id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/4685:142,Availability,error,error,142,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2247,Availability,error,error,2247,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2324,Availability,Error,Error,2324,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2408,Deployability,deploy,deployment-,2408,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:271,Safety,timeout,timeouts,271,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:285,Safety,timeout,timeout,285,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:93,Testability,log,log,93,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:556,Testability,test,tested,556,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:614,Testability,log,log,614,"```; # gsutil cat gs://hail-ci-0-1/ci/46808cb\*/038a6de7ce33b218b8d45160f224cae1feaf1c5a/job.log; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF% . ```; The three most recent commits:. ```; * 6d17db60a - (23 minutes ago) add batch client timeouts, set timeout in ci (#4586) - Daniel King (HEAD -> fix-batch-client, hi/master, master); * 74d5e7560 - (23 minutes ago) fix a few issues with hl.plot.histogram (#4681) - Tim Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""cr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:1565,Testability,log,login,1565,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2091,Testability,log,login,2091,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2285,Testability,log,logs,2285,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/issues/4685:2301,Testability,log,logs,2301,"m Poterba; * 038a6de7c - (45 minutes ago) refresh from batch (#4670) - Daniel King; ```. #4586 was never tested against 75d5e7560. This is bad. We can look at the log of statuses posted to GitHub:; ```; # curl -sSL api.github.com/repos/hail-is/hail/commits/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/statuses | less; ```; [46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt](https://github.com/hail-is/hail/files/2531246/46808cb224dbaa2d4fbae9f4fc90439e2eed8730-statuses.txt). Before the merge status goes in we see this one:; ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728320639,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MzIwNjM5"",; ""state"": ""success"",; ""description"": ""successful build"",; ""target_url"": ""https://storage.googleapis.com/hail-ci-0-1/ci/46808cb224dbaa2d4fbae9f4fc90439e2eed8730/038a6de7ce33b218b8d45160f224cae1feaf1c5a/index.html"",; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:51:09Z"",; ""updated_at"": ""2018-10-30T18:51:09Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. and before that:. ```; {; ""url"": ""https://api.github.com/repos/hail-is/hail/statuses/46808cb224dbaa2d4fbae9f4fc90439e2eed8730"",; ""avatar_url"": ""https://avatars2.githubusercontent.com/u/106194?v=4"",; ""id"": 5728220065,; ""node_id"": ""MDEzOlN0YXR1c0NvbnRleHQ1NzI4MjIwMDY1"",; ""state"": ""pending"",; ""description"": ""build 38 pending. target: 038a6de7ce33"",; ""target_url"": null,; ""context"": ""hail-ci-0-1"",; ""created_at"": ""2018-10-30T18:36:31Z"",; ""updated_at"": ""2018-10-30T18:36:31Z"",; ""creator"": {; ""login"": ""danking"", ...; }; },; ```. I don't understand what this Kubernetes state is, but this EOF thing is being incorrectly reported by batch to CI as an error. I cannot get the old batch pod logs.; ```; # k logs -l app=batch -p ; Error from server (BadRequest): previous terminated container ""batch"" in pod ""batch-deployment-769554dd84-kszwm"" not found. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4685
https://github.com/hail-is/hail/pull/4687:43,Deployability,deploy,deploy,43,"Previously everything was fine because the deploy script called; both the image creation and the deploy targets in make. However,; if we need to manually deploy, it is easy to forget this. I; modified the Makefile to make the dependency explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4687
https://github.com/hail-is/hail/pull/4687:97,Deployability,deploy,deploy,97,"Previously everything was fine because the deploy script called; both the image creation and the deploy targets in make. However,; if we need to manually deploy, it is easy to forget this. I; modified the Makefile to make the dependency explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4687
https://github.com/hail-is/hail/pull/4687:154,Deployability,deploy,deploy,154,"Previously everything was fine because the deploy script called; both the image creation and the deploy targets in make. However,; if we need to manually deploy, it is easy to forget this. I; modified the Makefile to make the dependency explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4687
https://github.com/hail-is/hail/pull/4687:226,Integrability,depend,dependency,226,"Previously everything was fine because the deploy script called; both the image creation and the deploy targets in make. However,; if we need to manually deploy, it is easy to forget this. I; modified the Makefile to make the dependency explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4687
https://github.com/hail-is/hail/pull/4689:32,Deployability,deploy,deployment,32,"had the wrong name. Not used by deployment, just for developers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4689
https://github.com/hail-is/hail/pull/4690:221,Availability,error,errors,221,"Now image fetcher asks the running notebook image what worker image its using and pulls that. Also, add a five second sleep after the service's endpoints are configured. Hopefully that prevents these intermittent gateway errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4690
https://github.com/hail-is/hail/pull/4690:158,Modifiability,config,configured,158,"Now image fetcher asks the running notebook image what worker image its using and pulls that. Also, add a five second sleep after the service's endpoints are configured. Hopefully that prevents these intermittent gateway errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4690
https://github.com/hail-is/hail/pull/4692:70,Availability,failure,failure,70,"If I can GET the URL I'm about to redirect to, then the only point of failure; remaining is the gateway nginx, and I trust that to work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4692
https://github.com/hail-is/hail/pull/4693:7,Deployability,deploy,deploy,7,I will deploy after this merges,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4693
https://github.com/hail-is/hail/pull/4701:80,Integrability,depend,dependencies,80,"This adds preliminary support for python 3.7 including a conda env with relaxed dependencies compared to what we currently ship. I've tested this manually and found no problems aside from a lot of warnings about upcoming depreciation in new versions of dependencies. The value of `__origin__` changed for many types from 3.6 to 3.7, which is why the check function map needed to be expanded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701
https://github.com/hail-is/hail/pull/4701:253,Integrability,depend,dependencies,253,"This adds preliminary support for python 3.7 including a conda env with relaxed dependencies compared to what we currently ship. I've tested this manually and found no problems aside from a lot of warnings about upcoming depreciation in new versions of dependencies. The value of `__origin__` changed for many types from 3.6 to 3.7, which is why the check function map needed to be expanded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701
https://github.com/hail-is/hail/pull/4701:134,Testability,test,tested,134,"This adds preliminary support for python 3.7 including a conda env with relaxed dependencies compared to what we currently ship. I've tested this manually and found no problems aside from a lot of warnings about upcoming depreciation in new versions of dependencies. The value of `__origin__` changed for many types from 3.6 to 3.7, which is why the check function map needed to be expanded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4701
https://github.com/hail-is/hail/pull/4707:615,Performance,Load,LoadBgen,615,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4707
https://github.com/hail-is/hail/pull/4707:524,Safety,safe,safely,524,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4707
https://github.com/hail-is/hail/pull/4711:530,Availability,error,error,530,"@cseed Can you look over this for structure before I assign it randomly? There's three things I am not happy about or want double checked:. 1. I had to replicate the parsing code for `quoted_literal` etc. between the two parser classes. I couldn't figure out how to have the function in one place and be able to call it. 2. I debated back and forth what the `repsepUntil` and `repUntil` interface should look like. I decided to make it take the desired tokens instead of comparing to a string or any value because that seems more error proof and explicit. However, the users of the function have something like this now `repsepUntil(it, f, PunctuationToken("",""), PunctuationToken(""}""))`, which is quite verbose. I thought about making a second function that took string arguments and converted it to PunctuationToken and then called the functions that took tokens, but decided it was better to be explicit. 3. Can you double check the regex for `float_literal` is still correct? This was to fix the empty string match I had during our check-in last week.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4711
https://github.com/hail-is/hail/pull/4711:387,Integrability,interface,interface,387,"@cseed Can you look over this for structure before I assign it randomly? There's three things I am not happy about or want double checked:. 1. I had to replicate the parsing code for `quoted_literal` etc. between the two parser classes. I couldn't figure out how to have the function in one place and be able to call it. 2. I debated back and forth what the `repsepUntil` and `repUntil` interface should look like. I decided to make it take the desired tokens instead of comparing to a string or any value because that seems more error proof and explicit. However, the users of the function have something like this now `repsepUntil(it, f, PunctuationToken("",""), PunctuationToken(""}""))`, which is quite verbose. I thought about making a second function that took string arguments and converted it to PunctuationToken and then called the functions that took tokens, but decided it was better to be explicit. 3. Can you double check the regex for `float_literal` is still correct? This was to fix the empty string match I had during our check-in last week.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4711
https://github.com/hail-is/hail/issues/4718:164,Availability,error,error,164,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9649,Availability,error,error,9649,"uite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:10380,Availability,error,error,10380,"ryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:170,Integrability,message,messages,170,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:11047,Integrability,depend,dependencies,11047,"64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9456,Safety,Unsafe,UnsafeSuite,9456,"testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9519,Safety,Unsafe,UnsafeSuite,9519,"eGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9622,Safety,Unsafe,UnsafeSuite,9622,"uite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9664,Safety,detect,detected,9664,"uite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:10040,Safety,Unsafe,UnsafeRow,10040,"hod testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:10553,Safety,Unsafe,UnsafeRow,10553,"64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:11518,Safety,abort,abort,11518,"dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e5318)[0x7fe4bd7ed318]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; [0x7fe4a85738ec]. 38 tests completed, 29 failed; :testCppCodegen FAILED; ```; [hs_err_pid23790.log](https://github.com/hail-is/hail/files/2540933/hs_err_pid23790.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:122,Testability,test,testCppCodegen,122,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:229,Testability,test,testCppCodegen,229,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:504,Testability,test,testClasses,504,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:529,Testability,test,testCppCodegen,529,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:553,Testability,test,test,553,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:559,Testability,Test,Test,559,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:571,Testability,test,testReadWrite,571,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:646,Testability,test,test,646,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:690,Testability,test,testReadWrite,690,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:795,Testability,Assert,AssertionError,795,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:819,Testability,test,test,819,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:825,Testability,Test,Test,825,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:837,Testability,test,testEmptyKeys,837,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:897,Testability,test,test,897,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:926,Testability,test,testEmptyKeys,926,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:958,Testability,Assert,AssertionError,958,"### Hail version:; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1005,Testability,test,test,1005,"; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1011,Testability,Test,Test,1011,"; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1023,Testability,test,testIntervalIterator,1023,"; fd300f29c00349d2a9d26835e35be2b142a3505f; ### What you did:; `make -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1090,Testability,test,test,1090,"ke -C src/main/c prebuilt && ./gradlew testCppCodegen`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1119,Testability,test,testIntervalIterator,1119,"`; ### What went wrong (all error messages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.Index",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1158,Testability,Assert,AssertionError,1158,"ssages here, including the full java stack trace):; ```; testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1205,Testability,test,test,1205,testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1211,Testability,Test,Test,1211,testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1223,Testability,test,testIntervalIteratorWorksWithGeneralEndpoints,1223,testCppCodegen; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1315,Testability,test,test,1315,zf libsimdpp-2.1.tar.gz; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1344,Testability,test,testIntervalIteratorWorksWithGeneralEndpoints,1344,ces UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.Assertio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1408,Testability,Assert,AssertionError,1408,Java UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1455,Testability,test,test,1455,; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1461,Testability,Test,Test,1461,; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1473,Testability,test,testIterateFromUntil,1473,; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :testCppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1540,Testability,test,test,1540,CppCodegen; Running test: Test method testReadWrite(is.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1569,Testability,test,testIterateFromUntil,1569,s.hail.annotations.AnnotationsSuite). Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1608,Testability,Assert,AssertionError,1608,. Gradle suite > Gradle test > is.hail.annotations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1655,Testability,test,test,1655,otations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at Inde,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1661,Testability,Test,Test,1661,otations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at Inde,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1673,Testability,test,testLowerBound,1673,otations.AnnotationsSuite.testReadWrite FAILED; org.apache.spark.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at Inde,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1734,Testability,test,test,1734,k.SparkException at AnnotationsSuite.scala:76; Caused by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1763,Testability,test,testLowerBound,1763,sed by: java.lang.AssertionError; Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1796,Testability,Assert,AssertionError,1796, Running test: Test method testEmptyKeys(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1843,Testability,test,test,1843,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1849,Testability,Test,Test,1849,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1861,Testability,test,testQueryByKey,1861,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1922,Testability,test,test,1922,IndexSuite.testEmptyKeys FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1951,Testability,test,testQueryByKey,1951,onError at IndexSuite.scala:42; Running test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:1984,Testability,Assert,AssertionError,1984,unning test: Test method testIntervalIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2031,Testability,test,test,2031,or(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2037,Testability,Test,Test,2037,or(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2049,Testability,test,testRangeIterator,2049,or(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2113,Testability,test,test,2113,o.IndexSuite.testIntervalIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2142,Testability,test,testRangeIterator,2142,.AssertionError at IndexSuite.scala:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2178,Testability,Assert,AssertionError,2178,:42; Running test: Test method testIntervalIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2225,Testability,test,test,2225,lIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2231,Testability,Test,Test,2231,lIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2243,Testability,test,testUpperBound,2243,lIteratorWorksWithGeneralEndpoints(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2304,Testability,test,test,2304,le suite > Gradle test > is.hail.io.IndexSuite.testIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2333,Testability,test,testUpperBound,2333,tIntervalIteratorWorksWithGeneralEndpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2366,Testability,Assert,AssertionError,2366,Endpoints FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2413,Testability,test,test,2413,Suite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2419,Testability,Test,Test,2419,Suite.scala:42; Running test: Test method testIterateFromUntil(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2536,Testability,test,test,2536,e > Gradle test > is.hail.io.IndexSuite.testIterateFromUntil FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2642,Testability,Assert,AssertionError,2642,est: Test method testLowerBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2689,Testability,test,test,2689,te). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2695,Testability,Test,Test,2695,te). Gradle suite > Gradle test > is.hail.io.IndexSuite.testLowerBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2812,Testability,test,test,2812,Error at IndexSuite.scala:42; Running test: Test method testQueryByKey(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2918,Testability,Assert,AssertionError,2918,hail.io.IndexSuite.testQueryByKey FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2965,Testability,test,test,2965,.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:2971,Testability,Test,Test,2971,.AssertionError at IndexSuite.scala:42; Running test: Test method testRangeIterator(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3088,Testability,test,test,3088,exSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testRangeIterator FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3194,Testability,Assert,AssertionError,3194,cala:42; Running test: Test method testUpperBound(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3241,Testability,test,test,3241,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3247,Testability,Test,Test,3247,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.testUpperBound FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3364,Testability,test,test,3364,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3470,Testability,Assert,AssertionError,3470,3eb0)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3517,Testability,test,test,3517, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3523,Testability,Test,Test,3523, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[0]([Ljava.lang.String;@49613eb0) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3640,Testability,test,test,3640,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3746,Testability,Assert,AssertionError,3746,09be)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3793,Testability,test,test,3793, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3799,Testability,Test,Test,3799, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[1]([Ljava.lang.String;@326709be) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:3916,Testability,test,test,3916,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4022,Testability,Assert,AssertionError,4022,570e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4069,Testability,test,test,4069, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4075,Testability,Test,Test,4075, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[2]([Ljava.lang.String;@7c5e570e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4192,Testability,test,test,4192,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test meth,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4298,Testability,Assert,AssertionError,4298,c5dc)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4345,Testability,test,test,4345, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4351,Testability,Test,Test,4351, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[3]([Ljava.lang.String;@3c90c5dc) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.la,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4468,Testability,test,test,4468,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test me,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4574,Testability,Assert,AssertionError,4574,0f2d)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4621,Testability,test,test,4621, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4627,Testability,Test,Test,4627, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[4]([Ljava.lang.String;@165d0f2d) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4744,Testability,test,test,4744,ava.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4850,Testability,Assert,AssertionError,4850,cb35)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4897,Testability,test,test,4897, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:4903,Testability,Test,Test,4903, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[5]([Ljava.lang.String;@2cb6cb35) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5019,Testability,test,test,5019,java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Te,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5124,Testability,Assert,AssertionError,5124,09f4c5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5171,Testability,test,test,5171,le test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5177,Testability,Test,Test,5177,le test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[6]([Ljava.lang.String;@4c09f4c5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5295,Testability,test,test,5295,java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5402,Testability,Assert,AssertionError,5402,10a4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5449,Testability,test,test,5449, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5455,Testability,Test,Test,5455, test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[7]([Ljava.lang.String;@67a910a4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5573,Testability,test,test,5573,va.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5680,Testability,Assert,AssertionError,5680,d4)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5727,Testability,test,test,5727,est > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Lja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5733,Testability,Test,Test,5733,est > is.hail.io.IndexSuite.writeReadGivesSameAsInput[8]([Ljava.lang.String;@3c215dd4) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Lja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5851,Testability,test,test,5851,.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:5958,Testability,Assert,AssertionError,5958,(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6005,Testability,test,test,6005, > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6011,Testability,Test,Test,6011, > is.hail.io.IndexSuite.writeReadGivesSameAsInput[9]([Ljava.lang.String;@5a40cc5) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6129,Testability,test,test,6129,g.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6236,Testability,Assert,AssertionError,6236,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6283,Testability,test,test,6283, is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6289,Testability,Test,Test,6289, is.hail.io.IndexSuite.writeReadGivesSameAsInput[10]([Ljava.lang.String;@69594c0e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6407,Testability,test,test,6407,g.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6514,Testability,Assert,AssertionError,6514,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6561,Testability,test,test,6561, is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6567,Testability,Test,Test,6567, is.hail.io.IndexSuite.writeReadGivesSameAsInput[11]([Ljava.lang.String;@5ea39a14) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6683,Testability,test,test,6683,ang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Te,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6788,Testability,Assert,AssertionError,6788,2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6835,Testability,test,test,6835,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6841,Testability,Test,Test,6841,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[12]([Ljava.lang.String;@21ebf782) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:6959,Testability,test,test,6959,.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7066,Testability,Assert,AssertionError,7066,a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7113,Testability,test,test,7113,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7119,Testability,Test,Test,7119,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[13]([Ljava.lang.String;@3bda762a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7237,Testability,test,test,7237,.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Grad,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7344,Testability,Assert,AssertionError,7344,e)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpee,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7391,Testability,test,test,7391,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7397,Testability,Test,Test,7397,st > is.hail.io.IndexSuite.writeReadGivesSameAsInput[14]([Ljava.lang.String;@4898b38e) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7515,Testability,test,test,7515,.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7622,Testability,Assert,AssertionError,7622,(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7669,Testability,test,test,7669, > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7675,Testability,Test,Test,7675, > is.hail.io.IndexSuite.writeReadGivesSameAsInput[15]([Ljava.lang.String;@51684a) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7793,Testability,test,test,7793,g.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.Nat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7900,Testability,Assert,AssertionError,7900,s.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Loggin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7947,Testability,test,test,7947,test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7953,Testability,Test,Test,7953,test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:7965,Testability,test,testCXXCodeFunctions,7965,test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[16]([Ljava.lang.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle tes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8030,Testability,Log,Logging,8030,g.String;@4a971368) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Runnin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8077,Testability,test,test,8077,rtionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8119,Testability,test,testCXXCodeFunctions,8119,[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.Native,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8156,Testability,test,test,8156,[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.Native,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8162,Testability,Test,Test,8162,[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.Native,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8174,Testability,test,testNativeBuild,8174,[17]([Ljava.lang.String;@47162b74)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.Native,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8249,Testability,test,test,8249,> is.hail.io.IndexSuite.writeReadGivesSameAsInput[17]([Ljava.lang.String;@47162b74) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMem,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8291,Testability,test,testNativeBuild,8291,AILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8323,Testability,test,test,8323,AILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8329,Testability,Test,Test,8329,AILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8341,Testability,test,testNativeCallSpeed,8341,AILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8420,Testability,test,test,8420,eReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8462,Testability,test,testNativeCallSpeed,8462, suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8498,Testability,test,test,8498, suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8504,Testability,Test,Test,8504, suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8516,Testability,test,testNativeGlobal,8516, suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[18]([Ljava.lang.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8592,Testability,test,test,8592,.String;@70bfbb33) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test m,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8634,Testability,test,testNativeGlobal,8634, test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been dete,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8667,Testability,test,test,8667, test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been dete,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8673,Testability,Test,Test,8673, test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been dete,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8685,Testability,test,testNativePtr,8685, test: Test method writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2)(is.hail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been dete,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8758,Testability,test,test,8758,"ail.io.IndexSuite). Gradle suite > Gradle test > is.hail.io.IndexSuite.writeReadGivesSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8800,Testability,test,testNativePtr,8800,"esSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8830,Testability,test,test,8830,"esSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8836,Testability,Test,Test,8836,"esSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8848,Testability,test,testNativeUpcall,8848,"esSameAsInput[19]([Ljava.lang.String;@3e391ee2) FAILED; java.lang.AssertionError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8909,Testability,Log,Logging,8909,"onError at IndexSuite.scala:42; Running test: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8956,Testability,test,test,8956,"st: Test method testCXXCodeFunctions(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:8998,Testability,test,testNativeUpcall,8998,"ging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9031,Testability,test,test,9031,"ging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9037,Testability,Test,Test,9037,"ging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9049,Testability,test,testObjectArray,9049,"ging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9124,Testability,test,test,9124,"e.testCXXCodeFunctions PASSED; Running test: Test method testNativeBuild(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9166,Testability,test,testObjectArray,9166,"ativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9198,Testability,test,test,9198,"ativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9204,Testability,Test,Test,9204,"ativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9216,Testability,test,testShuffleAndJoinDoesntMemoryLeak,9216,"ativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9304,Testability,test,test,9304,"tiveBuild PASSED; Running test: Test method testNativeCallSpeed(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9340,Testability,test,testShuffleAndJoinDoesntMemoryLeak,9340,"eSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error repo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9391,Testability,test,test,9391,"eSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error repo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9397,Testability,Test,Test,9397,"eSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error repo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9409,Testability,test,testBufferWriteReadDoubles,9409,"eSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeCallSpeed PASSED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error repo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9492,Testability,test,test,9492,"SED; Running test: Test method testNativeGlobal(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9531,Testability,test,testBufferWriteReadDoubles,9531,". Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9574,Testability,test,test,9574,". Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9580,Testability,Test,Test,9580,". Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:9592,Testability,test,testCodec,9592,". Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeGlobal PASSED; Running test: Test method testNativePtr(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativePtr PASSED; Running test: Test method testNativeUpcall(is.hail.nativecode.NativeCodeSuite); DEBUG: Logging set_test_msg ... Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testNativeUpcall PASSED; Running test: Test method testObjectArray(is.hail.nativecode.NativeCodeSuite). Gradle suite > Gradle test > is.hail.nativecode.NativeCodeSuite.testObjectArray PASSED; Running test: Test method testShuffleAndJoinDoesntMemoryLeak(is.hail.expr.ir.TableIRSuite). Gradle suite > Gradle test > is.hail.expr.ir.TableIRSuite.testShuffleAndJoinDoesntMemoryLeak PASSED; Running test: Test method testBufferWriteReadDoubles(is.hail.annotations.UnsafeSuite). Gradle suite > Gradle test > is.hail.annotations.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::rea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:10494,Testability,log,log,10494,"ons.UnsafeSuite.testBufferWriteReadDoubles PASSED; Running test: Test method testCodec(is.hail.annotations.UnsafeSuite); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe4a85738ec, pid=23790, tid=0x00007fe48cdfa700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.18.04.1-b13); # Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:10765,Testability,stub,stub,10765,"64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops); # Problematic frame:; # J 9008 C1 is.hail.annotations.UnsafeRow$.readBinary(Lis/hail/annotations/Region;J)[B (39 bytes) @ 0x00007fe4a85738ec [0x00007fe4a8573600+0x2ec]; #; # Core dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:12020,Testability,test,tests,12020,"dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e5318)[0x7fe4bd7ed318]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; [0x7fe4a85738ec]. 38 tests completed, 29 failed; :testCppCodegen FAILED; ```; [hs_err_pid23790.log](https://github.com/hail-is/hail/files/2540933/hs_err_pid23790.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:12049,Testability,test,testCppCodegen,12049,"dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e5318)[0x7fe4bd7ed318]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; [0x7fe4a85738ec]. 38 tests completed, 29 failed; :testCppCodegen FAILED; ```; [hs_err_pid23790.log](https://github.com/hail-is/hail/files/2540933/hs_err_pid23790.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:12094,Testability,log,log,12094,"dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e5318)[0x7fe4bd7ed318]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; [0x7fe4a85738ec]. 38 tests completed, 29 failed; :testCppCodegen FAILED; ```; [hs_err_pid23790.log](https://github.com/hail-is/hail/files/2540933/hs_err_pid23790.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/issues/4718:12161,Testability,log,log,12161,"dump written. Default location: /home/BROAD.MIT.EDU/cvittal/src/hail/hail/core or core.23790 (max size 9223372036854775 kB). To ensure a full core dump, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /home/BROAD.MIT.EDU/cvittal/src/hail/hail/hs_err_pid23790.log; Compiled method (c1) 33969 8500 2 is.hail.annotations.UnsafeRow$::readLocus (78 bytes); total in heap [0x00007fe4a8b81810,0x00007fe4a8b83430] = 7200; relocation [0x00007fe4a8b81938,0x00007fe4a8b81a98] = 352; main code [0x00007fe4a8b81aa0,0x00007fe4a8b82100] = 1632; stub code [0x00007fe4a8b82100,0x00007fe4a8b822b8] = 440; oops [0x00007fe4a8b822b8,0x00007fe4a8b822c0] = 8; metadata [0x00007fe4a8b822c0,0x00007fe4a8b82338] = 120; scopes data [0x00007fe4a8b82338,0x00007fe4a8b82f30] = 3064; scopes pcs [0x00007fe4a8b82f30,0x00007fe4a8b83340] = 1040; dependencies [0x00007fe4a8b83340,0x00007fe4a8b83348] = 8; nul chk table [0x00007fe4a8b83348,0x00007fe4a8b83430] = 232; #; FATAL: caught signal 6 SIGABRT; # If you would like to submit a bug report, please visit:; # http://bugreport.sun.com/bugreport/; #; /tmp/libhail8122447512081932366.so(+0x18f5f)[0x7fe3a7bf0f5f]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7fe4be507e97]; /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7fe4be509801]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e80b9)[0x7fe4bd7f00b9]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0xaaed23)[0x7fe4bd9b6d23]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(JVM_handle_linux_signal+0x1b4)[0x7fe4bd7fa694]; /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so(+0x8e5318)[0x7fe4bd7ed318]; /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fe4be507f20]; [0x7fe4a85738ec]. 38 tests completed, 29 failed; :testCppCodegen FAILED; ```; [hs_err_pid23790.log](https://github.com/hail-is/hail/files/2540933/hs_err_pid23790.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4718
https://github.com/hail-is/hail/pull/4719:78,Performance,queue,queue,78,"Applies on top of #4713 . Compared to previous versions, this uses a priority queue rather than a linear search to create the list of returned items. We return tuples so that the caller can construct arrays as appropriate rather than constructing an array of values here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4719
https://github.com/hail-is/hail/pull/4722:392,Integrability,wrap,wrapper,392,"This basically just pulls out the logic from the `NativeDecoder` and `NativeEncoder` stuff in RowStore.scala into their own objects, and dynamically generates a c++ class for the row type. (The only things that have changed between `NativeDecoder`/`cxx.PackDecoder` and the encoders are the `apply` functions; I'm generating an Encoder class that inherits NativeObj instead of relying on the wrapper classes in `Encoder.h` and `Decoder.h`. This mostly felt like it made things easier to reason about when I started writing the full-stage code generation stuff, but I've pulled it out here as a separate PR. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4722
https://github.com/hail-is/hail/pull/4722:347,Modifiability,inherit,inherits,347,"This basically just pulls out the logic from the `NativeDecoder` and `NativeEncoder` stuff in RowStore.scala into their own objects, and dynamically generates a c++ class for the row type. (The only things that have changed between `NativeDecoder`/`cxx.PackDecoder` and the encoders are the `apply` functions; I'm generating an Encoder class that inherits NativeObj instead of relying on the wrapper classes in `Encoder.h` and `Decoder.h`. This mostly felt like it made things easier to reason about when I started writing the full-stage code generation stuff, but I've pulled it out here as a separate PR. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4722
https://github.com/hail-is/hail/pull/4722:34,Testability,log,logic,34,"This basically just pulls out the logic from the `NativeDecoder` and `NativeEncoder` stuff in RowStore.scala into their own objects, and dynamically generates a c++ class for the row type. (The only things that have changed between `NativeDecoder`/`cxx.PackDecoder` and the encoders are the `apply` functions; I'm generating an Encoder class that inherits NativeObj instead of relying on the wrapper classes in `Encoder.h` and `Decoder.h`. This mostly felt like it made things easier to reason about when I started writing the full-stage code generation stuff, but I've pulled it out here as a separate PR. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4722
https://github.com/hail-is/hail/pull/4725:45,Modifiability,parameteriz,parameterize,45,"This is running on hail-vdc/vdc. Summary:; - parameterize project; - gateway option to forward to letsencrypt only for bootstrapping; - service accounts for gateway, letsencrypt; - mysql instance running. I did not make default:default cluster admin, so it should have no special privileges. Changes are not yet being applied automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4725
https://github.com/hail-is/hail/pull/4728:92,Usability,feedback,feedback,92,"@cseed It's still not quite as tidy as I'd like it to be, but I thought I'd throw it up for feedback. Builds on #4722.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4728
https://github.com/hail-is/hail/pull/4729:136,Integrability,interface,interface,136,"Pretty rough, not yet hooked into anything, but I wrote a test. This doesn't deal with the question of region management at the jvm/c++ interface, but it lays out a basic wrapper for passing iterators through c++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4729
https://github.com/hail-is/hail/pull/4729:171,Integrability,wrap,wrapper,171,"Pretty rough, not yet hooked into anything, but I wrote a test. This doesn't deal with the question of region management at the jvm/c++ interface, but it lays out a basic wrapper for passing iterators through c++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4729
https://github.com/hail-is/hail/pull/4729:58,Testability,test,test,58,"Pretty rough, not yet hooked into anything, but I wrote a test. This doesn't deal with the question of region management at the jvm/c++ interface, but it lays out a basic wrapper for passing iterators through c++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4729
https://github.com/hail-is/hail/issues/4733:77,Availability,error,errors,77,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:574,Availability,error,error,574,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:639,Availability,ERROR,ERROR,639,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:3884,Availability,ERROR,ERROR,3884,"xecute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapEntries.execute(MatrixIR.scala:1257); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:728); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); 	at is.hail.variant.MatrixTable.aggregateEntries(MatrixTable.scala:891); 	at is.hail.variant.MatrixTable.aggregateEntriesJSON(MatrixTable.scala:884); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/homes/nber/barronk-dua51929/local/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:11,Deployability,install,install,11,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:163,Deployability,release,released,163,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:306,Deployability,install,installed,306,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:327,Deployability,Install,Installed,327,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:580,Integrability,message,messages,580,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:4158,Integrability,protocol,protocol,4158,"xecute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapEntries.execute(MatrixIR.scala:1257); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:728); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); 	at is.hail.variant.MatrixTable.aggregateEntries(MatrixTable.scala:891); 	at is.hail.variant.MatrixTable.aggregateEntriesJSON(MatrixTable.scala:884); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/homes/nber/barronk-dua51929/local/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:1189,Performance,load,load,1189,"f any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:1299,Performance,load,loadLibrary,1299,"lled from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/issues/4733:1403,Performance,load,load,1403," did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733
https://github.com/hail-is/hail/pull/4737:127,Integrability,depend,dependency,127,"currently can be used to control whether the decoder/encoder uses c++ or jvm bytecode, although we're not yet bundling the lz4 dependency so I don't think this will work on a default cloudtools cluster yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4737
https://github.com/hail-is/hail/issues/4738:6,Testability,Test,Test,6,"```; @Test def testTableAggregate() {; val table = TableRange(3, 2); val collectSig = AggSignature(Collect(), Seq(), None, Seq(TInt32())); val collect = ApplyAggOp(FastIndexedSeq.empty, None, FastIndexedSeq(GetField(Ref(""row"", table.typ.rowType), ""idx"")), collectSig); assertEvalsTo(TableAggregate(table, MakeStruct(Seq(""foo"" -> collect))), Row(FastIndexedSeq(0, 1, 2))); }; ```. This will fail because interpret returns Array(2, 0, 1) instead of Array(0, 1, 2).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4738
https://github.com/hail-is/hail/issues/4738:15,Testability,test,testTableAggregate,15,"```; @Test def testTableAggregate() {; val table = TableRange(3, 2); val collectSig = AggSignature(Collect(), Seq(), None, Seq(TInt32())); val collect = ApplyAggOp(FastIndexedSeq.empty, None, FastIndexedSeq(GetField(Ref(""row"", table.typ.rowType), ""idx"")), collectSig); assertEvalsTo(TableAggregate(table, MakeStruct(Seq(""foo"" -> collect))), Row(FastIndexedSeq(0, 1, 2))); }; ```. This will fail because interpret returns Array(2, 0, 1) instead of Array(0, 1, 2).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4738
https://github.com/hail-is/hail/issues/4738:269,Testability,assert,assertEvalsTo,269,"```; @Test def testTableAggregate() {; val table = TableRange(3, 2); val collectSig = AggSignature(Collect(), Seq(), None, Seq(TInt32())); val collect = ApplyAggOp(FastIndexedSeq.empty, None, FastIndexedSeq(GetField(Ref(""row"", table.typ.rowType), ""idx"")), collectSig); assertEvalsTo(TableAggregate(table, MakeStruct(Seq(""foo"" -> collect))), Row(FastIndexedSeq(0, 1, 2))); }; ```. This will fail because interpret returns Array(2, 0, 1) instead of Array(0, 1, 2).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4738
https://github.com/hail-is/hail/pull/4745:19,Deployability,deploy,deploy,19,Should fix failing deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4745
https://github.com/hail-is/hail/issues/4746:102,Availability,error,error,102,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/issues/4746:249,Testability,Assert,AssertionError,249,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/issues/4746:265,Testability,assert,assertion,265,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/issues/4746:312,Testability,Assert,AssertionError,312,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/issues/4746:328,Testability,assert,assertion,328,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/issues/4746:364,Testability,assert,assert,364,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4746
https://github.com/hail-is/hail/pull/4747:20,Testability,test,testIfWithDifferentRequiredness,20,"I noticed that the `testIfWithDifferentRequiredness` and `testMakeArrayWithDifferentRequiredness` were failing after I added the assertion in `upcast` for #4585 . After discussing with @tpoterba , it seemed like it would be okay to allow a required type to be a subtype of an optional type. This PR also includes the assertion, which may cause other tests to fail, and so I have marked it as a WIP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4747
https://github.com/hail-is/hail/pull/4747:58,Testability,test,testMakeArrayWithDifferentRequiredness,58,"I noticed that the `testIfWithDifferentRequiredness` and `testMakeArrayWithDifferentRequiredness` were failing after I added the assertion in `upcast` for #4585 . After discussing with @tpoterba , it seemed like it would be okay to allow a required type to be a subtype of an optional type. This PR also includes the assertion, which may cause other tests to fail, and so I have marked it as a WIP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4747
https://github.com/hail-is/hail/pull/4747:129,Testability,assert,assertion,129,"I noticed that the `testIfWithDifferentRequiredness` and `testMakeArrayWithDifferentRequiredness` were failing after I added the assertion in `upcast` for #4585 . After discussing with @tpoterba , it seemed like it would be okay to allow a required type to be a subtype of an optional type. This PR also includes the assertion, which may cause other tests to fail, and so I have marked it as a WIP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4747
https://github.com/hail-is/hail/pull/4747:317,Testability,assert,assertion,317,"I noticed that the `testIfWithDifferentRequiredness` and `testMakeArrayWithDifferentRequiredness` were failing after I added the assertion in `upcast` for #4585 . After discussing with @tpoterba , it seemed like it would be okay to allow a required type to be a subtype of an optional type. This PR also includes the assertion, which may cause other tests to fail, and so I have marked it as a WIP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4747
https://github.com/hail-is/hail/pull/4747:350,Testability,test,tests,350,"I noticed that the `testIfWithDifferentRequiredness` and `testMakeArrayWithDifferentRequiredness` were failing after I added the assertion in `upcast` for #4585 . After discussing with @tpoterba , it seemed like it would be okay to allow a required type to be a subtype of an optional type. This PR also includes the assertion, which may cause other tests to fail, and so I have marked it as a WIP.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4747
https://github.com/hail-is/hail/pull/4752:29,Testability,test,tests,29,"Makes it easy to see failing tests, stack traces, and timings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4752
https://github.com/hail-is/hail/issues/4754:799,Availability,error,errors,799,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4754
https://github.com/hail-is/hail/issues/4754:816,Availability,Error,Error,816,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4754
https://github.com/hail-is/hail/issues/4754:831,Testability,Assert,AssertionError,831,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4754
https://github.com/hail-is/hail/issues/4754:847,Testability,assert,assertion,847,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4754
https://github.com/hail-is/hail/issues/4754:1107,Testability,log,log,1107,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4754
https://github.com/hail-is/hail/issues/4755:593,Availability,error,error,593,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-4f13f27cd28d. ### What you did:; ipython vcf2mt.py . import hail as hl; hl.init(default_reference='GRCh38'); chr=""22""; vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; hl.import_vcf(vcf).write(mt). ### What went wrong (all error messages here, including the full java stack trace):. GC Overhead limit exceeded on Stage 2 of import_vcf.write (See below). ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:776,Availability,avail,available,776,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-4f13f27cd28d. ### What you did:; ipython vcf2mt.py . import hail as hl; hl.init(default_reference='GRCh38'); chr=""22""; vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; hl.import_vcf(vcf).write(mt). ### What went wrong (all error messages here, including the full java stack trace):. GC Overhead limit exceeded on Stage 2 of import_vcf.write (See below). ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:2898,Availability,Error,Error,2898,"utions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3156,Availability,down,down,3156,"f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3260,Availability,down,down,3260,"ut, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHook",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7437,Availability,Error,Error,7437,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7514,Availability,down,down,7514,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:1839,Deployability,install,install,1839,"/__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:2152,Deployability,install,install,2152,"oerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:2407,Deployability,install,install,2407,"--------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:2720,Deployability,install,install,2720,"imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3286,Energy Efficiency,schedul,scheduler,3286,"f._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3397,Energy Efficiency,schedul,scheduler,3397,".2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3571,Energy Efficiency,schedul,scheduler,3571,"turn_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3665,Energy Efficiency,schedul,scheduler,3665,"emp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3818,Energy Efficiency,schedul,scheduler,3818,"lError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookMana",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:5377,Energy Efficiency,schedul,scheduler,5377,cala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188); at scala.util.Try$.apply(Try.scala:192); at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178); at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7663,Energy Efficiency,schedul,scheduler,7663,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7733,Energy Efficiency,schedul,scheduler,7733,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:599,Integrability,message,messages,599,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-4f13f27cd28d. ### What you did:; ipython vcf2mt.py . import hail as hl; hl.init(default_reference='GRCh38'); chr=""22""; vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; hl.import_vcf(vcf).write(mt). ### What went wrong (all error messages here, including the full java stack trace):. GC Overhead limit exceeded on Stage 2 of import_vcf.write (See below). ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:948,Integrability,Interface,Interfaces,948,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-4f13f27cd28d. ### What you did:; ipython vcf2mt.py . import hail as hl; hl.init(default_reference='GRCh38'); chr=""22""; vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; hl.import_vcf(vcf).write(mt). ### What went wrong (all error messages here, including the full java stack trace):. GC Overhead limit exceeded on Stage 2 of import_vcf.write (See below). ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-4f13f27cd28d; NOTE: This is a beta version. Interfaces may change; during the beta period. We recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:1910,Integrability,wrap,wrapper,1910,"g; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedExc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:1944,Integrability,wrap,wrapper,1944,"g; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedExc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:2112,Integrability,wrap,wrapper,2112,"g; the latest changes weekly.; [Stage 1:======================================================>(740 + 1) / 741]2018-11-10 22:55:07 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 24) / 741]Exception in thread ""refresh progress"" Exception in thread ""LeaseRenewer:farrell@scc"" java.lang.OutOfMemoryError: GC overhead limit exceeded; java.lang.OutOfMemoryError: GC overhead limit exceeded; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>(); 4 vcf=""/project/ukbiobank/imputation/ad.v1/vcf/ukbb.hg38.imputed.chr""+chr+"".dose.vcf.bgz""; 5 mt=""/project/ukbiobank/imputation/ad.v1/mt/ukbb.hg38.imputed.chr""+chr; ----> 6 hl.import_vcf(vcf).write(mt). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548; 549 update_wrapper(wrapper, f). /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, _codec_spec); 2111 """"""; 2112; -> 2113 self._jvds.write(output, overwrite, _codec_spec); 2114; 2115 def globals_table(self) -> Table:. /share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedExc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7859,Performance,concurren,concurrent,7859,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:7943,Performance,concurren,concurrent,7943,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3516,Security,Hash,HashSet,3516,"client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:3532,Security,Hash,HashSet,3532,"command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-06-18/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job 2 cancelled because SparkContext was shut down. Java stack trace:; org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/issues/4755:4685,Testability,log,logUncaughtExceptions,4685,Loop.onStop(DAGScheduler.scala:1732); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188); at scala.util.Try$.apply(Try.scala:192); at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188); at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178); at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4755
https://github.com/hail-is/hail/pull/4756:24,Deployability,deploy,deploy,24,This will hopefully fix deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4756
https://github.com/hail-is/hail/issues/4757:1291,Availability,error,error,1291," discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-e60bdb1a125a. ### What you did:; ```; def get_counts_agg_expr2(mt: hl.MatrixTable):; return (hl.case(missing_false=True); #0x; .when(hl.is_missing(mt.gt1) & ~mt.missing1,; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[1,0,0,0,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,1,0,0,0,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,1,0,0,0,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #1x; .when(mt.gt1.is_het(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,1,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,1,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,1,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #2x; .when(mt.gt1.is_hom_var(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,0,0,0,1,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,0,0,0,1,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,0,0,0,1]); .default([0,0,0,0,0,0,0,0,0])); .default([0,0,0,0,0,0,0,0,0])); mt = mt.annotate_rows(gt_counts=hl.agg.array_sum(get_counts_agg_expr2(mt))); ```; ### What went wrong (all error messages here, including the full java stack trace):; `gt_counts` is `[]` everywhere:; ```; mt.show(); +---------------+------------+---------------+------------+--------------+; | locus2 | alleles2 | locus1 | alleles1 | gt_counts |; +---------------+------------+---------------+------------+--------------+; | locus<GRCh37> | array<str> | locus<GRCh37> | array<str> | array<int64> |; +---------------+------------+---------------+------------+--------------+; | 1:69173 | [""A"",""T""] | 1:69166 | [""G"",""T""] | [] |; | 1:69946 | [""G"",""A""] | 1:69359 | [""G"",""A""] | [] |; | 1:69947 | [""A"",""G""] | 1:69359 | [""G"",""A""] | [] |; | 1:69735 | [""A"",""G""] | 1:69438 | [""T"",""C""] | [] |; | 1:69496 | [""G"",""A""] | 1:69453 | [""G"",""A""] | [] |; +---------------+------------+---------------+------------+--------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4757
https://github.com/hail-is/hail/issues/4757:1297,Integrability,message,messages,1297," discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-e60bdb1a125a. ### What you did:; ```; def get_counts_agg_expr2(mt: hl.MatrixTable):; return (hl.case(missing_false=True); #0x; .when(hl.is_missing(mt.gt1) & ~mt.missing1,; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[1,0,0,0,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,1,0,0,0,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,1,0,0,0,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #1x; .when(mt.gt1.is_het(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,1,0,0,0,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,1,0,0,0,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,1,0,0,0]); .default([0,0,0,0,0,0,0,0,0])); #2x; .when(mt.gt1.is_hom_var(),; hl.case(missing_false=True); .when(hl.is_missing(mt.gt2) & ~mt.missing2,[0,0,0,0,0,0,1,0,0]); .when(mt.gt2.is_het(), [0,0,0,0,0,0,0,1,0]); .when(mt.gt2.is_hom_var(), [0,0,0,0,0,0,0,0,1]); .default([0,0,0,0,0,0,0,0,0])); .default([0,0,0,0,0,0,0,0,0])); mt = mt.annotate_rows(gt_counts=hl.agg.array_sum(get_counts_agg_expr2(mt))); ```; ### What went wrong (all error messages here, including the full java stack trace):; `gt_counts` is `[]` everywhere:; ```; mt.show(); +---------------+------------+---------------+------------+--------------+; | locus2 | alleles2 | locus1 | alleles1 | gt_counts |; +---------------+------------+---------------+------------+--------------+; | locus<GRCh37> | array<str> | locus<GRCh37> | array<str> | array<int64> |; +---------------+------------+---------------+------------+--------------+; | 1:69173 | [""A"",""T""] | 1:69166 | [""G"",""T""] | [] |; | 1:69946 | [""G"",""A""] | 1:69359 | [""G"",""A""] | [] |; | 1:69947 | [""A"",""G""] | 1:69359 | [""G"",""A""] | [] |; | 1:69735 | [""A"",""G""] | 1:69438 | [""T"",""C""] | [] |; | 1:69496 | [""G"",""A""] | 1:69453 | [""G"",""A""] | [] |; +---------------+------------+---------------+------------+--------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4757
https://github.com/hail-is/hail/pull/4758:8,Deployability,deploy,deploy,8,another deploy fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4758
https://github.com/hail-is/hail/issues/4761:93,Testability,assert,assert,93,"In Python, we check that the row.dtype and entry.dtype are the same for all MTs. In scala we assert that the RVDTypes are the same. This means that both requiredness and entry array location can cause problems.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4761
https://github.com/hail-is/hail/issues/4766:23,Modifiability,extend,extend,23,"ArrayFunctions:. - [ ] extend; - [ ] argF; - [ ] uniqueIndex; - [ ] ""[]"" with negative argument; - [ ] ""[*:]""; - [ ] ""[:*]""; - [ ] ""[*:*]"". DictFunctions:. - [ ] contains; - [ ] get. SetFunctions:. - [ ] contains. StringFunctions:. - [ ] ""[*:]""; - [ ] ""[:*]""; - [ ] ""[*:*]"". UtilFunctions:. - [ ] min; - [ ] max",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4766
https://github.com/hail-is/hail/pull/4768:8,Deployability,deploy,deploy,8,another deploy fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4768
https://github.com/hail-is/hail/pull/4769:21,Performance,perform,performance,21,This should be a big performance boost.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4769
https://github.com/hail-is/hail/issues/4770:912,Integrability,wrap,wrapper,912,"```; mt = get_gnomad_data(data_type, release_samples=True, release_annotations=True); cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; 'subpop': hl.agg.collect_as_set(; hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; cut_data = mt.aggregate_cols(hl.struct(**cut_dict)); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-3-0d2fea45d693>"", line 1, in <module>; cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; File ""<decorator-gen-998>"", line 2, in filter; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/konradk/hail/hail/python/hail/expr/aggregators/aggregators.py"", line 26, in check; assert(len(x._ir.search(lambda node: isinstance(node, BaseApplyAggOp))) != 0); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770
https://github.com/hail-is/hail/issues/4770:1245,Testability,assert,assert,1245,"```; mt = get_gnomad_data(data_type, release_samples=True, release_annotations=True); cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; 'subpop': hl.agg.collect_as_set(; hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; cut_data = mt.aggregate_cols(hl.struct(**cut_dict)); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-3-0d2fea45d693>"", line 1, in <module>; cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; File ""<decorator-gen-998>"", line 2, in filter; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/konradk/hail/hail/python/hail/expr/aggregators/aggregators.py"", line 26, in check; assert(len(x._ir.search(lambda node: isinstance(node, BaseApplyAggOp))) != 0); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770
https://github.com/hail-is/hail/issues/4770:1324,Testability,Assert,AssertionError,1324,"```; mt = get_gnomad_data(data_type, release_samples=True, release_annotations=True); cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; 'subpop': hl.agg.collect_as_set(; hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; cut_data = mt.aggregate_cols(hl.struct(**cut_dict)); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-3-0d2fea45d693>"", line 1, in <module>; cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),; File ""<decorator-gen-998>"", line 2, in filter; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/konradk/hail/hail/python/hail/expr/aggregators/aggregators.py"", line 26, in check; assert(len(x._ir.search(lambda node: isinstance(node, BaseApplyAggOp))) != 0); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4770
https://github.com/hail-is/hail/issues/4773:317,Availability,echo,echo,317,"### Hail version:. Irrelevant feature branch based on `abfb656e2`. ### What you did:. 1. `cd batch ; make run`; 2. ; ```; curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/issues/4773:2600,Availability,ERROR,ERROR,2600,"1-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:17:20,179 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:17:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,732 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /jobs/create HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:19:31,745 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,764 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; ERROR	| 2018-11-13 18:19:31,779 	| app.py 	| log_exception:1761 | Exception on /pod_changed [POST]; Traceback (most recent call last):; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app; response = self.full_dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request; rv = self.handle_user_exception(e); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception; reraise(exc_type, exc_value, tb); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise; raise value; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request; rv = self.dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/issues/4773:1118,Performance,load,loading,1118,"curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:17:20,179 	| _internal.py 	| _log:88 | ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/issues/4773:438,Testability,log,log,438,"### Hail version:. Irrelevant feature branch based on `abfb656e2`. ### What you did:. 1. `cd batch ; make run`; 2. ; ```; curl -XPOST localhost:5000/jobs/create -H 'Content-Type: application/json' -d '{ ; ""spec"" : {; ""containers"" : [; { ""name"" : ""foobarbaz""; , ""image"" : ""alpine:3.8""; , ""command"": [""/bin/sh"", ""-c"", ""echo hi""] }] } }'; ```; 3. ; ```; curl localhost:5000/jobs; ```; 4. the job never transitions to Complete and the server log shows:; ```; (hail-batch) # make run; BATCH_USE_KUBE_CONFIG=1 python -c 'import batch.server; batch.server.serve()'; INFO	| 2018-11-13 18:12:19,124 	| server.py 	| <module>:25 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-11-13 18:12:19,125 	| server.py 	| <module>:28 | instance_id = 63aeb0cd4fa840a9864cfd909ce7f682; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target kube_event_loop; INFO	| 2018-11-13 18:12:19,130 	| server.py 	| run_forever:391 | run_forever: run target polling_event_loop; INFO	| 2018-11-13 18:12:19,131 	| server.py 	| run_forever:391 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; INFO	| 2018-11-13 18:12:19,168 	| _internal.py 	| _log:88 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); INFO	| 2018-11-13 18:12:20,141 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:12:20,159 	| server.py 	| refresh_k8s_state:379 | k8s state refresh complete; INFO	| 2018-11-13 18:12:20,160 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:20] ""POST /refresh_k8s_state HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:12:55,902 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:12:55] ""GET /jobs HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:17:20,174 	| server.py 	| refresh_k8s_state:360 | started k8s state refresh; INFO	| 2018-11-13 18:17:20,179 	| serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/issues/4773:3881,Testability,assert,assert,3881,".0.0.1 - - [13/Nov/2018 18:19:31] ""POST /jobs/create HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:19:31,745 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,764 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; ERROR	| 2018-11-13 18:19:31,779 	| app.py 	| log_exception:1761 | Exception on /pod_changed [POST]; Traceback (most recent call last):; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app; response = self.full_dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request; rv = self.handle_user_exception(e); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception; reraise(exc_type, exc_value, tb); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise; raise value; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request; rv = self.dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request; return self.view_functions[rule.endpoint](**req.view_args); File ""/Users/bking/projects/hail/batch/batch/server.py"", line 354, in pod_changed; update_job_with_pod(job, pod); File ""/Users/bking/projects/hail/batch/batch/server.py"", line 331, in update_job_with_pod; assert container_status.name == 'default'; AssertionError; INFO	| 2018-11-13 18:19:31,780 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 500 -; ```. ## Proposed Solution. Do not accept arbitrary specs, and in particular do not accept a name. I think batch should just take an image, a command, and maybe some volumes and volume mounts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/issues/4773:3924,Testability,Assert,AssertionError,3924,".0.0.1 - - [13/Nov/2018 18:19:31] ""POST /jobs/create HTTP/1.1"" 200 -; INFO	| 2018-11-13 18:19:31,745 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2018-11-13 18:19:31,764 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 204 -; ERROR	| 2018-11-13 18:19:31,779 	| app.py 	| log_exception:1761 | Exception on /pod_changed [POST]; Traceback (most recent call last):; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app; response = self.full_dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request; rv = self.handle_user_exception(e); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception; reraise(exc_type, exc_value, tb); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise; raise value; File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request; rv = self.dispatch_request(); File ""/Users/bking/miniconda3/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request; return self.view_functions[rule.endpoint](**req.view_args); File ""/Users/bking/projects/hail/batch/batch/server.py"", line 354, in pod_changed; update_job_with_pod(job, pod); File ""/Users/bking/projects/hail/batch/batch/server.py"", line 331, in update_job_with_pod; assert container_status.name == 'default'; AssertionError; INFO	| 2018-11-13 18:19:31,780 	| _internal.py 	| _log:88 | 127.0.0.1 - - [13/Nov/2018 18:19:31] ""POST /pod_changed HTTP/1.1"" 500 -; ```. ## Proposed Solution. Do not accept arbitrary specs, and in particular do not accept a name. I think batch should just take an image, a command, and maybe some volumes and volume mounts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4773
https://github.com/hail-is/hail/pull/4774:146,Testability,test,test,146,"I wrote a pretty basic TableEmit structure that can handle basic read/map/filter stuff. It's still not hooked up to anything, but I wrote a small test for it. Still just passing in the region from outside the generated code, but that'll be the next step. @tpoterba I've assigned this to you again since you had the last one, but I can reassign if you'd like.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4774
https://github.com/hail-is/hail/issues/4775:461,Availability,error,error,461,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-a2eaf89baa0c; ### What you did:; Executed first four lines in https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/tutorials/01-genome-wide-association-study.ipynb; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-758eefccad3d> in <module>; ----> 1 hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). <decorator-gen-1074> in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci). ~/bin/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in dec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2110,Availability,Error,Error,2110,"*kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2445,Availability,Error,ErrorHandling,2445,"l_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2471,Availability,Error,ErrorHandling,2471,"genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:3519,Availability,Error,Error,3519,"9 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png). directly provides 1kg.mt so the conversion step ; ```python; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True); ```; is unnecessary:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:467,Integrability,message,messages,467,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-a2eaf89baa0c; ### What you did:; Executed first four lines in https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/tutorials/01-genome-wide-association-study.ipynb; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-758eefccad3d> in <module>; ----> 1 hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). <decorator-gen-1074> in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci). ~/bin/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in dec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:1039,Integrability,wrap,wrapper,1039,"-----------------------------------------------------------------. ### Hail version:; 0.2-a2eaf89baa0c; ### What you did:; Executed first four lines in https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/tutorials/01-genome-wide-association-study.ipynb; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-758eefccad3d> in <module>; ----> 1 hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). <decorator-gen-1074> in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci). ~/bin/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:1090,Integrability,wrap,wrapper,1090,"-----------------------------------------------------------------. ### Hail version:; 0.2-a2eaf89baa0c; ### What you did:; Executed first four lines in https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/tutorials/01-genome-wide-association-study.ipynb; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-758eefccad3d> in <module>; ----> 1 hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). <decorator-gen-1074> in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci). ~/bin/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:1296,Integrability,wrap,wrapper,1296,"-----------------------------------------------------------------. ### Hail version:; 0.2-a2eaf89baa0c; ### What you did:; Executed first four lines in https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/tutorials/01-genome-wide-association-study.ipynb; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-758eefccad3d> in <module>; ----> 1 hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). <decorator-gen-1074> in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci). ~/bin/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/bin/anaconda3/lib/python3.6/site-packages/hail/methods/impex.py in import_vcf(path, force, force_bgz, header_file, min_partitions, drop_samples, call_fields, reference_genome, contig_recoding, array_elements_required, skip_invalid_loci); 1893 skip_invalid_loci,; 1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2566,Performance,Load,LoadVCF,2566,"1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2587,Performance,Load,LoadVCF,2587,"; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2649,Performance,Load,LoadVCF,2649,"conda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2688,Performance,Load,LoadVCF,2688,"a_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2705,Performance,Load,LoadVCF,2705,"in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2744,Performance,Load,LoadVCF,2744,"f.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4775:2760,Performance,Load,LoadVCF,2760,"ient.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Basically, the ; ```; hl.utils.get_1kg('data/'); ```; ![image](https://user-images.githubusercontent.com/10011161/48459558-9f645c80-e798-11e8-94db-0faa2e44e985.png). directly pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4775
https://github.com/hail-is/hail/issues/4776:569,Availability,error,error,569,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-48fb15983780. ### What you did:; Plotting a histogram of mt.sample_qc.r_ti_tv, using tutorial data:. import hail as hl; import bokeh. hl.init(); mt = hl.read_matrix_table('data/1kg.mt'); mt = hl.sample_qc(mt); p = hl.plot.histogram(mt.sample_qc.r_ti_tv); bokeh.io.save(p, 'test.html'). ### What went wrong (all error messages here, including the full java stack trace):. The resulting histogram has very high numbers for Frequency labels (y-axis). There are 284 samples in the tutorial dataset, so I expected frequencies to sum up to that, but y-axis labels are 5.000e+4, 1.000e+5, 1.500e+5, implying much higher counts. . I'm new to Hail and I could be plotting the values wrong and misunderstanding this particular plot (what I wanted to plot was the histogram of Ti/Tv ratios of all the samples). I've noticed the same y-axis labels in the GWAS tutorial in the docs (In [29]). . Thank you for your time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4776
https://github.com/hail-is/hail/issues/4776:575,Integrability,message,messages,575,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-48fb15983780. ### What you did:; Plotting a histogram of mt.sample_qc.r_ti_tv, using tutorial data:. import hail as hl; import bokeh. hl.init(); mt = hl.read_matrix_table('data/1kg.mt'); mt = hl.sample_qc(mt); p = hl.plot.histogram(mt.sample_qc.r_ti_tv); bokeh.io.save(p, 'test.html'). ### What went wrong (all error messages here, including the full java stack trace):. The resulting histogram has very high numbers for Frequency labels (y-axis). There are 284 samples in the tutorial dataset, so I expected frequencies to sum up to that, but y-axis labels are 5.000e+4, 1.000e+5, 1.500e+5, implying much higher counts. . I'm new to Hail and I could be plotting the values wrong and misunderstanding this particular plot (what I wanted to plot was the histogram of Ti/Tv ratios of all the samples). I've noticed the same y-axis labels in the GWAS tutorial in the docs (In [29]). . Thank you for your time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4776
https://github.com/hail-is/hail/issues/4776:531,Testability,test,test,531,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2-48fb15983780. ### What you did:; Plotting a histogram of mt.sample_qc.r_ti_tv, using tutorial data:. import hail as hl; import bokeh. hl.init(); mt = hl.read_matrix_table('data/1kg.mt'); mt = hl.sample_qc(mt); p = hl.plot.histogram(mt.sample_qc.r_ti_tv); bokeh.io.save(p, 'test.html'). ### What went wrong (all error messages here, including the full java stack trace):. The resulting histogram has very high numbers for Frequency labels (y-axis). There are 284 samples in the tutorial dataset, so I expected frequencies to sum up to that, but y-axis labels are 5.000e+4, 1.000e+5, 1.500e+5, implying much higher counts. . I'm new to Hail and I could be plotting the values wrong and misunderstanding this particular plot (what I wanted to plot was the histogram of Ti/Tv ratios of all the samples). I've noticed the same y-axis labels in the GWAS tutorial in the docs (In [29]). . Thank you for your time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4776
https://github.com/hail-is/hail/issues/4780:749,Availability,error,error,749,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: ; version 0.2-721af83bc30a. ### What you did: ; Import UK Biobank bgen chr10. import hail as hl; import sys; hl.init(); chr=sys.argv[1]; bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; hl.index_bgen(bgen); hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1962,Availability,Heartbeat,HeartbeatReceiver,1962,"7-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2006,Availability,Heartbeat,HeartbeatReceiver,2006,"7-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2049,Availability,Heartbeat,HeartbeatReceiver,2049,"er-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2099,Availability,Heartbeat,HeartbeatReceiver,2099,GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2143,Availability,Heartbeat,HeartbeatReceiver,2143,GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2186,Availability,Heartbeat,HeartbeatReceiver,2186,.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2789,Availability,Heartbeat,HeartbeatReceiver,2789,ssLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collecti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2824,Availability,Heartbeat,HeartbeatReceiver,2824,ClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2859,Availability,Heartbeat,HeartbeatReceiver,2859,ClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2909,Availability,Heartbeat,HeartbeatReceiver,2909,java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2966,Availability,Heartbeat,HeartbeatReceiver,2966,atReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.colle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:6654,Availability,Error,Error,6654,"func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:6973,Availability,failure,failure,6973,")._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:7030,Availability,failure,failure,7030,"c.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15075,Availability,Error,Error,15075,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15136,Availability,ERROR,ERROR,15136,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15846,Availability,Error,Error,15846,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15930,Availability,Error,Error,15930,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:5408,Deployability,install,install,5408,"ache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:5771,Deployability,install,install,5771,"imer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:6163,Deployability,install,install,6163,"gen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:6476,Deployability,install,install,6476,"pper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15246,Deployability,install,install,15246,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15592,Deployability,install,install,15592,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15755,Deployability,install,install,15755,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9677,Energy Efficiency,schedul,scheduler,9677,scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9717,Energy Efficiency,schedul,scheduler,9717,erator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9815,Energy Efficiency,schedul,scheduler,9815,:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9912,Energy Efficiency,schedul,scheduler,9912,pache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10163,Energy Efficiency,schedul,scheduler,10163,03); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10243,Energy Efficiency,schedul,scheduler,10243,apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10348,Energy Efficiency,schedul,scheduler,10348,la:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10496,Energy Efficiency,schedul,scheduler,10496,323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at o,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10584,Energy Efficiency,schedul,scheduler,10584,rtitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10681,Energy Efficiency,schedul,scheduler,10681,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.foreachPartition(RDD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10776,Energy Efficiency,schedul,scheduler,10776,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924); at is.hail.io.bgen.IndexBgen$.apply(IndexBgen.scala:99); at is.hail.HailContext.indexB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10939,Energy Efficiency,schedul,scheduler,10939,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926); at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924); at is.hail.io.bgen.IndexBgen$.apply(IndexBgen.scala:99); at is.hail.HailContext.indexBgen(HailContext.scala:374); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:755,Integrability,message,messages,755,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: ; version 0.2-721af83bc30a. ### What you did: ; Import UK Biobank bgen chr10. import hail as hl; import sys; hl.init(); chr=sys.argv[1]; bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; hl.index_bgen(bgen); hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3250,Integrability,Message,MessageLoop,3250,e$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:5479,Integrability,wrap,wrapper,5479,"7); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:5530,Integrability,wrap,wrapper,5530,"7); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:5735,Integrability,wrap,wrapper,5735,"7); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). <decorator-gen-1065> in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/methods/impex.py in index_bgen(path, index_file_map, reference_genome, contig_recoding, skip_invalid_loci); 1955 index_file_map = tdict(tstr, tstr)._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15399,Integrability,protocol,protocol,15399,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:15903,Integrability,protocol,protocol,15903,".serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185); at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: GC overhead limit exceeded; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1785,Performance,load,loadClass,1785,"ack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.Heartbe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1854,Performance,load,loadClass,1854," `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(Heart",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1909,Performance,load,loadClass,1909,"NG: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatRe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3302,Performance,concurren,concurrent,3302,la:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3386,Performance,concurren,concurrent,3386,99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3118,Safety,safe,safelyCall,3118,org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:6952,Safety,abort,aborted,6952,")._convert_to_j(index_file_map); 1956; -> 1957 Env.hc()._jhc.indexBgen(jindexed_seq_args(path), index_file_map, joption(rg), contig_recoding, skip_invalid_loci); 1958; 1959. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: GC overhead limit exceeded. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.immutable.VectorBuilder.<init>(Vector.scala:713); at scala.collection.immutable.Vector$.newBuilder(Vector.scala:22); at scala.collection.immutable.IndexedSeq$.newBuilder(IndexedSeq.scala:46); at scala.collection.IndexedSeq$.newBuilder(IndexedSeq.scala:36); at scala.collection.IndexedSeq$$anon$1.apply(IndexedSeq.scala:34); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:39); at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21); at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); at com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9847,Safety,abort,abortStage,9847,park.util.CompletionIterator.hasNext(CompletionIterator.scala:32); at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:9944,Safety,abort,abortStage,9944,bleIterator.hasNext(InterruptibleIterator.scala:37); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:10186,Safety,abort,abortStage,10186,huffledRDD.compute(ShuffledRDD.scala:105); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1642,Security,secur,security,1642,"t_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.muta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1651,Security,Access,AccessController,1651,"ample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2344,Security,Hash,HashMap,2344,rEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2377,Security,Hash,HashMap,2377,23); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2424,Security,Hash,HashMap,2424,sPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolE,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2457,Security,Hash,HashMap,2457,sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2504,Security,Hash,HashTable,2504,9); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.Out,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2533,Security,Hash,HashTable,2533,ClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC ov,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2583,Security,Hash,HashMap,2583,URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.List,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2604,Security,Hash,HashMap,2604,$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2651,Security,Hash,HashMap,2651,ssController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:2667,Security,Hash,HashMap,2667,.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$p,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3894,Security,Hash,HashMap,3894,artbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call las,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3935,Security,Hash,HashMap,3935,receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectn,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:3983,Security,Hash,HashMap,3983,"rg.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4024,Security,Hash,HashMap,4024,".Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4072,Security,Hash,HashTable,4072,"; at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4101,Security,Hash,HashTable,4101,"rk.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbioba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4151,Security,Hash,HashMap,4151,"pache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4172,Security,Hash,HashMap,4172,"c.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4219,Security,Hash,HashMap,4219,"k.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:4243,Security,Hash,HashMap,4243,"atcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.apache.spark.SparkStatusTracker.getActiveStageIds(SparkStatusTracker.scala:61); at org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:67); at org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:55); at java.util.TimerThread.mainLoop(Timer.java:555); at java.util.TimerThread.run(Timer.java:505); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen2mt.py in <module>; 6 sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; 7 mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; ----> 8 hl.index_bgen(bgen); 9 hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', '",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:915,Testability,LOG,LOGGING,915,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: ; version 0.2-721af83bc30a. ### What you did: ; Import UK Biobank bgen chr10. import hail as hl; import sys; hl.init(); chr=sys.argv[1]; bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; hl.index_bgen(bgen); hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/issues/4780:1021,Testability,log,log,1021,"ature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: ; version 0.2-721af83bc30a. ### What you did: ; Import UK Biobank bgen chr10. import hail as hl; import sys; hl.init(); chr=sys.argv[1]; bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; mt=""/project/ukbiobank/imp/uk.v3/mt/ukbb_imp_chr""+chr+""_v3_s487327.mt""; hl.index_bgen(bgen); hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT', 'GP','dosage']).write(mt). ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4780
https://github.com/hail-is/hail/pull/4785:332,Availability,failure,failure,332,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4785:508,Deployability,configurat,configuration,508,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4785:508,Modifiability,config,configuration,508,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4785:19,Testability,test,test-local,19,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4785:97,Testability,test,test-local,97,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4785:134,Testability,test,test,134,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4785
https://github.com/hail-is/hail/pull/4786:375,Availability,failure,failure,375,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4786:551,Deployability,configurat,configuration,551,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4786:551,Modifiability,config,configuration,551,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4786:19,Testability,test,test-local,19,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4786:97,Testability,test,test-local,97,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4786:134,Testability,test,test,134,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4786
https://github.com/hail-is/hail/pull/4787:7,Modifiability,enhance,enhancements,7,Future enhancements to batch will necessitate the proper use of modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4787
https://github.com/hail-is/hail/pull/4788:73,Integrability,depend,dependencies,73,"Now that we are using proper python packages and modules, we can use pip dependencies instead of explicitly writing out the dependencies in the `Dockerfile`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4788
https://github.com/hail-is/hail/pull/4788:124,Integrability,depend,dependencies,124,"Now that we are using proper python packages and modules, we can use pip dependencies instead of explicitly writing out the dependencies in the `Dockerfile`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4788
https://github.com/hail-is/hail/pull/4789:35,Availability,error,errors,35,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789
https://github.com/hail-is/hail/pull/4789:130,Availability,error,errors,130,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789
https://github.com/hail-is/hail/pull/4789:171,Availability,error,error,171,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789
https://github.com/hail-is/hail/pull/4789:205,Availability,error,error,205,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789
https://github.com/hail-is/hail/pull/4789:311,Availability,error,errors,311,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4789
https://github.com/hail-is/hail/pull/4791:23,Deployability,update,update,23,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:409,Deployability,update,update,409,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:355,Performance,cache,cache,355,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:455,Performance,cache,cache,455,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:627,Performance,cache,cache,627,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:649,Performance,perform,performs,649,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4791:964,Usability,clear,clear,964,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4791
https://github.com/hail-is/hail/pull/4794:52,Integrability,Wrap,Wrapping,52,I couldn't get `_counter` to work from `server.py`. Wrapping it in a function restored the functionality.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4794
https://github.com/hail-is/hail/pull/4798:88,Modifiability,refactor,refactor,88,"And `Batch`. Again, no code changes here. That's all for today. Monday I'll do one last refactor before I'm ready to start merging the DAG functionality.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4798
https://github.com/hail-is/hail/issues/4799:743,Availability,error,error,743,"I wonder if related to #4754?; ```; ht = hl.experimental.import_gtf('gs://konradk/gencode.v19.annotation.gtf.bgz', 'GRCh37', True, min_partitions=12); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0],; transcript_id=ht.transcript_id.split('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:860,Availability,error,error,860,"I wonder if related to #4754?; ```; ht = hl.experimental.import_gtf('gs://konradk/gencode.v19.annotation.gtf.bgz', 'GRCh37', True, min_partitions=12); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0],; transcript_id=ht.transcript_id.split('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:1515,Integrability,Wrap,WrappedArray,1515,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:1536,Integrability,Wrap,WrappedArray,1536,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:1954,Usability,Simpl,Simplify,1954,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:1997,Usability,Simpl,Simplify,1997,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:2038,Usability,Simpl,Simplify,2038,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/issues/4799:2081,Usability,Simpl,Simplify,2081,"it('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:394); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simplify.scala:251); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223); 	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4799
https://github.com/hail-is/hail/pull/4804:88,Modifiability,maintainab,maintainability,88,"This is a minor architectural change (cc'ing @cseed @tpoterba) that I hope will improve maintainability of `batch`. It foreshadows the DAG functionality. There may be shared data structures between the server and the client. At the very least, the client sends structured data to the server (e.g. a pod spec and metadata about the job). Often, the server parses this data into an object or series of objects which contain methods for performing the server's job (e.g. `batch/server/job.py`). I think this architecture is more or less a different way of defining the API (see `batch/api.py`). I think defining the API via data objects is appealing because; - it centralizes serialization and deserialization for each data structure in one class,; - it enables sharing (via object composition) of that basic data structure between potentially complex client and server objects that implement algorithms on that data structure (I want to do this with the forthcoming DAG stuff), and; - the client has objects representing its ideas (i.e. ""a job"") and those objects can have `__str__`'s and `__repr__`'s facilitating debugging of the client. Moreover, this change pushes the use of k8s' swagger models everywhere possible. This means it's harder for us to make code mistakes because pylint will notice when we, for example, misspell a parameter to a k8s model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4804
https://github.com/hail-is/hail/pull/4804:434,Performance,perform,performing,434,"This is a minor architectural change (cc'ing @cseed @tpoterba) that I hope will improve maintainability of `batch`. It foreshadows the DAG functionality. There may be shared data structures between the server and the client. At the very least, the client sends structured data to the server (e.g. a pod spec and metadata about the job). Often, the server parses this data into an object or series of objects which contain methods for performing the server's job (e.g. `batch/server/job.py`). I think this architecture is more or less a different way of defining the API (see `batch/api.py`). I think defining the API via data objects is appealing because; - it centralizes serialization and deserialization for each data structure in one class,; - it enables sharing (via object composition) of that basic data structure between potentially complex client and server objects that implement algorithms on that data structure (I want to do this with the forthcoming DAG stuff), and; - the client has objects representing its ideas (i.e. ""a job"") and those objects can have `__str__`'s and `__repr__`'s facilitating debugging of the client. Moreover, this change pushes the use of k8s' swagger models everywhere possible. This means it's harder for us to make code mistakes because pylint will notice when we, for example, misspell a parameter to a k8s model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4804
https://github.com/hail-is/hail/pull/4806:45,Testability,test,test,45,kind of just trying to figure out how to get test results to show up in the ci thing. will assign once i've figured out how that works.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4806
https://github.com/hail-is/hail/pull/4807:398,Usability,learn,learn,398,"I also harmonized the delete endpoint URLs so that `jobs`, `batch`, and `dag` all use `DELETE /{type}/{id}`. @jigold is reviewing all the infrastructural changes that prepare for this. This is relatively independent of the restructuring so I figure I can assign it to a new person. @jbloom22 if you'd rather not review `batch` things, I can roll the dice again. Otherwise, here's an opportunity to learn about how that's working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4807
https://github.com/hail-is/hail/pull/4809:65,Usability,feedback,feedback,65,"@patrick-schultz I've assigned this to you because I wanted your feedback, but I'm happy to spin the wheel if you'd prefer. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4809
https://github.com/hail-is/hail/pull/4811:86,Safety,timeout,timeout,86,"Unfortunately, as far as I can tell, swagger-codgen providss no way; to set a default timeout. Instead, I littered our code with explicit; timeout arguments to k8s API calls. The best documentation of this feature of swagger-codegen that I have; found is the PR that adds the feature:; https://github.com/swagger-api/swagger-codegen/pull/4173. cc: @cseed, we should do this any time we add new k8s API calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4811
https://github.com/hail-is/hail/pull/4811:139,Safety,timeout,timeout,139,"Unfortunately, as far as I can tell, swagger-codgen providss no way; to set a default timeout. Instead, I littered our code with explicit; timeout arguments to k8s API calls. The best documentation of this feature of swagger-codegen that I have; found is the PR that adds the feature:; https://github.com/swagger-api/swagger-codegen/pull/4173. cc: @cseed, we should do this any time we add new k8s API calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4811
https://github.com/hail-is/hail/pull/4812:215,Deployability,deploy,deploy,215,"Changes; ---; - introduce consistent naming of the three types of hail versions (see; build.gradle comment). - add `hail_pip_version` to `_generated_version_info.py` which is used by; `setup.py`. - add `hail/python/deploy.sh` which should not be called directly but handles; generated PYPI compatible packages and uploading to PYPI. - `generate-build-info.sh` is now the *only* authoritative source on hail; versioning (it generates *exactly* two files, one for JARs and one for the; python library). - fix: actually activate the anonymous conda env in `hail-ci-build.sh`'s ; pip test. Janitorial; ---; - ignore python/hail.egg-info. - sort lines in hail/.gitignore. - add `hail/python/hail/docs/__init__.py` so that we can use relative imports; in `conf.py`. - remove commented out code in makeDocs.sh. - makeDocs.sh now has `set -x`. Notes; ---. You can test deployment locally with:. ```; HAIL_TWINE_CREDS_FOLDER=/path/to/twine/creds ./gradlew deploy -D hail.pip-version-suffix=.devN; ```; where `.devN` is some currently unused dev version suffix. You can check these by running `python hail/list_pypi_versions.py hail`. These dev versions are not visible to end users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812
https://github.com/hail-is/hail/pull/4812:861,Deployability,deploy,deployment,861,"Changes; ---; - introduce consistent naming of the three types of hail versions (see; build.gradle comment). - add `hail_pip_version` to `_generated_version_info.py` which is used by; `setup.py`. - add `hail/python/deploy.sh` which should not be called directly but handles; generated PYPI compatible packages and uploading to PYPI. - `generate-build-info.sh` is now the *only* authoritative source on hail; versioning (it generates *exactly* two files, one for JARs and one for the; python library). - fix: actually activate the anonymous conda env in `hail-ci-build.sh`'s ; pip test. Janitorial; ---; - ignore python/hail.egg-info. - sort lines in hail/.gitignore. - add `hail/python/hail/docs/__init__.py` so that we can use relative imports; in `conf.py`. - remove commented out code in makeDocs.sh. - makeDocs.sh now has `set -x`. Notes; ---. You can test deployment locally with:. ```; HAIL_TWINE_CREDS_FOLDER=/path/to/twine/creds ./gradlew deploy -D hail.pip-version-suffix=.devN; ```; where `.devN` is some currently unused dev version suffix. You can check these by running `python hail/list_pypi_versions.py hail`. These dev versions are not visible to end users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812
https://github.com/hail-is/hail/pull/4812:947,Deployability,deploy,deploy,947,"Changes; ---; - introduce consistent naming of the three types of hail versions (see; build.gradle comment). - add `hail_pip_version` to `_generated_version_info.py` which is used by; `setup.py`. - add `hail/python/deploy.sh` which should not be called directly but handles; generated PYPI compatible packages and uploading to PYPI. - `generate-build-info.sh` is now the *only* authoritative source on hail; versioning (it generates *exactly* two files, one for JARs and one for the; python library). - fix: actually activate the anonymous conda env in `hail-ci-build.sh`'s ; pip test. Janitorial; ---; - ignore python/hail.egg-info. - sort lines in hail/.gitignore. - add `hail/python/hail/docs/__init__.py` so that we can use relative imports; in `conf.py`. - remove commented out code in makeDocs.sh. - makeDocs.sh now has `set -x`. Notes; ---. You can test deployment locally with:. ```; HAIL_TWINE_CREDS_FOLDER=/path/to/twine/creds ./gradlew deploy -D hail.pip-version-suffix=.devN; ```; where `.devN` is some currently unused dev version suffix. You can check these by running `python hail/list_pypi_versions.py hail`. These dev versions are not visible to end users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812
https://github.com/hail-is/hail/pull/4812:580,Testability,test,test,580,"Changes; ---; - introduce consistent naming of the three types of hail versions (see; build.gradle comment). - add `hail_pip_version` to `_generated_version_info.py` which is used by; `setup.py`. - add `hail/python/deploy.sh` which should not be called directly but handles; generated PYPI compatible packages and uploading to PYPI. - `generate-build-info.sh` is now the *only* authoritative source on hail; versioning (it generates *exactly* two files, one for JARs and one for the; python library). - fix: actually activate the anonymous conda env in `hail-ci-build.sh`'s ; pip test. Janitorial; ---; - ignore python/hail.egg-info. - sort lines in hail/.gitignore. - add `hail/python/hail/docs/__init__.py` so that we can use relative imports; in `conf.py`. - remove commented out code in makeDocs.sh. - makeDocs.sh now has `set -x`. Notes; ---. You can test deployment locally with:. ```; HAIL_TWINE_CREDS_FOLDER=/path/to/twine/creds ./gradlew deploy -D hail.pip-version-suffix=.devN; ```; where `.devN` is some currently unused dev version suffix. You can check these by running `python hail/list_pypi_versions.py hail`. These dev versions are not visible to end users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812
https://github.com/hail-is/hail/pull/4812:856,Testability,test,test,856,"Changes; ---; - introduce consistent naming of the three types of hail versions (see; build.gradle comment). - add `hail_pip_version` to `_generated_version_info.py` which is used by; `setup.py`. - add `hail/python/deploy.sh` which should not be called directly but handles; generated PYPI compatible packages and uploading to PYPI. - `generate-build-info.sh` is now the *only* authoritative source on hail; versioning (it generates *exactly* two files, one for JARs and one for the; python library). - fix: actually activate the anonymous conda env in `hail-ci-build.sh`'s ; pip test. Janitorial; ---; - ignore python/hail.egg-info. - sort lines in hail/.gitignore. - add `hail/python/hail/docs/__init__.py` so that we can use relative imports; in `conf.py`. - remove commented out code in makeDocs.sh. - makeDocs.sh now has `set -x`. Notes; ---. You can test deployment locally with:. ```; HAIL_TWINE_CREDS_FOLDER=/path/to/twine/creds ./gradlew deploy -D hail.pip-version-suffix=.devN; ```; where `.devN` is some currently unused dev version suffix. You can check these by running `python hail/list_pypi_versions.py hail`. These dev versions are not visible to end users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4812
https://github.com/hail-is/hail/pull/4814:80,Deployability,deploy,deploy,80,I will rebase after #4812 is in. This is a one-line change that adds `./gradlew deploy` to `hail-ci-deploy.sh`. The `python/deploy.sh` script only takes action if the latest deployed hail version does not match the current hail version. The current hail version is defined by `hailShortVersion` and `hailPipVersion` in `build.gradle`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4814
https://github.com/hail-is/hail/pull/4814:100,Deployability,deploy,deploy,100,I will rebase after #4812 is in. This is a one-line change that adds `./gradlew deploy` to `hail-ci-deploy.sh`. The `python/deploy.sh` script only takes action if the latest deployed hail version does not match the current hail version. The current hail version is defined by `hailShortVersion` and `hailPipVersion` in `build.gradle`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4814
https://github.com/hail-is/hail/pull/4814:124,Deployability,deploy,deploy,124,I will rebase after #4812 is in. This is a one-line change that adds `./gradlew deploy` to `hail-ci-deploy.sh`. The `python/deploy.sh` script only takes action if the latest deployed hail version does not match the current hail version. The current hail version is defined by `hailShortVersion` and `hailPipVersion` in `build.gradle`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4814
https://github.com/hail-is/hail/pull/4814:174,Deployability,deploy,deployed,174,I will rebase after #4812 is in. This is a one-line change that adds `./gradlew deploy` to `hail-ci-deploy.sh`. The `python/deploy.sh` script only takes action if the latest deployed hail version does not match the current hail version. The current hail version is defined by `hailShortVersion` and `hailPipVersion` in `build.gradle`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4814
https://github.com/hail-is/hail/issues/4816:223,Availability,error,error,223,"To reproduce:. ```; >>> import hail as hl; >>> hl._set_flags(cpp='true'); >>> mt = hl.read_table('gs://gnomad-public/release/2.1/ht/exomes/gnomad.exomes.r2.1.sites.ht'); >>> mt._force_count(); ```. gets:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x0000000113aae924, pid=29051, tid=0x0000000000004003; #; ```. This is on OSX. Smaller examples work fine with C++ on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4816
https://github.com/hail-is/hail/issues/4816:117,Deployability,release,release,117,"To reproduce:. ```; >>> import hail as hl; >>> hl._set_flags(cpp='true'); >>> mt = hl.read_table('gs://gnomad-public/release/2.1/ht/exomes/gnomad.exomes.r2.1.sites.ht'); >>> mt._force_count(); ```. gets:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x0000000113aae924, pid=29051, tid=0x0000000000004003; #; ```. This is on OSX. Smaller examples work fine with C++ on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4816
https://github.com/hail-is/hail/issues/4816:238,Safety,detect,detected,238,"To reproduce:. ```; >>> import hail as hl; >>> hl._set_flags(cpp='true'); >>> mt = hl.read_table('gs://gnomad-public/release/2.1/ht/exomes/gnomad.exomes.r2.1.sites.ht'); >>> mt._force_count(); ```. gets:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x0000000113aae924, pid=29051, tid=0x0000000000004003; #; ```. This is on OSX. Smaller examples work fine with C++ on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4816
https://github.com/hail-is/hail/issues/4822:55,Availability,echo,echo,55,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:81,Availability,failure,failure,81,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:121,Availability,echo,echo,121,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:268,Availability,echo,echo,268,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:918,Availability,echo,echo,918,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:1570,Availability,Toler,Tolerations,1570,"is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Container image ""alpine:3.8"" already present on machine; Normal Created 53s (x5 over 2m) kubelet, minikube Created container; Normal Started 53s (x5 over 2m) kubelet, minikube Started container; Warning BackOff 38s (x9 over 2m) kubelet, minikube Back-off restarting failed container; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:1760,Energy Efficiency,Schedul,Scheduled,1760,"is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Container image ""alpine:3.8"" already present on machine; Normal Created 53s (x5 over 2m) kubelet, minikube Created container; Normal Started 53s (x5 over 2m) kubelet, minikube Started container; Warning BackOff 38s (x9 over 2m) kubelet, minikube Back-off restarting failed container; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:1781,Energy Efficiency,schedul,scheduler,1781,"is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Container image ""alpine:3.8"" already present on machine; Normal Created 53s (x5 over 2m) kubelet, minikube Created container; Normal Started 53s (x5 over 2m) kubelet, minikube Started container; Warning BackOff 38s (x9 over 2m) kubelet, minikube Back-off restarting failed container; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4822:1713,Integrability,Message,Message,1713,"is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Container image ""alpine:3.8"" already present on machine; Normal Created 53s (x5 over 2m) kubelet, minikube Created container; Normal Started 53s (x5 over 2m) kubelet, minikube Started container; Warning BackOff 38s (x9 over 2m) kubelet, minikube Back-off restarting failed container; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4822
https://github.com/hail-is/hail/issues/4827:405,Availability,error,error,405," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4827
https://github.com/hail-is/hail/issues/4827:411,Integrability,message,messages,411," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4827
https://github.com/hail-is/hail/issues/4827:479,Performance,optimiz,optimization,479," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4827
https://github.com/hail-is/hail/pull/4828:101,Deployability,update,update,101,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:494,Deployability,update,update,494,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:439,Performance,cache,cache,439,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:540,Performance,cache,cache,540,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:715,Performance,cache,cache,715,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:738,Performance,perform,performs,738,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/pull/4828:1049,Usability,clear,clear,1049,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4828
https://github.com/hail-is/hail/issues/4833:142,Modifiability,extend,extends,142,"@patrick-schultz your assignment was totally random!. Here is the code for TableDistinct:; ```scala; case class TableDistinct(child: TableIR) extends TableIR {. ... protected[ir] override def execute(hc: HailContext): TableValue = {; val prev = child.execute(hc); prev.copy(rvd = prev.rvd.distinctByKey()); }; }; ```. I suspect that if the RVD key is longer than the table `typ.key`, this is going to produce incorrect results.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4833
https://github.com/hail-is/hail/pull/4845:39,Availability,error,error,39,Follow-up PRs will use this for better error messages in places like require_biallelic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4845
https://github.com/hail-is/hail/pull/4845:45,Integrability,message,messages,45,Follow-up PRs will use this for better error messages in places like require_biallelic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4845
https://github.com/hail-is/hail/pull/4847:80,Testability,test,tests,80,"Fixes #4816. I pulled this out of the other PR, since it's still failing the CI tests, so I'll remove this part there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4847
https://github.com/hail-is/hail/issues/4857:965,Availability,error,error,965," ### What you did: ; Ran hail's maximal independent set method with the following code:. ```; related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); ```. where related pairs is structured as:. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'i': struct {; data_type: str, ; s: str; } ; 'j': struct {; data_type: str, ; s: str; } ; 'id1_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; 'id2_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; ----------------------------------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:2696,Availability,Error,Error,2696,"original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_str)); 145 ; 146 nt = Table._from_java(nodes._jt.annotateGlobal(nodes_in_set, hl.tset(node_t)._jtype, 'nodes_in_set')). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: MatchError: cmg_exomes (of class java.lang.String). Java stack trace:; scala.MatchError: cmg_exomes (of class java.lang.String); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:537); 	at is.hail.annotations.RegionValueBuilder.addRow(RegionValueBuilder.scala:298); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:5401,Availability,Error,Error,5401,il.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-29fbaeaf265e; Error summary: MatchError: cmg_exomes (of class java.lang.String); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:971,Integrability,message,messages,971," ### What you did: ; Ran hail's maximal independent set method with the following code:. ```; related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); ```. where related pairs is structured as:. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'i': struct {; data_type: str, ; s: str; } ; 'j': struct {; data_type: str, ; s: str; } ; 'id1_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; 'id2_rank': struct {; id: struct {; data_type: str, ; s: str; }, ; rank: int32; } ; ----------------------------------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:1597,Integrability,wrap,wrapper,1597,"----------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_str)); 145 ; 146 nt = Table._from_java(nodes._jt.annotateGlobal(nodes_in_set, hl.tset(node_t)._jtype, 'nodes_in_set')). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:1648,Integrability,wrap,wrapper,1648,"----------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_str)); 145 ; 146 nt = Table._from_java(nodes._jt.annotateGlobal(nodes_in_set, hl.tset(node_t)._jtype, 'nodes_in_set')). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:1854,Integrability,wrap,wrapper,1854,"----------------; Key: ['i', 'j']; ----------------------------------------; ```. and tie_breaker is :. ```; def tie_breaker(l, r):; return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError Traceback (most recent call last); <ipython-input-220-88e7ce1066ed> in <module>; 11 return hl.or_else(l.rank, max_rank + 1) - hl.or_else(r.rank, max_rank + 1); 12 ; ---> 13 related_samples_to_drop_ranked = hl.maximal_independent_set(related_pairs.id1_rank, related_pairs.id2_rank,keep=False, tie_breaker=tie_breaker); 14 #return related_samples_to_drop_ranked.select(**related_samples_to_drop_ranked.node.id).key_by('data_type', 's'). <decorator-gen-1024> in maximal_independent_set(i, j, keep, tie_breaker). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 142 ; 143 edges = t.key_by().select('i', 'j'); --> 144 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_str)); 145 ; 146 nt = Table._from_java(nodes._jt.annotateGlobal(nodes_in_set, hl.tset(node_t)._jtype, 'nodes_in_set')). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4124,Security,Hash,HashMap,4124,onValueBuilder.addRow(RegionValueBuilder.scala:298); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4157,Security,Hash,HashMap,4157,(RegionValueBuilder.scala:298); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.Abstract,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4205,Security,Hash,HashMap,4205,onValueBuilder.addAnnotation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.comm,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4238,Security,Hash,HashMap,4238,otation(RegionValueBuilder.scala:541); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.exe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4286,Security,Hash,HashTable,4286,s.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4315,Security,Hash,HashTable,4315,$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:60); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4366,Security,Hash,HashMap,4366,	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). H,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4387,Security,Hash,HashMap,4387,ils.Graph$$anonfun$2$$anonfun$apply$2$$anonfun$apply$3.apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4435,Security,Hash,HashMap,4435,apply(Graph.scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-29fbaeaf265e; Error summary: MatchError: cmg_exomes ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/issues/4857:4451,Security,Hash,HashMap,4451,scala:54); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:54); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$2.apply(Graph.scala:53); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:91); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:90); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:90); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:76); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-29fbaeaf265e; Error summary: MatchError: cmg_exomes (of class ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4857
https://github.com/hail-is/hail/pull/4859:0,Modifiability,rewrite,rewrite,0,rewrite Region.scala to build on the RegionPool backed region.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4859
https://github.com/hail-is/hail/pull/4862:81,Energy Efficiency,power,powerful,81,"@tpoterba @jbloom22 a few more things to fix for the workshop. I was using a too powerful kubernetes command to look up worker pods and services for the admin page. I now use a restricted form of it that is permitted by our security policy. We also are missing the non-preemptible node pool (!), so this adds that to our gcp-config. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4862
https://github.com/hail-is/hail/pull/4862:325,Modifiability,config,config,325,"@tpoterba @jbloom22 a few more things to fix for the workshop. I was using a too powerful kubernetes command to look up worker pods and services for the admin page. I now use a restricted form of it that is permitted by our security policy. We also are missing the non-preemptible node pool (!), so this adds that to our gcp-config. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4862
https://github.com/hail-is/hail/pull/4862:224,Security,secur,security,224,"@tpoterba @jbloom22 a few more things to fix for the workshop. I was using a too powerful kubernetes command to look up worker pods and services for the admin page. I now use a restricted form of it that is permitted by our security policy. We also are missing the non-preemptible node pool (!), so this adds that to our gcp-config. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4862
https://github.com/hail-is/hail/pull/4863:123,Safety,avoid,avoid,123,- two network requests instead of four now that we know the name of both the service name and pod name; - use try-catch to avoid 500ing if a user hits `GET /new` and their service or pod is already gone (for whatever reason),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4863
https://github.com/hail-is/hail/issues/4866:21,Integrability,depend,dependence,21,We memoize full type dependence from each argument.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4866
https://github.com/hail-is/hail/pull/4871:54,Modifiability,Extend,ExtendedOrdering,54,"Builds on: https://github.com/hail-is/hail/pull/4869. ExtendedOrdering on rows and containers now matches CodeOrdering by using lt instead of compare in lt, etc. FYI @tpoterba since this could potentially (e.g. comparing arrays with nans) cause a user-visible change in behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4871
https://github.com/hail-is/hail/pull/4876:20,Deployability,install,installation,20,This is an outdated installation method.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4876
https://github.com/hail-is/hail/issues/4880:440,Availability,error,error,440,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4880:588,Availability,error,error,588,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4880:709,Availability,Error,Error,709,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4880:737,Availability,FAILURE,FAILURE,737,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4880:446,Integrability,message,messages,446,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4880:1010,Testability,log,log,1010,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; version 0.2.4-f1e6526d34b1. ### What you did:; cd hail/hail. ./gradlew -Dspark.version=2.2.1 -Dbreeze.version=0.13.1 -Dpy4j.version=0.10.4 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):; In file included from Decoder.cpp:3:0:; ../resources/include/hail/Decoder.h:3:10: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^~~~~~~; compilation terminated.; make: *** [build/Decoder.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 36.47 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4880
https://github.com/hail-is/hail/issues/4887:153,Integrability,interface,interface,153,"import plink and export plink annotations should be consistent for the same field. This will make read-write tests easier to write. . The current Python interface can also be improved by potentially looking whether 'cm_position', 'varid', 'mat_id', etc. are defined and the correct types in the dataset and if so, output those fields if the user doesn't specify a value. . However, I can also see the value in the current approach where the user has to specify each value explicitly or missing values are provided as default. Might be worth discussing what interface we want when we start working on 0.3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4887
https://github.com/hail-is/hail/issues/4887:557,Integrability,interface,interface,557,"import plink and export plink annotations should be consistent for the same field. This will make read-write tests easier to write. . The current Python interface can also be improved by potentially looking whether 'cm_position', 'varid', 'mat_id', etc. are defined and the correct types in the dataset and if so, output those fields if the user doesn't specify a value. . However, I can also see the value in the current approach where the user has to specify each value explicitly or missing values are provided as default. Might be worth discussing what interface we want when we start working on 0.3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4887
https://github.com/hail-is/hail/issues/4887:109,Testability,test,tests,109,"import plink and export plink annotations should be consistent for the same field. This will make read-write tests easier to write. . The current Python interface can also be improved by potentially looking whether 'cm_position', 'varid', 'mat_id', etc. are defined and the correct types in the dataset and if so, output those fields if the user doesn't specify a value. . However, I can also see the value in the current approach where the user has to specify each value explicitly or missing values are provided as default. Might be worth discussing what interface we want when we start working on 0.3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4887
https://github.com/hail-is/hail/pull/4888:240,Modifiability,extend,extended,240,"Summing a block-sparse matrix may result in a block-dense vector, in which case maybeBlocks should be None (otherwise the `bis.length < maxNBlocks` assert fails...when rebuilding BlockMatrix in Python/C++ I may change the invariants). Also extended test cases to serve as regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4888
https://github.com/hail-is/hail/pull/4888:148,Testability,assert,assert,148,"Summing a block-sparse matrix may result in a block-dense vector, in which case maybeBlocks should be None (otherwise the `bis.length < maxNBlocks` assert fails...when rebuilding BlockMatrix in Python/C++ I may change the invariants). Also extended test cases to serve as regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4888
https://github.com/hail-is/hail/pull/4888:249,Testability,test,test,249,"Summing a block-sparse matrix may result in a block-dense vector, in which case maybeBlocks should be None (otherwise the `bis.length < maxNBlocks` assert fails...when rebuilding BlockMatrix in Python/C++ I may change the invariants). Also extended test cases to serve as regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4888
https://github.com/hail-is/hail/pull/4888:283,Testability,test,test,283,"Summing a block-sparse matrix may result in a block-dense vector, in which case maybeBlocks should be None (otherwise the `bis.length < maxNBlocks` assert fails...when rebuilding BlockMatrix in Python/C++ I may change the invariants). Also extended test cases to serve as regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4888
https://github.com/hail-is/hail/pull/4891:426,Performance,Optimiz,Optimize,426,"Closes #4527; Closes #4761. This is a workaround to prevent issues with MatrixUnionRows when the; entries arrays are in different places in the rvRowType in each of the; children. Furthermore, it prevents issues if the entries array is; pruned and then re-added later in rebuild, where it will often be; inserted, likely by MatrixMapRows, at the end of the rvRowType. This; rearrangement caused the type equality assertion in Optimize to fail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891
https://github.com/hail-is/hail/pull/4891:413,Testability,assert,assertion,413,"Closes #4527; Closes #4761. This is a workaround to prevent issues with MatrixUnionRows when the; entries arrays are in different places in the rvRowType in each of the; children. Furthermore, it prevents issues if the entries array is; pruned and then re-added later in rebuild, where it will often be; inserted, likely by MatrixMapRows, at the end of the rvRowType. This; rearrangement caused the type equality assertion in Optimize to fail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4891
https://github.com/hail-is/hail/pull/4895:114,Integrability,interface,interface,114,@konradjk @bw2 Can you double check the test examples in `test_liftover_negative_strand` and give feedback on the interface?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895
https://github.com/hail-is/hail/pull/4895:40,Testability,test,test,40,@konradjk @bw2 Can you double check the test examples in `test_liftover_negative_strand` and give feedback on the interface?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895
https://github.com/hail-is/hail/pull/4895:98,Usability,feedback,feedback,98,@konradjk @bw2 Can you double check the test examples in `test_liftover_negative_strand` and give feedback on the interface?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4895
https://github.com/hail-is/hail/issues/4896:404,Availability,error,error,404,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.Closu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:490,Availability,ERROR,ERROR,490,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.Closu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:266,Deployability,install,installed,266,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.Closu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:4856,Deployability,install,installed,4856,"e(RDD.scala:362); at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:793); at is.hail.sparkextras.ContextRDD.cmapPartitions(ContextRDD.scala:351); at is.hail.HailContext.readRows(HailContext.scala:629); at is.hail.rvd.AbstractRVDSpec.read(AbstractRVDSpec.scala:151); at is.hail.variant.RVDComponentSpec.read(MatrixTable.scala:98); at is.hail.expr.ir.TableRead.execute(TableIR.scala:78); at is.hail.expr.ir.TableHead.execute(TableIR.scala:325); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:56); at is.hail.table.Table.x$3$lzycompute(Table.scala:216); at is.hail.table.Table.x$3(Table.scala:216); at is.hail.table.Table.value$lzycompute(Table.scala:216); at is.hail.table.Table.value(Table.scala:216); at is.hail.table.Table.rdd$lzycompute(Table.scala:220); at is.hail.table.Table.rdd(Table.scala:220); at is.hail.table.Table.collect(Table.scala:538); at is.hail.table.Table.showString(Table.scala:582); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:564); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:844). The problem here is that the message gives zero information on why the python hail code failed. Please implement a version checker; run the java binary and query its version, and if it's not compatible, raise a python exception with a clear message about which java version is installed and needs to be installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:4882,Deployability,install,installed,4882,"e(RDD.scala:362); at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:793); at is.hail.sparkextras.ContextRDD.cmapPartitions(ContextRDD.scala:351); at is.hail.HailContext.readRows(HailContext.scala:629); at is.hail.rvd.AbstractRVDSpec.read(AbstractRVDSpec.scala:151); at is.hail.variant.RVDComponentSpec.read(MatrixTable.scala:98); at is.hail.expr.ir.TableRead.execute(TableIR.scala:78); at is.hail.expr.ir.TableHead.execute(TableIR.scala:325); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:56); at is.hail.table.Table.x$3$lzycompute(Table.scala:216); at is.hail.table.Table.x$3(Table.scala:216); at is.hail.table.Table.value$lzycompute(Table.scala:216); at is.hail.table.Table.value(Table.scala:216); at is.hail.table.Table.rdd$lzycompute(Table.scala:220); at is.hail.table.Table.rdd(Table.scala:220); at is.hail.table.Table.collect(Table.scala:538); at is.hail.table.Table.showString(Table.scala:582); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:564); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:844). The problem here is that the message gives zero information on why the python hail code failed. Please implement a version checker; run the java binary and query its version, and if it's not compatible, raise a python exception with a clear message about which java version is installed and needs to be installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:410,Integrability,message,messages,410,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.Closu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:4608,Integrability,message,message,4608,"e(RDD.scala:362); at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:793); at is.hail.sparkextras.ContextRDD.cmapPartitions(ContextRDD.scala:351); at is.hail.HailContext.readRows(HailContext.scala:629); at is.hail.rvd.AbstractRVDSpec.read(AbstractRVDSpec.scala:151); at is.hail.variant.RVDComponentSpec.read(MatrixTable.scala:98); at is.hail.expr.ir.TableRead.execute(TableIR.scala:78); at is.hail.expr.ir.TableHead.execute(TableIR.scala:325); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:56); at is.hail.table.Table.x$3$lzycompute(Table.scala:216); at is.hail.table.Table.x$3(Table.scala:216); at is.hail.table.Table.value$lzycompute(Table.scala:216); at is.hail.table.Table.value(Table.scala:216); at is.hail.table.Table.rdd$lzycompute(Table.scala:220); at is.hail.table.Table.rdd(Table.scala:220); at is.hail.table.Table.collect(Table.scala:538); at is.hail.table.Table.showString(Table.scala:582); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:564); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:844). The problem here is that the message gives zero information on why the python hail code failed. Please implement a version checker; run the java binary and query its version, and if it's not compatible, raise a python exception with a clear message about which java version is installed and needs to be installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:4820,Integrability,message,message,4820,"e(RDD.scala:362); at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:793); at is.hail.sparkextras.ContextRDD.cmapPartitions(ContextRDD.scala:351); at is.hail.HailContext.readRows(HailContext.scala:629); at is.hail.rvd.AbstractRVDSpec.read(AbstractRVDSpec.scala:151); at is.hail.variant.RVDComponentSpec.read(MatrixTable.scala:98); at is.hail.expr.ir.TableRead.execute(TableIR.scala:78); at is.hail.expr.ir.TableHead.execute(TableIR.scala:325); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:56); at is.hail.table.Table.x$3$lzycompute(Table.scala:216); at is.hail.table.Table.x$3(Table.scala:216); at is.hail.table.Table.value$lzycompute(Table.scala:216); at is.hail.table.Table.value(Table.scala:216); at is.hail.table.Table.rdd$lzycompute(Table.scala:220); at is.hail.table.Table.rdd(Table.scala:220); at is.hail.table.Table.collect(Table.scala:538); at is.hail.table.Table.showString(Table.scala:582); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:564); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:844). The problem here is that the message gives zero information on why the python hail code failed. Please implement a version checker; run the java binary and query its version, and if it's not compatible, raise a python exception with a clear message about which java version is installed and needs to be installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1191,Security,Hash,HashMap,1191,"-------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(Closure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1232,Security,Hash,HashMap,1232,". Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1280,Security,Hash,HashMap,1280,"ss instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1321,Security,Hash,HashMap,1321,"re-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1369,Security,Hash,HashTable,1369,". ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(Clo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1398,Security,Hash,HashTable,1398,"ng (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:25",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1448,Security,Hash,HashMap,1448,ck trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:256); at org.apache.spark.util.ClosureCleaner$.clean(Clos,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1469,Security,Hash,HashMap,1469,18-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:256); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1516,Security,Hash,HashMap,1516,ull; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:256); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:156); at org.apache.spark.SparkContext.clean(SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:1540,Security,Hash,HashMap,1540,ang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257); at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256); at scala.collection.immutable.List.foreach(List.scala:381); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:256); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:156); at org.apache.spark.SparkContext.clean(SparkContext.scala:2294); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/issues/4896:4814,Usability,clear,clear,4814,"e(RDD.scala:362); at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:793); at is.hail.sparkextras.ContextRDD.cmapPartitions(ContextRDD.scala:351); at is.hail.HailContext.readRows(HailContext.scala:629); at is.hail.rvd.AbstractRVDSpec.read(AbstractRVDSpec.scala:151); at is.hail.variant.RVDComponentSpec.read(MatrixTable.scala:98); at is.hail.expr.ir.TableRead.execute(TableIR.scala:78); at is.hail.expr.ir.TableHead.execute(TableIR.scala:325); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:56); at is.hail.table.Table.x$3$lzycompute(Table.scala:216); at is.hail.table.Table.x$3(Table.scala:216); at is.hail.table.Table.value$lzycompute(Table.scala:216); at is.hail.table.Table.value(Table.scala:216); at is.hail.table.Table.rdd$lzycompute(Table.scala:220); at is.hail.table.Table.rdd(Table.scala:220); at is.hail.table.Table.collect(Table.scala:538); at is.hail.table.Table.showString(Table.scala:582); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:564); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.base/java.lang.Thread.run(Thread.java:844). The problem here is that the message gives zero information on why the python hail code failed. Please implement a version checker; run the java binary and query its version, and if it's not compatible, raise a python exception with a clear message about which java version is installed and needs to be installed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4896
https://github.com/hail-is/hail/pull/4902:10,Performance,optimiz,optimization,10,"With good optimization too:. ```; hl.eval(hl.utils.range_table(10).annotate(x=[1,2,3]).explode('x').annotate(y=10).index_globals()); ```. ```; 2018-12-05 18:33:33 root: INFO: optimize: before:; (TableGetGlobals; (TableMapRows; (TableExplode x; (TableMapRows; (TableRange 10 12); (InsertFields; (Ref row); (x; (Literal Array[Int32] ""[1,2,3]""))))); (InsertFields; (Ref row); (y; (I32 10))))); 2018-12-05 18:33:33 root: INFO: optimize: after:; (MakeStruct); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4902
https://github.com/hail-is/hail/pull/4902:175,Performance,optimiz,optimize,175,"With good optimization too:. ```; hl.eval(hl.utils.range_table(10).annotate(x=[1,2,3]).explode('x').annotate(y=10).index_globals()); ```. ```; 2018-12-05 18:33:33 root: INFO: optimize: before:; (TableGetGlobals; (TableMapRows; (TableExplode x; (TableMapRows; (TableRange 10 12); (InsertFields; (Ref row); (x; (Literal Array[Int32] ""[1,2,3]""))))); (InsertFields; (Ref row); (y; (I32 10))))); 2018-12-05 18:33:33 root: INFO: optimize: after:; (MakeStruct); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4902
https://github.com/hail-is/hail/pull/4902:423,Performance,optimiz,optimize,423,"With good optimization too:. ```; hl.eval(hl.utils.range_table(10).annotate(x=[1,2,3]).explode('x').annotate(y=10).index_globals()); ```. ```; 2018-12-05 18:33:33 root: INFO: optimize: before:; (TableGetGlobals; (TableMapRows; (TableExplode x; (TableMapRows; (TableRange 10 12); (InsertFields; (Ref row); (x; (Literal Array[Int32] ""[1,2,3]""))))); (InsertFields; (Ref row); (y; (I32 10))))); 2018-12-05 18:33:33 root: INFO: optimize: after:; (MakeStruct); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4902
https://github.com/hail-is/hail/issues/4907:27,Performance,perform,performance,27,This is causing unbearable performance slowdowns when big literals are added. Randomly assigned @chrisvittal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4907
https://github.com/hail-is/hail/issues/4908:20,Usability,feedback,feedback,20,"Mostly generated by feedback from Tobyn (thanks, Tobyn!). - [x] Rename Hailpedia back to something like ""Overview"" - ""pedia"" implies verbose, detailed, and independent records, not a high level survey of the library; - [ ] Link to the overview from the getting started page. Preferably in a very visible way; - [ ] (MatrixTable overview) explain how a structured matrix is different from a numeric matrix explicitly",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4908
https://github.com/hail-is/hail/pull/4924:169,Availability,failure,failure,169,"- conda environments are always up to date and enabled (important for developers); - flake8 and pylint are test (and, ergo, CI) dependencies; - use pytest with `--first-failure` so that re-runs run the failing tests first (important for developers)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924
https://github.com/hail-is/hail/pull/4924:128,Integrability,depend,dependencies,128,"- conda environments are always up to date and enabled (important for developers); - flake8 and pylint are test (and, ergo, CI) dependencies; - use pytest with `--first-failure` so that re-runs run the failing tests first (important for developers)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924
https://github.com/hail-is/hail/pull/4924:107,Testability,test,test,107,"- conda environments are always up to date and enabled (important for developers); - flake8 and pylint are test (and, ergo, CI) dependencies; - use pytest with `--first-failure` so that re-runs run the failing tests first (important for developers)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924
https://github.com/hail-is/hail/pull/4924:210,Testability,test,tests,210,"- conda environments are always up to date and enabled (important for developers); - flake8 and pylint are test (and, ergo, CI) dependencies; - use pytest with `--first-failure` so that re-runs run the failing tests first (important for developers)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4924
https://github.com/hail-is/hail/issues/4925:10,Availability,error,errors,10,"It causes errors on GRCh38. I propose that the default argument is None, and we if None we use a default dict for GRCh37 or GRCh38 and an empty dictionary for other references.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4925
https://github.com/hail-is/hail/pull/4929:19,Integrability,depend,dependency,19,"- remove shadowJar dependency of makeDocsNoTest; - cleanup markdown conversion, make general: any pair of .md and .xslt file defines a new top-level page; - harmonize naming to enable the above, landing.md becomes index.md; - ctrl-c actually kills the site docker container now; - `(cd site && make test)` tests the site image with the contents of `hail/build/www` (we need a top-level makefile to manage that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:410,Integrability,depend,dependency,410,"- remove shadowJar dependency of makeDocsNoTest; - cleanup markdown conversion, make general: any pair of .md and .xslt file defines a new top-level page; - harmonize naming to enable the above, landing.md becomes index.md; - ctrl-c actually kills the site docker container now; - `(cd site && make test)` tests the site image with the contents of `hail/build/www` (we need a top-level makefile to manage that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:732,Modifiability,rewrite,rewrite,732,"- remove shadowJar dependency of makeDocsNoTest; - cleanup markdown conversion, make general: any pair of .md and .xslt file defines a new top-level page; - harmonize naming to enable the above, landing.md becomes index.md; - ctrl-c actually kills the site docker container now; - `(cd site && make test)` tests the site image with the contents of `hail/build/www` (we need a top-level makefile to manage that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:1513,Performance,perform,performed,1513,"e that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/overview.html` redirect to `docs/0.1/overview.html`. Yes, the regex for rules two matches `/hail`, `/hail/`, `/hail/overview.html`, etc. and redirects, replacing the hail with `docs/0.1`. One last note: I used 301 Permanent Redirect for the `/hail` since that's a dead url. I used 307 Temporary Redirect for `/docs` since that will change when versions change. Here's a test interaction. I interleaved my c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:299,Testability,test,test,299,"- remove shadowJar dependency of makeDocsNoTest; - cleanup markdown conversion, make general: any pair of .md and .xslt file defines a new top-level page; - harmonize naming to enable the above, landing.md becomes index.md; - ctrl-c actually kills the site docker container now; - `(cd site && make test)` tests the site image with the contents of `hail/build/www` (we need a top-level makefile to manage that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:306,Testability,test,tests,306,"- remove shadowJar dependency of makeDocsNoTest; - cleanup markdown conversion, make general: any pair of .md and .xslt file defines a new top-level page; - harmonize naming to enable the above, landing.md becomes index.md; - ctrl-c actually kills the site docker container now; - `(cd site && make test)` tests the site image with the contents of `hail/build/www` (we need a top-level makefile to manage that dependency properly); - added a 404 page; ![screen shot 2018-12-07 at 3 33 13 pm](https://user-images.githubusercontent.com/106194/49671603-72713580-fa36-11e8-91cf-b24936257628.png); - fixed redirect rules for /docs and /hail see note below. Resolves #4919 . ---; ### On NGINX Redirects; The internet seems to think that `rewrite` for redirects is ""bad"", ergo, I ignore the deleted rule and explain the additions. ```; location = /docs/ {; return 307 $scheme://$http_host/docs/0.2;; }; location ~ ^/hail(|/.*)$ {; return 301 $scheme://$http_host/docs/0.1$1;; }; ```. The [location](http://nginx.org/en/docs/http/ngx_http_core_module.html#location) directive can match `=` exactly, `~` by regex, `~*` by case insensitive regex, and `^~` which I do not understand. Question one: does this redirect `hail.is/docs` to `/docs/0.2`? Yes, the last paragraph of the location docs:. > If a location is defined by a prefix string that ends with the slash character, and requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/over",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:2367,Testability,test,test,2367,"requests are processed by one of proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/overview.html` redirect to `docs/0.1/overview.html`. Yes, the regex for rules two matches `/hail`, `/hail/`, `/hail/overview.html`, etc. and redirects, replacing the hail with `docs/0.1`. One last note: I used 301 Permanent Redirect for the `/hail` since that's a dead url. I used 307 Temporary Redirect for `/docs` since that will change when versions change. Here's a test interaction. I interleaved my curl commands with the logs. ```; $ curl localhost/hail; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /hail HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /docs/0.1 HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /docs/0.1/ HTTP/1.1"" 200 11994 ""-"" ""curl/7.61.0""; $ curl localhost/hail/; 172.17.0.1 - - [07/Dec/2018:19:36:06 +0000] ""GET /hail/ HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:06 +0000] ""GET /docs/0.1/ HTTP/1.1"" 200 11994 ""-"" ""curl/7.61.0""; $ curl localhost/hail/overview.html; 172.17.0.1 - - [07/Dec/2018:19:36:27 +0000] ""GET /hail/overview.html HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:27 +0000] ""GET /docs/0.1/overview.html HTTP/1.1"" 200 37288 ""-"" ""curl/7.61.0""; $ curl localhost/docs; 172.17.0.1 - - [07/Dec/2018:19:36:37 +0000] ""GET /docs HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:37 +0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4929:2425,Testability,log,logs,2425,"y_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, or grpc_pass, then the special processing is performed. In response to a request with URI equal to this string, but without the trailing slash, a permanent redirect with the code 301 will be returned to the requested URI with the slash appended. The docs appear incomplete, though, because this is a `return` rule, but it gets the 301. Question two: does this redirect `hail.is/docs/foo` to `/docs/0.2/foo`. No, the docs redirect is an `=` or exact match so `hail.is/docs/foo` is a 404. Question three: does this redirect `/hail/overview.html` redirect to `docs/0.1/overview.html`. Yes, the regex for rules two matches `/hail`, `/hail/`, `/hail/overview.html`, etc. and redirects, replacing the hail with `docs/0.1`. One last note: I used 301 Permanent Redirect for the `/hail` since that's a dead url. I used 307 Temporary Redirect for `/docs` since that will change when versions change. Here's a test interaction. I interleaved my curl commands with the logs. ```; $ curl localhost/hail; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /hail HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /docs/0.1 HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:35:30 +0000] ""GET /docs/0.1/ HTTP/1.1"" 200 11994 ""-"" ""curl/7.61.0""; $ curl localhost/hail/; 172.17.0.1 - - [07/Dec/2018:19:36:06 +0000] ""GET /hail/ HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:06 +0000] ""GET /docs/0.1/ HTTP/1.1"" 200 11994 ""-"" ""curl/7.61.0""; $ curl localhost/hail/overview.html; 172.17.0.1 - - [07/Dec/2018:19:36:27 +0000] ""GET /hail/overview.html HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:27 +0000] ""GET /docs/0.1/overview.html HTTP/1.1"" 200 37288 ""-"" ""curl/7.61.0""; $ curl localhost/docs; 172.17.0.1 - - [07/Dec/2018:19:36:37 +0000] ""GET /docs HTTP/1.1"" 301 185 ""-"" ""curl/7.61.0""; 172.17.0.1 - - [07/Dec/2018:19:36:37 +0000] ""GET /docs/ HTTP/1.1"" 307 187 """,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4929
https://github.com/hail-is/hail/pull/4930:89,Integrability,depend,dependencies,89,"cc: @cseed . The scorecard die seems to turn up @jigold a lot for me ;). This implements dependencies between Batch jobs. Includes the other #4924 change, but I'll rebase when that's in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4930
https://github.com/hail-is/hail/pull/4931:294,Availability,avail,available,294,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:2341,Availability,down,downsides,2341,"ct/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6287,Availability,robust,robust,6287,"nterfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7101,Availability,avail,available,7101,"er; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be perform",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8008,Availability,down,download,8008,"up dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9045,Availability,avail,available,9045,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9295,Availability,avail,available,9295,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:1298,Deployability,update,update,1298,"chronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and mono",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7792,Deployability,Integrat,Integrate,7792,"party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8348,Deployability,pipeline,pipelines,8348,"(possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3604,Integrability,rout,route,3604,"ng server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've deve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:5039,Integrability,wrap,wrapper,5039,"getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:5320,Integrability,interface,interfaces,5320,"good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:5478,Integrability,wrap,wrapped,5478,"d has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6441,Integrability,interface,interface,6441,"bviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7593,Integrability,interface,interface,7593,"ther proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7792,Integrability,Integrat,Integrate,7792,"party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8096,Integrability,interface,interface,8096,"ll available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8274,Integrability,interface,interface,8274,"tep): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate mino",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9418,Integrability,depend,depends,9418,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:2001,Modifiability,variab,variable,2001,"(verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:4916,Modifiability,layers,layers,4916,"t.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9778,Modifiability,extend,extended,9778,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:68,Performance,perform,performance,68,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:163,Performance,load,loading,163,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:551,Performance,latency,latency,551,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:1533,Performance,perform,performs,1533,"MContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:2099,Performance,load,load,2099,"but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:2322,Performance,load,load,2322,"ct/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:2391,Performance,perform,performance,2391,"ct/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3343,Performance,Perform,Performance,3343,"es: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3567,Performance,load,loaded,3567,"ng server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've deve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3985,Performance,perform,performant,3985,"rks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8146,Performance,perform,performed,8146,"tes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-tes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:1873,Safety,avoid,avoid,1873," as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be sl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6684,Safety,avoid,avoid,6684,"oducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:1695,Security,XSS,XSS,1695,"mic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor develo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:1699,Security,attack,attack,1699,"mic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor develo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6010,Security,Authenticat,Authentication,6010,"r around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6366,Security,secur,security,6366,"nterfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6531,Security,access,access,6531,"bviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6837,Security,access,access,6837,"d by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api serve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:6941,Security,authenticat,authenticated,6941,"e} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7207,Security,Authenticat,Authentication,7207,"er; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be perform",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7650,Security,Authenticat,Authenticate,7650,"e Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8861,Security,validat,validate,8861,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9672,Security,authenticat,authentication,9672,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3006,Testability,benchmark,benchmarks,3006,"?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web Assembly, by using server-rendered pages, is ~impossible. We will achieve this interactivity without suffering the bundle-size-before-first-render cost, at minor developer costs vs server-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3669,Testability,benchmark,benchmarks,3669,"er-side-only rendering. Lastly, it is possible to abuse any technology. Javascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3744,Testability,benchmark,benchmark-on-a-real-life-application-,3744,"vascript brings to mind ""bloated""; this is an implementation issue. PHP/Python/Perl websites also used to be slow and ugly (Geocities).; * NodeJS/Javascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:3909,Testability,benchmark,benchmark,3909,"vascript/V8 JIT is consistently faster than PHP, Python, and ~Java: https://www.techempower.com/benchmarks/. ## Why NodeJS, React, etc; 1. Javascript is the only language supported by modern browsers. Web assembly will change this (compile target == web assembly, language == rust | go | python), but is not nearly as mature; 2. Ecosystem. Chosen technologies are (likely) by far the most popular. We should quantify this better; 3. Performance. NodeJS is faster than Flask, React is ~fastest JS view layer. Next makes it really easy to split app into page bundles, and (on localhost) achieves DOMContentLoaded of ~70-100ms, and faster interactivity: first loaded page (the page of the current route) is ~6-10ms.; * [Techempower]: https://www.techempower.com/benchmarks/; * [Node vs , ](https://medium.com/@mihaigeorge.c/web-rest-api-benchmark-on-a-real-life-application-ebb743a5d7a3). * React vs other client side micro bench (pay attention to ""Non-keyed""): https://krausest.github.io/js-framework-benchmark/current.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7626,Testability,log,logic,7626,"rough our own resource server... Firebase Auth seems to avoid these limitations. ## TODO:; 1. Create a structured description of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7753,Testability,test,tests,7753,"of this pull request; 2. Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7764,Testability,Mock,Mock,7764,". Incorporate Firebase Auth in place of Auth0 for 3rd party access token benefits.; 3: Scorecard; 3a. Draft working GraphQL V4 scorecard implementation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Chec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:7944,Testability,test,tests,7944,"mentation; 3b. Finish authenticated GraphQL V4 scorecard implementation; 4. Batch; 4a: Setup dev batch endpoint; 4b: Call batch endpoint (no auth), and return any data; 4c: List all available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:9149,Testability,test,test,9149,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:365,Usability,responsiv,responsiveness,365,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:5607,Usability,learn,learn,5607,"s no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:5648,Usability,Guid,Guide,5648,"l, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <div className='some-class'> Hello {name} </div>; ); }; ```. ## Challenges; 1. Auth ; Authentication is tricky, but not for any reason specific to React, Next, Node. Server-side rendered apps tie the web app to the resource server; as such it's easier to hide sensitive information. . Mobile and desktop apps have dealt with this for 2 decades. We should build a robust infrastructure, and not one that requires server-rendered web pages for security. Currently it seems Auth0 may not be the best choice: it does not interface for us with third-party API's; requires us to either insecurely store 3rd party access tokens (with at least 1 extra round trip), or altogether proxy all third-party requests through our own resource server..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4931:8088,Usability,simpl,simpler,8088,"ll available jobs; * By querying Batch api, or Kubernetes directly; 4d: Receive current status of 1 job; 4e: Authentication; 4f: Polish (longest step): make interacting with batch achievable within perceived 16ms.; * goal: subscribe to events in web socket; * may want to save user job state in a Hail-controlled database (possible to use Firebase or Mongo, may prefer relational db, maybe Postgres or MySQL).; 4other: Figure out state question (sufficient to use Kubernetes); 5. Basic notebook interface.; 6. Connect websocket logic (non-GraphQL); 7. Authenticate web socket via Oauth2; 8. Incorporate GraphQL subscriptions (first: GitHub API); 9. Write tests; 10. Mock GraphQL endpoints; 11. Integrate web and api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931
https://github.com/hail-is/hail/pull/4934:22,Testability,log,logic,22,"These follow the same logic as the `/hail` to `/docs/0.1` to redirect,; except they are 307 TEMPORARY REDIRECT because devel and stable may; change to different versions in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4934
https://github.com/hail-is/hail/pull/4936:263,Integrability,protocol,protocol-agnostic,263,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4936
https://github.com/hail-is/hail/pull/4936:182,Performance,load,load,182,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4936
https://github.com/hail-is/hail/pull/4936:370,Performance,load,load,370,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4936
https://github.com/hail-is/hail/pull/4936:293,Testability,test,testing,293,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4936
https://github.com/hail-is/hail/pull/4937:71,Deployability,pipeline,pipelines,71,"@cseed I got a version working!!!! I'd like to test on other potential pipelines and make a real example that will run for demo purposes. ```; In [1]: from pyapi import Pipeline; ...: p = Pipeline(); ...:; ...: subset = (p.new_task(); ...: .label('subset'); ...: .command('plink --bfile {{bfile}} --make-bed {{tmp1}}'); ...: .command(""awk '{ print $1, $2}' {{tmp1}}.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > {{tmp2}}""); ...: .command(""plink --bfile {{bfile}} --remove {{tmp2}} --make-bed {{ofile}}"")); ...:; ...: shapeit_tasks = []; ...: for contig in [str(x) for x in range(1, 4)]:; ...: shapeit = (p.new_task(); ...: .label('shapeit'); ...: .command('shapeit --bed-file {{bfile}} --chr ' + contig + ' --out {{ofile}}'); ...: .inputs(bfile=subset.ofile)); ...: shapeit_tasks.append(shapeit); ...:; ...: merger = (p.new_task(); ...: .label('merge'); ...: .command('cat {{files}} >> {{ofile}}'); ...: .inputs(files=[task.ofile for task in shapeit_tasks])); ...:; ...:; ...: p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.txt""); ...: p.run(); ...:; #! /usr/bash; set -ex. # __TASK__0 subset; __RESOURCE__0=/tmp/9CiA1t; __RESOURCE__1=/tmp/y7HdVA; __RESOURCE__2=/tmp/l7skDb; __RESOURCE__3=/tmp/McFulO; plink --bfile $__RESOURCE__1 --make-bed $__RESOURCE__0; awk '{ print $1, $2}' $__RESOURCE__0.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > $__RESOURCE__2; plink --bfile $__RESOURCE__1 --remove $__RESOURCE__2 --make-bed $__RESOURCE__3. # __TASK__1 shapeit; __RESOURCE__4=/tmp/PQiR68; __RESOURCE__5=/tmp/McFulO; shapeit --bed-file $__RESOURCE__5 --chr 1 --out $__RESOURCE__4. # __TASK__2 shapeit; __RESOURCE__6=/tmp/sjoOQX; __RESOURCE__7=/tmp/McFulO; shapeit --bed-file $__RESOURCE__7 --chr 2 --out $__RESOURCE__6. # __TASK__3 shapeit; __RESOURCE__8=/tmp/gNw0he; __RESOURCE__9=/tmp/McFulO; shapeit --bed-file $__RESOURCE__9 --chr 3 --out $__RESOURCE__8. # __TASK__4 merge; __RESOURCE__10=/tmp/RY0Raq; __RESOURCE__11=(/tmp/PQiR68 /tmp/sjoOQX /tmp/gNw0he)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937
https://github.com/hail-is/hail/pull/4937:169,Deployability,Pipeline,Pipeline,169,"@cseed I got a version working!!!! I'd like to test on other potential pipelines and make a real example that will run for demo purposes. ```; In [1]: from pyapi import Pipeline; ...: p = Pipeline(); ...:; ...: subset = (p.new_task(); ...: .label('subset'); ...: .command('plink --bfile {{bfile}} --make-bed {{tmp1}}'); ...: .command(""awk '{ print $1, $2}' {{tmp1}}.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > {{tmp2}}""); ...: .command(""plink --bfile {{bfile}} --remove {{tmp2}} --make-bed {{ofile}}"")); ...:; ...: shapeit_tasks = []; ...: for contig in [str(x) for x in range(1, 4)]:; ...: shapeit = (p.new_task(); ...: .label('shapeit'); ...: .command('shapeit --bed-file {{bfile}} --chr ' + contig + ' --out {{ofile}}'); ...: .inputs(bfile=subset.ofile)); ...: shapeit_tasks.append(shapeit); ...:; ...: merger = (p.new_task(); ...: .label('merge'); ...: .command('cat {{files}} >> {{ofile}}'); ...: .inputs(files=[task.ofile for task in shapeit_tasks])); ...:; ...:; ...: p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.txt""); ...: p.run(); ...:; #! /usr/bash; set -ex. # __TASK__0 subset; __RESOURCE__0=/tmp/9CiA1t; __RESOURCE__1=/tmp/y7HdVA; __RESOURCE__2=/tmp/l7skDb; __RESOURCE__3=/tmp/McFulO; plink --bfile $__RESOURCE__1 --make-bed $__RESOURCE__0; awk '{ print $1, $2}' $__RESOURCE__0.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > $__RESOURCE__2; plink --bfile $__RESOURCE__1 --remove $__RESOURCE__2 --make-bed $__RESOURCE__3. # __TASK__1 shapeit; __RESOURCE__4=/tmp/PQiR68; __RESOURCE__5=/tmp/McFulO; shapeit --bed-file $__RESOURCE__5 --chr 1 --out $__RESOURCE__4. # __TASK__2 shapeit; __RESOURCE__6=/tmp/sjoOQX; __RESOURCE__7=/tmp/McFulO; shapeit --bed-file $__RESOURCE__7 --chr 2 --out $__RESOURCE__6. # __TASK__3 shapeit; __RESOURCE__8=/tmp/gNw0he; __RESOURCE__9=/tmp/McFulO; shapeit --bed-file $__RESOURCE__9 --chr 3 --out $__RESOURCE__8. # __TASK__4 merge; __RESOURCE__10=/tmp/RY0Raq; __RESOURCE__11=(/tmp/PQiR68 /tmp/sjoOQX /tmp/gNw0he)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937
https://github.com/hail-is/hail/pull/4937:188,Deployability,Pipeline,Pipeline,188,"@cseed I got a version working!!!! I'd like to test on other potential pipelines and make a real example that will run for demo purposes. ```; In [1]: from pyapi import Pipeline; ...: p = Pipeline(); ...:; ...: subset = (p.new_task(); ...: .label('subset'); ...: .command('plink --bfile {{bfile}} --make-bed {{tmp1}}'); ...: .command(""awk '{ print $1, $2}' {{tmp1}}.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > {{tmp2}}""); ...: .command(""plink --bfile {{bfile}} --remove {{tmp2}} --make-bed {{ofile}}"")); ...:; ...: shapeit_tasks = []; ...: for contig in [str(x) for x in range(1, 4)]:; ...: shapeit = (p.new_task(); ...: .label('shapeit'); ...: .command('shapeit --bed-file {{bfile}} --chr ' + contig + ' --out {{ofile}}'); ...: .inputs(bfile=subset.ofile)); ...: shapeit_tasks.append(shapeit); ...:; ...: merger = (p.new_task(); ...: .label('merge'); ...: .command('cat {{files}} >> {{ofile}}'); ...: .inputs(files=[task.ofile for task in shapeit_tasks])); ...:; ...:; ...: p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.txt""); ...: p.run(); ...:; #! /usr/bash; set -ex. # __TASK__0 subset; __RESOURCE__0=/tmp/9CiA1t; __RESOURCE__1=/tmp/y7HdVA; __RESOURCE__2=/tmp/l7skDb; __RESOURCE__3=/tmp/McFulO; plink --bfile $__RESOURCE__1 --make-bed $__RESOURCE__0; awk '{ print $1, $2}' $__RESOURCE__0.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > $__RESOURCE__2; plink --bfile $__RESOURCE__1 --remove $__RESOURCE__2 --make-bed $__RESOURCE__3. # __TASK__1 shapeit; __RESOURCE__4=/tmp/PQiR68; __RESOURCE__5=/tmp/McFulO; shapeit --bed-file $__RESOURCE__5 --chr 1 --out $__RESOURCE__4. # __TASK__2 shapeit; __RESOURCE__6=/tmp/sjoOQX; __RESOURCE__7=/tmp/McFulO; shapeit --bed-file $__RESOURCE__7 --chr 2 --out $__RESOURCE__6. # __TASK__3 shapeit; __RESOURCE__8=/tmp/gNw0he; __RESOURCE__9=/tmp/McFulO; shapeit --bed-file $__RESOURCE__9 --chr 3 --out $__RESOURCE__8. # __TASK__4 merge; __RESOURCE__10=/tmp/RY0Raq; __RESOURCE__11=(/tmp/PQiR68 /tmp/sjoOQX /tmp/gNw0he)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937
https://github.com/hail-is/hail/pull/4937:47,Testability,test,test,47,"@cseed I got a version working!!!! I'd like to test on other potential pipelines and make a real example that will run for demo purposes. ```; In [1]: from pyapi import Pipeline; ...: p = Pipeline(); ...:; ...: subset = (p.new_task(); ...: .label('subset'); ...: .command('plink --bfile {{bfile}} --make-bed {{tmp1}}'); ...: .command(""awk '{ print $1, $2}' {{tmp1}}.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > {{tmp2}}""); ...: .command(""plink --bfile {{bfile}} --remove {{tmp2}} --make-bed {{ofile}}"")); ...:; ...: shapeit_tasks = []; ...: for contig in [str(x) for x in range(1, 4)]:; ...: shapeit = (p.new_task(); ...: .label('shapeit'); ...: .command('shapeit --bed-file {{bfile}} --chr ' + contig + ' --out {{ofile}}'); ...: .inputs(bfile=subset.ofile)); ...: shapeit_tasks.append(shapeit); ...:; ...: merger = (p.new_task(); ...: .label('merge'); ...: .command('cat {{files}} >> {{ofile}}'); ...: .inputs(files=[task.ofile for task in shapeit_tasks])); ...:; ...:; ...: p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.txt""); ...: p.run(); ...:; #! /usr/bash; set -ex. # __TASK__0 subset; __RESOURCE__0=/tmp/9CiA1t; __RESOURCE__1=/tmp/y7HdVA; __RESOURCE__2=/tmp/l7skDb; __RESOURCE__3=/tmp/McFulO; plink --bfile $__RESOURCE__1 --make-bed $__RESOURCE__0; awk '{ print $1, $2}' $__RESOURCE__0.fam | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > $__RESOURCE__2; plink --bfile $__RESOURCE__1 --remove $__RESOURCE__2 --make-bed $__RESOURCE__3. # __TASK__1 shapeit; __RESOURCE__4=/tmp/PQiR68; __RESOURCE__5=/tmp/McFulO; shapeit --bed-file $__RESOURCE__5 --chr 1 --out $__RESOURCE__4. # __TASK__2 shapeit; __RESOURCE__6=/tmp/sjoOQX; __RESOURCE__7=/tmp/McFulO; shapeit --bed-file $__RESOURCE__7 --chr 2 --out $__RESOURCE__6. # __TASK__3 shapeit; __RESOURCE__8=/tmp/gNw0he; __RESOURCE__9=/tmp/McFulO; shapeit --bed-file $__RESOURCE__9 --chr 3 --out $__RESOURCE__8. # __TASK__4 merge; __RESOURCE__10=/tmp/RY0Raq; __RESOURCE__11=(/tmp/PQiR68 /tmp/sjoOQX /tmp/gNw0he)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937
https://github.com/hail-is/hail/pull/4938:66,Availability,resilien,resiliently,66,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:85,Availability,failure,failure,85,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:901,Availability,resilien,resilient,901,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:924,Availability,failure,failures,924,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1041,Safety,safe,safely,1041,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1098,Testability,Test,Testing,1098,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1116,Testability,test,test,1116,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1283,Testability,test,test,1283,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1388,Testability,test,testing,1388,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:1482,Testability,test,tests,1482,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/pull/4938:524,Usability,simpl,simply,524,"### Problem Description. `ExportPlink` was previously modified to resiliently handle failure of a; Spark task by including a per-task UUID. `copyMerge` was not modified to; correctly handle the directories generated by this modified; `ExportPlink`. For example, if exactly one task out of N fails during `ExportPlink`, the; temporary output directory will contain N+1 partition files. One of; these partition files is corrupted and invalid. The other N are the; output of successful task completion. The invalid file should simply be; ignored by `copyMerge`. ### Changes Made. This PR modifies `copyMerge` to take an optional list of files to; merge. If that argument is set to `None`, the original behavior; persists. The original behavior is used by `RichRDD.writeTable` and; `RichRDDByteArray.saveFromByteArrays`. These two methods use the default; Spark parallel export system. This system is not resilient to all task; failures, but *does* ensure failed tasks do not generate garbage; partitions in the output directory. Ergo, they can safely use the; original behavior of `copyMerge`. ### On Testing. I do not test this behavior because failing a task during write is a; little bit tricky. I have verified that all users of `copyMerge` now use; `copyMerge` correctly. Adding a test to `ExportPlink` would not save us; from incorrectly using `copyMerge` in the future. A longer term testing strategy that includes a Chaos Monkey that kills; entire containers during Hail Scala tests execution would protect; against this type of bug. ---. Fixes #4932",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4938
https://github.com/hail-is/hail/issues/4941:3126,Availability,Error,Error,3126,"xpr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1007); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1022); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.5-e017bef1c26c; Error summary: AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:229,Testability,test,test,229,"```python; mt = hl.utils.range_matrix_table(1, 1).annotate_cols(y=0, cov1=1).annotate_entries(x=1). # x_expr uses a col field; x_expr = hl.case().when(mt.cov1 == 1, 2).default(mt.x). hl.logistic_regression_rows(y=mt.y, x=x_expr, test='wald', covariates=[1]); ```. ```; Java stack trace:; java.lang.AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:74); 	at is.hail.expr.ir.InferIR$class.typ(IR.scala:62); 	at is.hail.expr.ir.GetField.typ(IR.scala:215); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:561); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:536); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:298,Testability,Assert,AssertionError,298,"```python; mt = hl.utils.range_matrix_table(1, 1).annotate_cols(y=0, cov1=1).annotate_entries(x=1). # x_expr uses a col field; x_expr = hl.case().when(mt.cov1 == 1, 2).default(mt.x). hl.logistic_regression_rows(y=mt.y, x=x_expr, test='wald', covariates=[1]); ```. ```; Java stack trace:; java.lang.AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:74); 	at is.hail.expr.ir.InferIR$class.typ(IR.scala:62); 	at is.hail.expr.ir.GetField.typ(IR.scala:215); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:561); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:536); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:314,Testability,assert,assertion,314,"```python; mt = hl.utils.range_matrix_table(1, 1).annotate_cols(y=0, cov1=1).annotate_entries(x=1). # x_expr uses a col field; x_expr = hl.case().when(mt.cov1 == 1, 2).default(mt.x). hl.logistic_regression_rows(y=mt.y, x=x_expr, test='wald', covariates=[1]); ```. ```; Java stack trace:; java.lang.AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:74); 	at is.hail.expr.ir.InferIR$class.typ(IR.scala:62); 	at is.hail.expr.ir.GetField.typ(IR.scala:215); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:561); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:536); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:401,Testability,assert,assert,401,"```python; mt = hl.utils.range_matrix_table(1, 1).annotate_cols(y=0, cov1=1).annotate_entries(x=1). # x_expr uses a col field; x_expr = hl.case().when(mt.cov1 == 1, 2).default(mt.x). hl.logistic_regression_rows(y=mt.y, x=x_expr, test='wald', covariates=[1]); ```. ```; Java stack trace:; java.lang.AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:74); 	at is.hail.expr.ir.InferIR$class.typ(IR.scala:62); 	at is.hail.expr.ir.GetField.typ(IR.scala:215); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:561); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:536); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:507); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:3141,Testability,Assert,AssertionError,3141,"xpr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1007); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1022); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.5-e017bef1c26c; Error summary: AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/issues/4941:3157,Testability,assert,assertion,3157,"xpr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:494); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:281); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:489); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:670); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:511); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:920); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:923); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:885); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$$anonfun$parse_matrix_ir$2.apply(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1007); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1023); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1022); 	at is.hail.expr.ir.IRParser.parse_matrix_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.5-e017bef1c26c; Error summary: AssertionError: assertion failed: cov1 not in struct{__y: float64, __cov0: float64}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941
https://github.com/hail-is/hail/pull/4945:188,Integrability,inject,inject,188,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4945
https://github.com/hail-is/hail/pull/4945:269,Safety,safe,safe,269,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4945
https://github.com/hail-is/hail/pull/4945:188,Security,inject,inject,188,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4945
https://github.com/hail-is/hail/pull/4945:331,Security,expose,expose,331,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4945
https://github.com/hail-is/hail/pull/4945:0,Usability,Simpl,Simply,0,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4945
https://github.com/hail-is/hail/pull/4946:35,Deployability,install,install,35,"> If all software was this easy to install, no one would ever use software. ~ everyone, ever",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4946
https://github.com/hail-is/hail/pull/4947:188,Integrability,inject,inject,188,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947
https://github.com/hail-is/hail/pull/4947:269,Safety,safe,safe,269,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947
https://github.com/hail-is/hail/pull/4947:188,Security,inject,inject,188,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947
https://github.com/hail-is/hail/pull/4947:331,Security,expose,expose,331,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947
https://github.com/hail-is/hail/pull/4947:0,Usability,Simpl,Simply,0,"Simply enables CORS from all domains. This would be insecure, but our endpoints are read-only operations on public GitHub resources, against a fixed list of users, there is no database to inject, and there are no cookies or local storage entries to steal. I think it's safe enough for the time being, but open to suggestions. Also expose /json endpoint to return all index.html data without rendering a page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947
https://github.com/hail-is/hail/issues/4948:132,Availability,error,errors,132,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:95,Performance,load,loading,95,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:166,Performance,load,load,166,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:239,Performance,load,load,239,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:313,Performance,load,load,313,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:379,Performance,load,load,379,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/issues/4948:446,Performance,load,load,446,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948
https://github.com/hail-is/hail/pull/4949:96,Deployability,release,release,96,"--chown was added in 17.09.0 on September 26, 2017, but GKE has an old version: 17.03.2-ce. GKE release notes don't indicate any updates and they only moved to 17.03 in February of 2018. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4949
https://github.com/hail-is/hail/pull/4949:129,Deployability,update,updates,129,"--chown was added in 17.09.0 on September 26, 2017, but GKE has an old version: 17.03.2-ce. GKE release notes don't indicate any updates and they only moved to 17.03 in February of 2018. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4949
https://github.com/hail-is/hail/issues/4951:49,Availability,error,error,49,"Keys mismatch by type, not number of fields, but error suggests it's the wrong number.; ```; constraint_ht.key; Out[29]: <StructExpression of type struct{gene: str, expressed: str}>; caf_ht.key; Out[30]: <StructExpression of type struct{gene: str, expressed: bool}>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-31-6560bd06e877>"", line 1, in <module>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 349, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1299, in index; raise ExpressionException(f'Key mismatch: table has {len(self.key)} key fields, '; hail.expr.expressions.base_expression.ExpressionException: Key mismatch: table has 2 key fields, found 1 index expressions.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4951
https://github.com/hail-is/hail/pull/4953:392,Testability,test,testing,392,"I misunderstood what `//` and `/` meant. `//` is used for things like `//google.com/index.html` which resolves to `https://google.com/index.html` when served by `https` and `http://google.com/index.html` when served by `http`. `/` is the same but for local URLs. For example, `/index.html` resolves to `https://hail.is/index.html` on our website but `http://localhost:8080/index.html` if I'm testing locally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4953
https://github.com/hail-is/hail/pull/4955:94,Integrability,wrap,wrapper,94,"Basically what it says on the box. NativeStatus is now only used in the generated code in the wrapper function signature; internally, we just throw a HailFatalError object that's caught in the wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4955
https://github.com/hail-is/hail/pull/4955:193,Integrability,wrap,wrapper,193,"Basically what it says on the box. NativeStatus is now only used in the generated code in the wrapper function signature; internally, we just throw a HailFatalError object that's caught in the wrapper.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4955
https://github.com/hail-is/hail/pull/4956:28,Integrability,wrap,wrapper,28,"I didn't like how the scala wrapper was written, so I rewrote it? Should make it easier to use as an endpoint for code-generated stages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4956
https://github.com/hail-is/hail/issues/4960:19,Usability,simpl,simple,19,at least something simple.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4960
https://github.com/hail-is/hail/pull/4974:682,Availability,alive,alive,682,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:708,Availability,error,error,708,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:748,Availability,error,error,748,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:887,Availability,error,error,887,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:1483,Availability,error,errors,1483,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:1571,Deployability,deploy,deploy,1571,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:311,Integrability,depend,depending,311,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:615,Modifiability,config,config,615,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:513,Safety,timeout,timeout,513,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:1242,Security,access,access,1242,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:166,Testability,test,testing,166,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:521,Testability,log,logic,521,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:585,Testability,test,testing,585,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:1535,Testability,test,tested,1535,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4974:1310,Usability,user experience,user experience,1310,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974
https://github.com/hail-is/hail/pull/4976:7,Energy Efficiency,reduce,reduced,7,I also reduced the layers and size of the notebook a bit. It's still ~8GB. I added `time` to the make command for curiosity's sake.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4976
https://github.com/hail-is/hail/pull/4976:19,Modifiability,layers,layers,19,I also reduced the layers and size of the notebook a bit. It's still ~8GB. I added `time` to the make command for curiosity's sake.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4976
https://github.com/hail-is/hail/pull/4978:435,Availability,error,errors,435,"Spaces were screwing things up because the `location` directive matches the decoded string (e.g. ""%20"" is converted back to "" ""). We don't need to reconstruct the URL explicitly with the regex pieces and the `$args` (which refers to HTTP query parameters), `$request_uri` is all that, but still encoded. The notebook was receiving requests without encoded spaces which appear as a bunch of weird hex characters. Obviously the notebook errors when it sees such demonic lettering.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4978
https://github.com/hail-is/hail/pull/4979:98,Availability,error,error,98,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/pull/4979:4,Deployability,deploy,deploy,4,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/pull/4979:175,Deployability,deploy,deploy,175,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/pull/4979:274,Deployability,deploy,deploy,274,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/pull/4979:328,Deployability,deploy,deploy,328,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/pull/4979:371,Deployability,deploy,deploy,371,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4979
https://github.com/hail-is/hail/issues/4984:93,Deployability,deploy,deployment,93,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:161,Deployability,deploy,deployment,161,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:252,Deployability,deploy,deployment,252,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:317,Deployability,deploy,deployment,317,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:185,Testability,log,log,185,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:242,Testability,log,log,242,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:338,Testability,log,log,338,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/issues/4984:392,Testability,log,log,392,Batch started hanging which prevented CI from making progress. Unclear what happened. [batch-deployment.txt](https://github.com/hail-is/hail/files/2681748/batch-deployment.txt); [batch.log](https://github.com/hail-is/hail/files/2681749/batch.log); [ci-deployment.txt](https://github.com/hail-is/hail/files/2681750/ci-deployment.txt); [ci.log](https://github.com/hail-is/hail/files/2681751/ci.log),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984
https://github.com/hail-is/hail/pull/4994:7,Integrability,depend,dependence,7,Remove dependence on tag dk-test and zone us-central1-a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994
https://github.com/hail-is/hail/pull/4994:28,Testability,test,test,28,Remove dependence on tag dk-test and zone us-central1-a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994
https://github.com/hail-is/hail/pull/5004:102,Integrability,depend,dependence,102,"Builds on: https://github.com/hail-is/hail/pull/4995. First of several (many) changes to rip out Java dependence from the mainline Python code. Move _to_java_ir to the SparkBackend. Eventually, only the Spark backend should call Java. Matrix/Table now gets it types from BaseIR.typ (which will eventually implement the (virtual) typing rules in Python). Matrix/Table explicitly call into the Spark backend to create _jmt/_jt, but those can be removed once the methods are all using IR. @tpoterba giving this to you since you got the last one. Let me know if you would prefer me to spin the wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5004
https://github.com/hail-is/hail/pull/5009:116,Deployability,deploy,deploy,116,Should actually figure out how to unify all these variables in one file since they're at least used in both hail-ci-deploy.sh and get-deployed-sha.sh right now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5009
https://github.com/hail-is/hail/pull/5009:134,Deployability,deploy,deployed-sha,134,Should actually figure out how to unify all these variables in one file since they're at least used in both hail-ci-deploy.sh and get-deployed-sha.sh right now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5009
https://github.com/hail-is/hail/pull/5009:50,Modifiability,variab,variables,50,Should actually figure out how to unify all these variables in one file since they're at least used in both hail-ci-deploy.sh and get-deployed-sha.sh right now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5009
https://github.com/hail-is/hail/pull/5011:25,Testability,test,testing,25,noticed this while smoke testing notebooks,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5011
https://github.com/hail-is/hail/pull/5015:188,Modifiability,Rewrite,Rewrite,188,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015
https://github.com/hail-is/hail/pull/5015:350,Modifiability,rewrite,rewrite,350,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015
https://github.com/hail-is/hail/pull/5015:554,Performance,optimiz,optimization,554,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015
https://github.com/hail-is/hail/pull/5016:823,Deployability,deploy,deploy,823,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:200,Modifiability,layers,layers,200,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:380,Modifiability,layers,layers,380,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:674,Modifiability,layers,layers,674,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:53,Performance,cache,caches,53,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:108,Performance,cache,cache-from,108,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:168,Performance,cache,cache-from,168,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:212,Performance,cache,cache,212,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:251,Performance,cache,cache,251,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:374,Performance,cache,cache,374,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:466,Performance,cache,cache,466,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5016:892,Performance,cache,cache,892,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5016
https://github.com/hail-is/hail/pull/5026:312,Testability,test,tests,312,"@jbloom22 opening this PR so we have something to work from. I think the dimensions where the ability to scale with Hail will be beneficial are:; * heritability estimation of many phenotypes at once; * partitioned LDSC with potentially thousands of covariates (Ran is currently exploring this space); * multiple tests of enrichment across different annotations (fitting the ""baseline"" partitioned model with ~50 covariates + 1 annotation of interest thousands of times -- Kate has been working in this area). In this first effort, I'm trying to implement the univariate, ""vanilla"" version of LD score regression -- no partitioning/annotations, just simple linear regression of chi-squared statistics on LD scores -- though I have tried to build for the first bullet point above, heritability estimation of many phenotypes at once.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5026
https://github.com/hail-is/hail/pull/5026:649,Usability,simpl,simple,649,"@jbloom22 opening this PR so we have something to work from. I think the dimensions where the ability to scale with Hail will be beneficial are:; * heritability estimation of many phenotypes at once; * partitioned LDSC with potentially thousands of covariates (Ran is currently exploring this space); * multiple tests of enrichment across different annotations (fitting the ""baseline"" partitioned model with ~50 covariates + 1 annotation of interest thousands of times -- Kate has been working in this area). In this first effort, I'm trying to implement the univariate, ""vanilla"" version of LD score regression -- no partitioning/annotations, just simple linear regression of chi-squared statistics on LD scores -- though I have tried to build for the first bullet point above, heritability estimation of many phenotypes at once.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5026
https://github.com/hail-is/hail/pull/5035:111,Availability,failure,failure,111,Batches themselves can now have callbacks. You receive a callback for each member job's completion (success or failure). CI will use this to wait for completion of the entire DAG. Stacked on https://github.com/hail-is/hail/pull/4930,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5035
https://github.com/hail-is/hail/issues/5040:66,Performance,load,loaded,66,We should be able to check something in netlib to determine if it loaded the natives or the reference library.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5040
https://github.com/hail-is/hail/issues/5042:4,Deployability,deploy,deployed-sha,4,The deployed-sha logic seems broken.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5042
https://github.com/hail-is/hail/issues/5042:17,Testability,log,logic,17,The deployed-sha logic seems broken.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5042
https://github.com/hail-is/hail/pull/5043:18,Performance,optimiz,optimize,18,Use this to fully optimize `hl.read_matrix_table().count_cols()`. Stacked on #5041,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5043
https://github.com/hail-is/hail/pull/5046:150,Safety,safe,safe,150,"@danking I rebased in the process of addressing your requested changes, so git won't allow me to re-open the closed branch #4842. I think this is now safe with regard to closing resources. Do you see a way to not include the branch on binary twice? I'd prefer to consider changing the types of streams/buffers as another PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5046
https://github.com/hail-is/hail/pull/5050:10,Performance,perform,performance,10,"This is a performance improvement (and also returns explosion after the right number of iterations which is more logical). It is only necessary to check the first element because I'm confident NaN appears nowhere or everywhere in deltaB (and certainly if it appears somewhere in one iteration, it spreads to everywhere in the next).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5050
https://github.com/hail-is/hail/pull/5050:113,Testability,log,logical,113,"This is a performance improvement (and also returns explosion after the right number of iterations which is more logical). It is only necessary to check the first element because I'm confident NaN appears nowhere or everywhere in deltaB (and certainly if it appears somewhere in one iteration, it spreads to everywhere in the next).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5050
https://github.com/hail-is/hail/pull/5052:50,Deployability,deploy,deployed,50,@jbloom22 I suspect this is why the website isn't deployed. We were missing this file so the ccg-workshop image couldn't be built.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5052
https://github.com/hail-is/hail/pull/5054:84,Availability,Failure,Failure,84,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5054
https://github.com/hail-is/hail/pull/5054:296,Availability,alive,alive,296,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5054
https://github.com/hail-is/hail/pull/5054:422,Deployability,deploy,deploy,422,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5054
https://github.com/hail-is/hail/pull/5054:243,Testability,test,test,243,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5054
https://github.com/hail-is/hail/pull/5054:260,Testability,test,test,260,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5054
https://github.com/hail-is/hail/pull/5055:9,Integrability,depend,depend,9,"Does not depend on PTypes 53, despite the name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5055
https://github.com/hail-is/hail/pull/5059:425,Deployability,update,updated,425,"Sphinx 1.8 broke compatibility (https://github.com/sphinx-doc/sphinx/issues/5460) with a number of themes including `sphinx_rtd_theme`. Sphinx 1.8.3 restores compatibility (https://github.com/sphinx-doc/sphinx/pull/5590) with said themes. Moreover, sphinx_rtd_theme 0.4.2 (https://github.com/rtfd/sphinx_rtd_theme/pull/672) fixed itself to be compatible with Sphinx 1.8. I tested this locally and search works for me. I also updated our `pandas` dependency because our `setup.py` declares compatibility with only `0.23.x`. Because I edited the environment files, I rebuilt the pr builder image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5059
https://github.com/hail-is/hail/pull/5059:446,Integrability,depend,dependency,446,"Sphinx 1.8 broke compatibility (https://github.com/sphinx-doc/sphinx/issues/5460) with a number of themes including `sphinx_rtd_theme`. Sphinx 1.8.3 restores compatibility (https://github.com/sphinx-doc/sphinx/pull/5590) with said themes. Moreover, sphinx_rtd_theme 0.4.2 (https://github.com/rtfd/sphinx_rtd_theme/pull/672) fixed itself to be compatible with Sphinx 1.8. I tested this locally and search works for me. I also updated our `pandas` dependency because our `setup.py` declares compatibility with only `0.23.x`. Because I edited the environment files, I rebuilt the pr builder image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5059
https://github.com/hail-is/hail/pull/5059:373,Testability,test,tested,373,"Sphinx 1.8 broke compatibility (https://github.com/sphinx-doc/sphinx/issues/5460) with a number of themes including `sphinx_rtd_theme`. Sphinx 1.8.3 restores compatibility (https://github.com/sphinx-doc/sphinx/pull/5590) with said themes. Moreover, sphinx_rtd_theme 0.4.2 (https://github.com/rtfd/sphinx_rtd_theme/pull/672) fixed itself to be compatible with Sphinx 1.8. I tested this locally and search works for me. I also updated our `pandas` dependency because our `setup.py` declares compatibility with only `0.23.x`. Because I edited the environment files, I rebuilt the pr builder image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5059
https://github.com/hail-is/hail/pull/5065:58,Availability,error,errors,58,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065
https://github.com/hail-is/hail/pull/5065:474,Performance,bottleneck,bottleneck,474,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065
https://github.com/hail-is/hail/pull/5065:30,Testability,test,tests,30,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065
https://github.com/hail-is/hail/pull/5065:78,Testability,test,test,78,"WIP pull request. - [x] Write tests to replicate existing errors. - [ ] Write test to replicate stalled request. - [ ] Resolve stalled request. However, I think we should consider writing a more complete solution to this. As far as I can tell, our use of threads is fragile; following Flask recommendations w.r.t reliance on production-ready WSGI server seems a good idea. Happy to take that on. I'd also like to move away from Flask for API stuff. While not likely to be a bottleneck for many moons, there are solutions rumored to be far faster (Falcon, esp using Cpython).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065
https://github.com/hail-is/hail/pull/5066:320,Availability,down,down,320,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5066
https://github.com/hail-is/hail/pull/5066:81,Modifiability,config,configured,81,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5066
https://github.com/hail-is/hail/pull/5066:156,Modifiability,config,configure,156,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5066
https://github.com/hail-is/hail/pull/5066:98,Performance,load,loadconda,98,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5066
https://github.com/hail-is/hail/pull/5066:328,Testability,test,test,328,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5066
https://github.com/hail-is/hail/pull/5069:96,Integrability,interface,interface,96,"This makes it possible to explode nested fields in TableExplode,; but I don't want to solve the interface problem in this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5069
https://github.com/hail-is/hail/pull/5071:14,Security,hash,hash,14,"the entries! [hash] must die; will use Sym instead of String throughout IR, types, etc. Identifiers from the user program are as before, colon is used for internal symbols like `:foo` and generated symbols like `:jlen-5`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5071
https://github.com/hail-is/hail/pull/5072:73,Performance,optimiz,optimizations,73,"First crack at supporting multi phenotype logistic regression. No matrix optimizations, as is implemented in multi phenotype linear regression, but I attempt to follow a similar approach as far as far as API and single call of mapPartitions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5072
https://github.com/hail-is/hail/pull/5072:42,Testability,log,logistic,42,"First crack at supporting multi phenotype logistic regression. No matrix optimizations, as is implemented in multi phenotype linear regression, but I attempt to follow a similar approach as far as far as API and single call of mapPartitions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5072
https://github.com/hail-is/hail/pull/5073:244,Usability,simpl,simpler,244,"Batch currently only supports a single container with the name default. This change encodes this requirement in the create_job endpoint. A better solution is to change the API to not accept multiple containers or name parameters, but this is a simpler fix. resolves #4773",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5073
https://github.com/hail-is/hail/pull/5075:227,Performance,optimiz,optimizer,227,"Some context:. We have 3 kinds of IRs currently: ; - Value IRs (MakeStruct, I32, ApplyComparisonOp, etc); - TableIRs (TableRange, TableFilter, etc); - MatrixIRs (MatrixMapRows, MatrixExplodeCols, etc). One of the passes of the optimizer is to reformulate MatrixIRs in terms of TableIRs, and we're in the middle of this push to write lowerers for each MatrixIR node (which means writing an algorithm in LowerMatrixTable.scala, and removing the node's 'execute' method). Here I implement lowering MatrixAnnotateRowsTable as either TableIntervalJoin or TableLeftJoinRightDistinct (a future PR should split MatrixAnnotateRowsTable into 2 nodes like Table has, probably). The one bit of extra complexity in Python comes from implementing foreign-key joins explicitly (this was previously handled by the IR node itself).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075
https://github.com/hail-is/hail/pull/5077:36,Performance,load,loading,36,This PR adds support for optionally loading a dosage values as the call value for genotype loci. This is to support VCF files (such as the ones we use at 23andMe) that have dosages (`DS`) instead of genotype call information (typically `GT`).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077
https://github.com/hail-is/hail/pull/5078:41,Availability,down,down,41,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:841,Deployability,install,install,841,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:947,Deployability,deploy,deploy,947,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:307,Performance,Perform,Performance,307,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:519,Performance,optimiz,optimization,519,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:545,Safety,avoid,avoid,545,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:747,Safety,avoid,avoiding,747,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:1036,Testability,Test,Test,1036,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5078:1056,Testability,test,test,1056,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078
https://github.com/hail-is/hail/pull/5080:71,Security,hash,hash,71,"Sorry in advance for the spicy meatball and rebase pain. the entries! [hash] is now gone!. Added Symbol hierarchy in Scala and Python. Symbols have 3 types: user-level identifiers, generated symbols and ""internal"" symbols like :row and :entries. Generated and internal symbols are printed with a leading colon (with no backtick quotes). Internal symbols are never visible to the user. On the Scala side, symbols are all `Sym` but there is implicit conversions from String to Sym so client code can just write strings. On the Python side, symbols are `str` or `Symbol`. I didn't change Python gen_uid to produce Generated symbols yet. I will do that in a second PR. Generally, we pass strings through the py-j boundary and parse on either side. I turned off color on testPython because it leaves the logs full of unreadable escape codes. There is one user-visible change: the JSON exporter now escapes strings in structs so we will have ```{""`$foo`"": 5}``` instead of `{""$foo"": 5}`. This is necessary to disambiguate complex names and internal symbols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5080
https://github.com/hail-is/hail/pull/5080:766,Testability,test,testPython,766,"Sorry in advance for the spicy meatball and rebase pain. the entries! [hash] is now gone!. Added Symbol hierarchy in Scala and Python. Symbols have 3 types: user-level identifiers, generated symbols and ""internal"" symbols like :row and :entries. Generated and internal symbols are printed with a leading colon (with no backtick quotes). Internal symbols are never visible to the user. On the Scala side, symbols are all `Sym` but there is implicit conversions from String to Sym so client code can just write strings. On the Python side, symbols are `str` or `Symbol`. I didn't change Python gen_uid to produce Generated symbols yet. I will do that in a second PR. Generally, we pass strings through the py-j boundary and parse on either side. I turned off color on testPython because it leaves the logs full of unreadable escape codes. There is one user-visible change: the JSON exporter now escapes strings in structs so we will have ```{""`$foo`"": 5}``` instead of `{""$foo"": 5}`. This is necessary to disambiguate complex names and internal symbols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5080
https://github.com/hail-is/hail/pull/5080:799,Testability,log,logs,799,"Sorry in advance for the spicy meatball and rebase pain. the entries! [hash] is now gone!. Added Symbol hierarchy in Scala and Python. Symbols have 3 types: user-level identifiers, generated symbols and ""internal"" symbols like :row and :entries. Generated and internal symbols are printed with a leading colon (with no backtick quotes). Internal symbols are never visible to the user. On the Scala side, symbols are all `Sym` but there is implicit conversions from String to Sym so client code can just write strings. On the Python side, symbols are `str` or `Symbol`. I didn't change Python gen_uid to produce Generated symbols yet. I will do that in a second PR. Generally, we pass strings through the py-j boundary and parse on either side. I turned off color on testPython because it leaves the logs full of unreadable escape codes. There is one user-visible change: the JSON exporter now escapes strings in structs so we will have ```{""`$foo`"": 5}``` instead of `{""$foo"": 5}`. This is necessary to disambiguate complex names and internal symbols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5080
https://github.com/hail-is/hail/pull/5084:55,Modifiability,rewrite,rewrites,55,the former `ascending` and `onKey` parameters are just rewrites of the comparison expression.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5084
https://github.com/hail-is/hail/issues/5087:117,Modifiability,variab,variable,117,Modify the [scorecard.py](https://github.com/hail-is/hail/blob/master/scorecard/scorecard/scorecard.py#L40)'s `user` variable to include Daniel Goldstein's GitHub handle so that he shows up in the random user list.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5087
https://github.com/hail-is/hail/pull/5091:72,Performance,perform,performance,72,Please don't approve yet. Some initial experiments are showing that the performance here is pretty terrible - probably related to more interpreter usage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5091
https://github.com/hail-is/hail/issues/5095:398,Availability,error,error,398,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; Visited top hit for ""hail matrix table"": https://hail.is/docs/0.2/hailpedia/matrix_table.html. ### What went wrong (all error messages here, including the full java stack trace):; 404 / Not Found . ### Solution; We should add a 301 / permanent redirect: https://support.google.com/webmasters/answer/93633?hl=en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5095
https://github.com/hail-is/hail/issues/5095:404,Integrability,message,messages,404,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; Visited top hit for ""hail matrix table"": https://hail.is/docs/0.2/hailpedia/matrix_table.html. ### What went wrong (all error messages here, including the full java stack trace):; 404 / Not Found . ### Solution; We should add a 301 / permanent redirect: https://support.google.com/webmasters/answer/93633?hl=en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5095
https://github.com/hail-is/hail/issues/5100:2470,Availability,error,error,2470,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5100
https://github.com/hail-is/hail/issues/5100:2476,Integrability,message,messages,2476,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5100
https://github.com/hail-is/hail/issues/5100:278,Performance,optimiz,optimize,278,"### Hail version:; ```; 0.2.7-14ce9228174e; ```; ### What you did:; ```; mt = hl.import_bgen('... a bgen file with ~500k samples ...', entry_fields=['GT']); mt = mt.select_rows().select_cols().select_entries('GT'); mt.count(); ```; becomes; ```; 2019-01-08 18:19:48 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5100
https://github.com/hail-is/hail/issues/5100:1335,Performance,optimiz,optimize,1335,"s""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5100
https://github.com/hail-is/hail/issues/5100:2542,Performance,optimiz,optimized,2542,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5100
https://github.com/hail-is/hail/pull/5103:157,Performance,perform,performance,157,"This allows us to insert fields elsewhere in the struct, and; is necessary for a forthcoming Python IR generation change, which will fix the BGEN allocation performance problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5103
https://github.com/hail-is/hail/pull/5107:314,Testability,test,tested,314,"Stacked on #5103 . Fixes #5100 . This PR changes the Python select and key_by operators to generate; the IR we'd expect them to be generating (e.g. `ht.select('x')` emits; a `SelectFields` instead of a `MakeStruct`). In the process, I found and fixed a bug in group expressions for ; `GroupedMatrixTable`. This is tested for both tables and matrix tables; in the new tests in `test_table` and `test_matrix_table`. Some timings:. >>> mt = hl.read_matrix_table('/Users/tpoterba/data/profile.mt'); >>> %timeit mt.select_entries('GT')._force_count_rows(); ; master: . 1.64 s  106 ms per loop; ; PR: . 967 ms  61.1 ms per loop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5107
https://github.com/hail-is/hail/pull/5107:367,Testability,test,tests,367,"Stacked on #5103 . Fixes #5100 . This PR changes the Python select and key_by operators to generate; the IR we'd expect them to be generating (e.g. `ht.select('x')` emits; a `SelectFields` instead of a `MakeStruct`). In the process, I found and fixed a bug in group expressions for ; `GroupedMatrixTable`. This is tested for both tables and matrix tables; in the new tests in `test_table` and `test_matrix_table`. Some timings:. >>> mt = hl.read_matrix_table('/Users/tpoterba/data/profile.mt'); >>> %timeit mt.select_entries('GT')._force_count_rows(); ; master: . 1.64 s  106 ms per loop; ; PR: . 967 ms  61.1 ms per loop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5107
https://github.com/hail-is/hail/pull/5110:14,Availability,error,error,14,Now raises an error instead of asserting. resolves #4770 by clarifying problem with old syntax introduced by [breaking change](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5110
https://github.com/hail-is/hail/pull/5110:192,Integrability,interface,interface,192,Now raises an error instead of asserting. resolves #4770 by clarifying problem with old syntax introduced by [breaking change](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5110
https://github.com/hail-is/hail/pull/5110:31,Testability,assert,asserting,31,Now raises an error instead of asserting. resolves #4770 by clarifying problem with old syntax introduced by [breaking change](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5110
https://github.com/hail-is/hail/issues/5111:2414,Deployability,install,installation,2414,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2851,Integrability,Interface,Interface,2851,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2987,Integrability,interface,interface,2987,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:289,Modifiability,sandbox,sandbox,289,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:728,Modifiability,sandbox,sandboxing,728,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:790,Modifiability,sandbox,sandbox,790,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2271,Modifiability,sandbox,sandboxed,2271,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2811,Modifiability,Plugin,Plugin,2811,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:312,Performance,perform,performance,312,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:864,Performance,perform,performance,864,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1900,Performance,Perform,Performance,1900,"aster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1922,Performance,perform,performance,1922,"aster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1074,Security,access,access,1074,"es not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal At",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1151,Security,Authoriz,Authorization,1151,"al outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1216,Security,access,access-authn-authz,1216,"a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networkin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1475,Security,Secur,Secure,1475,"ement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### Refer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1612,Security,secur,secure-fast-microvm-serverless,1612,"tion across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-cont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2099,Security,Attack,Attack,2099,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:289,Testability,sandbox,sandbox,289,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:497,Testability,test,test,497,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:728,Testability,sandbox,sandboxing,728,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:790,Testability,sandbox,sandbox,790,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelles Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:1934,Testability,benchmark,benchmark,1934,"aster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/issues/5111:2271,Testability,sandbox,sandboxed,2271,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesnt work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation  Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O  GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesnt replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5111
https://github.com/hail-is/hail/pull/5113:33,Testability,test,tested,33,"Sorry about that! Thought it was tested, it was not!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5113
https://github.com/hail-is/hail/pull/5117:50,Safety,Safe,SafeRow,50,Added:; - NDArray class with toJSON; - NDArray to SafeRow.read to go from RV to Annotation; - NDArrayto RegionValueBuilder.addAnnotation to go from Annotation to RV; - NDArray to JSONAnnotationImpex. Next steps:; ndarray 4: genValue on TNDArray in Scala with tests in TypeSuite; ndarray 5: JSON importers/exporters for NumPy NDArrays; ndarray 6: MakeNDArray IR; ndarray 7: Add MakeNDArray to cxx.emit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5117
https://github.com/hail-is/hail/pull/5117:259,Testability,test,tests,259,Added:; - NDArray class with toJSON; - NDArray to SafeRow.read to go from RV to Annotation; - NDArrayto RegionValueBuilder.addAnnotation to go from Annotation to RV; - NDArray to JSONAnnotationImpex. Next steps:; ndarray 4: genValue on TNDArray in Scala with tests in TypeSuite; ndarray 5: JSON importers/exporters for NumPy NDArrays; ndarray 6: MakeNDArray IR; ndarray 7: Add MakeNDArray to cxx.emit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5117
https://github.com/hail-is/hail/issues/5118:955,Availability,error,error,955,"I will submit a larger batch PR soon, not sure this is worth addressing until then, because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to addres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5118
https://github.com/hail-is/hail/issues/5118:1358,Performance,race condition,race conditions,1358," because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to address state questions in the upcoming PR, and will close this when / if we approve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5118
https://github.com/hail-is/hail/issues/5118:1919,Security,validat,validate,1919," because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to address state questions in the upcoming PR, and will close this when / if we approve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5118
https://github.com/hail-is/hail/issues/5118:1706,Testability,log,log,1706," because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to address state questions in the upcoming PR, and will close this when / if we approve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5118
https://github.com/hail-is/hail/issues/5118:992,Usability,Simpl,Simple,992,"I will submit a larger batch PR soon, not sure this is worth addressing until then, because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to addres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5118
https://github.com/hail-is/hail/issues/5119:539,Availability,error,error,539,"### Hail version:; reported on Jan 11 9:45 by @nikbaya on [zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/import_table.20IncompleteParseError). `00f9b64d8`; ### What you did:. ```; import hail as hl; phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5119
https://github.com/hail-is/hail/issues/5119:545,Integrability,message,messages,545,"### Hail version:; reported on Jan 11 9:45 by @nikbaya on [zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/import_table.20IncompleteParseError). `00f9b64d8`; ### What you did:. ```; import hail as hl; phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5119
https://github.com/hail-is/hail/issues/5119:1518,Integrability,wrap,wrapper,1518,"--------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impute, quote,; 1328 skip_blank_lines, force_bgz); -> 1329 return Table._from_java(jt); 1330; 1331. /home/hail/hail.zip/hail/table.py in _from_java(jt); 319 @staticmethod; 320 def _from_java(jt):; --> 321 return Table(JavaTable(jt.tir())); 322; 323 def __init__(self, tir):. /home/hail/hail.zip/hail/table.py in __init__(self, tir); 328 Env.hc()._jhc, Env.hc()._backend._to_java_ir(self._tir)); 329; --> 330 self._type = self._tir.typ; 331; 332 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 78 def typ(self):; 79 jtir = Env.hc()._backend._to_java_ir(self); ---> 80 return ttable._from_java(jtir.typ()); 81; 82 def parse",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5119
https://github.com/hail-is/hail/issues/5119:1569,Integrability,wrap,wrapper,1569,"--------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impute, quote,; 1328 skip_blank_lines, force_bgz); -> 1329 return Table._from_java(jt); 1330; 1331. /home/hail/hail.zip/hail/table.py in _from_java(jt); 319 @staticmethod; 320 def _from_java(jt):; --> 321 return Table(JavaTable(jt.tir())); 322; 323 def __init__(self, tir):. /home/hail/hail.zip/hail/table.py in __init__(self, tir); 328 Env.hc()._jhc, Env.hc()._backend._to_java_ir(self._tir)); 329; --> 330 self._type = self._tir.typ; 331; 332 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 78 def typ(self):; 79 jtir = Env.hc()._backend._to_java_ir(self); ---> 80 return ttable._from_java(jtir.typ()); 81; 82 def parse",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5119
https://github.com/hail-is/hail/issues/5119:1774,Integrability,wrap,wrapper,1774,"--------------------------------; IncompleteParseError Traceback (most recent call last); <ipython-input-2-ebe253e83367> in <module>(); 1 phesant_still_more1 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.1.tsv',; ----> 2 missing='',impute=True,types={'all_sexes$userId':hl.tstr}).rename({'all_sexes$userId':'s'}); 3 # phesant_still_more2 = hl.import_table('gs://ukb31063-mega-gwas/phenotype-files/still-more-phesant/neale_lab_parsed_and_restricted_to_QCed_samples_cat_variables_both_sexes.2.tsv',; 4 # missing='',impute=True,types={'all_sexes$userId':hl.tstr}). <decorator-gen-1108> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1327 delimiter, missing, no_header, impute, quote,; 1328 skip_blank_lines, force_bgz); -> 1329 return Table._from_java(jt); 1330; 1331. /home/hail/hail.zip/hail/table.py in _from_java(jt); 319 @staticmethod; 320 def _from_java(jt):; --> 321 return Table(JavaTable(jt.tir())); 322; 323 def __init__(self, tir):. /home/hail/hail.zip/hail/table.py in __init__(self, tir); 328 Env.hc()._jhc, Env.hc()._backend._to_java_ir(self._tir)); 329; --> 330 self._type = self._tir.typ; 331; 332 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 78 def typ(self):; 79 jtir = Env.hc()._backend._to_java_ir(self); ---> 80 return ttable._from_java(jtir.typ()); 81; 82 def parse",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5119
https://github.com/hail-is/hail/pull/5121:445,Availability,Failure,Failures,445,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1260,Availability,failure,failure,1260,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1326,Availability,down,down,1326,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:175,Testability,test,tests,175,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:218,Testability,test,tests,218,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:356,Testability,test,tests,356,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:428,Testability,test,testPython,428,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:473,Testability,test,test,473,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:515,Testability,Test,Tests,515,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:544,Testability,test,test,544,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:639,Testability,test,test,639,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:671,Testability,Test,Tests,671,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:720,Testability,test,test,720,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:752,Testability,Test,Tests,752,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:802,Testability,test,test,802,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:834,Testability,Test,Tests,834,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:884,Testability,test,test,884,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:916,Testability,Test,Tests,916,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:971,Testability,test,test,971,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1003,Testability,Test,Tests,1003,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1046,Testability,test,test,1046,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1076,Testability,Test,Tests,1076,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1113,Testability,test,test,1113,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1148,Testability,Test,Tests,1148,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5121:1350,Testability,test,tests,1350,"Builds on: https://github.com/hail-is/hail/pull/5101. I'm not going to assign this directly, but break it up because it is pretty spicy (>5K lines changed). This gets us to 9 tests failing against the service, with 22 tests skipped that use to/from_spark or BlockMatrix. Start the server:. ```; $ hail python/hail-apiserver/hail-apiserver.py; ```. Run the tests:. ```; $ HAIL_TEST_SERVICE_BACKEND_URL='http://localhost:5000' gw testPython; ```. Failures:. ```; FAIL python/test/hail/methods/test_family_methods.py::Tests::test_tdt; FAIL python/test/hail/methods/test_impex.py::ImportMatrixTableTests::test_import_matrix_table; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set2; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set3; FAIL python/test/hail/methods/test_misc.py::Tests::test_maximal_independent_set_types; FAIL python/test/hail/methods/test_misc.py::Tests::test_rename_duplicates; FAIL python/test/hail/methods/test_qc.py::Tests::test_concordance; FAIL python/test/hail/methods/test_statgen.py::Tests::test_ibd; ======= 9 failed, 460 passed, 22 skipped, 24 warnings in 233.52 seconds ========; ```. The tdt failure is due to a latent pruner bug I haven't finished tracking down yet. The remaining tests are easily fixed but adding relational functions for: import table, MIS and IBD. Rename duplicates and concordance should just be re-written in Python. Reference genomes will need some work to be multi-user. We need to eliminate the global reference genome state in the JVM, add it to Python, and include references with requests. This means the reference function registration will need to get revamped.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5121
https://github.com/hail-is/hail/pull/5123:25,Modifiability,variab,variable,25,"This PR adds environment variable options to skip tests requiring plink/Rscript executables. - If `HAIL_TEST_SKIP_PLINK` is set, skip tests requiring the `plink` binary.; - If `HAIL_TEST_SKIP_R` is set, skip tests requiring `RScript`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5123
https://github.com/hail-is/hail/pull/5123:50,Testability,test,tests,50,"This PR adds environment variable options to skip tests requiring plink/Rscript executables. - If `HAIL_TEST_SKIP_PLINK` is set, skip tests requiring the `plink` binary.; - If `HAIL_TEST_SKIP_R` is set, skip tests requiring `RScript`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5123
https://github.com/hail-is/hail/pull/5123:134,Testability,test,tests,134,"This PR adds environment variable options to skip tests requiring plink/Rscript executables. - If `HAIL_TEST_SKIP_PLINK` is set, skip tests requiring the `plink` binary.; - If `HAIL_TEST_SKIP_R` is set, skip tests requiring `RScript`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5123
https://github.com/hail-is/hail/pull/5123:208,Testability,test,tests,208,"This PR adds environment variable options to skip tests requiring plink/Rscript executables. - If `HAIL_TEST_SKIP_PLINK` is set, skip tests requiring the `plink` binary.; - If `HAIL_TEST_SKIP_R` is set, skip tests requiring `RScript`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5123
https://github.com/hail-is/hail/pull/5124:8,Testability,test,tests,8,and add tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5124
https://github.com/hail-is/hail/pull/5125:22,Performance,perform,performance,22,"Required for adequate performance in forthcoming lowered MatrixIRs involving the cols. There are a few changes here. 1) LiftLiterals is renamed to LiftNonCompilable. This pass functions to lift non-compilable IRs into MapGlobals nodes.; 2) Introduced EvaluateNonCompilable pass. This evaluates all non-compilable nodes, and replaces them either with primitives (I32, Str, NA), or with references to a struct of compound values.; 3) TableMapGlobals uses EvaluateNonCompilable and the compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5125
https://github.com/hail-is/hail/pull/5126:8,Modifiability,extend,extends,8,This PR extends environment variable options to skip Scala tests requiring plink/Rscript executables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5126
https://github.com/hail-is/hail/pull/5126:28,Modifiability,variab,variable,28,This PR extends environment variable options to skip Scala tests requiring plink/Rscript executables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5126
https://github.com/hail-is/hail/pull/5126:59,Testability,test,tests,59,This PR extends environment variable options to skip Scala tests requiring plink/Rscript executables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5126
https://github.com/hail-is/hail/pull/5127:377,Availability,down,downstream,377,"In `SparkBackend`, TableIRs get lowered into SparkStages and value IRs get lowered into SparkPipelines. `SparkStage` represents the necessary computation on a partition of a table, as well as the partitioning information. This can either directly represent a TableIR, in which case the partition IR (`body`) is an array of all the rows of that given partition, or whatever the downstream ValueIR needs---e.g. for `TableCount`, the length of that array; for `TableWrite`, the filename that the partition was written out to, etc. `SparkPipeline` represents a local value that can use the results from the referenced stages. One assumption that I've made in this PR is that none of the bindings across all `SparkStage.globals` will have the same name, and none of them will be named ""context"". (I think this is a fairly reasonably assumption, since we'll just use genUID() to generate unique IDs for all of them and then use unique symbols once #5080 goes in.). In this PR, I've lowered:; - TableCount; - TableCollect; - TableGetGlobals; - TableRange; - TableMapGlobals; - TableMapRows. in order to write some tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127
https://github.com/hail-is/hail/pull/5127:1107,Testability,test,tests,1107,"In `SparkBackend`, TableIRs get lowered into SparkStages and value IRs get lowered into SparkPipelines. `SparkStage` represents the necessary computation on a partition of a table, as well as the partitioning information. This can either directly represent a TableIR, in which case the partition IR (`body`) is an array of all the rows of that given partition, or whatever the downstream ValueIR needs---e.g. for `TableCount`, the length of that array; for `TableWrite`, the filename that the partition was written out to, etc. `SparkPipeline` represents a local value that can use the results from the referenced stages. One assumption that I've made in this PR is that none of the bindings across all `SparkStage.globals` will have the same name, and none of them will be named ""context"". (I think this is a fairly reasonably assumption, since we'll just use genUID() to generate unique IDs for all of them and then use unique symbols once #5080 goes in.). In this PR, I've lowered:; - TableCount; - TableCollect; - TableGetGlobals; - TableRange; - TableMapGlobals; - TableMapRows. in order to write some tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127
https://github.com/hail-is/hail/issues/5128:59,Safety,avoid,avoid,59,Next time we make backwards incompatible changes we should avoid keying this table because it forces a sort now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5128
https://github.com/hail-is/hail/issues/5129:8,Availability,Error,Error,8,throws `Error summary: FileNotFoundException: Item not found` which leads to confusion when the file (directory) obviously exists,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5129
https://github.com/hail-is/hail/pull/5130:544,Deployability,deploy,deploy,544,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:575,Deployability,deploy,deploy,575,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1706,Deployability,install,install,1706,"all change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1750,Deployability,configurat,configuration,1750,"ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2019,Deployability,release,released,2019,"r, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2161,Deployability,install,installed,2161,"multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2279,Deployability,install,installed,2279,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:3838,Deployability,release,release,3838,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:56,Integrability,depend,depend,56,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1421,Integrability,depend,dependencies,1421,"tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IG",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1605,Integrability,depend,dependencies,1605,"eploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line vari",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:3264,Integrability,depend,dependent,3264,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1750,Modifiability,config,configuration,1750,"ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2369,Modifiability,variab,variable,2369,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2572,Modifiability,variab,variable,2572,"ly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:3083,Modifiability,variab,variable,3083,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:3350,Modifiability,variab,variable,3350,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2185,Performance,load,loading,2185,"multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2422,Safety,safe,safe,2422,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:50,Testability,test,tests,50,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:383,Testability,test,tests,383,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:966,Testability,test,tests,966,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1101,Testability,test,tests,1101,"failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1527,Testability,test,tests,1527,"eploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line vari",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1737,Testability,test,test,1737,"all change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:1907,Testability,test,tests,1907,"ocs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2153,Testability,test,test,2153,"multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:558,Usability,simpl,simplify,558,"### Summary of Changes. - make the vectorized IBS tests depend on libsmidpp, I have no idea how this wasn't failing builds before. - fix `generate-build-info.sh` to actually set `BRANCH` instead of setting the `DATE` to the current git branch. Also remove unnecessary parameter passing to `echo_build_properties`. - remove `doctest` as a separate build step (it's now part of python tests). - fix `DOCS_STATUS` in `hail-ci-build.sh`. - remove some unused files: `list_pypi_versions.py`, `publish-to-pypi.sh` (which duplicates, wrongly, `python/deploy.sh`. - simplify `python/deploy.sh`, by managing copied files in make, we no longer clean up files, but the JAR is only 30 MB anyway. - small change to `conftest.py` triggered a bad diff: I added a `try`/`finally` block to ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/pull/5130:2411,Usability,simpl,simple,2411,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130
https://github.com/hail-is/hail/issues/5143:161,Testability,test,test,161,[`hail/hail-ci-build.sh` has a cluster](https://github.com/hail-is/hail/blob/master/hail/hail-ci-build.sh#L235-L250) and we should use it to run the full python test suite. The archiveZip should contain the tests (it's just a zip of the python folder). We should be able to invoke pytest on the test dir as a module from a `cluster submit`ed python script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5143
https://github.com/hail-is/hail/issues/5143:207,Testability,test,tests,207,[`hail/hail-ci-build.sh` has a cluster](https://github.com/hail-is/hail/blob/master/hail/hail-ci-build.sh#L235-L250) and we should use it to run the full python test suite. The archiveZip should contain the tests (it's just a zip of the python folder). We should be able to invoke pytest on the test dir as a module from a `cluster submit`ed python script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5143
https://github.com/hail-is/hail/issues/5143:295,Testability,test,test,295,[`hail/hail-ci-build.sh` has a cluster](https://github.com/hail-is/hail/blob/master/hail/hail-ci-build.sh#L235-L250) and we should use it to run the full python test suite. The archiveZip should contain the tests (it's just a zip of the python folder). We should be able to invoke pytest on the test dir as a module from a `cluster submit`ed python script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5143
https://github.com/hail-is/hail/issues/5144:53,Availability,error,error-indexing-bgen-files,53,"See the discuss post here: https://discuss.hail.is/t/error-indexing-bgen-files/833. @catoverdrive At check-in, we discussed this error that occurs in `PackCodecSpec.buildEncoder` and thought you'd be the best person to fix this bug since you added this feature. If not, I'll try and come up with a fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5144
https://github.com/hail-is/hail/issues/5144:129,Availability,error,error,129,"See the discuss post here: https://discuss.hail.is/t/error-indexing-bgen-files/833. @catoverdrive At check-in, we discussed this error that occurs in `PackCodecSpec.buildEncoder` and thought you'd be the best person to fix this bug since you added this feature. If not, I'll try and come up with a fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5144
https://github.com/hail-is/hail/pull/5145:0,Safety,avoid,avoid,0,avoid unnecessary copying,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5145
https://github.com/hail-is/hail/issues/5147:3915,Integrability,wrap,wrapper,3915,"9 nfe_ds_freq=ht.freq[nfe_ds_index], nfe_freq=ht.freq[nfe_index]); 10 ; ---> 11 ht = process_consequences(ht); 12 thresholds = hl.literal([0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 1e-5, 1e-6, 0]); 13 . /home/hail/gnomad_hail/utils/generic.py in process_consequences(mt, vep_root, penalize_flags); 220 ensg_with_most_severe_csq=ensg_with_worst_csq); 221 ; --> 222 return mt.annotate_rows(**{vep_root: vep_data}) if isinstance(mt, hl.MatrixTable) else mt.annotate(**{vep_root: vep_data}); 223 ; 224 . /home/hail/hail.zip/hail/table.py in annotate(self, **named_exprs); 753 caller = ""Table.annotate""; 754 e = get_annotate_exprs(caller, named_exprs, self._row_indices); --> 755 return self._select(caller, self.row.annotate(**e)); 756 ; 757 @typecheck_method(expr=expr_bool,. <decorator-gen-830> in _select(self, caller, row). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in _select(self, caller, row); 428 analyze(caller, row, self._row_indices); 429 base, cleanup = self._process_joins(row); --> 430 return cleanup(Table(TableMapRows(base._tir, row._ir))); 431 ; 432 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/table.py in __init__(self, tir); 334 ; 335 self._tir = tir; --> 336 self._type = self._tir.typ; 337 ; 338 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 89 def typ(self):; 90 if self._type is None:; ---> 91 self._compute_type(); 92 assert self._type is not None, self; 93 return self._type. /home/hail/hail.zip/hail/ir/table_ir.py in _compute_type(self); 166 def _compute_type(self):; 167 # agg_env for scans; --> 168 self.new_row._compute_type(self.child.typ.row_env(), self.child.typ.row_env()); 169 self._type = hl.ttable(; 170",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5147
https://github.com/hail-is/hail/issues/5147:3966,Integrability,wrap,wrapper,3966,"9 nfe_ds_freq=ht.freq[nfe_ds_index], nfe_freq=ht.freq[nfe_index]); 10 ; ---> 11 ht = process_consequences(ht); 12 thresholds = hl.literal([0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 1e-5, 1e-6, 0]); 13 . /home/hail/gnomad_hail/utils/generic.py in process_consequences(mt, vep_root, penalize_flags); 220 ensg_with_most_severe_csq=ensg_with_worst_csq); 221 ; --> 222 return mt.annotate_rows(**{vep_root: vep_data}) if isinstance(mt, hl.MatrixTable) else mt.annotate(**{vep_root: vep_data}); 223 ; 224 . /home/hail/hail.zip/hail/table.py in annotate(self, **named_exprs); 753 caller = ""Table.annotate""; 754 e = get_annotate_exprs(caller, named_exprs, self._row_indices); --> 755 return self._select(caller, self.row.annotate(**e)); 756 ; 757 @typecheck_method(expr=expr_bool,. <decorator-gen-830> in _select(self, caller, row). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in _select(self, caller, row); 428 analyze(caller, row, self._row_indices); 429 base, cleanup = self._process_joins(row); --> 430 return cleanup(Table(TableMapRows(base._tir, row._ir))); 431 ; 432 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/table.py in __init__(self, tir); 334 ; 335 self._tir = tir; --> 336 self._type = self._tir.typ; 337 ; 338 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 89 def typ(self):; 90 if self._type is None:; ---> 91 self._compute_type(); 92 assert self._type is not None, self; 93 return self._type. /home/hail/hail.zip/hail/ir/table_ir.py in _compute_type(self); 166 def _compute_type(self):; 167 # agg_env for scans; --> 168 self.new_row._compute_type(self.child.typ.row_env(), self.child.typ.row_env()); 169 self._type = hl.ttable(; 170",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5147
https://github.com/hail-is/hail/issues/5147:4172,Integrability,wrap,wrapper,4172,"9 nfe_ds_freq=ht.freq[nfe_ds_index], nfe_freq=ht.freq[nfe_index]); 10 ; ---> 11 ht = process_consequences(ht); 12 thresholds = hl.literal([0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 1e-5, 1e-6, 0]); 13 . /home/hail/gnomad_hail/utils/generic.py in process_consequences(mt, vep_root, penalize_flags); 220 ensg_with_most_severe_csq=ensg_with_worst_csq); 221 ; --> 222 return mt.annotate_rows(**{vep_root: vep_data}) if isinstance(mt, hl.MatrixTable) else mt.annotate(**{vep_root: vep_data}); 223 ; 224 . /home/hail/hail.zip/hail/table.py in annotate(self, **named_exprs); 753 caller = ""Table.annotate""; 754 e = get_annotate_exprs(caller, named_exprs, self._row_indices); --> 755 return self._select(caller, self.row.annotate(**e)); 756 ; 757 @typecheck_method(expr=expr_bool,. <decorator-gen-830> in _select(self, caller, row). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in _select(self, caller, row); 428 analyze(caller, row, self._row_indices); 429 base, cleanup = self._process_joins(row); --> 430 return cleanup(Table(TableMapRows(base._tir, row._ir))); 431 ; 432 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/table.py in __init__(self, tir); 334 ; 335 self._tir = tir; --> 336 self._type = self._tir.typ; 337 ; 338 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 89 def typ(self):; 90 if self._type is None:; ---> 91 self._compute_type(); 92 assert self._type is not None, self; 93 return self._type. /home/hail/hail.zip/hail/ir/table_ir.py in _compute_type(self); 166 def _compute_type(self):; 167 # agg_env for scans; --> 168 self.new_row._compute_type(self.child.typ.row_env(), self.child.typ.row_env()); 169 self._type = hl.ttable(; 170",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5147
https://github.com/hail-is/hail/issues/5147:4747,Testability,assert,assert,4747,"er, self.row.annotate(**e)); 756 ; 757 @typecheck_method(expr=expr_bool,. <decorator-gen-830> in _select(self, caller, row). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in _select(self, caller, row); 428 analyze(caller, row, self._row_indices); 429 base, cleanup = self._process_joins(row); --> 430 return cleanup(Table(TableMapRows(base._tir, row._ir))); 431 ; 432 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/table.py in __init__(self, tir); 334 ; 335 self._tir = tir; --> 336 self._type = self._tir.typ; 337 ; 338 self._row_axis = 'row'. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 89 def typ(self):; 90 if self._type is None:; ---> 91 self._compute_type(); 92 assert self._type is not None, self; 93 return self._type. /home/hail/hail.zip/hail/ir/table_ir.py in _compute_type(self); 166 def _compute_type(self):; 167 # agg_env for scans; --> 168 self.new_row._compute_type(self.child.typ.row_env(), self.child.typ.row_env()); 169 self._type = hl.ttable(; 170 self.child.typ.global_type,. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1236 self.old._compute_type(env, agg_env); 1237 for f, x in self.fields:; -> 1238 x._compute_type(env, agg_env); 1239 self._type = self.old.typ._insert_fields(**{f: x.typ for f, x in self.fields}); 1240 if self.field_order:. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1236 self.old._compute_type(env, agg_env); 1237 for f, x in self.fields:; -> 1238 x._compute_type(env, agg_env); 1239 self._type = self.old.typ._insert_fields(**{f: x.typ for f, x in self.fields}); 1240 if self.field_order:. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 14",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5147
https://github.com/hail-is/hail/pull/5148:161,Modifiability,Refactor,Refactored,161,"Created IR nodes for reading, writing and adding BlockMatrix objects as well as a BlockMatrixLiteral node to interop with functionality that doesn't use the IR. Refactored `blockmatrix.py` to stay fully functional while methods are converted into the IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5148
https://github.com/hail-is/hail/pull/5150:49,Testability,log,logistic,49,clean up on Scala side; there is a full suite of logistic regression tests on the Python side (in methods/test_statgen.py),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150
https://github.com/hail-is/hail/pull/5150:69,Testability,test,tests,69,clean up on Scala side; there is a full suite of logistic regression tests on the Python side (in methods/test_statgen.py),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150
https://github.com/hail-is/hail/pull/5152:288,Deployability,release,release,288,"remove _convert_to_j, no longer used (pending some other PRs). note, this is a breaking change, I'm removing overlaps and contains from the Interval interface. I doubt anyone is using them. Users should use hl.eval and the corresponding functions on IntervalExpression. FYI @tpoterba for release notes (or pushback)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152
https://github.com/hail-is/hail/pull/5152:149,Integrability,interface,interface,149,"remove _convert_to_j, no longer used (pending some other PRs). note, this is a breaking change, I'm removing overlaps and contains from the Interval interface. I doubt anyone is using them. Users should use hl.eval and the corresponding functions on IntervalExpression. FYI @tpoterba for release notes (or pushback)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152
https://github.com/hail-is/hail/pull/5155:234,Availability,ping,ping,234,"This caught a bug in the type inference in `GroupByKey`, which is also fixed in the PR. @konradjk I'm pretty sure that this makes #5147 work, but I haven't confirmed directly so if you could confirm that it does and then close (or re-ping) once this goes in, that would be great.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5155
https://github.com/hail-is/hail/issues/5157:129,Availability,error,errors,129,Consider that `hl.len(mt.AD) == hl.len(mt.alleles)`. We should `die` if this is unsatisfied instead of throw index out of bounds errors when we index into arrays.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5157
https://github.com/hail-is/hail/pull/5158:118,Performance,cache,cache,118,"Default persist (used by ServiceBackend) currently does nothing. As we've discussed, there's more work to flesh out a cache/persist strategy in the new setting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5158
https://github.com/hail-is/hail/pull/5162:2216,Availability,down,download,2216,"avigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:3814,Availability,down,down,3814,"; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/Jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:5123,Availability,Error,Error,5123,"the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, bef",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:5641,Availability,Error,Error,5641,"lying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, before Promises were in stdlib (bluebird). ```js; async function usePromise() {; const arg = someSyncOperation();; ; let result;; try {; result = await asyncPromise(arg);; } catch(e) {; // without wrapping catch, will just throw on reject(), unwinding the call stack; doSomethingWIthError(e) ; }; ; doStuffWithResult(result);; }; ```; ### React; What is a react component? A function that returns JSX. React components accept props (HTML attributes `<Component propName={propValue} />`); Stateless vs stateful components; ``",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:9143,Availability,down,down,9143,"ment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:11745,Availability,avail,available,11745,"tml#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12073,Availability,avail,available,12073,"ypescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15607,Availability,reliab,reliable,15607," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15851,Availability,avail,available,15851," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2185,Deployability,release,release,2185,"00 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2360,Deployability,install,install,2360,"iles]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2391,Deployability,install,install,2391,"iles]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2809,Deployability,canary,canary,2809,"hen impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:10054,Deployability,update,update,10054,"nciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ###",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15425,Energy Efficiency,schedul,scheduler,15425," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:16188,Energy Efficiency,schedul,scheduling,16188," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2245,Integrability,depend,dependency,2245,"e from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2342,Integrability,depend,dependencies,2342,"iles]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2431,Integrability,depend,dependencies,2431,"iles]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2499,Integrability,depend,dependencies,2499,"e, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a funct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:2775,Integrability,depend,dependencies,2775,"abelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:4618,Integrability,depend,depends,4618,"ther it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. Th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:6311,Integrability,wrap,wrapping,6311," Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; return;; } ; ; resolve(result);; });; }. asyncPromise(arg).then( r => doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, before Promises were in stdlib (bluebird). ```js; async function usePromise() {; const arg = someSyncOperation();; ; let result;; try {; result = await asyncPromise(arg);; } catch(e) {; // without wrapping catch, will just throw on reject(), unwinding the call stack; doSomethingWIthError(e) ; }; ; doStuffWithResult(result);; }; ```; ### React; What is a react component? A function that returns JSX. React components accept props (HTML attributes `<Component propName={propValue} />`); Stateless vs stateful components; ```jsx; # Stateful; class Stuff extends React.Component {; static getInitialProps() {; ; }. render() {; return <div>Hello World</div>; } ; }. # Stateless; # Note that arrow syntax has an implicit return if you don't create a function block, i. => { return <div>Hello World</div> } is valid too.; () => <div> Hello World </div> ; ```. Stateless ones are typically cheaper, but not necessarily:; * See: [PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent); * See: [shouldComponentUpdate lifecycle method](https://reactjs.org/docs/react-component.html#shouldcomponentupdate). #### JSX differences from html; 1. `className` : ""class"" is a reserved word in JSX; used to specify the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:7780,Integrability,wrap,wrap,7780,"ote that arrow syntax has an implicit return if you don't create a function block, i. => { return <div>Hello World</div> } is valid too.; () => <div> Hello World </div> ; ```. Stateless ones are typically cheaper, but not necessarily:; * See: [PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent); * See: [shouldComponentUpdate lifecycle method](https://reactjs.org/docs/react-component.html#shouldcomponentupdate). #### JSX differences from html; 1. `className` : ""class"" is a reserved word in JSX; used to specify the component class (every HTML element is modeled as an object). [This will go away in 2019, maybe](https://github.com/facebook/react/issues/13525); 2. There should alway be one and only one non-leaf node in the component tree. Leaf siblings are allowed; note that this requires that there is only one root node as well. ```jsx; export default () => <div>OK</div>; export default () => (<div>OK</div><span>Not OK</span>); ```. To get around this, wrap in a ""<Fragment>"", an html element, or array. Basically, react wants a top level / root object. https://reactjs.org/docs/fragments.html, https://pawelgrzybek.com/return-multiple-elements-from-a-component-with-react-16/. ```jsx; import { Fragment } from 'react';. const good = () => [<div>OK</div><span>GOOD!</span>];; const good2 = () => <Fragment><div>OK</div><span>GOOD!</span></Fragment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:11174,Integrability,Wrap,Wraps,11174,"ing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:11570,Integrability,Wrap,Wraps,11570,"function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12021,Integrability,rout,routing,12021,"ypescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12156,Integrability,wrap,wrapped,12156,"itted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12287,Integrability,rout,router,12287,"nge pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12295,Integrability,Rout,Routes,12295,"er, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12852,Integrability,rout,routing,12852,"nd again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or seri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13424,Integrability,rout,routing,13424," pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13433,Integrability,Wrap,Wrap,13433," pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13582,Integrability,rout,routing,13582,"ents, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14013,Integrability,rout,routing,14013,"ct containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in genera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15223,Integrability,Depend,Depends,15223,"g, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architectu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:321,Modifiability,variab,variables,321,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:1856,Modifiability,config,configurable,1856,"irst commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:6668,Modifiability,extend,extends,6668,"> doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, before Promises were in stdlib (bluebird). ```js; async function usePromise() {; const arg = someSyncOperation();; ; let result;; try {; result = await asyncPromise(arg);; } catch(e) {; // without wrapping catch, will just throw on reject(), unwinding the call stack; doSomethingWIthError(e) ; }; ; doStuffWithResult(result);; }; ```; ### React; What is a react component? A function that returns JSX. React components accept props (HTML attributes `<Component propName={propValue} />`); Stateless vs stateful components; ```jsx; # Stateful; class Stuff extends React.Component {; static getInitialProps() {; ; }. render() {; return <div>Hello World</div>; } ; }. # Stateless; # Note that arrow syntax has an implicit return if you don't create a function block, i. => { return <div>Hello World</div> } is valid too.; () => <div> Hello World </div> ; ```. Stateless ones are typically cheaper, but not necessarily:; * See: [PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent); * See: [shouldComponentUpdate lifecycle method](https://reactjs.org/docs/react-component.html#shouldcomponentupdate). #### JSX differences from html; 1. `className` : ""class"" is a reserved word in JSX; used to specify the component class (every HTML element is modeled as an object). [This will go away in 2019, maybe](https://github.com/facebook/react/issues/13525); 2. There should alway be one and only one non-leaf node in the component tree. Leaf siblings are allowed; note that t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:9213,Modifiability,variab,variable,9213,"ment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:10437,Modifiability,extend,extend,10437,"s work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:263,Performance,perform,performance,263,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:10631,Performance,cache,cached,10631," the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resourc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:10647,Performance,cache,cached,10647," the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resourc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:11633,Performance,load,load,11633,"hed, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, des",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13750,Performance,load,loading,13750,"ex;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14496,Performance,cache,cache,14496,"k ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14844,Performance,Perform,Performance,14844,"series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14902,Performance,perform,performance-cost-of-server-side-rendered-react-node-js,14902," ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14973,Performance,perform,performance,14973," ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15438,Performance,tune,tuned,15438," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15668,Performance,perform,performs,15668," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15828,Performance,perform,performing,15828," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15909,Performance,optimiz,optimized,15909," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:4582,Safety,avoid,avoid,4582,"llback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/JavaScript/EventLoop#Event_loop. #### Using callbacks; ```js. # Callback-based; function asyncCall(arg, cb => {; const (err, result) = someSynchronousOperation();. cb(err, result);; }. asyncCall(arg,(r, err) => { if(err){ throw new Error(err); doSomething(r)} ); ```. #### Using async/await; Deeply nested callbacks are hard to follow. This is called ""callback hell"". To help combat this, JS, in both NodeJS and Web context, developed Promises. Promises flatten the callback tree. ```js; function asyncPromise(arg) {; return new Promise((resolve, reject) => {; const (err, result) = someSynchronousOperation();; ; if(err) {; reject(err);; retu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:3878,Security,hash,hashing,3878,"; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed from kernel threads that Node maintains in the background, effectively making these operations non-blocking (until the thread pool is exhausted). Browsers and NodeJS use different event loops:. NodeJS: libuv event loop; * Node maintains a hidden worker thread pool (kernel threads) through which it issues sys calls, to avoid blocking the event loop. Web: depends on the underlying Javascript Engine; * Chromium: V8: libevent: https://stackoverflow.com/questions/25750884/are-there-significant-differences-between-the-chrome-browser-event-loop-versus-t; * Firefox: Spidermonkey: ?; * https://developer.mozilla.org/en-US/docs/Web/Jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:12606,Security,expose,expose,12606,"lace to define external resource you want to load, such as some external stylesheet, font, whatever. . 3) `getInitialProps`: a lifecycle method that is only available to components in the `pages/` folder. `getInitialProps ` runs once during server-side rendering, and again if you navigate to the page that defines it. Only components in pages can specify this property. This is because it is effectively a function triggered during routing and:; * `getInitialProps` is of course only available if you define a stateful component. See [functional components (just JSX wrapped in a function, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14712,Security,authenticat,authentication,14712,"s how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many ti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:354,Testability,log,logout,354,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:489,Testability,log,login,489,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:542,Testability,test,test,542,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:642,Testability,log,logic,642,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13169,Testability,log,log,13169,"tion, rather than a class)](https://reactjs.org/docs/components-and-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13235,Testability,log,log,13235,"d-props.html). 4) NextJS includes a light, fast router. Routes are matched based on the names of files in `pages/`, with index.js mapping to `/`. For instance, to navigate to `domain.com/scorecard/users`, you'd make the folder structure:. pages/; * scorecard.tsx; * scorecard/; * users.tsx. These 'pages' components are just like normal react components, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser cac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13590,Testability,log,logic,13590,"ents, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:15341,Testability,benchmark,benchmark,15341," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:371,Usability,responsiv,responsive,371,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:502,Usability,clear,clear,502,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:3204,Usability,guid,guides,3204,"ement; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",; ""react"": ""^16.7.0"",; ""react-dom"": ""^16.7.0""; },; ""devDependencies"": {; }; }; ```; * Scripts are thing that can be run by typing, in shell `npm run`. Ex: `npm run dev`. ### Async, Await, Promises and callback (WIP); Javascript is async-first. This is most obvious in Node.js, which is the most popular library for server-side JS.; * [How event loop works](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/); <img width=""765"" alt=""screen shot 2019-01-18 at 11 20 51 am"" src=""https://user-images.githubusercontent.com/5543229/51399094-1f999c00-1b13-11e9-8dfb-da8aa20807b0.png"">. * The event loop call stack: https://www.youtube.com/watch?v=8aGhZQkoFbQ. At a high level, a function that defines a callback will return immediately. The callback is pushed on to the event-loop stack, and on each tick, is checked to determine whether it has returned or not. Blocking operations within the callbacks will block the event loop. This is how CPU viruses, like blockchain manage to slow down web pages that are hijacked to include some mining script: hashing something 30 million times, takes a long time, and JS cannot do anything besides waiting for those operations to finish in a synchronous fashion. Luckily, asynchronous functions are the norm in the JS ecosystem, such that both in the browser, and nodejs, IO functions are (mostly?) asynchronous.; * For NodeJS: Transparently to the user, blocking operations (IO) are executed ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:8472,Usability,simpl,simple,8472,"ebook/react/issues/13525); 2. There should alway be one and only one non-leaf node in the component tree. Leaf siblings are allowed; note that this requires that there is only one root node as well. ```jsx; export default () => <div>OK</div>; export default () => (<div>OK</div><span>Not OK</span>); ```. To get around this, wrap in a ""<Fragment>"", an html element, or array. Basically, react wants a top level / root object. https://reactjs.org/docs/fragments.html, https://pawelgrzybek.com/return-multiple-elements-from-a-component-with-react-16/. ```jsx; import { Fragment } from 'react';. const good = () => [<div>OK</div><span>GOOD!</span>];; const good2 = () => <Fragment><div>OK</div><span>GOOD!</span></Fragment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when acc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:13554,Usability,simpl,simply,13554,"ents, except they expose `getInitialProps`, described above. Each page file must export 1 default component:. ```js; #Page file; import React from 'react';. const index = () => <div>Hello World</div>; export default index;; ```. There is nothing else to do to get routing to work, a quite nice solution. ### JS pragma; 1. `this` is different than in most (every?) other language. scope of this is bound to caller, not object containing the method; * Solution: use arrow functions. ```js; class Something {; constructor() {; this.bar = 'foo';; } ; //Do; onSubmit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5162:14131,Usability,Simpl,Simply,14131,"Submit = () => {; console.log(this.bar) //prints foo; }. // Don't; onSubmitBad() {; console.log(this.bar) //may be undefined; }; }. const barrer = new Something();; console.info(""good"", barrer.onSubmit());; console.info(""bad"", barrer.onSubmitBad());; ```. # Tips . ### Client-side routing; Wrap a normal anchor tag in `<Link ></Link>`; ex:; ```jsx; <Link href='/path/to/page'><a>Page Name</a></Link>; ```. This simply adds the client-side routing logic, and passes the href to <a href=. . ### Prefetching; One of the neat things about Next is how easy it makes prefetching pages. This allows perceived page loading times on the order of 5ms, even when the page requires very complex state (say a GraphQL or series of REST calls with large responses). ```jsx; <Link href='/expensive-page' prefetch><a>Expensive Page</a></Link>; ```; ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162
https://github.com/hail-is/hail/pull/5164:27,Testability,log,logistic,27,Should be analogous to the logistic regression change,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5164
https://github.com/hail-is/hail/issues/5168:1068,Availability,Toler,Tolerations,1068,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1474,Availability,error,error,1474,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1504,Availability,error,error,1504,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1533,Availability,Error,Error,1533,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1757,Availability,error,error,1757,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1857,Availability,Error,Error,1857,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1940,Availability,avail,available,1940,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1902,Energy Efficiency,allocate,allocate,1902,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1211,Integrability,Message,Message,1211,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1258,Modifiability,Sandbox,SandboxChanged,1258,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1348,Modifiability,sandbox,sandbox,1348,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1744,Modifiability,sandbox,sandbox,1744,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:127,Testability,test,test,127,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:175,Testability,test,test,175,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1258,Testability,Sandbox,SandboxChanged,1258,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1348,Testability,sandbox,sandbox,1348,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/issues/5168:1744,Testability,sandbox,sandbox,1744,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168
https://github.com/hail-is/hail/pull/5172:22,Usability,Simpl,Simplify,22,Fix deoptimization in Simplify.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5172
https://github.com/hail-is/hail/issues/5173:389,Availability,error,error,389,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.8-844023079796. ### What you did:. ```; x = hl.utils.range_table(100); x = x.filter(False); x.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-89-65d54ebb6a64> in <module>; 1 x = hl.utils.range_table(100); 2 x = x.filter(False); ----> 3 x.show(). /home/hail/gnomad_hail/utils/plotting.py in new_show(t, n, width, truncate, types); 25 ; 26 def new_show(t, n=10, width=140, truncate=40, types=True):; ---> 27 old_show(t, n, width, truncate, types); 28 hl.Table.show = new_show; 29 . <decorator-gen-848> in show(self, n, width, truncate, types, handler). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types, handler); 1331 Handler function for data string.; 1332 """"""; -> 1333 handler(self._show(n, width, truncate, types)); 1334 ; 1335 def index(self, *exprs):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. /home/hail/hail.zip/hail/table.py in <listcomp>(.0); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5173
https://github.com/hail-is/hail/issues/5173:395,Integrability,message,messages,395,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.8-844023079796. ### What you did:. ```; x = hl.utils.range_table(100); x = x.filter(False); x.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-89-65d54ebb6a64> in <module>; 1 x = hl.utils.range_table(100); 2 x = x.filter(False); ----> 3 x.show(). /home/hail/gnomad_hail/utils/plotting.py in new_show(t, n, width, truncate, types); 25 ; 26 def new_show(t, n=10, width=140, truncate=40, types=True):; ---> 27 old_show(t, n, width, truncate, types); 28 hl.Table.show = new_show; 29 . <decorator-gen-848> in show(self, n, width, truncate, types, handler). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types, handler); 1331 Handler function for data string.; 1332 """"""; -> 1333 handler(self._show(n, width, truncate, types)); 1334 ; 1335 def index(self, *exprs):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. /home/hail/hail.zip/hail/table.py in <listcomp>(.0); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5173
https://github.com/hail-is/hail/issues/5173:1048,Integrability,wrap,wrapper,1048,"support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.8-844023079796. ### What you did:. ```; x = hl.utils.range_table(100); x = x.filter(False); x.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-89-65d54ebb6a64> in <module>; 1 x = hl.utils.range_table(100); 2 x = x.filter(False); ----> 3 x.show(). /home/hail/gnomad_hail/utils/plotting.py in new_show(t, n, width, truncate, types); 25 ; 26 def new_show(t, n=10, width=140, truncate=40, types=True):; ---> 27 old_show(t, n, width, truncate, types); 28 hl.Table.show = new_show; 29 . <decorator-gen-848> in show(self, n, width, truncate, types, handler). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types, handler); 1331 Handler function for data string.; 1332 """"""; -> 1333 handler(self._show(n, width, truncate, types)); 1334 ; 1335 def index(self, *exprs):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. /home/hail/hail.zip/hail/table.py in <listcomp>(.0); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. ValueError: max() arg is an empty sequence; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5173
https://github.com/hail-is/hail/issues/5173:1099,Integrability,wrap,wrapper,1099,"support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.8-844023079796. ### What you did:. ```; x = hl.utils.range_table(100); x = x.filter(False); x.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-89-65d54ebb6a64> in <module>; 1 x = hl.utils.range_table(100); 2 x = x.filter(False); ----> 3 x.show(). /home/hail/gnomad_hail/utils/plotting.py in new_show(t, n, width, truncate, types); 25 ; 26 def new_show(t, n=10, width=140, truncate=40, types=True):; ---> 27 old_show(t, n, width, truncate, types); 28 hl.Table.show = new_show; 29 . <decorator-gen-848> in show(self, n, width, truncate, types, handler). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types, handler); 1331 Handler function for data string.; 1332 """"""; -> 1333 handler(self._show(n, width, truncate, types)); 1334 ; 1335 def index(self, *exprs):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. /home/hail/hail.zip/hail/table.py in <listcomp>(.0); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. ValueError: max() arg is an empty sequence; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5173
https://github.com/hail-is/hail/issues/5173:1305,Integrability,wrap,wrapper,1305,"support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.8-844023079796. ### What you did:. ```; x = hl.utils.range_table(100); x = x.filter(False); x.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-89-65d54ebb6a64> in <module>; 1 x = hl.utils.range_table(100); 2 x = x.filter(False); ----> 3 x.show(). /home/hail/gnomad_hail/utils/plotting.py in new_show(t, n, width, truncate, types); 25 ; 26 def new_show(t, n=10, width=140, truncate=40, types=True):; ---> 27 old_show(t, n, width, truncate, types); 28 hl.Table.show = new_show; 29 . <decorator-gen-848> in show(self, n, width, truncate, types, handler). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/table.py in show(self, n, width, truncate, types, handler); 1331 Handler function for data string.; 1332 """"""; -> 1333 handler(self._show(n, width, truncate, types)); 1334 ; 1335 def index(self, *exprs):. /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. /home/hail/hail.zip/hail/table.py in <listcomp>(.0); 1238 ; 1239 column_width = [max(len(fields[i]), len(types[i]), max([len(row[i]) for row in rows])); -> 1240 for i in range(n_fields)]; 1241 ; 1242 column_blocks = []. ValueError: max() arg is an empty sequence; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5173
https://github.com/hail-is/hail/issues/5174:175,Availability,failure,failure,175,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:233,Availability,failure,failure,233,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:2487,Energy Efficiency,schedul,scheduler,2487,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:2558,Energy Efficiency,schedul,scheduler,2558,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:2681,Performance,concurren,concurrent,2681,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:2765,Performance,concurren,concurrent,2765,nonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/issues/5174:154,Safety,abort,aborted,154,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174
https://github.com/hail-is/hail/pull/5175:271,Availability,error,error,271,"cc: @cseed . ---. Root issue is that `alpine` does not have `/bin/bash`, so we use `/bin/sh` instead. The syntax for the loops is POSIX compliant. ---. Interesting, these pods were failing due to:. ```; State: Terminated; Reason: ContainerCannotRun; Message: oci runtime error: container_linux.go:247: starting container process caused ""exec: \""/bin/bash\"": stat /bin/bash: no such file or directory"". Exit Code: 127; Started: Fri, 18 Jan 2019 16:15:45 -0500; Finished: Fri, 18 Jan 2019 16:15:45 -0500; Ready: False; ```. But this didn't fail the tests. I suppose because we cancel it faster than k8s realizes the container cannot run and notifies batch?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5175
https://github.com/hail-is/hail/pull/5175:250,Integrability,Message,Message,250,"cc: @cseed . ---. Root issue is that `alpine` does not have `/bin/bash`, so we use `/bin/sh` instead. The syntax for the loops is POSIX compliant. ---. Interesting, these pods were failing due to:. ```; State: Terminated; Reason: ContainerCannotRun; Message: oci runtime error: container_linux.go:247: starting container process caused ""exec: \""/bin/bash\"": stat /bin/bash: no such file or directory"". Exit Code: 127; Started: Fri, 18 Jan 2019 16:15:45 -0500; Finished: Fri, 18 Jan 2019 16:15:45 -0500; Ready: False; ```. But this didn't fail the tests. I suppose because we cancel it faster than k8s realizes the container cannot run and notifies batch?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5175
https://github.com/hail-is/hail/pull/5175:547,Testability,test,tests,547,"cc: @cseed . ---. Root issue is that `alpine` does not have `/bin/bash`, so we use `/bin/sh` instead. The syntax for the loops is POSIX compliant. ---. Interesting, these pods were failing due to:. ```; State: Terminated; Reason: ContainerCannotRun; Message: oci runtime error: container_linux.go:247: starting container process caused ""exec: \""/bin/bash\"": stat /bin/bash: no such file or directory"". Exit Code: 127; Started: Fri, 18 Jan 2019 16:15:45 -0500; Finished: Fri, 18 Jan 2019 16:15:45 -0500; Ready: False; ```. But this didn't fail the tests. I suppose because we cancel it faster than k8s realizes the container cannot run and notifies batch?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5175
https://github.com/hail-is/hail/issues/5182:219,Availability,error,error,219,"### Hail version:; ```; 0.2.8-590ea4ae3b83; ```; ### What you did:; ```; hl.import_bgen(; ""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen"",; entry_fields=[]; ).count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):; A spark stage was triggered. I expected it to read the number of rows from the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5182
https://github.com/hail-is/hail/issues/5182:225,Integrability,message,messages,225,"### Hail version:; ```; 0.2.8-590ea4ae3b83; ```; ### What you did:; ```; hl.import_bgen(; ""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen"",; entry_fields=[]; ).count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):; A spark stage was triggered. I expected it to read the number of rows from the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5182
https://github.com/hail-is/hail/pull/5191:138,Security,expose,expose,138,"@cseed You're really the only one who understands this well enough to review it. resolves #5168 . In order to create a test, we'd have to expose the pod_name to the clients and the tests would have to talk to k8s to ensure said pod was really deleted. Not a terrible test, but maybe more work than I care to do right now given my other commitments. See the [description of the issue in a comment on #5168](; https://github.com/hail-is/hail/issues/5168#issuecomment-456618542). cc: k8s-and-services team: @jigold @tpoterba @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5191
https://github.com/hail-is/hail/pull/5191:119,Testability,test,test,119,"@cseed You're really the only one who understands this well enough to review it. resolves #5168 . In order to create a test, we'd have to expose the pod_name to the clients and the tests would have to talk to k8s to ensure said pod was really deleted. Not a terrible test, but maybe more work than I care to do right now given my other commitments. See the [description of the issue in a comment on #5168](; https://github.com/hail-is/hail/issues/5168#issuecomment-456618542). cc: k8s-and-services team: @jigold @tpoterba @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5191
https://github.com/hail-is/hail/pull/5191:181,Testability,test,tests,181,"@cseed You're really the only one who understands this well enough to review it. resolves #5168 . In order to create a test, we'd have to expose the pod_name to the clients and the tests would have to talk to k8s to ensure said pod was really deleted. Not a terrible test, but maybe more work than I care to do right now given my other commitments. See the [description of the issue in a comment on #5168](; https://github.com/hail-is/hail/issues/5168#issuecomment-456618542). cc: k8s-and-services team: @jigold @tpoterba @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5191
https://github.com/hail-is/hail/pull/5191:267,Testability,test,test,267,"@cseed You're really the only one who understands this well enough to review it. resolves #5168 . In order to create a test, we'd have to expose the pod_name to the clients and the tests would have to talk to k8s to ensure said pod was really deleted. Not a terrible test, but maybe more work than I care to do right now given my other commitments. See the [description of the issue in a comment on #5168](; https://github.com/hail-is/hail/issues/5168#issuecomment-456618542). cc: k8s-and-services team: @jigold @tpoterba @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5191
https://github.com/hail-is/hail/issues/5193:2864,Availability,reliab,reliably,2864,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:3559,Availability,alive,alive,3559,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:3808,Deployability,deploy,deployment,3808,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:2080,Energy Efficiency,allocate,allocates,2080,"eives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:436,Integrability,depend,dependsOn,436,"# People. @cseed @jigold @akotlar @danking @tpoterba. # Plan. We want to describe build processes like this:. ```; - type: namespace; name: ns; - type: image; name: hail-pr-builder; context: ../; dockerfile: ../Dockerfile; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:620,Integrability,depend,depend,620,"# People. @cseed @jigold @akotlar @danking @tpoterba. # Plan. We want to describe build processes like this:. ```; - type: namespace; name: ns; - type: image; name: hail-pr-builder; context: ../; dockerfile: ../Dockerfile; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1060,Integrability,depend,dependencies,1060,"poterba. # Plan. We want to describe build processes like this:. ```; - type: namespace; name: ns; - type: image; name: hail-pr-builder; context: ../; dockerfile: ../Dockerfile; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1599,Integrability,depend,dependencies,1599,"`. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1625,Integrability,depend,dependencies,1625,"`. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1804,Integrability,depend,dependencies,1804,"matically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1866,Integrability,depend,dependencies,1866,"matically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:2063,Integrability,depend,dependencies,2063,"t dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:2310,Integrability,depend,dependencies,2310,"t to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:2553,Modifiability,parameteriz,parameterized,2553,"ng. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:2670,Modifiability,parameteriz,parameterized,2670,"ng. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:3694,Modifiability,config,configure,3694,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1311,Security,secur,secure,1311,"""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:317,Testability,test,test,317,"# People. @cseed @jigold @akotlar @danking @tpoterba. # Plan. We want to describe build processes like this:. ```; - type: namespace; name: ns; - type: image; name: hail-pr-builder; context: ../; dockerfile: ../Dockerfile; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:485,Testability,test,tests-using-input-jar,485,"# People. @cseed @jigold @akotlar @danking @tpoterba. # Plan. We want to describe build processes like this:. ```; - type: namespace; name: ns; - type: image; name: hail-pr-builder; context: ../; dockerfile: ../Dockerfile; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. A series of steps that get us there:. - [x] (https://github.com/hail-is/hail/pull/5231) jobs may only depend on other jobs in the same batch ; - [x] (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/issues/5193:1686,Testability,log,logging,1686," (https://github.com/hail-is/hail/pull/5232) a batch can be `closed` indicating no more jobs will be added; - [ ] (https://github.com/hail-is/hail/pull/5233) a batch is automatically closed after 30 minutes; - [ ] add ""outputs"" to batch jobs. When a batch job is complete, the batch system copies its outputs to some durable storage (e.g. GCS), this is not user-visible; - [ ] add input/output dependencies to batch jobs. A batch job always receives the output of its parents. The data is copied (somehow) from the durable storage to the pod's filesystem at the path `/input/PARENT_JOB_NAME` and set to permissions 777. The copying is done in a secure way. In particular, the job container can be completely unprivileged (e.g. no credentials in the job's image, no credentials in the mounted volumes, no way to escalate to these credentials); - [ ] parse and run a series of `exec` commands that execute in parallel but no namespace dependencies and no image dependencies. This immediately puts us in a better place wrt logging. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193
https://github.com/hail-is/hail/pull/5194:158,Availability,avail,available,158,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:120,Deployability,deploy,deploy,120,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:138,Deployability,deploy,deploys,138,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:227,Deployability,install,install,227,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:507,Deployability,update,updated,507,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:674,Integrability,depend,dependency,674,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1389,Integrability,depend,dependency,1389,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1018,Modifiability,variab,variable,1018,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:111,Testability,test,test-pip-deploy,111,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:272,Testability,test,test,272,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1115,Testability,test,test,1115,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1183,Testability,test,test,1183,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1222,Testability,test,tests,1222,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/pull/5194:1235,Testability,test,test-python,1235,"- move environment.yml files out of the packaged directory so they don't get shipped to end users. - add `make test-pip-deploy` which pip deploys to the next available `devN` version (you have to wait a bit before you can `pip install` it, so I didn't include that in the test, but you can do that manually, or a motivated person can write a polling script). - add `build/dev-conda` which ensures that if the dev-environment file changes since you last ran `make build/dev-conda`, your conda environment is updated. - pedantically use the correct conda environment _everywhere_. - use python to determine cpu count instead of fixing it at 2. - add `jq` as an `env-setup.sh` dependency. - add `make build/credentials.json` which `scp`s a new JSON file containing credentials to the local machine, moreover there are two rules for automatically extracting the credentials for PYPI from this JSON file. - use `ENV_VAR`, a make macro, to ensure we rebuild the appropriate targets (but no more) when a relevant environment variable is changed since last build. - added several missing breeze versions, now we can easily test against new spark versions, just run `SPARK_VERSION=4.0.0 make test`. - fold doctests in with regular tests under `test-python` which uses pytest, no more unnecessary copying as well. - fix build-info. - delete two unused python files in hail root. - correct LIBSIMDPP dependency in C makefile. # Not Doing Yet. - incorporate native lib into this Makefile. Instead, if anything changed in src/main/c since we last built, we rebuild. - fix the directory structure to be compliant with pytests recommended structure",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194
https://github.com/hail-is/hail/issues/5195:2271,Deployability,pipeline,pipeline,2271,"SelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the above rule needs care wrt aggregations, namely the let must be pushed under the aggregation, but no further. All these rules need to be careful because we don't want to lose the ability to send something through BLAS. Perhaps these rules should be left entirely to the ""pipeline""-level (see: Arcturus' recent work) optimizer (after translation to tables of small tensors is complete, at which point BLAS operations are explicit). ## Compilation. At first, we pattern match the items that can be represented via BlockMatrix, err'ing on unrepresentable expressions. C2[[ TensorMap(u, ApplyUnaryPrimOp(Plus(), Ref(""e""), F64(n))) ]]; =; u.scalarAdd(n). C2[[ TensorMap2(u, v, ApplyBinaryPrimOp(Plus(), Ref(""l""), Ref(""r""))) ]]; =; u.add(v). C2[[ TensorContraction(u, v, (ApplyAggOp Sum () None ((ApplyBinaryPrimOp `*` (Ref l) (Ref r))))) ]]; =; u.dot(v)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5195
https://github.com/hail-is/hail/issues/5195:168,Modifiability,variab,variable,168,"# --- DRAFT ---. # People. @hail-is/tensors . # Scope of Document; Python API, types, IR, optimizer, compiler. # Notation. e[[x/v]] means substitute occurrences of the variable v with the expression x in; the expression e. f[[ e ]] is just application of the function `f` but with the advantage that it doesn't look like any python or Scala syntax (so it's obviously referring to the meta-language rather than the languages we're building here). I'm going to consistently use ""distributed"" to talk about BlockMatrix-y things and ""small"" to refer to things that live in the ""value"" IR. # DistributedTensorIR. Some thoughts on TensorIR (fruits of discussion among the group):. TensorIR ::= TensorLiteral(); | TensorContract(TensorIR, TensorIR, Int, Int, body: IR); | TensorMap2(TensorIR, TensorIR, body: IR); | TensorMap(TensorIR, body: IR); | TensorSelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5195
https://github.com/hail-is/hail/issues/5195:90,Performance,optimiz,optimizer,90,"# --- DRAFT ---. # People. @hail-is/tensors . # Scope of Document; Python API, types, IR, optimizer, compiler. # Notation. e[[x/v]] means substitute occurrences of the variable v with the expression x in; the expression e. f[[ e ]] is just application of the function `f` but with the advantage that it doesn't look like any python or Scala syntax (so it's obviously referring to the meta-language rather than the languages we're building here). I'm going to consistently use ""distributed"" to talk about BlockMatrix-y things and ""small"" to refer to things that live in the ""value"" IR. # DistributedTensorIR. Some thoughts on TensorIR (fruits of discussion among the group):. TensorIR ::= TensorLiteral(); | TensorContract(TensorIR, TensorIR, Int, Int, body: IR); | TensorMap2(TensorIR, TensorIR, body: IR); | TensorMap(TensorIR, body: IR); | TensorSelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5195
https://github.com/hail-is/hail/issues/5195:2316,Performance,optimiz,optimizer,2316,"SelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the above rule needs care wrt aggregations, namely the let must be pushed under the aggregation, but no further. All these rules need to be careful because we don't want to lose the ability to send something through BLAS. Perhaps these rules should be left entirely to the ""pipeline""-level (see: Arcturus' recent work) optimizer (after translation to tables of small tensors is complete, at which point BLAS operations are explicit). ## Compilation. At first, we pattern match the items that can be represented via BlockMatrix, err'ing on unrepresentable expressions. C2[[ TensorMap(u, ApplyUnaryPrimOp(Plus(), Ref(""e""), F64(n))) ]]; =; u.scalarAdd(n). C2[[ TensorMap2(u, v, ApplyBinaryPrimOp(Plus(), Ref(""l""), Ref(""r""))) ]]; =; u.add(v). C2[[ TensorContraction(u, v, (ApplyAggOp Sum () None ((ApplyBinaryPrimOp `*` (Ref l) (Ref r))))) ]]; =; u.dot(v)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5195
https://github.com/hail-is/hail/pull/5196:386,Testability,Test,TestUtils,386,"Builds on: https://github.com/hail-is/hail/pull/5161 (approved, should land shortly). Add reference operations to Backend. This is a first cut/stop gap. The backend needs to be per-user stateless, so we should eventually track the references in Python send references along with IR in execute and resolve references during parsing. Moved defaultReference to Python. A little cleanup in TestUtils. This is the last PR of my big service push. There some additional things to do, I'll create an issue with bullet points to track them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5196
https://github.com/hail-is/hail/issues/5197:418,Deployability,Pipeline,Pipeline,418,"Remaining things to do:; - [ ] store references in Python, send references along with IR, and resolve them during parsing; - [ ] IBD (MatrixToTableApply); - [ ] index_bgen (Backend function); - [ ] maximal independent set; - [ ] import matrix table (MatrixReader); - [ ] concordance; - [x] MatrixTable.head (new IR?); - [ ] to/from_pandas that doesn't go through Spark (Arrow?). VEP and Nirvana should be converted to Pipeline when it is ready. I don't have a plan for ImportVCFs yet. Not counting linear algebra. Not counting making the Python front-end Java-free, those are other threads.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5197
https://github.com/hail-is/hail/issues/5199:297,Availability,down,download,297,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:207,Deployability,install,install,207,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:84,Integrability,depend,dependencies,84,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:193,Integrability,depend,dependencies,193,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:362,Integrability,depend,dependencies,362,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:72,Testability,test,tests,72,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/issues/5199:380,Testability,test,test,380,"- [ ] use make dry-runs w/ git to ensure that CI executes exactly those tests whose dependencies have changed since the last commit. - [ ] banish `archiveZip`. create `whl` files without spark dependencies, install those on the leader node (do we need to specify the jar separately still?). - [ ] download plink, qctool, and R packages in hail/Makefile and make dependencies for `test`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199
https://github.com/hail-is/hail/pull/5200:99,Availability,error,error,99,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5200
https://github.com/hail-is/hail/pull/5200:7,Deployability,pipeline,pipeline,7,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5200
https://github.com/hail-is/hail/pull/5200:129,Deployability,pipeline,pipeline,129,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5200
https://github.com/hail-is/hail/pull/5200:169,Testability,log,log,169,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5200
https://github.com/hail-is/hail/pull/5208:58,Deployability,pipeline,pipeline,58,"Relevant to CNV work. Should hook up nicely with Jackie's pipeline work too, when that's done!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208
https://github.com/hail-is/hail/issues/5212:236,Availability,error,error,236,"### Hail version: 0.2.8. ### What you did:. ```python; smt = hl.methods.balding_nichols_model(3, 100, 200); smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:2415,Availability,Error,Error,2415,"ark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:3156,Availability,Error,Error,3156,"lf.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:3253,Availability,Error,ErrorHandling,3253,"3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:3279,Availability,Error,ErrorHandling,3279,"il/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at is.hail.table.Table.x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:8060,Availability,Error,Error,8060,"leIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at is.hail.table.Table.x$3(Table.scala:211); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.8-70304a52d33d; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:242,Integrability,message,messages,242,"### Hail version: 0.2.8. ### What you did:. ```python; smt = hl.methods.balding_nichols_model(3, 100, 200); smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:990,Integrability,wrap,wrapper,990,"nnotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:1041,Integrability,wrap,wrapper,1041,"nnotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:1247,Integrability,wrap,wrapper,1247,"nnotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); smt.make_table().to_pandas(); ```; ### What went wrong (all error messages here, including the full java stack trace):. ```; 2019-01-26 19:03:28 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 200 variants...; 2019-01-26 19:03:28 Hail: INFO: Coerced sorted dataset; 2019-01-26 19:03:29 Hail: INFO: Coerced sorted dataset; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-59-9addf4eaf59b> in <module>; 1 smt = hl.methods.balding_nichols_model(3, 100, 200); 2 smt = smt.annotate_cols(s=hl.str(smt.sample_idx)).key_cols_by('s'); ----> 3 smt.make_table().to_pandas(). <decorator-gen-1366> in to_pandas(self, flatten). ~/anaconda3/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda3/lib/python3.6/site-packages/hail/table.py in to_pandas(self, flatten); 2703 ; 2704 """"""; -> 2705 return Env.spark_backend('to_pandas').to_pandas(self, flatten); 2706 ; 2707 @staticmethod. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_pandas(self, t, flatten); 66 ; 67 def to_pandas(self, t, flatten):; ---> 68 return self.to_spark(t, flatten).toPandas(); 69 ; 70 def from_pandas(self, df, key):. ~/anaconda3/lib/python3.6/site-packages/hail/backend/backend.py in to_spark(self, t, flatten); 63 if flatten:; 64 t = t.flatten(); ---> 65 return pyspark.sql.DataFrame(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:2611,Testability,Assert,AssertionError,2611,"me(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:2627,Testability,assert,assertion,2627,"me(t._jt.toDF(Env.hc()._jsql_context), Env.sql_context()); 66 ; 67 def to_pandas(self, t, flatten):. ~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}. Java stack trace:; is.hail.utils.HailException: Error while typechecking IR:; (MakeStruct; (bn; (GetField bn; (Ref global)))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:16); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:5094,Testability,Assert,AssertionError,5094,"e.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:60); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:237); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:5110,Testability,assert,assertion,5110,"e.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:60); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:237); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:5609,Testability,assert,assert,5609,"java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:60); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:237); 	at is.hail.expr.ir.TypeCheck$.is$hail$expr$ir$TypeCheck$$check$1(TypeCheck.scala:22); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$$anonfun$_apply$18.apply(TypeCheck.scala:213); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.TypeCheck$._apply(TypeCheck.scala:213); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:14); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:32); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:856); 	at is.hail.expr.ir.Tabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:8075,Testability,Assert,AssertionError,8075,"leIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at is.hail.table.Table.x$3(Table.scala:211); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.8-70304a52d33d; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/issues/5212:8091,Testability,assert,assertion,8091,"leIR.scala:856); 	at is.hail.expr.ir.TableMapGlobals$$anonfun$38.apply(TableIR.scala:846); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.annotations.Region$.scoped(Region.scala:13); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:846); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:237); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:838); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:696); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:42); 	at is.hail.table.Table.x$3$lzycompute(Table.scala:211); 	at is.hail.table.Table.x$3(Table.scala:211); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:211); 	at is.hail.table.Table.rvd(Table.scala:211); 	at is.hail.table.Table.toDF(Table.scala:455); 	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.8-70304a52d33d; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__uid_882:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; expect: Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean},__cols:Array[Struct{sample_idx:Int32,pop:Int32,s:String}]}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212
https://github.com/hail-is/hail/pull/5213:8,Deployability,deploy,deployed,8,"Not yet deployed by default. Builds Jupyter image that runs against apiserver by default. This will be used by the notebook service to spin up a demo notebook. The make target `run-hail-jupyter-pod` launches a single pod running the image by hand. Image includes Hadoop Google Storage connector. Deployment mounts a secret with a service account key to access files which can access the bucket gs://haas-scratch. This completes the backend work for the Feb 5 data meeting demo. Round trip for a trivial job is about 80ms. From the Jupyter notebook running agains the apiserver:. ```; %%time; hl.utils.range_matrix_table(346, 100).count(); CPU times: user 20.6 ms, sys: 5.17 ms, total: 25.8 ms; Wall time: 81.2 ms; ```. For a job that hits Spark, around 300ms:. ```; mt = hl.utils.range_matrix_table(346, 100); mt = mt.annotate_rows(x = mt.row_idx*mt.row_idx); t = mt.rows(). %%time; t.aggregate(hl.agg.sum(t.x)); CPU times: user 12.6 ms, sys: 1.21 ms, total: 13.8 ms; Wall time: 304 ms; 13747445; ```. Things that hit Google storage are noticeably slower:. ```; %%time; mt = hl.read_matrix_table('gs://haas-scratch/sample.mt'); mt.aggregate_rows(hl.agg.sum(hl.len(mt.alleles))); CPU times: user 28.3 ms, sys: 2.63 ms, total: 30.9 ms; Wall time: 1.58 s; ```. I think we can speed this up by caching read IR. Also, the RVD metadata is stored in a separate file, so executing the read requires an additional read besides reading the actual data file. We might want to rethink our directory layout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5213
https://github.com/hail-is/hail/pull/5213:296,Deployability,Deploy,Deployment,296,"Not yet deployed by default. Builds Jupyter image that runs against apiserver by default. This will be used by the notebook service to spin up a demo notebook. The make target `run-hail-jupyter-pod` launches a single pod running the image by hand. Image includes Hadoop Google Storage connector. Deployment mounts a secret with a service account key to access files which can access the bucket gs://haas-scratch. This completes the backend work for the Feb 5 data meeting demo. Round trip for a trivial job is about 80ms. From the Jupyter notebook running agains the apiserver:. ```; %%time; hl.utils.range_matrix_table(346, 100).count(); CPU times: user 20.6 ms, sys: 5.17 ms, total: 25.8 ms; Wall time: 81.2 ms; ```. For a job that hits Spark, around 300ms:. ```; mt = hl.utils.range_matrix_table(346, 100); mt = mt.annotate_rows(x = mt.row_idx*mt.row_idx); t = mt.rows(). %%time; t.aggregate(hl.agg.sum(t.x)); CPU times: user 12.6 ms, sys: 1.21 ms, total: 13.8 ms; Wall time: 304 ms; 13747445; ```. Things that hit Google storage are noticeably slower:. ```; %%time; mt = hl.read_matrix_table('gs://haas-scratch/sample.mt'); mt.aggregate_rows(hl.agg.sum(hl.len(mt.alleles))); CPU times: user 28.3 ms, sys: 2.63 ms, total: 30.9 ms; Wall time: 1.58 s; ```. I think we can speed this up by caching read IR. Also, the RVD metadata is stored in a separate file, so executing the read requires an additional read besides reading the actual data file. We might want to rethink our directory layout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5213
https://github.com/hail-is/hail/pull/5213:353,Security,access,access,353,"Not yet deployed by default. Builds Jupyter image that runs against apiserver by default. This will be used by the notebook service to spin up a demo notebook. The make target `run-hail-jupyter-pod` launches a single pod running the image by hand. Image includes Hadoop Google Storage connector. Deployment mounts a secret with a service account key to access files which can access the bucket gs://haas-scratch. This completes the backend work for the Feb 5 data meeting demo. Round trip for a trivial job is about 80ms. From the Jupyter notebook running agains the apiserver:. ```; %%time; hl.utils.range_matrix_table(346, 100).count(); CPU times: user 20.6 ms, sys: 5.17 ms, total: 25.8 ms; Wall time: 81.2 ms; ```. For a job that hits Spark, around 300ms:. ```; mt = hl.utils.range_matrix_table(346, 100); mt = mt.annotate_rows(x = mt.row_idx*mt.row_idx); t = mt.rows(). %%time; t.aggregate(hl.agg.sum(t.x)); CPU times: user 12.6 ms, sys: 1.21 ms, total: 13.8 ms; Wall time: 304 ms; 13747445; ```. Things that hit Google storage are noticeably slower:. ```; %%time; mt = hl.read_matrix_table('gs://haas-scratch/sample.mt'); mt.aggregate_rows(hl.agg.sum(hl.len(mt.alleles))); CPU times: user 28.3 ms, sys: 2.63 ms, total: 30.9 ms; Wall time: 1.58 s; ```. I think we can speed this up by caching read IR. Also, the RVD metadata is stored in a separate file, so executing the read requires an additional read besides reading the actual data file. We might want to rethink our directory layout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5213
https://github.com/hail-is/hail/pull/5213:376,Security,access,access,376,"Not yet deployed by default. Builds Jupyter image that runs against apiserver by default. This will be used by the notebook service to spin up a demo notebook. The make target `run-hail-jupyter-pod` launches a single pod running the image by hand. Image includes Hadoop Google Storage connector. Deployment mounts a secret with a service account key to access files which can access the bucket gs://haas-scratch. This completes the backend work for the Feb 5 data meeting demo. Round trip for a trivial job is about 80ms. From the Jupyter notebook running agains the apiserver:. ```; %%time; hl.utils.range_matrix_table(346, 100).count(); CPU times: user 20.6 ms, sys: 5.17 ms, total: 25.8 ms; Wall time: 81.2 ms; ```. For a job that hits Spark, around 300ms:. ```; mt = hl.utils.range_matrix_table(346, 100); mt = mt.annotate_rows(x = mt.row_idx*mt.row_idx); t = mt.rows(). %%time; t.aggregate(hl.agg.sum(t.x)); CPU times: user 12.6 ms, sys: 1.21 ms, total: 13.8 ms; Wall time: 304 ms; 13747445; ```. Things that hit Google storage are noticeably slower:. ```; %%time; mt = hl.read_matrix_table('gs://haas-scratch/sample.mt'); mt.aggregate_rows(hl.agg.sum(hl.len(mt.alleles))); CPU times: user 28.3 ms, sys: 2.63 ms, total: 30.9 ms; Wall time: 1.58 s; ```. I think we can speed this up by caching read IR. Also, the RVD metadata is stored in a separate file, so executing the read requires an additional read besides reading the actual data file. We might want to rethink our directory layout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5213
https://github.com/hail-is/hail/pull/5215:929,Availability,mask,mask,929,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1670,Availability,error,errors,1670," Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:310,Deployability,update,updates,310,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:353,Deployability,Deploy,Deploy,353,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:378,Deployability,Deploy,Deploy,378,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1838,Deployability,patch,patch,1838,"instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteris",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2363,Deployability,update,updates,2363,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2407,Deployability,update,update,2407,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2708,Deployability,integrat,integration,2708,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1608,Energy Efficiency,green,greenlet,1608,"ser ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1822,Energy Efficiency,green,greenlet,1822,"instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteris",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:710,Integrability,rout,route,710,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1162,Integrability,rout,route,1162,"y. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C librar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:1313,Integrability,depend,dependent,1313,"ns, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2708,Integrability,integrat,integration,2708,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2840,Integrability,rout,routing,2840,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:763,Performance,latency,latency,763,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:952,Performance,latency,latency,952,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2778,Performance,perform,performance,2778,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2848,Performance,latency,latency,2848,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:223,Security,validat,validated,223,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:102,Testability,test,testing,102,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:557,Testability,Test,Test,557,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2425,Testability,test,test,2425,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2608,Testability,test,testing,2608,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5215:2720,Testability,test,tests,2720,"ill look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be. However, that will be next implementation, for velocity/documentation reasons. . A better, third, more unwieldy solution is to use the C library (MySQLDb) establish a connection pool, N threads, and use deqeue. No implementation for waiting state, but will be the same; effectively, browser will connect to notebook socket server, notebook will issue periodic updates. Same thing, just . Need help/ok to update gateway to test this in production environment. Preferably, as I mentioned to Dan, we would have a staging gateway, which *.dev.hail.is points to, and which is used for more than automated / ci testing, allowing for human interaction, which by some likelihood catches classes of bugs that unit/integration tests do not, and allows us to explore production context performance characteristics (for instance k8 internal network routing latency). cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215
https://github.com/hail-is/hail/pull/5220:1834,Deployability,Update,Update,1834,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:1125,Energy Efficiency,reduce,reduces,1125,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:440,Integrability,wrap,wraps,440,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:1329,Integrability,wrap,wrap,1329,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:1435,Integrability,interface,interface,1435,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:1575,Integrability,interface,interface,1575,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:638,Performance,perform,perform,638,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/pull/5220:1918,Testability,Test,Test,1918,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5220
https://github.com/hail-is/hail/issues/5221:172,Availability,error,error,172,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:1952,Availability,Error,Error,1952,"func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 228 except pyspark.sql.utils.CapturedException as e:; 229 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: MalformedInputException: Input length = 1. Java stack trace:; java.nio.charset.MalformedInputException: Input length = 1; 	at java.nio.charset.CoderResult.throwException(CoderResult.java:281); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:339); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at java.io.BufferedReader.fill(BufferedReader.java:161); 	at java.io.BufferedReader.readLine(BufferedReader.java:324); 	at java.io.BufferedReader.readLine(BufferedReader.java:389); 	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:72); 	at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:836); 	at scala.collection.Iterator$$anon$11.hasNext(Iterato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:4964,Availability,Error,Error,4964,$class.isEmpty(Iterator.scala:330); 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336); 	at is.hail.utils.TextTableReader$$anonfun$14.apply(TextTableReader.scala:218); 	at is.hail.utils.TextTableReader$$anonfun$14.apply(TextTableReader.scala:215); 	at is.hail.utils.richUtils.RichHadoopConfiguration$$anonfun$readLines$extension$1.apply(RichHadoopConfiguration.scala:301); 	at is.hail.utils.richUtils.RichHadoopConfiguration$$anonfun$readLines$extension$1.apply(RichHadoopConfiguration.scala:292); 	at is.hail.utils.package$.using(package.scala:587); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:285); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readLines$extension(RichHadoopConfiguration.scala:292); 	at is.hail.utils.TextTableReader$.read(TextTableReader.scala:215); 	at is.hail.HailContext$$anonfun$importTables$3.apply(HailContext.scala:516); 	at is.hail.HailContext$$anonfun$importTables$3.apply(HailContext.scala:518); 	at is.hail.HailContext.maybeGZipAsBGZip(HailContext.scala:586); 	at is.hail.HailContext.importTables(HailContext.scala:515); 	at is.hail.HailContext.importTable(HailContext.scala:477); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.8-70304a52d33d; Error summary: MalformedInputException: Input length = 1; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:178,Integrability,message,message,178,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:777,Integrability,wrap,wrapper,777,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:828,Integrability,wrap,wrapper,828,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5221:1034,Integrability,wrap,wrapper,1034,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221
https://github.com/hail-is/hail/issues/5222:1276,Integrability,wrap,wrapper,1276,"re-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:1327,Integrability,wrap,wrapper,1327,"re-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:1533,Integrability,wrap,wrapper,1533,"re-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:2568,Integrability,wrap,wrapper,2568,"; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-504> in __getitem__(self, item). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1316 """"""; 1317 if isinstance(item, str):; -> 1318 return self._get_field(item); 1319 else:; 1320 return self._get_field(self.dtype.fields[item]). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in _get_field(self, item); 1281 return self._fields[item]; 1282 else:; -> 1283 raise KeyError(get_nice_field_error(self, item)); 1284 ; 1285 def __getattr__(self, item):. KeyError: ""StructExpression instance has no field 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;...'\n Did you mean:\n 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)'\n Hint: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:2619,Integrability,wrap,wrapper,2619,"; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-504> in __getitem__(self, item). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1316 """"""; 1317 if isinstance(item, str):; -> 1318 return self._get_field(item); 1319 else:; 1320 return self._get_field(self.dtype.fields[item]). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in _get_field(self, item); 1281 return self._fields[item]; 1282 else:; -> 1283 raise KeyError(get_nice_field_error(self, item)); 1284 ; 1285 def __getattr__(self, item):. KeyError: ""StructExpression instance has no field 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;...'\n Did you mean:\n 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)'\n Hint: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:2825,Integrability,wrap,wrapper,2825,"; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in <listcomp>(.0); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in fields]; 1228 right_align = [hl.expr.types.is_numeric(t.row[f].dtype) for f in fields]; 1229 . </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-504> in __getitem__(self, item). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in __getitem__(self, item); 1316 """"""; 1317 if isinstance(item, str):; -> 1318 return self._get_field(item); 1319 else:; 1320 return self._get_field(self.dtype.fields[item]). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in _get_field(self, item); 1281 return self._fields[item]; 1282 else:; -> 1283 raise KeyError(get_nice_field_error(self, item)); 1284 ; 1285 def __getattr__(self, item):. KeyError: ""StructExpression instance has no field 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;...'\n Did you mean:\n 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)'\n Hint: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/issues/5222:561,Performance,Load,Loading,561,"Consider this snippet; ```; broken_ht = hl.import_table('../data/bikes.csv'); # Look at the first 3 rows; broken_ht.show(3); ```; [`bikes.csv`](https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv), but I removed the diacritic characters (AFAICT), the first line is now this:; ```; Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5222
https://github.com/hail-is/hail/pull/5226:83,Modifiability,variab,variable,83,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5226
https://github.com/hail-is/hail/pull/5226:185,Modifiability,Variab,Variables,185,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5226
https://github.com/hail-is/hail/pull/5226:210,Modifiability,Variab,Variables,210,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5226
https://github.com/hail-is/hail/pull/5226:27,Performance,cache,cache,27,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5226
https://github.com/hail-is/hail/pull/5227:136,Deployability,deploy,deploy,136,FYI @tpoterba. See https://github.com/pypa/pip/issues/6197 and proposed fix PR https://github.com/pypa/pip/pull/6219. This resolves the deploy issue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5227
https://github.com/hail-is/hail/pull/5228:289,Availability,failure,failure,289,"There is still work to do here, but it is now complete enough that InterpretSuite can be run properly on a minimal example. Current TODOs:; - [x] Add C++ emit; - [x] Add Python api (experimental, for now); - [x] Proper type checking in python? *yes, but no type inference*; - [ ] Test ALL failure pathways; - [x] Mismatched Number of args between `Loop` and matching `Recur`; - [x] Mismatched types of args between `Loop` and matching `Recur`; - [ ] Infinite loop detection; - [ ] Not tail recursive detection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5228
https://github.com/hail-is/hail/pull/5228:464,Safety,detect,detection,464,"There is still work to do here, but it is now complete enough that InterpretSuite can be run properly on a minimal example. Current TODOs:; - [x] Add C++ emit; - [x] Add Python api (experimental, for now); - [x] Proper type checking in python? *yes, but no type inference*; - [ ] Test ALL failure pathways; - [x] Mismatched Number of args between `Loop` and matching `Recur`; - [x] Mismatched types of args between `Loop` and matching `Recur`; - [ ] Infinite loop detection; - [ ] Not tail recursive detection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5228
https://github.com/hail-is/hail/pull/5228:500,Safety,detect,detection,500,"There is still work to do here, but it is now complete enough that InterpretSuite can be run properly on a minimal example. Current TODOs:; - [x] Add C++ emit; - [x] Add Python api (experimental, for now); - [x] Proper type checking in python? *yes, but no type inference*; - [ ] Test ALL failure pathways; - [x] Mismatched Number of args between `Loop` and matching `Recur`; - [x] Mismatched types of args between `Loop` and matching `Recur`; - [ ] Infinite loop detection; - [ ] Not tail recursive detection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5228
https://github.com/hail-is/hail/pull/5228:280,Testability,Test,Test,280,"There is still work to do here, but it is now complete enough that InterpretSuite can be run properly on a minimal example. Current TODOs:; - [x] Add C++ emit; - [x] Add Python api (experimental, for now); - [x] Proper type checking in python? *yes, but no type inference*; - [ ] Test ALL failure pathways; - [x] Mismatched Number of args between `Loop` and matching `Recur`; - [x] Mismatched types of args between `Loop` and matching `Recur`; - [ ] Infinite loop detection; - [ ] Not tail recursive detection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5228
https://github.com/hail-is/hail/pull/5229:25,Availability,Failure,Failure,25,`to_json` was broken on `Failure` objects,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5229
https://github.com/hail-is/hail/pull/5230:85,Deployability,deploy,deploy,85,"Sleeping used to work when we had a small number of PRs and other jobs. Now, test-CI deploy jobs need to wait a long time to start running. We should have always been polling for CI to be finished deploying, I was just lazy. Fixed now. Also should free up @jigold 's PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5230
https://github.com/hail-is/hail/pull/5230:197,Deployability,deploy,deploying,197,"Sleeping used to work when we had a small number of PRs and other jobs. Now, test-CI deploy jobs need to wait a long time to start running. We should have always been polling for CI to be finished deploying, I was just lazy. Fixed now. Also should free up @jigold 's PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5230
https://github.com/hail-is/hail/pull/5230:77,Testability,test,test-CI,77,"Sleeping used to work when we had a small number of PRs and other jobs. Now, test-CI deploy jobs need to wait a long time to start running. We should have always been polling for CI to be finished deploying, I was just lazy. Fixed now. Also should free up @jigold 's PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5230
https://github.com/hail-is/hail/pull/5231:111,Integrability,depend,dependencies,111,"Jobs are now only allowed to have parents in the same batch. This is necessary for a long term goal: inter-job dependencies. We plan to temporarily store the output of a job. We need to know when a job can no longer have children (ergo it is safe to delete the job's output). We will add a `batch.close` which prevents a batch from receiving new children. If batch jobs may only depend on other jobs in the batch, then a `close` means that we can delete any output from a job whose children have already read its output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5231
https://github.com/hail-is/hail/pull/5231:379,Integrability,depend,depend,379,"Jobs are now only allowed to have parents in the same batch. This is necessary for a long term goal: inter-job dependencies. We plan to temporarily store the output of a job. We need to know when a job can no longer have children (ergo it is safe to delete the job's output). We will add a `batch.close` which prevents a batch from receiving new children. If batch jobs may only depend on other jobs in the batch, then a `close` means that we can delete any output from a job whose children have already read its output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5231
https://github.com/hail-is/hail/pull/5231:242,Safety,safe,safe,242,"Jobs are now only allowed to have parents in the same batch. This is necessary for a long term goal: inter-job dependencies. We plan to temporarily store the output of a job. We need to know when a job can no longer have children (ergo it is safe to delete the job's output). We will add a `batch.close` which prevents a batch from receiving new children. If batch jobs may only depend on other jobs in the batch, then a `close` means that we can delete any output from a job whose children have already read its output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5231
https://github.com/hail-is/hail/pull/5232:109,Safety,safe,safely,109,"When a batch is closed the DAG is complete and we do not permit any new jobs to be added. This permits us to safely clean up any job garbage when a job's children are finished. (We don't do any of that right now, but this is necessary for that).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5232
https://github.com/hail-is/hail/pull/5233:289,Energy Efficiency,schedul,scheduler,289,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/pull/5233:364,Energy Efficiency,schedul,scheduled,364,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/pull/5233:423,Energy Efficiency,schedul,scheduled,423,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/pull/5233:138,Integrability,depend,depend,138,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/pull/5233:456,Security,expose,exposes,456,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/pull/5233:534,Testability,test,tests,534,"Batch threads are closed after at most 30 minutes (meaning no more jobs may be submitted in that batch; ergo, crucially, no more jobs may depend on the output of jobs in the batch). The user can specify a shorter time-to-live via the `ttl` parameter. The batch server achieves this via a [scheduler](https://docs.python.org/3/library/sched.html) thread which runs scheduled events. When a batch is created a close event is scheduled for its TTL. This also exposes `is_open` in the JSON response to `GET /batches/<batch_id>` which the tests use to verify a batch has been closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5233
https://github.com/hail-is/hail/issues/5236:50,Security,secur,secure,50,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/issues/5236:79,Security,attack,attackers,79,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/issues/5236:120,Security,expose,expose,120,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/issues/5236:305,Security,firewall,firewall-rules,305,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/issues/5236:340,Security,expose,expose-jupyter,340,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/issues/5236:553,Testability,log,logs,553,"Jupyter notebook by default uses random tokens to secure itself against public attackers. Let's just use that token and expose jupyter publicly. cloudtools can open the port:; ```; gcloud compute instances add-tags CLUSTER_NAME-m \; --zone [ZONE] \; --tags cloudtools-CLUSTER_NAME-jupyter; gcloud compute firewall-rules create CLUSTER_NAME-expose-jupyter \; --action allow \; --direction ingress \; --rules tcp:8123 \; --priority 1 \; --target-tags cloudtools-CLUSTER_NAME-jupyter; ```. Then cloudtools can ssh there and read the token from the jupyter logs, then it can direct the user to the instance's public IP (look at `gcloud compute instances describe CLUSTER_NAME-m`) with the appropriate token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236
https://github.com/hail-is/hail/pull/5237:35,Testability,test,testing,35,"This is only relevant to the local testing case. `pylint` is really slow (even if you run it on one file, which was my first attempt to mitigate slowness).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5237
https://github.com/hail-is/hail/issues/5238:104,Performance,cache,cache-dir,104,"Just a heads up, any image that relies on pip, may want to pin to version 18.1 (if you want to use --no-cache-dir, as was done in notebook). https://github.com/pypa/pip/issues/6197",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5238
https://github.com/hail-is/hail/issues/5240:167,Testability,Assert,AssertionError,167,"```python; >>> ht = hl.utils.range_table(1).annotate(alleles = ['A', 'T'], gt = hl.call(0, 0)); >>> ht.aggregate(hl.agg.call_stats(ht.gt, ht.alleles)); ```. ```; ...; AssertionError: row not found in {'global': dtype('struct{}')}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5240
https://github.com/hail-is/hail/pull/5242:315,Deployability,deploy,deployed,315,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:49,Performance,cache,cache,49,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:201,Performance,optimiz,optimize,201,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:369,Performance,perform,performance,369,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:450,Performance,response time,response time,450,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:494,Performance,perform,performance,494,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:571,Testability,benchmark,benchmarks,571,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5242:433,Usability,simpl,simple,433,"Use Sanic + ujson instead of Flask + flask.json, cache more, and fix missing favicon in HTML templates. Sanic should be 3+x faster than Flask, and ujson ~2-3x faster than the standard json lib. I also optimize away the unnecessary re-generation of user_data (via get_users()) and json equivalent for /json. This is deployed currently on scorecard.hail.is, and improves performance by ~20%, even for 1 single connection, and for that simple workload (response time from ~50ms to ~40ms). Besides performance ( https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/ ) Sanic also builds in a production-oriented web server, so need for WSGI or aWSGI, Gunicorn, etc. Can easily run multiple workers if desired. ```python; app.run(host='0.0.0.0', port=5000, workers=4); ```. This serves as a demonstration or migration to faster web frameworks, and in particular to uvloop-based asyncio implementations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242
https://github.com/hail-is/hail/pull/5243:71,Testability,test,test,71,for use by new notebook server. Let's make sure this is working in the test setup before we merge it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243
https://github.com/hail-is/hail/pull/5246:24,Availability,error,error,24,"Danfeng was seeing this error trying to unpersist a matrix table:. ```; >>> mt = hl.utils.range_matrix_table(10, 10); >>> mt = mt.persist(); >>> mt = mt.unpersist(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/wang/code/hail/hail/python/hail/matrixtable.py"", line 2896, in unpersist; return Env.backend().unpersist_matrix_table(self); TypeError: unpersist_matrix_table() missing 1 required positional argument: 'storage_level'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5246
https://github.com/hail-is/hail/pull/5248:41,Testability,test,test,41,"@cseed I'm not actually sure how best to test that the regions are being handled correctly in the generated code; I've manually checked the output of a few of the tests, but unsure how to do it in general. (I also deleted TableEmit.scala and corresponding tests, since they're dead code after the SparkBackend stuff went in.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5248
https://github.com/hail-is/hail/pull/5248:163,Testability,test,tests,163,"@cseed I'm not actually sure how best to test that the regions are being handled correctly in the generated code; I've manually checked the output of a few of the tests, but unsure how to do it in general. (I also deleted TableEmit.scala and corresponding tests, since they're dead code after the SparkBackend stuff went in.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5248
https://github.com/hail-is/hail/pull/5248:256,Testability,test,tests,256,"@cseed I'm not actually sure how best to test that the regions are being handled correctly in the generated code; I've manually checked the output of a few of the tests, but unsure how to do it in general. (I also deleted TableEmit.scala and corresponding tests, since they're dead code after the SparkBackend stuff went in.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5248
https://github.com/hail-is/hail/issues/5252:341,Modifiability,enhance,enhanced,341,"https://github.com/hail-is/hail/pull/5196 introduced a bug where `hl.get_reference('GRCh37')` only works *after* a call to `hl.init()`. ```; (hail) dking@wmb16-359 # ipython; import Python 3.6.7 | packaged by conda-forge | (default, Nov 20 2018, 18:37:09) ; Type 'copyright', 'credits' or 'license' for more information; IPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; In [2]: hl.get_reference('GRCh37'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-2-3880f3d97a41> in <module>(); ----> 1 hl.get_reference('GRCh37'). ~/anaconda2/envs/hail/lib/python3.6/site-packages/hail/context.py in get_reference(name); 308 return default_reference(); 309 else:; --> 310 return ReferenceGenome._references[name]; 311 ; 312 . KeyError: 'GRCh37'; ```. This issue is considered fixed when:; - [ ] there is a test that would fail against current master `dcf43490c732`; - [ ] there is a fix for the issue. First reported by Claudia Dastmalchi [on Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/KeyError.20for.20get_reference).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5252
https://github.com/hail-is/hail/issues/5252:930,Testability,test,test,930,"https://github.com/hail-is/hail/pull/5196 introduced a bug where `hl.get_reference('GRCh37')` only works *after* a call to `hl.init()`. ```; (hail) dking@wmb16-359 # ipython; import Python 3.6.7 | packaged by conda-forge | (default, Nov 20 2018, 18:37:09) ; Type 'copyright', 'credits' or 'license' for more information; IPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; In [2]: hl.get_reference('GRCh37'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-2-3880f3d97a41> in <module>(); ----> 1 hl.get_reference('GRCh37'). ~/anaconda2/envs/hail/lib/python3.6/site-packages/hail/context.py in get_reference(name); 308 return default_reference(); 309 else:; --> 310 return ReferenceGenome._references[name]; 311 ; 312 . KeyError: 'GRCh37'; ```. This issue is considered fixed when:; - [ ] there is a test that would fail against current master `dcf43490c732`; - [ ] there is a fix for the issue. First reported by Claudia Dastmalchi [on Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/KeyError.20for.20get_reference).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5252
https://github.com/hail-is/hail/pull/5253:41,Security,authoriz,authorized,41,"This is the last change needed to enable authorized notebooks. Completely handles all N requests after redirect to Jupyter server, including all requests in subsequent within-Jupyter operations such as notebook creation (which still pass through our proxy)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5253
https://github.com/hail-is/hail/issues/5254:25,Usability,guid,guides,25,https://hail.is/docs/0.2/guides/genetics.html#filter-loci-by-a-list-of-locus-intervals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5254
https://github.com/hail-is/hail/issues/5266:140,Availability,error,error,140,"### Hail version: ; 0.2.9-8588a25687af. ### What you did: ; tbl.export(""filename"", header=False, types_file=None). ### What went wrong (all error messages here, including the full java stack trace):; No errors, but two files were written to my working directory; None and .None.crc. The file contains column types as if the 'types_file = None' was interpreted as 'types_file = ""None""'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5266
https://github.com/hail-is/hail/issues/5266:203,Availability,error,errors,203,"### Hail version: ; 0.2.9-8588a25687af. ### What you did: ; tbl.export(""filename"", header=False, types_file=None). ### What went wrong (all error messages here, including the full java stack trace):; No errors, but two files were written to my working directory; None and .None.crc. The file contains column types as if the 'types_file = None' was interpreted as 'types_file = ""None""'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5266
https://github.com/hail-is/hail/issues/5266:146,Integrability,message,messages,146,"### Hail version: ; 0.2.9-8588a25687af. ### What you did: ; tbl.export(""filename"", header=False, types_file=None). ### What went wrong (all error messages here, including the full java stack trace):; No errors, but two files were written to my working directory; None and .None.crc. The file contains column types as if the 'types_file = None' was interpreted as 'types_file = ""None""'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5266
https://github.com/hail-is/hail/issues/5269:177,Availability,error,error,177,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:633,Deployability,deploy,deployment-,633,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:686,Deployability,deploy,deployment-,686,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:828,Deployability,deploy,deployment-,828,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:882,Deployability,deploy,deployment-,882,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:1175,Deployability,deploy,deployment-,1175,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:1229,Deployability,deploy,deployment-,1229,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:1568,Deployability,deploy,deployment-,1568,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:1620,Deployability,deploy,deployment-,1620,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:1858,Deployability,deploy,deployment-,1858,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:96,Energy Efficiency,schedul,schedule,96,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:183,Integrability,message,messages,183,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:735,Testability,test,test-,735,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/issues/5269:236,Usability,Simpl,Simply,236,"###Hail version; N/A Kubernetes v1 API, cluster version 1.10.11. ### What you did; Attempted to schedule a pod through app.hail.is. Waited ~20 minutes. ### What went wrong (all error messages here, including the full java stack trace); Simply stuck in Not PodScheduled (status.condition contains an entry of {status: False, type: PodScheduled} ). This status is also verified using kubectl get pods -w. Total number of pods did not seem onerous by quantity alone, so this must be an issue of resource utilization by some of these pods. ```sh; NAME READY STATUS RESTARTS AGE; apiserver-8658d59d48-r8p6w 1/1 Running 0 9d; auth-gateway-deployment-7d7cf8846f-l5m9b 1/1 Running 0 14h; batch-deployment-6448f84d9c-gxn2c 1/1 Running 0 1h; dk-test-58dffcd944-9xkkx 1/1 Running 0 11d; frontend-766c875db4-cmpvx 1/1 Running 0 8d; gateway-deployment-78c4dd64f5-tdnnc 1/1 Running 0 1h; hail-ci-deployment-5744fd6964-s29xb 1/1 Running 0 1h; image-fetcher-bkpcc 1/1 Running 0 23m; image-fetcher-gb9rs 1/1 Running 0 26m; image-fetcher-glj5p 1/1 Running 0 25m; image-fetcher-kjd7z 1/1 Running 0 23m; image-fetcher-vhv74 1/1 Running 0 25m; image-fetcher-zppvc 1/1 Running 0 24m; notebook-api-deployment-7bb85bfd-z6mvp 1/1 Running 0 12h; notebook-deployment-8546dbcb7c-zfc4r 1/1 Running 0 1h; notebook-worker-2lt2l 1/1 Running 0 46m; notebook-worker-77nqq 1/1 Running 0 1h; notebook-worker-fljx6 1/1 Running 0 3h; notebook-worker-gm6lz 1/1 Running 0 36m; notebook-worker-kj7bb 1/1 Running 0 3h; notebook-worker-n8dgv 0/1 Pending 0 4m; notebook-worker-pshdf 1/1 Running 0 35m; scorecard-deployment-654f774444-vwpzr 1/1 Running 0 51m; site-deployment-6789bd6c5b-lxbxk 1/1 Running 0 51m; spark-master-6f7678b449-jcbnp 1/1 Running 0 9d; spark-worker-569866dff7-l452k 1/1 Running 0 9d; spark-worker-569866dff7-xzmx4 1/1 Running 0 9d; upload-658d7f8c7d-gvj4h 1/1 Running 0 51m; web-deployment-bc6497cdb-qfc9g 1/1 Running 0 2h; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269
https://github.com/hail-is/hail/pull/5274:18,Safety,safe,safe,18,"The `distinct` is safe to remove since `ToDict` will do its own distinct. The only occasion when we'd want the distinct there is if we have tons of highly duplicated keys in `table`, which is very rare. Otherwise, removing the distinct and inserting an unkey will generate better IRs with fewer shuffles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5274
https://github.com/hail-is/hail/pull/5275:157,Testability,test,tests,157,"Fixes #5106. (I realized halfway through that cseed fixed the problem with the artifacts not getting copied in a PR a few weeks ago, but I think the codegen tests should probably be writing to their own directory and not overwriting other tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5275
https://github.com/hail-is/hail/pull/5275:239,Testability,test,tests,239,"Fixes #5106. (I realized halfway through that cseed fixed the problem with the artifacts not getting copied in a PR a few weeks ago, but I think the codegen tests should probably be writing to their own directory and not overwriting other tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5275
https://github.com/hail-is/hail/issues/5277:262,Availability,Error,ErrorHandling,262,"```; In [1]: hl.utils.range_table(1).annotate(**{'a b c' : 5}).order_by('a b c')._force_count(); ```. ```. is.hail.utils.HailException: invalid struct filter operation: fields [ ``, ` b c` ] not found; Existing struct fields: [ idx, `a b c` ]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.types.virtual.TStruct.filterSet(TStruct.scala:295); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5277
https://github.com/hail-is/hail/issues/5277:288,Availability,Error,ErrorHandling,288,"```; In [1]: hl.utils.range_table(1).annotate(**{'a b c' : 5}).order_by('a b c')._force_count(); ```. ```. is.hail.utils.HailException: invalid struct filter operation: fields [ ``, ` b c` ] not found; Existing struct fields: [ idx, `a b c` ]; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.types.virtual.TStruct.filterSet(TStruct.scala:295); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5277
https://github.com/hail-is/hail/pull/5280:5,Performance,perform,performs,5,This performs the operations described in the `NOTE` at the end of `vcf_combiner.py`. The only thing that I feel may be confusing is that we recompute the `DP` info fields as the sum of the `DP` entry fields. We do this as `densify` (currently a placeholder) may populate a previously missing entry.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5280
https://github.com/hail-is/hail/pull/5281:236,Deployability,pipeline,pipeline,236,"I made `_prev_nonnull` private since it's pretty specific to this use case, but I can imagine we'll want to generalize it to something like `take` which aggregates the last `n` values instead of the first `n` values. Here is an example pipeline against `test-chr22.mt`:. ```; import hail as hl. mt = hl.read_matrix_table('test-chr22.mt'); mt = mt._filter_partitions(range(8)) # make small. # restrict to two samples; mt = mt.filter_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5281
https://github.com/hail-is/hail/pull/5281:254,Testability,test,test-,254,"I made `_prev_nonnull` private since it's pretty specific to this use case, but I can imagine we'll want to generalize it to something like `take` which aggregates the last `n` values instead of the first `n` values. Here is an example pipeline against `test-chr22.mt`:. ```; import hail as hl. mt = hl.read_matrix_table('test-chr22.mt'); mt = mt._filter_partitions(range(8)) # make small. # restrict to two samples; mt = mt.filter_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5281
https://github.com/hail-is/hail/pull/5281:322,Testability,test,test-,322,"I made `_prev_nonnull` private since it's pretty specific to this use case, but I can imagine we'll want to generalize it to something like `take` which aggregates the last `n` values instead of the first `n` values. Here is an example pipeline against `test-chr22.mt`:. ```; import hail as hl. mt = hl.read_matrix_table('test-chr22.mt'); mt = mt._filter_partitions(range(8)) # make small. # restrict to two samples; mt = mt.filter_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5281
https://github.com/hail-is/hail/pull/5281:2354,Usability,simpl,simple,2354,"er_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511207 | [NA,(0/0)] |; | chr22:10511272 | [(0/0),(0/0)] |; | chr22:10511391 | [NA,(1/1)] |; | chr22:10511392 | [(0/0),(0/0)] |; | chr22:10511397 | [(0/0),(0/0)] |; | chr22:10511406 | [(0/0),(0/0)] |; | chr22:10511420 | [(0/0),(0/0)] |; +----------------+-------------------------+; ```. Thanks to @tpoterba for the totally sick `array_agg`. It's crazy how simple this was. (Unfortunately, it won't be so simple to make it go fast.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5281
https://github.com/hail-is/hail/pull/5281:2402,Usability,simpl,simple,2402,"er_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511207 | [NA,(0/0)] |; | chr22:10511272 | [(0/0),(0/0)] |; | chr22:10511391 | [NA,(1/1)] |; | chr22:10511392 | [(0/0),(0/0)] |; | chr22:10511397 | [(0/0),(0/0)] |; | chr22:10511406 | [(0/0),(0/0)] |; | chr22:10511420 | [(0/0),(0/0)] |; +----------------+-------------------------+; ```. Thanks to @tpoterba for the totally sick `array_agg`. It's crazy how simple this was. (Unfortunately, it won't be so simple to make it go fast.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5281
https://github.com/hail-is/hail/issues/5289:76,Testability,Assert,AssertionError,76,https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/AssertionError,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5289
https://github.com/hail-is/hail/pull/5292:199,Deployability,pipeline,pipeline,199,"informational page for last 100 job created (with command line) and job finished (with log) events. This is for @jigold's demo on Monday so she can give some sense of progress of jobs when running a pipeline demo. Note, batch isn't exposed publicly so you'll have to `kubectl` port forward to access it. Not the most beautiful, but should be functional. Here's a screenshot:. ![screenshot](https://user-images.githubusercontent.com/1244990/52447370-8997d680-2afe-11e9-92f5-2e56121b4af7.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5292
https://github.com/hail-is/hail/pull/5292:232,Security,expose,exposed,232,"informational page for last 100 job created (with command line) and job finished (with log) events. This is for @jigold's demo on Monday so she can give some sense of progress of jobs when running a pipeline demo. Note, batch isn't exposed publicly so you'll have to `kubectl` port forward to access it. Not the most beautiful, but should be functional. Here's a screenshot:. ![screenshot](https://user-images.githubusercontent.com/1244990/52447370-8997d680-2afe-11e9-92f5-2e56121b4af7.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5292
https://github.com/hail-is/hail/pull/5292:293,Security,access,access,293,"informational page for last 100 job created (with command line) and job finished (with log) events. This is for @jigold's demo on Monday so she can give some sense of progress of jobs when running a pipeline demo. Note, batch isn't exposed publicly so you'll have to `kubectl` port forward to access it. Not the most beautiful, but should be functional. Here's a screenshot:. ![screenshot](https://user-images.githubusercontent.com/1244990/52447370-8997d680-2afe-11e9-92f5-2e56121b4af7.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5292
https://github.com/hail-is/hail/pull/5292:87,Testability,log,log,87,"informational page for last 100 job created (with command line) and job finished (with log) events. This is for @jigold's demo on Monday so she can give some sense of progress of jobs when running a pipeline demo. Note, batch isn't exposed publicly so you'll have to `kubectl` port forward to access it. Not the most beautiful, but should be functional. Here's a screenshot:. ![screenshot](https://user-images.githubusercontent.com/1244990/52447370-8997d680-2afe-11e9-92f5-2e56121b4af7.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5292
https://github.com/hail-is/hail/issues/5293:389,Integrability,wrap,wrapper,389,"```. /home/hail/hail.zip/hail/methods/family_methods.py in trio_matrix(dataset, pedigree, complete_trios); 67 samples = mt[k].collect(); 68 ; ---> 69 pedigree = pedigree.filter_to(samples); 70 trios = pedigree.complete_trios() if complete_trios else pedigree.trios(); 71 n_trios = len(trios). <decorator-gen-848> in filter_to(self, samples). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 557 @decorator; 558 def wrapper(__original_func, *args, **kwargs):; --> 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); 560 return __original_func(*args_, **kwargs_); 561 . /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 511 expected=checker.expects(),; 512 found=checker.format(arg); --> 513 )) from e; 514 elif param.kind == param.VAR_POSITIONAL:; 515 # consume the rest of the positional arguments. TypeError: filter_to: parameter 'samples': expected Sequence[str], found list: <...>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5293
https://github.com/hail-is/hail/issues/5293:456,Integrability,wrap,wrapper,456,"```. /home/hail/hail.zip/hail/methods/family_methods.py in trio_matrix(dataset, pedigree, complete_trios); 67 samples = mt[k].collect(); 68 ; ---> 69 pedigree = pedigree.filter_to(samples); 70 trios = pedigree.complete_trios() if complete_trios else pedigree.trios(); 71 n_trios = len(trios). <decorator-gen-848> in filter_to(self, samples). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 557 @decorator; 558 def wrapper(__original_func, *args, **kwargs):; --> 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); 560 return __original_func(*args_, **kwargs_); 561 . /home/hail/hail.zip/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method); 511 expected=checker.expects(),; 512 found=checker.format(arg); --> 513 )) from e; 514 elif param.kind == param.VAR_POSITIONAL:; 515 # consume the rest of the positional arguments. TypeError: filter_to: parameter 'samples': expected Sequence[str], found list: <...>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5293
https://github.com/hail-is/hail/pull/5294:49,Availability,error,error,49,Fixes #5293. @tpoterba I decided not to throw an error if the key is null and instead just let the `flatMap` in `restrictTo` take care of it. Let me know if you think we should error out.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5294
https://github.com/hail-is/hail/pull/5294:177,Availability,error,error,177,Fixes #5293. @tpoterba I decided not to throw an error if the key is null and instead just let the `flatMap` in `restrictTo` take care of it. Let me know if you think we should error out.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5294
https://github.com/hail-is/hail/issues/5296:487,Availability,error,error,487,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.9-c3b1183e4246. ### What you did:; ```; pops_for_subpop = pops_ht.aggregate(; hl.agg.filter(; hl.is_defined(pops_ht.pop) & (pops_ht.pop != ""oth""),; hl.agg.group_by(pops_ht.pop, hl.agg.count()); ); ); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 530, in <module>; main(args); File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 458, in main; hl.agg.group_by(pops_ht.pop, hl.agg.count()); File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-764>"", line 2, in aggregate; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call las",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/issues/5296:1837,Availability,ERROR,ERROR,1837,"g.count()); File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-764>"", line 2, in aggregate; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 132, in <module>; main(args, pass_through_args); File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 113, in main; subprocess.check_output(job); File ""/anaconda3/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/anaconda3/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); ```. Note that first filtering the table, then running agg.group_by it works, if I just run agg_filter without agg.group_by (just agg.count()), it also works. For ref, this is `pops_ht` schema:; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 's': str ; 'known_pop': str ; 'known_subpop': str ; '_kgp': bool ; 'pop': st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/issues/5296:1936,Availability,ERROR,ERROR,1936,"51cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 132, in <module>; main(args, pass_through_args); File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 113, in main; subprocess.check_output(job); File ""/anaconda3/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/anaconda3/lib/python3.6/subprocess.py"", line 418, in run; output=stdout, stderr=stderr); ```. Note that first filtering the table, then running agg.group_by it works, if I just run agg_filter without agg.group_by (just agg.count()), it also works. For ref, this is `pops_ht` schema:; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 's': str ; 'known_pop': str ; 'known_subpop': str ; '_kgp': bool ; 'pop': str ; 'prob_afr': float64 ; 'prob_amr': float64 ; 'prob_eas': float64 ; 'prob_eur': float64 ; 'prob_sas': float64 ; -------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/issues/5296:493,Integrability,message,messages,493,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.9-c3b1183e4246. ### What you did:; ```; pops_for_subpop = pops_ht.aggregate(; hl.agg.filter(; hl.is_defined(pops_ht.pop) & (pops_ht.pop != ""oth""),; hl.agg.group_by(pops_ht.pop, hl.agg.count()); ); ); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 530, in <module>; main(args); File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 458, in main; hl.agg.group_by(pops_ht.pop, hl.agg.count()); File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-764>"", line 2, in aggregate; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call las",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/issues/5296:1017,Integrability,wrap,wrapper,1017,"on below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.9-c3b1183e4246. ### What you did:; ```; pops_for_subpop = pops_ht.aggregate(; hl.agg.filter(; hl.is_defined(pops_ht.pop) & (pops_ht.pop != ""oth""),; hl.agg.group_by(pops_ht.pop, hl.agg.count()); ); ); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 530, in <module>; main(args); File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 458, in main; hl.agg.group_by(pops_ht.pop, hl.agg.count()); File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-764>"", line 2, in aggregate; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/Users/laurent/tools/gnoma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/issues/5296:1346,Integrability,wrap,wrapper,1346,"ps_ht.pop) & (pops_ht.pop != ""oth""),; hl.agg.group_by(pops_ht.pop, hl.agg.count()); ); ); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 530, in <module>; main(args); File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/sample_qc.py"", line 458, in main; hl.agg.group_by(pops_ht.pop, hl.agg.count()); File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-764>"", line 2, in aggregate; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/table.py"", line 1133, in aggregate; File ""</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-436>"", line 2, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/typecheck/check.py"", line 560, in wrapper; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 94, in analyze; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 237, in get_refs; File ""/tmp/251cbf7beb5f4503ba74e4d69bd09ec3/hail-0.2-c3b1183e4246.zip/hail/expr/expressions/expression_utils.py"", line 212, in _get_refs; AttributeError: 'NoneType' object has no attribute '_indices_from_ref'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [251cbf7beb5f4503ba74e4d69bd09ec3] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 132, in <module>; main(args, pass_through_args); File ""/Users/laurent/tools/gnomad_hail/pyhail.py"", line 113, in main; subprocess.check_output(job); File ""/anaconda3/lib/python3.6/subprocess.py"", line 336, in check_output; **kwargs).stdout; File ""/anaconda3/lib/python3.6/subprocess.py"", line 418, in run;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5296
https://github.com/hail-is/hail/pull/5299:0,Deployability,Update,Update,0,Update summarize in response to Laura's feedback. Also add a helper method to compute the 'real' GT from Local GT and Local Alleles.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5299
https://github.com/hail-is/hail/pull/5299:40,Usability,feedback,feedback,40,Update summarize in response to Laura's feedback. Also add a helper method to compute the 'real' GT from Local GT and Local Alleles.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5299
https://github.com/hail-is/hail/pull/5300:164,Availability,toler,tolerate,164,"Add the `filter` and `find_replace` arguments to `import_table` and `import_vcf`. These are regex filter and substitution arguments, which will make it possible to tolerate some invalid inputs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5300
https://github.com/hail-is/hail/issues/5304:126,Modifiability,Extend,Extended,126,"https://numpydoc.readthedocs.io/en/latest/format.html. ```; def foo(a, b):; """"""One-line summary. Deprecation warning, if any. Extended summary (a few sentences, clarify functionality). Parameters. Returns. Other parameters, for infrequently used parameters on functions with large parameter lists. Warnings. See also (reference related functions that may be what the user actually wants or needs). Notes section, a possibly lengthy discussion of the algorithm and other gotchas. References for the notes section. Examples; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5304
https://github.com/hail-is/hail/pull/5305:25,Performance,cache,cache,25,You were right about the cache'ing. Thanks @jbloom22 !,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5305
https://github.com/hail-is/hail/pull/5310:35,Deployability,install,installed,35,The template files weren't getting installed so the /recent endpoint was breaking. I followed the Flask instructions for installing non-Python files: http://flask.pocoo.org/docs/0.12/patterns/distribute/.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5310
https://github.com/hail-is/hail/pull/5310:121,Deployability,install,installing,121,The template files weren't getting installed so the /recent endpoint was breaking. I followed the Flask instructions for installing non-Python files: http://flask.pocoo.org/docs/0.12/patterns/distribute/.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5310
https://github.com/hail-is/hail/pull/5314:162,Deployability,install,install,162,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5314:280,Deployability,install,install,280,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5314:392,Deployability,install,install,392,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5314:148,Integrability,depend,dependencies,148,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5314:428,Integrability,depend,dependency,428,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5314:38,Usability,Simpl,Simply,38,"First of several web-related commits. Simply adds Lerna to manage (from future PRs) web apps and related micro services, as a monorepo. Deduplicate dependencies, install using a single command. The package.lock file is the majority of LOC, and it's automatically generated by npm install for a given package.json at a given point in time. We should commit these to ensure that subsequent npm install commands generated the same dependency tree. I think we should assume that this file is correct for the moment. https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5. Lerna: https://lernajs.io. We can also choose not to use Lerna here and treat all web-related micro services as completely independent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5314
https://github.com/hail-is/hail/pull/5321:12,Availability,error,error,12,We throw an error if field is nested and the name argument is passed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5321
https://github.com/hail-is/hail/pull/5326:325,Security,expose,exposed,325,"This PR defines a `ReadPartition` IR node which takes a file path, spec, and written/requested row types and reads in the rows in the file. I haven't supported/defined this IR node in either JVM Emit or in python, since for now this node can only exist after a TableRead gets lowered. In order to be able to read the file, I exposed some things to be able to pass in a (java) HadoopConfiguration and use it to create a (C++) InputStream for the decoder; The way these extra arguments are passed into Emit right now are a little hacky, but I've started tidying some stuff up in #5248 and was going to wait on that to write a better way to pipe around potentially unused arguments. Warning: Until #5248 goes in, the ReadPartition IR node will put all the rows into a single region. Since the SparkBackend lowerer/execution is not exposed at the python level, and we're only using it for some small Scala tests right now, I think that's ok; just wanted to note it here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5326
https://github.com/hail-is/hail/pull/5326:828,Security,expose,exposed,828,"This PR defines a `ReadPartition` IR node which takes a file path, spec, and written/requested row types and reads in the rows in the file. I haven't supported/defined this IR node in either JVM Emit or in python, since for now this node can only exist after a TableRead gets lowered. In order to be able to read the file, I exposed some things to be able to pass in a (java) HadoopConfiguration and use it to create a (C++) InputStream for the decoder; The way these extra arguments are passed into Emit right now are a little hacky, but I've started tidying some stuff up in #5248 and was going to wait on that to write a better way to pipe around potentially unused arguments. Warning: Until #5248 goes in, the ReadPartition IR node will put all the rows into a single region. Since the SparkBackend lowerer/execution is not exposed at the python level, and we're only using it for some small Scala tests right now, I think that's ok; just wanted to note it here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5326
https://github.com/hail-is/hail/pull/5326:902,Testability,test,tests,902,"This PR defines a `ReadPartition` IR node which takes a file path, spec, and written/requested row types and reads in the rows in the file. I haven't supported/defined this IR node in either JVM Emit or in python, since for now this node can only exist after a TableRead gets lowered. In order to be able to read the file, I exposed some things to be able to pass in a (java) HadoopConfiguration and use it to create a (C++) InputStream for the decoder; The way these extra arguments are passed into Emit right now are a little hacky, but I've started tidying some stuff up in #5248 and was going to wait on that to write a better way to pipe around potentially unused arguments. Warning: Until #5248 goes in, the ReadPartition IR node will put all the rows into a single region. Since the SparkBackend lowerer/execution is not exposed at the python level, and we're only using it for some small Scala tests right now, I think that's ok; just wanted to note it here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5326
https://github.com/hail-is/hail/pull/5329:16,Availability,down,download,16,- automatically download catch.hpp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5329
https://github.com/hail-is/hail/pull/5331:16,Availability,down,download,16,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1170,Availability,error,error,1170,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:2019,Availability,error,error,2019,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:2229,Availability,down,downloading,2229,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:2321,Availability,error,error,2321,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:2381,Availability,error,error,2381,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:409,Integrability,depend,dependencies,409,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:509,Integrability,depend,depend,509,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:571,Integrability,depend,dependency,571,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:888,Integrability,depend,dependency,888,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1176,Integrability,message,messages,1176,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1264,Integrability,depend,dependency,1264,"ne reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1338,Integrability,depend,depend,1338,"ne reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1457,Integrability,depend,depends,1457,"citly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:171,Modifiability,variab,variable,171,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:1800,Modifiability,variab,variable,1800,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5331:121,Performance,perform,performance,121,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331
https://github.com/hail-is/hail/pull/5332:1294,Energy Efficiency,allocate,allocate,1294,"which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of any element making it to the; third buffer is 1/4 because it had to survive two coin flips. ```; +-------------------------------+; 3 | # # # # # # # |; +-------------------------------+; 2 | |; +-------------------------------+; 1 | |; +-------------------------------+; ```. When the stream is exhausted, we compact all the buffers (even if th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:381,Testability,test,test,381,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:426,Testability,test,testing,426,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:507,Testability,test,test,507,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:904,Testability,log,logarithmic,904,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:350,Usability,simpl,simplifies,350,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5332:626,Usability,simpl,simple,626,"Stacked on https://github.com/hail-is/hail/pull/5354. This is a very naive implementation of approximate quantiles that @patrick-schultz and I wrote last Friday. The diff will only contain ApproximateQuantiles.cpp and ApproximateQuantiles_test.cpp and a one-line change to the Makefile when https://github.com/hail-is/hail/pull/5331 is merged (which simplifies the addition of new test files to `Makefile`). I'm not sure what testing means in this context because it is an approximate algorithm. We added a test that calculates the ranks for a bunch of elements and prints them. This at least verifies we do not segfault on a simple example. Some subset of the interested parties: @jbloom22 @cseed @catoverdrive @patrick-schultz. Next steps:; - translate to Scala and hook into an actual aggregator; - hook into some future C++ aggregator infrastructure. ---; # The Algorithm Idea. The idea is to keep a logarithmic amount of data but still be able to reproduce an approximation of the original rank (how many elements are less than the given element). We start with a buffer of size `2^N`. ```; +-------------+; | |; +-------------+; ```. We insert elements from the stream until the buffer is full:. ```; +-------------+; | 1 2 ... 2^N |; +-------------+; ```. Then we 1) sort the buffer, 2) allocate another buffer of equal size, and 3); copy half the elements, randomly choosing to start from the zeroth or oneth; element. In the figure below we started with the zeroth element. We now consider; the first buffer empty. Note that the second buffer is only half-full. ```; +-------------------------------+; 2 | 1 3 ... 2^N - 1 |; +-------------------------------+; 1 | |; +-------------------------------+; ```. We now fill the first buffer again and repeat the process, now filling the; second buffer entirely and emptying the first buffer again. Because the second; buffer is now full, we run this compaction process on it, producing a third; buffer which is one half full. The probability of an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332
https://github.com/hail-is/hail/pull/5338:519,Performance,perform,performed,519,"Using index expressions to represent abstract transformations on tensors. E.g. broadcasts from scalars and vectors to matrices would be represented by. ```; O(i, j) <- A(); O(i, j) <- A(i); O(i, j) <- A(j); ```; where A is the input tensor and O is the output tensor. Similarly, transpose and identity look like. ```; O(i, j) <- A(j, i); O(i, j) <- A(i, j); ```. Since the `O(i,j)` is the same in all of these, we only need to look at the ""in"" index expressions for broadcasts to distinguish what operation needs to be performed. There's also a Simplify rule to cut out identity broadcasts. ## Workaround; This method of index expressions treats scalars and vectors as 0-dimensional and 1-dimensional tensors, respectively. As such, their shapes in the IR are 0 and 1-dimensional. However, BlockMatrix (Py API and Scala backend) assumes that all BlockMatrices are 2-dimensional, with the potential to have dimensions of length 1. To satisfy the current user API while maintaining a tensor-like mindset in the IR, the BlockMatrixType now has a `isRowVector` flag to help the front-end BlockMatrix discern whether a 1-tensor IR should be treated as a row or column vector. As BlockMatrix generalizes to n-tensors this flag can be removed. The IR should not rely on this flag for any execution logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5338
https://github.com/hail-is/hail/pull/5338:1291,Testability,log,logic,1291,"Using index expressions to represent abstract transformations on tensors. E.g. broadcasts from scalars and vectors to matrices would be represented by. ```; O(i, j) <- A(); O(i, j) <- A(i); O(i, j) <- A(j); ```; where A is the input tensor and O is the output tensor. Similarly, transpose and identity look like. ```; O(i, j) <- A(j, i); O(i, j) <- A(i, j); ```. Since the `O(i,j)` is the same in all of these, we only need to look at the ""in"" index expressions for broadcasts to distinguish what operation needs to be performed. There's also a Simplify rule to cut out identity broadcasts. ## Workaround; This method of index expressions treats scalars and vectors as 0-dimensional and 1-dimensional tensors, respectively. As such, their shapes in the IR are 0 and 1-dimensional. However, BlockMatrix (Py API and Scala backend) assumes that all BlockMatrices are 2-dimensional, with the potential to have dimensions of length 1. To satisfy the current user API while maintaining a tensor-like mindset in the IR, the BlockMatrixType now has a `isRowVector` flag to help the front-end BlockMatrix discern whether a 1-tensor IR should be treated as a row or column vector. As BlockMatrix generalizes to n-tensors this flag can be removed. The IR should not rely on this flag for any execution logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5338
https://github.com/hail-is/hail/pull/5338:545,Usability,Simpl,Simplify,545,"Using index expressions to represent abstract transformations on tensors. E.g. broadcasts from scalars and vectors to matrices would be represented by. ```; O(i, j) <- A(); O(i, j) <- A(i); O(i, j) <- A(j); ```; where A is the input tensor and O is the output tensor. Similarly, transpose and identity look like. ```; O(i, j) <- A(j, i); O(i, j) <- A(i, j); ```. Since the `O(i,j)` is the same in all of these, we only need to look at the ""in"" index expressions for broadcasts to distinguish what operation needs to be performed. There's also a Simplify rule to cut out identity broadcasts. ## Workaround; This method of index expressions treats scalars and vectors as 0-dimensional and 1-dimensional tensors, respectively. As such, their shapes in the IR are 0 and 1-dimensional. However, BlockMatrix (Py API and Scala backend) assumes that all BlockMatrices are 2-dimensional, with the potential to have dimensions of length 1. To satisfy the current user API while maintaining a tensor-like mindset in the IR, the BlockMatrixType now has a `isRowVector` flag to help the front-end BlockMatrix discern whether a 1-tensor IR should be treated as a row or column vector. As BlockMatrix generalizes to n-tensors this flag can be removed. The IR should not rely on this flag for any execution logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5338
https://github.com/hail-is/hail/issues/5340:29,Integrability,interface,interface,29,"From the under development R interface, we want to call this method definition in `is.hail.expr.ir.IRParser`:. ```; def parse_table_ir(s: String, refMap: java.util.HashMap[String, String], irMap: java.util.HashMap[String, BaseIR]); ```. But the method signature is using `java.util.HashMap` instead of `java.util.Map` and the sparklyr JVM layer translates the R map equivalent (an environment) to a different implementation of `java.util.Map`, so dispatch fails. Since the conversion is transparent, there is no way to _force_ a particular implementation of `java.util.Map`. Is there any reason to require `java.util.HashMap` there, or could the signature be generalized? There are cases like this throughout the Hail API, for all collection types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340
https://github.com/hail-is/hail/issues/5340:164,Security,Hash,HashMap,164,"From the under development R interface, we want to call this method definition in `is.hail.expr.ir.IRParser`:. ```; def parse_table_ir(s: String, refMap: java.util.HashMap[String, String], irMap: java.util.HashMap[String, BaseIR]); ```. But the method signature is using `java.util.HashMap` instead of `java.util.Map` and the sparklyr JVM layer translates the R map equivalent (an environment) to a different implementation of `java.util.Map`, so dispatch fails. Since the conversion is transparent, there is no way to _force_ a particular implementation of `java.util.Map`. Is there any reason to require `java.util.HashMap` there, or could the signature be generalized? There are cases like this throughout the Hail API, for all collection types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340
https://github.com/hail-is/hail/issues/5340:206,Security,Hash,HashMap,206,"From the under development R interface, we want to call this method definition in `is.hail.expr.ir.IRParser`:. ```; def parse_table_ir(s: String, refMap: java.util.HashMap[String, String], irMap: java.util.HashMap[String, BaseIR]); ```. But the method signature is using `java.util.HashMap` instead of `java.util.Map` and the sparklyr JVM layer translates the R map equivalent (an environment) to a different implementation of `java.util.Map`, so dispatch fails. Since the conversion is transparent, there is no way to _force_ a particular implementation of `java.util.Map`. Is there any reason to require `java.util.HashMap` there, or could the signature be generalized? There are cases like this throughout the Hail API, for all collection types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340
https://github.com/hail-is/hail/issues/5340:282,Security,Hash,HashMap,282,"From the under development R interface, we want to call this method definition in `is.hail.expr.ir.IRParser`:. ```; def parse_table_ir(s: String, refMap: java.util.HashMap[String, String], irMap: java.util.HashMap[String, BaseIR]); ```. But the method signature is using `java.util.HashMap` instead of `java.util.Map` and the sparklyr JVM layer translates the R map equivalent (an environment) to a different implementation of `java.util.Map`, so dispatch fails. Since the conversion is transparent, there is no way to _force_ a particular implementation of `java.util.Map`. Is there any reason to require `java.util.HashMap` there, or could the signature be generalized? There are cases like this throughout the Hail API, for all collection types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340
https://github.com/hail-is/hail/issues/5340:617,Security,Hash,HashMap,617,"From the under development R interface, we want to call this method definition in `is.hail.expr.ir.IRParser`:. ```; def parse_table_ir(s: String, refMap: java.util.HashMap[String, String], irMap: java.util.HashMap[String, BaseIR]); ```. But the method signature is using `java.util.HashMap` instead of `java.util.Map` and the sparklyr JVM layer translates the R map equivalent (an environment) to a different implementation of `java.util.Map`, so dispatch fails. Since the conversion is transparent, there is no way to _force_ a particular implementation of `java.util.Map`. Is there any reason to require `java.util.HashMap` there, or could the signature be generalized? There are cases like this throughout the Hail API, for all collection types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340
https://github.com/hail-is/hail/pull/5344:98,Performance,cache,cached,98,"Fixes #5095 and tips Google that we don't want it to index /devel /stable (also 301 redirects are cached, should be faster).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5344
https://github.com/hail-is/hail/issues/5345:1772,Testability,assert,assert,1772,"to replicate:. ```python; def tmt2(table, row_key, col_key, row_fields=[], col_fields=[], n_partitions=None):; ht = table.key_by(). non_entry_fields = set(itertools.chain(row_key, col_key, row_fields, col_fields)); entry_fields = [x for x in ht.row if x not in non_entry_fields]. if not entry_fields:; raise ValueError(...). col_data = hl.rbind(; hl.array(; hl.dict(; ht.aggregate(; hl.agg.collect_as_set(; (ht.row.select(*col_key),; ht.row.select(*col_fields)))))),; lambda data: hl.struct(data=data,; key_to_index=hl.dict(hl.range(0, hl.len(data)).map(lambda i: (data[i][0], i)))); ). col_data_uid = Env.get_uid(); ht = ht.drop(*col_fields); ht = ht.annotate_globals(**{col_data_uid: col_data}). entries_uid = Env.get_uid(); ht = (ht.group_by(*row_key); .partition_hint(n_partitions); .aggregate(**{x: hl.agg._prev_nonnull(ht[x]) for x in row_fields},; **{entries_uid: hl.rbind(; hl.dict(hl.agg.collect((ht[col_data_uid]['key_to_index'][ht.row.select(*col_key)],; ht.row.select(*entry_fields)))),; lambda entry_dict: hl.range(0, hl.len(ht[col_data_uid]['key_to_index'])); .map(lambda i: entry_dict.get(i))); })); ht = ht.annotate_globals(; **{col_data_uid: hl.array(ht[col_data_uid]['data'].map(lambda elt: hl.struct(**elt[0], **elt[1])))}); return ht._unlocalize_entries(entries_uid, col_data_uid, col_key). N, M = 50, 50; mt = hl.utils.range_matrix_table(N, M); mt = mt.key_cols_by(s = 'Col' + hl.str(M - mt.col_idx)); mt = mt.annotate_cols(c1 = hl.rand_bool(0.5)); mt = mt.annotate_rows(r1 = hl.rand_bool(0.5)); mt = mt.annotate_entries(e1 = hl.rand_bool(0.5)). re_mt = tmt2(mt.entries(), ['row_idx'], ['s'], ['r1'], ['col_idx', 'c1']); new_col_order = re_mt.col_idx.collect(); mapping = [t[1] for t in sorted([(old, new) for new, old in enumerate(new_col_order)])]. assert re_mt.choose_cols(mapping).drop('col_idx')._same(mt.drop('col_idx')); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5345
https://github.com/hail-is/hail/pull/5350:71,Testability,log,log,71,"- Add a `BlockMatrixMap` IR node that handles operations like: `neg`, `log`, `sqrt`, `abs`; - Change the function on Map2 to be an `IR`, not just `ApplyBinaryPrimOp`. This allows Map2 to support `pow` and other registered functions as well as arbitrary functions on tensors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5350
https://github.com/hail-is/hail/pull/5353:327,Performance,load,loaded,327,"Add components and list of reviewers for those components to the index.html page. It looks like this:. <img width=""667"" alt=""screen shot 2019-02-14 at 11 07 43 am"" src=""https://user-images.githubusercontent.com/1244990/52800012-ea686700-3048-11e9-84bd-33adb86e4820.png"">. The order of the names is random each time the page is loaded. Idea is to take the first name for the component you're reviewing when creating PRs. @danking I removed you from Hail front-end. Now everyone appears on 2-3 components. @akotlar FYI, I ripped out the JSON endpoints since I'm happy with the Flask implementation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5353
https://github.com/hail-is/hail/pull/5355:126,Testability,test,test,126,fixes #5352. @patrick-schultz Please double check my Interval change doesn't break anything else.; @konradjk Please check the test covers the edge cases appropriately.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5355
https://github.com/hail-is/hail/pull/5360:0,Integrability,depend,depends,0,depends on #5356,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5360
https://github.com/hail-is/hail/pull/5362:243,Deployability,install,installation,243,"The lock file is auto-generated. There isn't any need to review its contents; it matches the dependency tree for the packages/versions specified in package.json. Committing it allows all future users to follow the same dependency graph during installation. Ref: https://docs.npmjs.com/files/package-lock.json , https://stackoverflow.com/questions/44297803/package-lock-json-role; - Counter-argument: https://github.com/npm/npm/issues/20603. We can accept this PR first, to make the dependent PR small.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5362
https://github.com/hail-is/hail/pull/5362:93,Integrability,depend,dependency,93,"The lock file is auto-generated. There isn't any need to review its contents; it matches the dependency tree for the packages/versions specified in package.json. Committing it allows all future users to follow the same dependency graph during installation. Ref: https://docs.npmjs.com/files/package-lock.json , https://stackoverflow.com/questions/44297803/package-lock-json-role; - Counter-argument: https://github.com/npm/npm/issues/20603. We can accept this PR first, to make the dependent PR small.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5362
https://github.com/hail-is/hail/pull/5362:219,Integrability,depend,dependency,219,"The lock file is auto-generated. There isn't any need to review its contents; it matches the dependency tree for the packages/versions specified in package.json. Committing it allows all future users to follow the same dependency graph during installation. Ref: https://docs.npmjs.com/files/package-lock.json , https://stackoverflow.com/questions/44297803/package-lock-json-role; - Counter-argument: https://github.com/npm/npm/issues/20603. We can accept this PR first, to make the dependent PR small.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5362
https://github.com/hail-is/hail/pull/5362:482,Integrability,depend,dependent,482,"The lock file is auto-generated. There isn't any need to review its contents; it matches the dependency tree for the packages/versions specified in package.json. Committing it allows all future users to follow the same dependency graph during installation. Ref: https://docs.npmjs.com/files/package-lock.json , https://stackoverflow.com/questions/44297803/package-lock-json-role; - Counter-argument: https://github.com/npm/npm/issues/20603. We can accept this PR first, to make the dependent PR small.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5362
https://github.com/hail-is/hail/pull/5369:22,Testability,log,logic,22,"Contains the business logic, and a small readme to explain expected use. No '.env' file is committed here, because this is dangerous. In a future revision of this code, there may be secrets needed; if they are stored in .env, as is idiomatic, we could more easily commit them with a checked-in .env. Ref: https://devcenter.heroku.com/articles/node-best-practices#be-environmentally-aware",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369
https://github.com/hail-is/hail/issues/5371:105,Availability,error,errors,105,"Trying to annotate a table with a reference genome creates tons of temp files, and ultimately fails with errors like:; ```; Mkdirs failed to create file:/tmp/hail.aHapwOHwB9LA (exists=false, cwd=file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1550247966765_0004/container_1550247966765_0004_02_000051). at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:8091,Energy Efficiency,schedul,scheduler,8091,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:8162,Energy Efficiency,schedul,scheduler,8162,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:1604,Performance,concurren,concurrent,1604, at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hail.variant.ReferenceGenome.addSequenceFromReader(ReferenceGenome.scala:354); at is.hail.codegen.generated.C2.method2(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.rvd.RVD$$anonfun$filterWithContext$1$$anonfun$apply$9.apply(RVD.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:8285,Performance,concurren,concurrent,8285,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:8369,Performance,concurren,concurrent,8369,pply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$30.apply(ContextRDD.scala:373); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:153); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:346,Security,Checksum,ChecksumFileSystem,346,"Trying to annotate a table with a reference genome creates tons of temp files, and ultimately fails with errors like:; ```; Mkdirs failed to create file:/tmp/hail.aHapwOHwB9LA (exists=false, cwd=file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1550247966765_0004/container_1550247966765_0004_02_000051). at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:372,Security,Checksum,ChecksumFileSystem,372,"Trying to annotate a table with a reference genome creates tons of temp files, and ultimately fails with errors like:; ```; Mkdirs failed to create file:/tmp/hail.aHapwOHwB9LA (exists=false, cwd=file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1550247966765_0004/container_1550247966765_0004_02_000051). at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:426,Security,Checksum,ChecksumFileSystem,426,"Trying to annotate a table with a reference genome creates tons of temp files, and ultimately fails with errors like:; ```; Mkdirs failed to create file:/tmp/hail.aHapwOHwB9LA (exists=false, cwd=file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1550247966765_0004/container_1550247966765_0004_02_000051). at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5371:452,Security,Checksum,ChecksumFileSystem,452,"Trying to annotate a table with a reference genome creates tons of temp files, and ultimately fails with errors like:; ```; Mkdirs failed to create file:/tmp/hail.aHapwOHwB9LA (exists=false, cwd=file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1550247966765_0004/container_1550247966765_0004_02_000051). at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:929); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:910); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371
https://github.com/hail-is/hail/issues/5377:7,Safety,safe,safer,7,Make a safer implementation of this transformation. The best solution is probably a hash.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5377
https://github.com/hail-is/hail/issues/5377:84,Security,hash,hash,84,Make a safer implementation of this transformation. The best solution is probably a hash.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5377
https://github.com/hail-is/hail/issues/5378:37,Security,authoriz,authorized,37,"Currently spawn one kube watcher per authorized user, with the benefit that each kube watch is light. We could use a one (or a handful) of kube watchers for many users, but this is much more complicated, unless we just have 1 kube watcher watching all pods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5378
https://github.com/hail-is/hail/issues/5379:220,Deployability,deploy,deployment,220,"Some, if not most of the delay in reaching a running notebook server appears to be due to the use of services. Services provide little apparent benefit at the moment, esp. since we're already managing these pods using a deployment controller. Remove them in favor of accessing pods directly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5379
https://github.com/hail-is/hail/issues/5379:267,Security,access,accessing,267,"Some, if not most of the delay in reaching a running notebook server appears to be due to the use of services. Services provide little apparent benefit at the moment, esp. since we're already managing these pods using a deployment controller. Remove them in favor of accessing pods directly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5379
https://github.com/hail-is/hail/issues/5380:13,Deployability,pipeline,pipelines,13,- [ ] Enable pipelines to be able to run in the background for both Local and Batch modes; - [ ] Add dry-run support to BatchBackend; - [ ] Don't serially copy files in BatchBackend; - [ ] Docker within docker needs to work to test the LocalBackend docker mode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5380
https://github.com/hail-is/hail/issues/5380:227,Testability,test,test,227,- [ ] Enable pipelines to be able to run in the background for both Local and Batch modes; - [ ] Add dry-run support to BatchBackend; - [ ] Don't serially copy files in BatchBackend; - [ ] Docker within docker needs to work to test the LocalBackend docker mode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5380
https://github.com/hail-is/hail/pull/5383:256,Energy Efficiency,reduce,reduce,256,"Stacked on: https://github.com/hail-is/hail/pull/5382. When that goes in, this will almost all be deletions. I removed Scala Table and all Scala tests that depended on it. Most deleted tests have analogous in Python, although not all. This will definitely reduce test coverage, esp. things like LDPrune, IBD and PCRelate that haven't been lifted Python yet. They're not changing and on the short list to get lifted, so maybe this is OK, although I'm open to pushback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383
https://github.com/hail-is/hail/pull/5383:156,Integrability,depend,depended,156,"Stacked on: https://github.com/hail-is/hail/pull/5382. When that goes in, this will almost all be deletions. I removed Scala Table and all Scala tests that depended on it. Most deleted tests have analogous in Python, although not all. This will definitely reduce test coverage, esp. things like LDPrune, IBD and PCRelate that haven't been lifted Python yet. They're not changing and on the short list to get lifted, so maybe this is OK, although I'm open to pushback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383
https://github.com/hail-is/hail/pull/5383:145,Testability,test,tests,145,"Stacked on: https://github.com/hail-is/hail/pull/5382. When that goes in, this will almost all be deletions. I removed Scala Table and all Scala tests that depended on it. Most deleted tests have analogous in Python, although not all. This will definitely reduce test coverage, esp. things like LDPrune, IBD and PCRelate that haven't been lifted Python yet. They're not changing and on the short list to get lifted, so maybe this is OK, although I'm open to pushback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383
https://github.com/hail-is/hail/pull/5383:185,Testability,test,tests,185,"Stacked on: https://github.com/hail-is/hail/pull/5382. When that goes in, this will almost all be deletions. I removed Scala Table and all Scala tests that depended on it. Most deleted tests have analogous in Python, although not all. This will definitely reduce test coverage, esp. things like LDPrune, IBD and PCRelate that haven't been lifted Python yet. They're not changing and on the short list to get lifted, so maybe this is OK, although I'm open to pushback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383
https://github.com/hail-is/hail/pull/5383:263,Testability,test,test,263,"Stacked on: https://github.com/hail-is/hail/pull/5382. When that goes in, this will almost all be deletions. I removed Scala Table and all Scala tests that depended on it. Most deleted tests have analogous in Python, although not all. This will definitely reduce test coverage, esp. things like LDPrune, IBD and PCRelate that haven't been lifted Python yet. They're not changing and on the short list to get lifted, so maybe this is OK, although I'm open to pushback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383
https://github.com/hail-is/hail/pull/5385:101,Integrability,depend,depended,101,Stacked on: https://github.com/hail-is/hail/pull/5383. Removed Scala MatrixTable and everything that depended on it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5385
https://github.com/hail-is/hail/pull/5386:82,Testability,test,test,82,"Stacked on: https://github.com/hail-is/hail/pull/5384. Right now just running one test, since the Python tests are quite long (~10m). When they get parallelized (ongoing batch changes), we enable the full tests. The full tests are currently passing when run as:. HAIL_TEST_SERVICE_BACKEND_URL=http://localhost:5000 gw testPython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386
https://github.com/hail-is/hail/pull/5386:105,Testability,test,tests,105,"Stacked on: https://github.com/hail-is/hail/pull/5384. Right now just running one test, since the Python tests are quite long (~10m). When they get parallelized (ongoing batch changes), we enable the full tests. The full tests are currently passing when run as:. HAIL_TEST_SERVICE_BACKEND_URL=http://localhost:5000 gw testPython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386
https://github.com/hail-is/hail/pull/5386:205,Testability,test,tests,205,"Stacked on: https://github.com/hail-is/hail/pull/5384. Right now just running one test, since the Python tests are quite long (~10m). When they get parallelized (ongoing batch changes), we enable the full tests. The full tests are currently passing when run as:. HAIL_TEST_SERVICE_BACKEND_URL=http://localhost:5000 gw testPython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386
https://github.com/hail-is/hail/pull/5386:221,Testability,test,tests,221,"Stacked on: https://github.com/hail-is/hail/pull/5384. Right now just running one test, since the Python tests are quite long (~10m). When they get parallelized (ongoing batch changes), we enable the full tests. The full tests are currently passing when run as:. HAIL_TEST_SERVICE_BACKEND_URL=http://localhost:5000 gw testPython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386
https://github.com/hail-is/hail/pull/5386:318,Testability,test,testPython,318,"Stacked on: https://github.com/hail-is/hail/pull/5384. Right now just running one test, since the Python tests are quite long (~10m). When they get parallelized (ongoing batch changes), we enable the full tests. The full tests are currently passing when run as:. HAIL_TEST_SERVICE_BACKEND_URL=http://localhost:5000 gw testPython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386
https://github.com/hail-is/hail/issues/5390:19,Testability,test,tests,19,"The combiner needs tests. Given it's general newness, those tests may not be anything other than comparing output to make sure it doesn't change, but it needs tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5390
https://github.com/hail-is/hail/issues/5390:60,Testability,test,tests,60,"The combiner needs tests. Given it's general newness, those tests may not be anything other than comparing output to make sure it doesn't change, but it needs tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5390
https://github.com/hail-is/hail/issues/5390:159,Testability,test,tests,159,"The combiner needs tests. Given it's general newness, those tests may not be anything other than comparing output to make sure it doesn't change, but it needs tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5390
https://github.com/hail-is/hail/pull/5392:213,Modifiability,refactor,refactored,213,"Previously, the BlockMatrix IR had nodes for reading and writing that only covered the BlockMatrix part file format. Implemented readers and writers for both native and binary file formats (compatible with numpy) refactored read/write nodes, and implemented `tofile` and `fromfile` BlockMatrix methods in terms of the IR. Also hardcoded the front end default block size so now tests running IO/basic algebra should be able to run on the service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5392
https://github.com/hail-is/hail/pull/5392:377,Testability,test,tests,377,"Previously, the BlockMatrix IR had nodes for reading and writing that only covered the BlockMatrix part file format. Implemented readers and writers for both native and binary file formats (compatible with numpy) refactored read/write nodes, and implemented `tofile` and `fromfile` BlockMatrix methods in terms of the IR. Also hardcoded the front end default block size so now tests running IO/basic algebra should be able to run on the service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5392
https://github.com/hail-is/hail/pull/5394:245,Availability,error,erroring,245,"This is a pretty barebones implementation of split_multi with none of the flags that the usual split_multi has. Because I think the gVCF combiner ensures that min-repping alleles doesn't move them, I'm returning the min-repped locus/alleles and erroring if the locus changes (since this would mess up the ref blocks in that row, although I suppose I could always fall back to the non-minrepped version instead). It constructs 5 table nodes (two of which are key-bys and shouldn't need to touch the actual partitioned data) and three passes---map, explode, map---which is more-or-less the same as the usual split-multi. I haven't timed this on anything yet but I'm happy to if someone points me at a dataset I can use. @chrisvittal I've assigned this to you because I figure you're in the best position to make sure I've understood the format correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5394
https://github.com/hail-is/hail/issues/5396:355,Availability,error,error,355,"Code:; ```python3; import hail as hl; from hail import ir. hl.init(); mt = hl.import_vcf('path/to/vcf'); mt = hl.MatrixTable(ir.MatrixKeyRowsBy(mt._mir, ['locus'], is_sorted=True)); ht = mt._localize_entries('_e', '_c'); j = hl.Table._multi_way_zip_join([ht, ht], 'd', 'g'); j.write('tst.ht'); ```. Java Stack Trace:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretJSON.; : java.util.NoSuchElementException: key not found: alleles; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:24); 	at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:669); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:775); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:93); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:63); 	at is.hail.expr.ir.Interpret$.interpretJSON(Interpret.scala:22); 	at is.hail.expr.ir.Interpret.interpretJSON(Interpret.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5396
https://github.com/hail-is/hail/issues/5396:328,Integrability,protocol,protocol,328,"Code:; ```python3; import hail as hl; from hail import ir. hl.init(); mt = hl.import_vcf('path/to/vcf'); mt = hl.MatrixTable(ir.MatrixKeyRowsBy(mt._mir, ['locus'], is_sorted=True)); ht = mt._localize_entries('_e', '_c'); j = hl.Table._multi_way_zip_join([ht, ht], 'd', 'g'); j.write('tst.ht'); ```. Java Stack Trace:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretJSON.; : java.util.NoSuchElementException: key not found: alleles; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:24); 	at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:669); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:775); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:93); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:63); 	at is.hail.expr.ir.Interpret$.interpretJSON(Interpret.scala:22); 	at is.hail.expr.ir.Interpret.interpretJSON(Interpret.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.Reflec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5396
https://github.com/hail-is/hail/pull/5400:44,Availability,down,downstream,44,"SB is a reserved INFO field in VCFs, and so downstream tools may; overwrite the header. SB_TABLE is more what we want and the header will; be correct for the datatype (array of 4 ints)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5400
https://github.com/hail-is/hail/pull/5401:3,Usability,clear,clear,3,Be clear we need a scratch folder not a scratch bucket.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5401
https://github.com/hail-is/hail/pull/5402:140,Security,access,access,140,"- Added IR node to go from a BlockMatrix to a value; - Added BlockMatrixToValueFunction to get an element at a certain index and used it to access elements in BlockMatrix.__getitem__; - This raises an interesting question of whether accessing an element of a tensor should return a Python value or a Hail expr. Right now, it just builds a BlockMatrixToValue IR and immediately executes it to return a Python number (matches existing behavior), but we could just as easily return the IR node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5402
https://github.com/hail-is/hail/pull/5402:233,Security,access,accessing,233,"- Added IR node to go from a BlockMatrix to a value; - Added BlockMatrixToValueFunction to get an element at a certain index and used it to access elements in BlockMatrix.__getitem__; - This raises an interesting question of whether accessing an element of a tensor should return a Python value or a Hail expr. Right now, it just builds a BlockMatrixToValue IR and immediately executes it to return a Python number (matches existing behavior), but we could just as easily return the IR node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5402
https://github.com/hail-is/hail/issues/5405:249,Usability,usab,usable,249,I have seen a number of folks try and do matrix table like things on tables. Latest example: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Sum.20rows.20in.20hail.20table . Let's make unlocalize_entries more natural and usable for everyone and encourage it!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5405
https://github.com/hail-is/hail/issues/5406:16,Integrability,interface,interface,16,"the choose_cols interface is really annoying. See: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Merging.20MT/near/159059047. In order to join two matrix tables whose keys are not in the same order, we need to use `choose_cols`. But `choose_cols` works on indices, not values, and this is unnecessarily difficult. A method like this would be very helpful:. ```; def reorder_cols(self, desired_key_order):; """"""Reorder the cols such that they match the order given by `desired_key_order`""""""; current_keys = self.col_key.collect(); current_index = {key: i for i, key in enumerate(current_keys)}; new_order_of_indices = [current_index(key) for key in desired_key_order]; return self.choose_cols(new_order_of_indices); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5406
https://github.com/hail-is/hail/pull/5408:109,Energy Efficiency,reduce,reduce,109,"This makes ContextRDD.aggregate deterministic (aggregators are folded in partition-order). treeAggregate and reduce (at least) also need to be fixed. This shouldn't be much slower, but will increase memory usage on the master. Better than a wrong answer. Longer-term, we should aggregate commutative aggregators on the fly, and merge adjacent non-commutative aggregators on the fly instead of collecting all of them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5408
https://github.com/hail-is/hail/pull/5409:197,Deployability,Update,Updated,197,"The front-end accepted a boolean flag to make the a random BlockMatrix with a uniform distribution and would default to gaussian, whereas the backend method it's calling accepts a `gaussian` flag. Updated the front-end API to match the backend and added a test to check that matrices made explicitly uniform have no negative values and are (very likely) not gaussian.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5409
https://github.com/hail-is/hail/pull/5409:256,Testability,test,test,256,"The front-end accepted a boolean flag to make the a random BlockMatrix with a uniform distribution and would default to gaussian, whereas the backend method it's calling accepts a `gaussian` flag. Updated the front-end API to match the backend and added a test to check that matrices made explicitly uniform have no negative values and are (very likely) not gaussian.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5409
https://github.com/hail-is/hail/issues/5410:36,Deployability,pipeline,pipeline,36,"Still waiting on info about Jacob's pipeline, but that appears to be where Alicia's pipeline stalled in #5320. Konrad is also seeing a ~40 minute delay between two stages inside the `count_cols()` before the initial (temp) BlockMatrix is written for ld_matrix, which seems to be for (local) computation of the lowered MatrixAnnotateColsTable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5410
https://github.com/hail-is/hail/issues/5410:84,Deployability,pipeline,pipeline,84,"Still waiting on info about Jacob's pipeline, but that appears to be where Alicia's pipeline stalled in #5320. Konrad is also seeing a ~40 minute delay between two stages inside the `count_cols()` before the initial (temp) BlockMatrix is written for ld_matrix, which seems to be for (local) computation of the lowered MatrixAnnotateColsTable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5410
https://github.com/hail-is/hail/pull/5412:129,Deployability,deploy,deployment,129,"Only changes are as we discussed: rename/add notebook2 labels where appropriate. Tested in cluster, appears to work although the deployment is stuck in Desired == 1, so I may have missed one of the notebook labels, or maybe `make deploy` not enough (I find the makefile a bit confusing still). Describe shows `ReplicaFailure True FailedCreate`, will figure out tomorrow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5412
https://github.com/hail-is/hail/pull/5412:230,Deployability,deploy,deploy,230,"Only changes are as we discussed: rename/add notebook2 labels where appropriate. Tested in cluster, appears to work although the deployment is stuck in Desired == 1, so I may have missed one of the notebook labels, or maybe `make deploy` not enough (I find the makefile a bit confusing still). Describe shows `ReplicaFailure True FailedCreate`, will figure out tomorrow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5412
https://github.com/hail-is/hail/pull/5412:81,Testability,Test,Tested,81,"Only changes are as we discussed: rename/add notebook2 labels where appropriate. Tested in cluster, appears to work although the deployment is stuck in Desired == 1, so I may have missed one of the notebook labels, or maybe `make deploy` not enough (I find the makefile a bit confusing still). Describe shows `ReplicaFailure True FailedCreate`, will figure out tomorrow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5412
https://github.com/hail-is/hail/issues/5413:159,Deployability,deploy,deploy,159,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/issues/5413:222,Deployability,deploy,deploy,222,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/issues/5413:242,Deployability,deploy,deploy,242,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/issues/5413:263,Deployability,deploy,deploy,263,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/issues/5413:102,Integrability,interface,interface,102,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/issues/5413:197,Integrability,depend,dependencies,197,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413
https://github.com/hail-is/hail/pull/5414:31,Testability,test,testing,31,"I left in the previous one for testing purposes. When we're confident the new one dominates, I will remove it. This is the first of several PRs to speed up densify (and scans and aggregates in general). The rough plan is:; - add CompiledPackEncoder so we aren't interpreting types in the prev_nonnull seqOp,; - make RegionValueAggregator.result staged so we don't use RegionValueBuilder to construct aggregator and scan results. I will do some more benchmarking at that point.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414
https://github.com/hail-is/hail/pull/5414:449,Testability,benchmark,benchmarking,449,"I left in the previous one for testing purposes. When we're confident the new one dominates, I will remove it. This is the first of several PRs to speed up densify (and scans and aggregates in general). The rough plan is:; - add CompiledPackEncoder so we aren't interpreting types in the prev_nonnull seqOp,; - make RegionValueAggregator.result staged so we don't use RegionValueBuilder to construct aggregator and scan results. I will do some more benchmarking at that point.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414
https://github.com/hail-is/hail/issues/5415:687,Availability,error,error,687,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.10-91149b50a53c. ### What you did:. ```; qc_ht = qc_mt.annotate_cols(; dp_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.DP)),; gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); ).rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._selec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:853,Availability,error,error,853,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.10-91149b50a53c. ### What you did:. ```; qc_ht = qc_mt.annotate_cols(; dp_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.DP)),; gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); ).rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._selec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:693,Integrability,message,messages,693,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.10-91149b50a53c. ### What you did:. ```; qc_ht = qc_mt.annotate_cols(; dp_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.DP)),; gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); ).rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._selec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:859,Integrability,message,message,859,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.10-91149b50a53c. ### What you did:. ```; qc_ht = qc_mt.annotate_cols(; dp_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.DP)),; gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); ).rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._selec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:1522,Integrability,wrap,wrapper,1522,"ws(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:1573,Integrability,wrap,wrapper,1573,"ws(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:1779,Integrability,wrap,wrapper,1779,"ws(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:2267,Integrability,wrap,wrapper,2267,"ackages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._col_indices, {self._row_axis}); 3056 base, cleanup = self._process_joins(col); -> 3057 return cleanup(MatrixTable(MatrixMapCols(base._mir, col._ir, new_key))); 3058 ; 3059 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/matrixtable.py in __init__(self, mir); 556 self._entry_indices = Indices(self, {self._row_axis, self._col_axis}); 557 ; --> 558 self._type = self._mir.typ; 559 ; 560 self._global_type = self._type.global_type. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 106 def typ(self):; 107 if self._type is None:; --> 108 self._compute_type(); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:2318,Integrability,wrap,wrapper,2318,"ackages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._col_indices, {self._row_axis}); 3056 base, cleanup = self._process_joins(col); -> 3057 return cleanup(MatrixTable(MatrixMapCols(base._mir, col._ir, new_key))); 3058 ; 3059 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/matrixtable.py in __init__(self, mir); 556 self._entry_indices = Indices(self, {self._row_axis, self._col_axis}); 557 ; --> 558 self._type = self._mir.typ; 559 ; 560 self._global_type = self._type.global_type. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 106 def typ(self):; 107 if self._type is None:; --> 108 self._compute_type(); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:2524,Integrability,wrap,wrapper,2524,"ackages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._select_cols(caller, self.col.annotate(**named_exprs)); 998 ; 999 @typecheck_method(named_exprs=expr_any). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-1038> in _select_cols(self, caller, col, new_key). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._col_indices, {self._row_axis}); 3056 base, cleanup = self._process_joins(col); -> 3057 return cleanup(MatrixTable(MatrixMapCols(base._mir, col._ir, new_key))); 3058 ; 3059 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/matrixtable.py in __init__(self, mir); 556 self._entry_indices = Indices(self, {self._row_axis, self._col_axis}); 557 ; --> 558 self._type = self._mir.typ; 559 ; 560 self._global_type = self._type.global_type. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 106 def typ(self):; 107 if self._type is None:; --> 108 self._compute_type(); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:951,Testability,Assert,AssertionError,951,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2.10-91149b50a53c. ### What you did:. ```; qc_ht = qc_mt.annotate_cols(; dp_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.DP)),; gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); ).rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. So I was distracted and used `annotate_rows` instead of `annotate_cols` and I got what seems to be an 0.1 error message:. ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-44-7491b514f674> in <module>; 3 gq_stats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.stats(qc_mt.GQ)),; 4 callstats=hl.agg.group_by(qc_mt.qc_platform, hl.agg.call_stats(qc_mt.GT, qc_mt.alleles)),; ----> 5 call_rate=hl.agg.group_by(qc_mt.qc_platform, hl.agg.fraction(hl.is_defined(qc_mt.GQ))); 6 ).rows(); 7 # qc_ht = qc_ht.key_by('locus','alleles'). </opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-976> in annotate_cols(self, **named_exprs). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in annotate_cols(self, **named_exprs); 995 caller = ""MatrixTable.annotate_cols""; 996 check_annotate_exprs(caller, named_exprs, self._col_indices); --> 997 return self._selec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:3240,Testability,assert,assert,3240,".zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 559 def wrapper(__original_func, *args, **kwargs):; 560 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 561 return __original_func(*args_, **kwargs_); 562 ; 563 return wrapper. /home/hail/hail.zip/hail/matrixtable.py in _select_cols(self, caller, col, new_key); 3055 analyze(caller, col, self._col_indices, {self._row_axis}); 3056 base, cleanup = self._process_joins(col); -> 3057 return cleanup(MatrixTable(MatrixMapCols(base._mir, col._ir, new_key))); 3058 ; 3059 @typecheck_method(caller=str, s=expr_struct()). /home/hail/hail.zip/hail/matrixtable.py in __init__(self, mir); 556 self._entry_indices = Indices(self, {self._row_axis, self._col_axis}); 557 ; --> 558 self._type = self._mir.typ; 559 ; 560 self._global_type = self._type.global_type. /home/hail/hail.zip/hail/ir/base_ir.py in typ(self); 106 def typ(self):; 107 if self._type is None:; --> 108 self._compute_type(); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 child_typ = self.child.typ; ---> 83 self.new_col._compute_type(child_typ.col_env(), child_typ.entry_env()); 84 self._type = hl.tmatrix(; 85 child_typ.global_type,. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1328 self.old._compute_type(env, agg_env); 1329 for f, x in self.fields:; -> 1330 x._compute_type(env, agg_env); 1331 self._type = self.old.typ._insert_fields(**{f: x.typ for f, x in self.fields}); 1332 if self.field_order:. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1082 def _compute_type(self, env, agg_env):; 1083 self.key._compute_type(agg_env, None); -> 1084 self.agg_ir._compute_type(env, agg_env); 1085 self._type = tdict(self.key.typ, self.agg_ir.typ); 1086 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1200 if self.init_op_args:; 1201 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:4917,Testability,assert,assert,4917,"); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 child_typ = self.child.typ; ---> 83 self.new_col._compute_type(child_typ.col_env(), child_typ.entry_env()); 84 self._type = hl.tmatrix(; 85 child_typ.global_type,. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1328 self.old._compute_type(env, agg_env); 1329 for f, x in self.fields:; -> 1330 x._compute_type(env, agg_env); 1331 self._type = self.old.typ._insert_fields(**{f: x.typ for f, x in self.fields}); 1332 if self.field_order:. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1082 def _compute_type(self, env, agg_env):; 1083 self.key._compute_type(agg_env, None); -> 1084 self.agg_ir._compute_type(env, agg_env); 1085 self._type = tdict(self.key.typ, self.agg_ir.typ); 1086 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1200 if self.init_op_args:; 1201 for a in self.init_op_args:; -> 1202 a._compute_type(env, agg_env); 1203 for a in self.seq_op_args:; 1204 a._compute_type(agg_env, None). /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 515 ; 516 def _compute_type(self, env, agg_env):; --> 517 self.a._compute_type(env, agg_env); 518 self._type = tint32; 519 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1359 ; 1360 def _compute_type(self, env, agg_env):; -> 1361 self.o._compute_type(env, agg_env); 1362 self._type = self.o.typ[self.name]; 1363 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 351 ; 352 def _compute_type(self, env, agg_env):; --> 353 assert self.name in env, f'{self.name} not found in {env}'; 354 self._type = env[self.name]; 355 . AssertionError: va not found in {'global': dtype('struct{qc_mt_params: struct{min_af: float64, min_callrate: float64, ld_r2: float64}}'), 'sa': dtype('struct{s: str, sample_callrate: float64, qc_platform: str}')}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/issues/5415:5016,Testability,Assert,AssertionError,5016,"); 109 assert self._type is not None, self; 110 return self._type. /home/hail/hail.zip/hail/ir/matrix_ir.py in _compute_type(self); 81 def _compute_type(self):; 82 child_typ = self.child.typ; ---> 83 self.new_col._compute_type(child_typ.col_env(), child_typ.entry_env()); 84 self._type = hl.tmatrix(; 85 child_typ.global_type,. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1328 self.old._compute_type(env, agg_env); 1329 for f, x in self.fields:; -> 1330 x._compute_type(env, agg_env); 1331 self._type = self.old.typ._insert_fields(**{f: x.typ for f, x in self.fields}); 1332 if self.field_order:. /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1082 def _compute_type(self, env, agg_env):; 1083 self.key._compute_type(agg_env, None); -> 1084 self.agg_ir._compute_type(env, agg_env); 1085 self._type = tdict(self.key.typ, self.agg_ir.typ); 1086 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1200 if self.init_op_args:; 1201 for a in self.init_op_args:; -> 1202 a._compute_type(env, agg_env); 1203 for a in self.seq_op_args:; 1204 a._compute_type(agg_env, None). /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 515 ; 516 def _compute_type(self, env, agg_env):; --> 517 self.a._compute_type(env, agg_env); 518 self._type = tint32; 519 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 1359 ; 1360 def _compute_type(self, env, agg_env):; -> 1361 self.o._compute_type(env, agg_env); 1362 self._type = self.o.typ[self.name]; 1363 . /home/hail/hail.zip/hail/ir/ir.py in _compute_type(self, env, agg_env); 351 ; 352 def _compute_type(self, env, agg_env):; --> 353 assert self.name in env, f'{self.name} not found in {env}'; 354 self._type = env[self.name]; 355 . AssertionError: va not found in {'global': dtype('struct{qc_mt_params: struct{min_af: float64, min_callrate: float64, ld_r2: float64}}'), 'sa': dtype('struct{s: str, sample_callrate: float64, qc_platform: str}')}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5415
https://github.com/hail-is/hail/pull/5417:55,Safety,Unsafe,UnsafeSuite,55,"Stacked on: https://github.com/hail-is/hail/pull/5414. UnsafeSuite.testCodec verifies this aggressively. I also turned the up the test count massively in hand testing and everything looks good. This should give a modest speed boost to writes, shuffles, anything that uses the encoder generally. Next up is staging the result from RegionValueAggregators.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5417
https://github.com/hail-is/hail/pull/5417:67,Testability,test,testCodec,67,"Stacked on: https://github.com/hail-is/hail/pull/5414. UnsafeSuite.testCodec verifies this aggressively. I also turned the up the test count massively in hand testing and everything looks good. This should give a modest speed boost to writes, shuffles, anything that uses the encoder generally. Next up is staging the result from RegionValueAggregators.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5417
https://github.com/hail-is/hail/pull/5417:130,Testability,test,test,130,"Stacked on: https://github.com/hail-is/hail/pull/5414. UnsafeSuite.testCodec verifies this aggressively. I also turned the up the test count massively in hand testing and everything looks good. This should give a modest speed boost to writes, shuffles, anything that uses the encoder generally. Next up is staging the result from RegionValueAggregators.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5417
https://github.com/hail-is/hail/pull/5417:159,Testability,test,testing,159,"Stacked on: https://github.com/hail-is/hail/pull/5414. UnsafeSuite.testCodec verifies this aggressively. I also turned the up the test count massively in hand testing and everything looks good. This should give a modest speed boost to writes, shuffles, anything that uses the encoder generally. Next up is staging the result from RegionValueAggregators.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5417
https://github.com/hail-is/hail/pull/5418:359,Availability,error,error,359,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:1530,Availability,ERROR,ERROR,1530,"019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request; response = self.make_response(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1740, in make_response; rv = self.response_class.force_type(rv, request.environ); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/wrappers.py"", line 885, in force_type; response = BaseResponse(*_run_wsgi_app(response, environ)); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/test.py"", line 884, in run_wsgi_app; app_rv = app(environ, start_response); TypeError: 'int' object is not callable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:365,Integrability,message,message,365,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:2271,Integrability,wrap,wrappers,2271,"019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request; response = self.make_response(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1740, in make_response; rv = self.response_class.force_type(rv, request.environ); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/wrappers.py"", line 885, in force_type; response = BaseResponse(*_run_wsgi_app(response, environ)); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/test.py"", line 884, in run_wsgi_app; app_rv = app(environ, start_response); TypeError: 'int' object is not callable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:251,Modifiability,refactor,refactor,251,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:308,Testability,test,tests,308,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:319,Testability,test,tests,319,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:758,Testability,log,log,758,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:787,Testability,log,logs,787,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:804,Testability,log,log,804,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:1558,Testability,test,test,1558,"019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request; response = self.make_response(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1740, in make_response; rv = self.response_class.force_type(rv, request.environ); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/wrappers.py"", line 885, in force_type; response = BaseResponse(*_run_wsgi_app(response, environ)); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/test.py"", line 884, in run_wsgi_app; app_rv = app(environ, start_response); TypeError: 'int' object is not callable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:2440,Testability,test,test,2440,"019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request; response = self.make_response(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1740, in make_response; rv = self.response_class.force_type(rv, request.environ); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/wrappers.py"", line 885, in force_type; response = BaseResponse(*_run_wsgi_app(response, environ)); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/werkzeug/test.py"", line 884, in run_wsgi_app; app_rv = app(environ, start_response); TypeError: 'int' object is not callable; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5418:233,Usability,clear,clearer,233,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418
https://github.com/hail-is/hail/pull/5420:523,Energy Efficiency,reduce,reducers,523,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5420
https://github.com/hail-is/hail/pull/5420:754,Performance,perform,performance,754,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5420
https://github.com/hail-is/hail/pull/5420:642,Safety,safe,safer,642,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5420
https://github.com/hail-is/hail/pull/5421:543,Availability,down,down,543,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5421:398,Performance,perform,performing,398,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5421:114,Testability,test,test,114,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5421:69,Usability,clear,clear,69,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5421:531,Usability,clear,cleared,531,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5421:650,Usability,clear,clearing,650,"cc: @tpoterba @patrick-schultz @catoverdrive . We are not allowed to clear a region we do not own. Someone should test this doesn't blow memory on a severe filter in the cloud. ---. Prevent segfaults when joining two tables using `t1.join(t2)`. This syntax does a ""product join"", i.e., a normal join. The `t2[t1.key]` syntax takes only one matching element from `t2` for each element in `t1`. When performing a ""product join"", hail keeps a side-buffer of region values from the right-hand-side table. This side buffer *must not be cleared* by down stream operations (it is owned by the join node). Unfortunately, hail's filter method was incorrectly clearing regions it might not own. This bug only appeared as a segfault when `t1.join(t2)` was followed by a filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421
https://github.com/hail-is/hail/pull/5422:110,Deployability,pipeline,pipelines,110,Only create serializable and broadcasted HadoopConf once in HailContext and use everywhere else. I was seeing pipelines with MANY duplicate broadcasts the Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5422
https://github.com/hail-is/hail/pull/5422:162,Deployability,configurat,configuration,162,Only create serializable and broadcasted HadoopConf once in HailContext and use everywhere else. I was seeing pipelines with MANY duplicate broadcasts the Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5422
https://github.com/hail-is/hail/pull/5422:162,Modifiability,config,configuration,162,Only create serializable and broadcasted HadoopConf once in HailContext and use everywhere else. I was seeing pipelines with MANY duplicate broadcasts the Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5422
https://github.com/hail-is/hail/pull/5424:1051,Availability,error,error,1051,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1616,Availability,failure,failures,1616,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1638,Availability,failure,failures,1638,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1876,Availability,down,down,1876,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:308,Deployability,pipeline,pipeline,308,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:484,Deployability,pipeline,pipeline,484,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:909,Deployability,pipeline,pipeline,909,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1794,Performance,optimiz,optimization,1794,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:143,Safety,avoid,avoid,143,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1066,Safety,detect,detected,1066,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:979,Testability,test,test,979,"First, I changed import_vcfs to return a MatrixTable only keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1587,Testability,test,tests,1587,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1756,Usability,clear,clearly,1756,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5424:1974,Usability,clear,clearing,1974,"y keyed by locus, and removed the MatrixKeyRowsBy in combine_gvcfs. To goal here is to avoid re-buidling an re-broadcasting the partitioner once for each gVCF. We'll need to re-key at the very end. I'm not so familiar with the end of the joint calling pipeline. @chrisvittal can you take care of that?. Second, I don't repartition in TableMultiWayZipJoin if the partitioners all match (which they should in in the joint calling pipeline). For that to work right, I need allowedOverlap == 0 (or to verify the partitions are in fact disjoint). Turns out allowedOverlap wasn't being propagated in various places. I fixed that. @patrick-schultz can you look at the RVDPartitioner changes? They just look like oversights to me, but maybe there was a reason why, for example, copy and coarsen wasn't preserving allowedOverlap?. Finally, now the joint calling pipeline/test_combiner_works segfaults, ugh:. ```; $ hail -m unittest test.hail.methods.test_impex.VCFTests.test_combiner_works; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010e5fa090, pid=64905, tid=33795; #; # JRE version: Java(TM) SE Runtime Environment (8.0_45-b14) (build 1.8.0_45-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.45-b02 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # J 8877 C1 is.hail.expr.types.physical.PLocus$$anon$1.compare(Lis/hail/annotations/Region;JLis/hail/annotations/Region;J)I (117 bytes) @ 0x000000010e5fa090 [0x000000010e5f9de0+0x2b0]; #; ```. The rest of the tests pass (the other Python failures are cascaded failures from test_combiner_works, I double-checked in the hopes of finding an easier example to debug.) It is pretty clearly related to the no repartition optimization. If I disable it, test_combiner_works passes. I haven't tracked this down, but I do have one question @chrisvittal: who's responsible for freeing the inputs (that is, clearing the input regions) to multi-way zip join? I don't see where that happens.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424
https://github.com/hail-is/hail/pull/5426:11,Performance,cache,cache,11,"Add a code cache. 50 is was chosen somewhat randomly. Normalize incoming IR so name differences don't case a recompile. Move ApplyIR `conversion` since it shouldn't be involved in equality. Add hashCode to GR because you should always define hashCode, equals as a pair (and it was behaving very strangely without it). @chrisvittal I think this resolves the last of the issues you ran into on Friday.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426
https://github.com/hail-is/hail/pull/5426:194,Security,hash,hashCode,194,"Add a code cache. 50 is was chosen somewhat randomly. Normalize incoming IR so name differences don't case a recompile. Move ApplyIR `conversion` since it shouldn't be involved in equality. Add hashCode to GR because you should always define hashCode, equals as a pair (and it was behaving very strangely without it). @chrisvittal I think this resolves the last of the issues you ran into on Friday.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426
https://github.com/hail-is/hail/pull/5426:242,Security,hash,hashCode,242,"Add a code cache. 50 is was chosen somewhat randomly. Normalize incoming IR so name differences don't case a recompile. Move ApplyIR `conversion` since it shouldn't be involved in equality. Add hashCode to GR because you should always define hashCode, equals as a pair (and it was behaving very strangely without it). @chrisvittal I think this resolves the last of the issues you ran into on Friday.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426
https://github.com/hail-is/hail/pull/5429:21,Integrability,wrap,wrapping,21,"This simply adds the wrapping structure + header from app.hail.is. Includes no scss from app.hail.is. That will be the next PR unless you want it here. We should also decide whether we want a home page (the smoothly-rising Hail that Arcturus liked), i.e whether this will be extended with a link to batch, or not (we discussed the option at our last meeting). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5429
https://github.com/hail-is/hail/pull/5429:275,Modifiability,extend,extended,275,"This simply adds the wrapping structure + header from app.hail.is. Includes no scss from app.hail.is. That will be the next PR unless you want it here. We should also decide whether we want a home page (the smoothly-rising Hail that Arcturus liked), i.e whether this will be extended with a link to batch, or not (we discussed the option at our last meeting). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5429
https://github.com/hail-is/hail/pull/5429:5,Usability,simpl,simply,5,"This simply adds the wrapping structure + header from app.hail.is. Includes no scss from app.hail.is. That will be the next PR unless you want it here. We should also decide whether we want a home page (the smoothly-rising Hail that Arcturus liked), i.e whether this will be extended with a link to batch, or not (we discussed the option at our last meeting). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5429
https://github.com/hail-is/hail/pull/5430:896,Modifiability,variab,variable,896,"### Stacked PR; Stacks on #5429, adding the scss that was excluded from that PR. I separated them to make the story a bit cleaner. When #5429 is approved, this PR will contain 3 lines of change to index.html, 5 to notebook.py (to compile scss css), 1 to environment.yml (to include libsass), 1 to gitignore (to ignore the compiled css). The remaining 437 lines are the top-scope styles (margins, buttons that are frequently re-used, etc), the header styles, and styles for the login and notebook pages. We can exclude login page in this PR, but it only adds ~ 12 lines. Some of the global SCSS we will be able to removed later this week, once we understand which pieces we truly don't need. I can also remove them now if needed, but we won't have a single ""styles"" PR. ### SCSS vs CSS; I find SCSS preferable to CSS because it makes style scoping and re-use easier. For instance, we may define a variable $margin, to standardize spacing across all pages, and $color to define a font color for all text. SCSS can be written identically to css if the developer is totally unfamiliar or doesn't have time to learn. My style scoping solution is reminiscent of [BEM](http://getbem.com/introduction/). Each page defines an ID, and any styles that are private to that page are scope that way `#page { ...styles }` in the corresponding `pages/page.scss`. It is lightweight (no additional tooling needed), and should be easy to follow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430
https://github.com/hail-is/hail/pull/5430:477,Testability,log,login,477,"### Stacked PR; Stacks on #5429, adding the scss that was excluded from that PR. I separated them to make the story a bit cleaner. When #5429 is approved, this PR will contain 3 lines of change to index.html, 5 to notebook.py (to compile scss css), 1 to environment.yml (to include libsass), 1 to gitignore (to ignore the compiled css). The remaining 437 lines are the top-scope styles (margins, buttons that are frequently re-used, etc), the header styles, and styles for the login and notebook pages. We can exclude login page in this PR, but it only adds ~ 12 lines. Some of the global SCSS we will be able to removed later this week, once we understand which pieces we truly don't need. I can also remove them now if needed, but we won't have a single ""styles"" PR. ### SCSS vs CSS; I find SCSS preferable to CSS because it makes style scoping and re-use easier. For instance, we may define a variable $margin, to standardize spacing across all pages, and $color to define a font color for all text. SCSS can be written identically to css if the developer is totally unfamiliar or doesn't have time to learn. My style scoping solution is reminiscent of [BEM](http://getbem.com/introduction/). Each page defines an ID, and any styles that are private to that page are scope that way `#page { ...styles }` in the corresponding `pages/page.scss`. It is lightweight (no additional tooling needed), and should be easy to follow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430
https://github.com/hail-is/hail/pull/5430:518,Testability,log,login,518,"### Stacked PR; Stacks on #5429, adding the scss that was excluded from that PR. I separated them to make the story a bit cleaner. When #5429 is approved, this PR will contain 3 lines of change to index.html, 5 to notebook.py (to compile scss css), 1 to environment.yml (to include libsass), 1 to gitignore (to ignore the compiled css). The remaining 437 lines are the top-scope styles (margins, buttons that are frequently re-used, etc), the header styles, and styles for the login and notebook pages. We can exclude login page in this PR, but it only adds ~ 12 lines. Some of the global SCSS we will be able to removed later this week, once we understand which pieces we truly don't need. I can also remove them now if needed, but we won't have a single ""styles"" PR. ### SCSS vs CSS; I find SCSS preferable to CSS because it makes style scoping and re-use easier. For instance, we may define a variable $margin, to standardize spacing across all pages, and $color to define a font color for all text. SCSS can be written identically to css if the developer is totally unfamiliar or doesn't have time to learn. My style scoping solution is reminiscent of [BEM](http://getbem.com/introduction/). Each page defines an ID, and any styles that are private to that page are scope that way `#page { ...styles }` in the corresponding `pages/page.scss`. It is lightweight (no additional tooling needed), and should be easy to follow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430
https://github.com/hail-is/hail/pull/5430:1105,Usability,learn,learn,1105,"### Stacked PR; Stacks on #5429, adding the scss that was excluded from that PR. I separated them to make the story a bit cleaner. When #5429 is approved, this PR will contain 3 lines of change to index.html, 5 to notebook.py (to compile scss css), 1 to environment.yml (to include libsass), 1 to gitignore (to ignore the compiled css). The remaining 437 lines are the top-scope styles (margins, buttons that are frequently re-used, etc), the header styles, and styles for the login and notebook pages. We can exclude login page in this PR, but it only adds ~ 12 lines. Some of the global SCSS we will be able to removed later this week, once we understand which pieces we truly don't need. I can also remove them now if needed, but we won't have a single ""styles"" PR. ### SCSS vs CSS; I find SCSS preferable to CSS because it makes style scoping and re-use easier. For instance, we may define a variable $margin, to standardize spacing across all pages, and $color to define a font color for all text. SCSS can be written identically to css if the developer is totally unfamiliar or doesn't have time to learn. My style scoping solution is reminiscent of [BEM](http://getbem.com/introduction/). Each page defines an ID, and any styles that are private to that page are scope that way `#page { ...styles }` in the corresponding `pages/page.scss`. It is lightweight (no additional tooling needed), and should be easy to follow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430
https://github.com/hail-is/hail/pull/5435:132,Testability,assert,assertEqual,132,"@chrisvittal your functions, sir:. ```; f = hl.experimental.define_function(; lambda a, b: (a + 7) * b, hl.tint32, hl.tint32); self.assertEqual(hl.eval(f(1, 3)), 24). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435
https://github.com/hail-is/hail/pull/5437:185,Deployability,update,update,185,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:101,Integrability,rout,routes,101,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:405,Modifiability,extend,extend,405,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:126,Security,authoriz,authorized,126,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:334,Security,authoriz,authorized,334,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:88,Testability,log,login,88,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:94,Testability,log,logout,94,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:250,Testability,log,login,250,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:363,Testability,log,login,363,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5437:378,Testability,log,login,378,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5437
https://github.com/hail-is/hail/pull/5442:74,Availability,error,error,74,"if the comparison returns missing for two non-missing values, we throw an error (and user-facing comparison functions are constructed to never hit this case). I accidentally did this in one of the test cases and the interpreter didn't have the same behavior so the tests were passing. I've fixed the Interpret behavior as well; this was caught in the tests for #5283.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5442
https://github.com/hail-is/hail/pull/5442:197,Testability,test,test,197,"if the comparison returns missing for two non-missing values, we throw an error (and user-facing comparison functions are constructed to never hit this case). I accidentally did this in one of the test cases and the interpreter didn't have the same behavior so the tests were passing. I've fixed the Interpret behavior as well; this was caught in the tests for #5283.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5442
https://github.com/hail-is/hail/pull/5442:265,Testability,test,tests,265,"if the comparison returns missing for two non-missing values, we throw an error (and user-facing comparison functions are constructed to never hit this case). I accidentally did this in one of the test cases and the interpreter didn't have the same behavior so the tests were passing. I've fixed the Interpret behavior as well; this was caught in the tests for #5283.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5442
https://github.com/hail-is/hail/pull/5442:351,Testability,test,tests,351,"if the comparison returns missing for two non-missing values, we throw an error (and user-facing comparison functions are constructed to never hit this case). I accidentally did this in one of the test cases and the interpreter didn't have the same behavior so the tests were passing. I've fixed the Interpret behavior as well; this was caught in the tests for #5283.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5442
https://github.com/hail-is/hail/pull/5448:229,Integrability,rout,routes,229,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:298,Integrability,rout,route,298,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:440,Integrability,rout,routes,440,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:134,Testability,log,login,134,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:247,Testability,log,login,247,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:253,Testability,log,logout,253,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5448:312,Usability,clear,clear,312,"Stacks on #5437. When #5437 is merged, only change will be the addition of a `requires_auth` decorator - which redirects users to the login page when unauthorized, keeping a reference to the referring url - the protection of all routes other than login/logout, and 4 lines to the `/auth0-callback` route to read/clear the referrer session cookie. Admin pages are currently protected in the same way, but I can drop protection from whatever routes you wish. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448
https://github.com/hail-is/hail/pull/5451:0,Integrability,Depend,Dependency,0,"Dependency was only added to environment.yml, causing the container to fail to run. Would be nice if we could specify in one place.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451
https://github.com/hail-is/hail/pull/5452:360,Testability,log,login,360,"Stacks on #5448. Just moves `/` to `/notebook` (` index.html` to `notebook.html`), and adds a home page with the bouncy Hail that people liked. Spoke with Cotton, he wanted it. Only other change is adding cursor:pointer on `input[type=""submit""]` so that the ""Start a Hail Notebook"" button shows a hand. . edit2: Added 2 more commits, +7 -6 lines total, to fix login redirect link, and remove the /user link (which didn't work), to make this completely working as of this PR.; 430a01902c7994d64c4b2986f1b174fb3f32bbb9; 6a4c842c6a8e7a23a83cb7f624974a61e6f85962. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5452
https://github.com/hail-is/hail/pull/5455:12,Availability,error,error,12,"Plus better error checking!. - Some bioinformatic tools expect a secondary implied file to be present. For example, sample.vcf and it's index file sample.vcf.tbi. This PR adds file localization such that if any file in a resource group is used, the entire resource group will be copied and not just the mentioned file. - Added a mentioned set that tracks whether a resource was defined in the command or declare resource group functions. Otherwise, you could do something like this which would throw an error upon execution:. ```python; p = Pipeline(); t = p.new_task(); p.write_output(t.undefined_variable, 'gs://foo/foo'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5455
https://github.com/hail-is/hail/pull/5455:503,Availability,error,error,503,"Plus better error checking!. - Some bioinformatic tools expect a secondary implied file to be present. For example, sample.vcf and it's index file sample.vcf.tbi. This PR adds file localization such that if any file in a resource group is used, the entire resource group will be copied and not just the mentioned file. - Added a mentioned set that tracks whether a resource was defined in the command or declare resource group functions. Otherwise, you could do something like this which would throw an error upon execution:. ```python; p = Pipeline(); t = p.new_task(); p.write_output(t.undefined_variable, 'gs://foo/foo'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5455
https://github.com/hail-is/hail/pull/5455:541,Deployability,Pipeline,Pipeline,541,"Plus better error checking!. - Some bioinformatic tools expect a secondary implied file to be present. For example, sample.vcf and it's index file sample.vcf.tbi. This PR adds file localization such that if any file in a resource group is used, the entire resource group will be copied and not just the mentioned file. - Added a mentioned set that tracks whether a resource was defined in the command or declare resource group functions. Otherwise, you could do something like this which would throw an error upon execution:. ```python; p = Pipeline(); t = p.new_task(); p.write_output(t.undefined_variable, 'gs://foo/foo'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5455
https://github.com/hail-is/hail/pull/5457:54,Modifiability,refactor,refactoring,54,Does not/should not change current behavior; I'm just refactoring some code in anticipation of future changes needing the individual pieces instead of the whole.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5457
https://github.com/hail-is/hail/pull/5459:326,Deployability,configurat,configuration,326,"Changes the first argument that Emit expects to be a SparkFunctionContext, which currently holds a region and a SparkEnv (currently a stub; will be fleshed out as we start writing code to call back into Spark.) This should let us be more flexible in our ability to pass other necessary (non-IR-value) inputs, such as a hadoop configuration, to the function without relying on function argument ordering and accounting. builds on #5457.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5459
https://github.com/hail-is/hail/pull/5459:238,Modifiability,flexible,flexible,238,"Changes the first argument that Emit expects to be a SparkFunctionContext, which currently holds a region and a SparkEnv (currently a stub; will be fleshed out as we start writing code to call back into Spark.) This should let us be more flexible in our ability to pass other necessary (non-IR-value) inputs, such as a hadoop configuration, to the function without relying on function argument ordering and accounting. builds on #5457.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5459
https://github.com/hail-is/hail/pull/5459:326,Modifiability,config,configuration,326,"Changes the first argument that Emit expects to be a SparkFunctionContext, which currently holds a region and a SparkEnv (currently a stub; will be fleshed out as we start writing code to call back into Spark.) This should let us be more flexible in our ability to pass other necessary (non-IR-value) inputs, such as a hadoop configuration, to the function without relying on function argument ordering and accounting. builds on #5457.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5459
https://github.com/hail-is/hail/pull/5459:134,Testability,stub,stub,134,"Changes the first argument that Emit expects to be a SparkFunctionContext, which currently holds a region and a SparkEnv (currently a stub; will be fleshed out as we start writing code to call back into Spark.) This should let us be more flexible in our ability to pass other necessary (non-IR-value) inputs, such as a hadoop configuration, to the function without relying on function argument ordering and accounting. builds on #5457.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5459
https://github.com/hail-is/hail/pull/5460:191,Integrability,interface,interface,191,"This defines (but doesn't yet implement) a ReadPartition value IR node. For an idea of how it would be implemented and used, see #5326 for a (non-working) example. I think I need to make the interface for c++ to call back into spark a little better before I implement this, but in the meantime I'm PRing the IR bits.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5460
https://github.com/hail-is/hail/pull/5462:288,Deployability,deploy,deploy,288,"The status parameter was being set as the scratch parameter. https://github.com/hail-is/hail/pull/5418 introduced another constructor parameter before `_status`, changing the meaning of the fifth argument. CI is currently broken as a result. I'm going to force merge this and manually re-deploy CI and batch. I'll follow up with a commit that enforced inter-project dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5462
https://github.com/hail-is/hail/pull/5462:366,Integrability,depend,dependencies,366,"The status parameter was being set as the scratch parameter. https://github.com/hail-is/hail/pull/5418 introduced another constructor parameter before `_status`, changing the meaning of the fifth argument. CI is currently broken as a result. I'm going to force merge this and manually re-deploy CI and batch. I'll follow up with a commit that enforced inter-project dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5462
https://github.com/hail-is/hail/pull/5464:19,Deployability,pipeline,pipeline,19,This forces CI and pipeline to retest themselves if batch changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5464
https://github.com/hail-is/hail/pull/5465:44,Modifiability,refactor,refactor,44,"Contains one piece of a fix for #5262. This refactor was initially undertaken to resolve the above issue, which; manifested due to the nested stack frames created by the IR renderer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465
https://github.com/hail-is/hail/pull/5468:49,Performance,perform,performance,49,"Fixes #5449. We don't have machinery for testing performance behavior of something; like show() right now, so I can't test it easily. But I did verify by; hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5468
https://github.com/hail-is/hail/pull/5468:41,Testability,test,testing,41,"Fixes #5449. We don't have machinery for testing performance behavior of something; like show() right now, so I can't test it easily. But I did verify by; hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5468
https://github.com/hail-is/hail/pull/5468:118,Testability,test,test,118,"Fixes #5449. We don't have machinery for testing performance behavior of something; like show() right now, so I can't test it easily. But I did verify by; hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5468
https://github.com/hail-is/hail/pull/5470:86,Testability,log,log,86,"If batch job fails we should still increment the _task_idx.; Otherwise, we ignore the log for the failing task.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5470
https://github.com/hail-is/hail/pull/5471:388,Security,hash,hashCode,388,"@chrisvittal This will fix your define_function issue. I was wrong, the types were the same. @danking Something very strange is going on here. I can verify only one GenomeReference is being constructed, so this and concrete must be the same object, but eq is returning false. I don't see how this can be possible. I some something similar last week which caused me to add ReferenceGenome.hashCode (we were defining one but not the other). However, that shouldn't matter because there was only one reference genome object, but two different pointers captured at different times were returning different values of hashCode. I don't see how that's possible but I don't see an alternate explanation. This fixes the immediate bug. I think ReferenceGenome shouldn't be a case class, and it should only use ref equality (don't override equals, hashCode). That will take a little work because the case class is being used for JSON serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471
https://github.com/hail-is/hail/pull/5471:612,Security,hash,hashCode,612,"@chrisvittal This will fix your define_function issue. I was wrong, the types were the same. @danking Something very strange is going on here. I can verify only one GenomeReference is being constructed, so this and concrete must be the same object, but eq is returning false. I don't see how this can be possible. I some something similar last week which caused me to add ReferenceGenome.hashCode (we were defining one but not the other). However, that shouldn't matter because there was only one reference genome object, but two different pointers captured at different times were returning different values of hashCode. I don't see how that's possible but I don't see an alternate explanation. This fixes the immediate bug. I think ReferenceGenome shouldn't be a case class, and it should only use ref equality (don't override equals, hashCode). That will take a little work because the case class is being used for JSON serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471
https://github.com/hail-is/hail/pull/5471:837,Security,hash,hashCode,837,"@chrisvittal This will fix your define_function issue. I was wrong, the types were the same. @danking Something very strange is going on here. I can verify only one GenomeReference is being constructed, so this and concrete must be the same object, but eq is returning false. I don't see how this can be possible. I some something similar last week which caused me to add ReferenceGenome.hashCode (we were defining one but not the other). However, that shouldn't matter because there was only one reference genome object, but two different pointers captured at different times were returning different values of hashCode. I don't see how that's possible but I don't see an alternate explanation. This fixes the immediate bug. I think ReferenceGenome shouldn't be a case class, and it should only use ref equality (don't override equals, hashCode). That will take a little work because the case class is being used for JSON serialization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471
https://github.com/hail-is/hail/issues/5475:60,Security,password,password,60,As mentioned in #5448 admins need to enter a separate admin password (basic auth). This can be solved by implementing account roles and/or scopes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5475
https://github.com/hail-is/hail/pull/5476:232,Deployability,update,updates,232,"Stacks on #5452. See next comment for some additional details. . This gets notebook2 into a state equivalent to that of notebook 1 for user-facing content, but with the styling of app.hail.is. 2 PRs remain: bring fine-grained state updates, remove services. PR-specific commits to preview diff after #5452 merged:; https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc; https://github.com/hail-is/hail/pull/5476/commits/a60780e506e77cddf6afbcf388f9bfb027a32f8f; https://github.com/hail-is/hail/pull/5476/commits/1d58947696043f16ad864ee38c40e6dccf87cb1a. edit notes: Added relevant commits.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5476
https://github.com/hail-is/hail/pull/5478:260,Deployability,Update,Updated,260,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5478
https://github.com/hail-is/hail/pull/5478:2,Modifiability,Refactor,Refactored,2,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5478
https://github.com/hail-is/hail/pull/5478:386,Performance,load,loaded,386,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5478
https://github.com/hail-is/hail/pull/5478:268,Testability,test,tests,268,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5478
https://github.com/hail-is/hail/pull/5478:295,Testability,test,tests,295,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5478
https://github.com/hail-is/hail/pull/5483:113,Deployability,update,updated,113,Also... - fix inheriting from str so `join` will work; - fix Makefile for checking whether Batch files have been updated; - fix copying same input multiple times in LocalBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5483
https://github.com/hail-is/hail/pull/5483:14,Modifiability,inherit,inheriting,14,Also... - fix inheriting from str so `join` will work; - fix Makefile for checking whether Batch files have been updated; - fix copying same input multiple times in LocalBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5483
https://github.com/hail-is/hail/pull/5485:37,Integrability,interface,interface,37,"Not sure how this affects the Python interface, but it seems to be working via sparklyr. It generalizes `ArrayList` to `List` and `HashMap` to `Map` in every place I could find. Fixes #5340.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5485
https://github.com/hail-is/hail/pull/5485:131,Security,Hash,HashMap,131,"Not sure how this affects the Python interface, but it seems to be working via sparklyr. It generalizes `ArrayList` to `List` and `HashMap` to `Map` in every place I could find. Fixes #5340.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5485
https://github.com/hail-is/hail/issues/5486:365,Energy Efficiency,schedul,scheduled,365,"Pod can be Running when all containers terminated. I don't think that this is strictly disallowed, but it came up in a previous conversation with @danking, so thought I would investigate. . I think the answer is, from our dev forum discussion, that pods continue running for at least some TTL, after all containers terminated. More interesting, a pod that has been scheduled for deletion remains in ""Running"" state in pod.status.phase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5486
https://github.com/hail-is/hail/pull/5488:316,Deployability,deploy,deploy,316,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:555,Deployability,deploy,deploy,555,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:661,Deployability,deploy,deployed,661,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:832,Deployability,deploy,deploy,832,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:866,Deployability,deploy,deploy,866,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:1176,Deployability,deploy,deploy,1176,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:62,Testability,test,test,62,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:346,Testability,test,test,346,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:681,Testability,test,test,681,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:743,Testability,test,test,743,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:917,Testability,test,test,917,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:1260,Testability,test,test,1260,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:1337,Testability,test,test,1337,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5488:159,Usability,learn,learn,159,"Currently, garbage pods will sit around in the batch-pods and test namespaces forever. In anticipation of adding expensive resources (storage), batch needs to learn to clean up after itself. Batch creates garbage whenever it is killed without warning. This happens in two circumstances:; - when batch is killed by a deploy; - CI job is running a test batch instance and is killed because master or the feature branch changed. To mitigate this issue we delete all PVCs (storage, ergo monetarily expensive resources) from the batch-pods namespace before we deploy batch. These PVCs are no longer needed because the batch instance that owns them is about to be re-deployed. Since the test namespace (where CI jobs will spin up batch instances to test) might also contain PVCs, we delete the whole namespace. We can do this because the deploy job (the one running `make deploy` is in the `batch-pods` namespace, not the `test` namespace). Since we delete the whole namespace, we need to recreate anything that's expected to exist there. ---. This is a short term fix. The long term fix mitigates these two situations differently:; - persistence of batch jobs ensures that after a deploy, the new batch instance finds the orphaned resources and adopts them; - each test job will get a fresh namespace in which it creates whatever it needs to test, batch then ensures this namespace is destroyed when the job is finished (which, of course, requires persistence of batch jobs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5488
https://github.com/hail-is/hail/pull/5489:10,Deployability,deploy,deploys,10,"notebook2 deploys are currently broken, which breaks all deploys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489
https://github.com/hail-is/hail/pull/5489:57,Deployability,deploy,deploys,57,"notebook2 deploys are currently broken, which breaks all deploys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489
https://github.com/hail-is/hail/pull/5491:550,Modifiability,refactor,refactoring,550,- Added a single `BlockMatrixFilter` IR node for filtering a subset of rows and columns.; - I don't really have a great way of talking about filtering on some dimensions and not others. Filter takes a list of list of indices (one list per dimension) and if the list is empty that dimension is not filtered on. While there's no other meaningful interpretation of an empty list it feels like you're saying you don't want to keep any of the indices. Could use optionals for each dimension but `None` seems just as confusing and would require some small refactoring of the parser. The more consistent thing to do is pass along something like `range(n_rows)` but that just seems wasteful.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5491
https://github.com/hail-is/hail/issues/5500:54,Testability,test,tests,54,"@daniel-goldstein, I'm vaguely worried given that the tests didn't fail on your PR the other day. Let's get a simple test that filters and/or sparsifies a block and then exports.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5500
https://github.com/hail-is/hail/issues/5500:117,Testability,test,test,117,"@daniel-goldstein, I'm vaguely worried given that the tests didn't fail on your PR the other day. Let's get a simple test that filters and/or sparsifies a block and then exports.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5500
https://github.com/hail-is/hail/issues/5500:110,Usability,simpl,simple,110,"@daniel-goldstein, I'm vaguely worried given that the tests didn't fail on your PR the other day. Let's get a simple test that filters and/or sparsifies a block and then exports.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5500
https://github.com/hail-is/hail/pull/5502:153,Deployability,deploy,deploy,153,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:367,Deployability,deploy,deploy,367,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:484,Deployability,deploy,deploy-svc,484,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:597,Deployability,deploy,deploy-svc,597,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:196,Security,secur,security,196,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:105,Testability,test,test,105,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:396,Testability,test,tested,396,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5502:509,Testability,test,test,509,"Basically, I had no idea how RBAC worked. Now I have some idea. I now feel a bit uneasy about having the test namespace destroyed and recreated by batch deploy. Maybe when I better understand k8s security, I'll change to that. For now, we just grant the minimal permissions to delete any PVCs (i.e. hard drives, i.e. expensive shit) that are sitting around before we deploy a new batch system. I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502
https://github.com/hail-is/hail/pull/5503:1075,Testability,test,test,1075,"Add volumes to `batch`, which are mounted by each pod, but are not written to or read from. Key Changes; ---; - Introduce a k8s [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) and associated [ResourceQuota](https://kubernetes.io/docs/concepts/policy/resource-quotas/#storage-resource-quota) which creates and destroys disks for use by k8s pods whenever a PersistentVolumeClaim is created or delete, respectively. 1GB of disk [costs 0.04 USD per month](https://cloud.google.com/compute/pricing#disk), so I feel pretty fine with a 100GB limit for each, limiting our total possible monthly cost to 8 USD per month. - If there is more than one task (ergo there is a need for a shared disk), then we create a PVC and mount it to `/volume`. We delete this PVC if 1) all tasks complete, 2) a task fails, or 3) the job is cancelled or deleted. We may want to copy data off this volume for debugging purposes. We leave such changes for future PRs. Minor Changes; ---; - decreased the exponential backoff parameter from 2 to 1.5, which in practice ~halves test time when waiting on a volume; - print POD_NAMESPACE on startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503
https://github.com/hail-is/hail/issues/5504:460,Integrability,interface,interface,460,"Danfeng needs to multiply a Hail Table by a Hail MatrixTable and get back a Hail MatrixTable again. The Table is sample by ""cluster"" (I think cluster is more or less an ancestry group). The MatrixTable is variant by ""cluster"". She wants to take the product of T and MT along the cluster dimension, returning to a variant by sample MatrixTable. There's two missing pieces:; - convert a Table to a MatrixTable; - convert a BlockMatrix to a MatrixTable. ---. The interface for `_unlocalize_entries` is pretty terrible right now:; ```; t = t.transmute(entries=[hl.struct(x=t[str(i)]) for i in range(6)]); t = t.annotate_globals(cols=hl.array([hl.struct(s=str(i)) for i in range(6)])); mt = t._unlocalize_entries('entries', 'cols', ['s']); ```; I want something like:; ```; mt = t.to_matrix_table_columns_to_entries(; columns=['0', '1', '2', '3', '4', '5']; ); ```; `columns` is a list of column names, the columns must have the same type. If they're not a struct, they're transformed to a struct this way: `hl.struct(entry=t[column])`. There is one column field called `""s""`, its values is corresponding column's name. ---. I want; ```; mt = bm.to_matrix_table(); ```; which produces a matrix table with three fields: `row_idx` `col_idx` and `entry`. They have the obvious values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5504
