id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/4#issuecomment-67049771:62,Usability,simpl,simplest,62,"I'm going to assign it to @kshakir for now - to implement the simplest -L (one set of intervals, provided by a file). He may choose to split this issue into smaller ones for more granular features. The approach we're going to take is to implement only the features of -L that we need. The first milestone reflects the first feature to implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4#issuecomment-67049771
https://github.com/broadinstitute/gatk/issues/9#issuecomment-66529138:117,Deployability,upgrade,upgrade,117,Tell me about it :). Biggest support burden of upping the java version was due to Apple making it hard to seamlessly upgrade the java version. Users themselves didn't care all that much as long as the requirements were clear. . So far we've been lucky that no other major tool seems to dictate which version of java users should have on their machine. Otherwise collisions could happen.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9#issuecomment-66529138
https://github.com/broadinstitute/gatk/issues/9#issuecomment-66529138:219,Usability,clear,clear,219,Tell me about it :). Biggest support burden of upping the java version was due to Apple making it hard to seamlessly upgrade the java version. Users themselves didn't care all that much as long as the requirements were clear. . So far we've been lucky that no other major tool seems to dictate which version of java users should have on their machine. Otherwise collisions could happen.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9#issuecomment-66529138
https://github.com/broadinstitute/gatk/issues/16#issuecomment-66795521:324,Usability,simpl,simpleMerge,324,"There is likely common code between the two, but we would need to pre-process the inputs to determine whether they are GVCFs: the merging algorithm is different for the two cases even if all input variants at a site are non-reference-blocks: see `ReferenceConfidenceVariantContextMerger.merge()` vs `GATKVariantContextUtils.simpleMerge()`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66795521
https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:263,Modifiability,rewrite,rewrite,263,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167
https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:99,Usability,clear,clear,99,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167
https://github.com/broadinstitute/gatk/issues/18#issuecomment-66629944:2,Usability,simpl,simply,2,"I simply mean migrate to hellbender. On Thu, Dec 11, 2014 at 9:40 AM, rpoplin notifications@github.com wrote:. > What does as a command line program mean?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/18#issuecomment-66627537; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/18#issuecomment-66629944
https://github.com/broadinstitute/gatk/issues/33#issuecomment-94484114:41,Usability,clear,clearer,41,i'm going to delete this until we have a clearer picture of locus walkers in hellbender,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/33#issuecomment-94484114
https://github.com/broadinstitute/gatk/issues/72#issuecomment-71662187:23,Usability,simpl,simple,23,"We ended up using jopt-simple alongside some of our existing code. It was chosen somewhat arbitrarily after examining a number of different options. It supports arguments on the style that we wanted, is under active development, and was easy to plug into our existing code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/72#issuecomment-71662187
https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719:72,Availability,error,errors,72,"Should I follow the existing convention of using UserException for user errors and GATKException for everything else that doesn't clearly fall under a standard exception type?. The alternative would be to port PicardException, which we decided not to do IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719
https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719:130,Usability,clear,clearly,130,"Should I follow the existing convention of using UserException for user errors and GATKException for everything else that doesn't clearly fall under a standard exception type?. The alternative would be to port PicardException, which we decided not to do IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719
https://github.com/broadinstitute/gatk/pull/74#issuecomment-67869375:632,Usability,clear,clearly,632,"Ah, I was not aware until now that we had a tool called SplitByRG -- which is probably because I'm mostly familiar with public tools, and SplitByRG appears to be in private. . After further thought, my guess is that the difference is: SplitByRG writes out a separate bam for each readgroup, and they are all output (core function modality is scattering), whereas the PrintReads + filters solution only writes out a specified subset, to a single output bam (core function modality is subsetting). If so then perhaps it does make sense to keep them separate, except that instead of making it a PrintReadsBy[blah](a name that does not clearly distinguish the core functionality), I would recommend making it a generic tool called SplitReads (analogous to PrintReads, but with a distinct scattering modality), and offer several options for how to split (e.g. you could choose a specific RG tag or other non-RG property of the read data -- including randomness, which would cover the functionality of SplitSamFile).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/74#issuecomment-67869375
https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:65,Modifiability,refactor,refactored,65,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630
https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:139,Performance,cache,cache,139,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630
https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:87,Usability,simpl,simplified,87,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:194,Availability,down,downsampling-with-haplotypecaller,194,"Recording key comments from the old pivotal entry ""Downsampling HC is wonky and misunderstood"" for posterity as a cautionary tale. ---. Reported in http://gatk.vanillaforums.com/discussion/3094/downsampling-with-haplotypecaller. User specifies -dcov 200 but DP per sample in VCF is higher than that. Local cmdline:. ```; java -Xmx16g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/1kg/reference/hs37d5.fa -nct 8 -ped /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/families.ped -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droaz",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:1651,Availability,down,down-sampling,1651,"th_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the un",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:1749,Availability,down,down-sampling,1749,"th_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the un",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:1921,Availability,down,down-sampling,1921,"_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2094,Availability,down,downsamplers,2094,"ov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2796,Availability,down,downsampling,2796," Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP val",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2998,Availability,down,downsampling,2998,"his. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlyin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:3668,Availability,down,downsampling,3668,"k added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4169,Availability,down,downsampling,4169,"eads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4758,Availability,down,downsampling,4758,"is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid stor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5486,Availability,down,downsampling,5486,"make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5853,Availability,down,downsampling,5853,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6043,Availability,down,downsampling,6043,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6323,Availability,down,downsampling,6323,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6702,Availability,down,downsampling,6702,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6852,Availability,down,downsampling,6852,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4331,Performance,perform,performance,4331,"ll overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4890,Performance,perform,performance,4890,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5690,Safety,avoid,avoid,5690,"ually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6173,Safety,avoid,avoid,6173,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:1196,Testability,test,test,1196,tp://gatk.vanillaforums.com/discussion/3094/downsampling-with-haplotypecaller. User specifies -dcov 200 but DP per sample in VCF is higher than that. Local cmdline:. ```; java -Xmx16g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/1kg/reference/hs37d5.fa -nct 8 -ped /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/families.ped -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provid,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2124,Testability,test,test,2124,"ov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4902,Testability,test,testing,4902,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2696,Usability,undo,undownsampled,2696," Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP val",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:2863,Usability,undo,undownsampled,2863,"ook like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droazen said (over multiple comments):. I am looking into this. LIBS is actually calling into the downsamplers correctly in the test case that Eric provided. You can see this by examining readStates.size() for each locus -- it never exceeds the -dcov target of 200. The problem must lie elsewhere -- I'll continue to step through this in the debugger. [...]. After some more debugging and consultation with Ryan, I've found that DP values exceeding dcov are to be expected given the way the ActiveRegion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:3463,Usability,undo,undownsampled,3463,"egion traversal currently works. Here's a summary of what's going on:. -dcov 200 does cause LIBS to cap the depth at each locus to 200, but due to code Mark added a while back LIBS will save all of the undownsampled reads in memory during active region traversals (which kind of defeats the purpose of downsampling in the first place!). -TraverseActiveRegions gets the undownsampled reads from LIBS, and adds them to the active regions that get passed to the walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the Activ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:3937,Usability,undo,undownsampled,3937," walker. -The HaplotypeCaller does a post-hoc downsampling pass on the reads in the active region in finalizeActiveRegion() to a hardcoded (!!!) and completely arbitrary depth of 1000, ignoring dcov. -The HaplotypeCaller does realignment of reads to the haplotypes, potentially causes the depth of coverage to vary at the locus in question. -The GenotypingEngine then computes DP based on the reads that still overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Wi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4629,Usability,undo,undownsampled,4629,"is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid stor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5374,Usability,undo,undownsampled,5374,"if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5708,Usability,undo,undownsampled,5708,"ually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5940,Usability,undo,undownsampled,5940,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6198,Usability,undo,undownsampled,6198,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345
https://github.com/broadinstitute/gatk/issues/104#issuecomment-77252124:6,Usability,simpl,simple,6,first simple and naive impl merged in by pull #252,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/104#issuecomment-77252124
https://github.com/broadinstitute/gatk/issues/118#issuecomment-70675377:67,Usability,guid,guide,67,"once we get to 0 for AyeAye, we should just change this to a style-guide item and not accept pull reqs with warnings in them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/118#issuecomment-70675377
https://github.com/broadinstitute/gatk/issues/133#issuecomment-70925401:279,Usability,intuit,intuitive,279,"I prefer the classic GATK paradigm, in which by default every boolean is false, until you pass the flag to set it to true. This allows you to be able to just add the flag to the CL without specifying a value. From user POV this provides valuable consistency. It seems a lot more intuitive as well. It's like you're asking ""do you see a flag?"" and in the answer, no means no, and yes means yes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-70925401
https://github.com/broadinstitute/gatk/issues/133#issuecomment-94413465:71,Usability,simpl,simplest,71,Is there a way we can use constants for common arg names? Would be the simplest way to keep them all in sync IMO.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94413465
https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510:368,Deployability,pipeline,pipeline,368,"@lbergelson, @akiezun AFAIK picard tools need you to specify specifically FLAG=true/ FLAG=false if it's a boolean flag. it is true that, if you want, any argument can have a default value (true or false) but to change it you will still need to assign true or false (i.e even if there is a default you cannot simply have FLAG on the commandline). Yes, the logic of the pipeline specifies all the commandline arguments, regardless of defaults so that if the defaults change (which the GATK used to do all the time!) the pipeline will not change. Thus the use case has to include being able to set all arguments to their value boolean or otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510
https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510:518,Deployability,pipeline,pipeline,518,"@lbergelson, @akiezun AFAIK picard tools need you to specify specifically FLAG=true/ FLAG=false if it's a boolean flag. it is true that, if you want, any argument can have a default value (true or false) but to change it you will still need to assign true or false (i.e even if there is a default you cannot simply have FLAG on the commandline). Yes, the logic of the pipeline specifies all the commandline arguments, regardless of defaults so that if the defaults change (which the GATK used to do all the time!) the pipeline will not change. Thus the use case has to include being able to set all arguments to their value boolean or otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510
https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510:355,Testability,log,logic,355,"@lbergelson, @akiezun AFAIK picard tools need you to specify specifically FLAG=true/ FLAG=false if it's a boolean flag. it is true that, if you want, any argument can have a default value (true or false) but to change it you will still need to assign true or false (i.e even if there is a default you cannot simply have FLAG on the commandline). Yes, the logic of the pipeline specifies all the commandline arguments, regardless of defaults so that if the defaults change (which the GATK used to do all the time!) the pipeline will not change. Thus the use case has to include being able to set all arguments to their value boolean or otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510
https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510:308,Usability,simpl,simply,308,"@lbergelson, @akiezun AFAIK picard tools need you to specify specifically FLAG=true/ FLAG=false if it's a boolean flag. it is true that, if you want, any argument can have a default value (true or false) but to change it you will still need to assign true or false (i.e even if there is a default you cannot simply have FLAG on the commandline). Yes, the logic of the pipeline specifies all the commandline arguments, regardless of defaults so that if the defaults change (which the GATK used to do all the time!) the pipeline will not change. Thus the use case has to include being able to set all arguments to their value boolean or otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/133#issuecomment-94434510
https://github.com/broadinstitute/gatk/pull/135#issuecomment-70915198:5,Usability,simpl,simple,5,"jopt-simple docs are here http://pholser.github.io/jopt-simple/. This should close #81, #111 and #67",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/135#issuecomment-70915198
https://github.com/broadinstitute/gatk/pull/135#issuecomment-70915198:56,Usability,simpl,simple,56,"jopt-simple docs are here http://pholser.github.io/jopt-simple/. This should close #81, #111 and #67",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/135#issuecomment-70915198
https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:439,Integrability,depend,dependent,439,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930
https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:836,Integrability,wrap,wrappers,836,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930
https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:538,Testability,log,logic,538,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930
https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930:1214,Usability,intuit,intuitive,1214,"I'm not completely opposed to that way of dealing with this, but I'm not yet convinced either. . I'm not sure I see how having an extra argument is somehow shorter than having one special value that is included in the description of the original argument. As in:. --trimWhatever | -trimWvr -- bla bla bla; default w; min x max y; to disable trimming, use z. . As for the documentation auto-generator showing the two args together, that is dependent on setting up the arguments so that the code specifies they are related, and adding some logic to the auto-generation to pull related arguments together. (As a contributing developer to a documentation auto-generator --the GATKDocs-- I can tell you that is not necessarily trivial and adds even more moving parts.) This also generates additional complexity for third-party developers of wrappers (such as Galaxy). Finally, it can be a source of confusion for users who are trying to look up an argument called ""-dont-Trim-whatever"" since presumably it's only going to be listed under T (-Trim-whatever) and not under D in the alphabetical list. Or should it be listed twice? . A reference manual can be very ""nice"" and helpful, and it must be organized in the most intuitive way possible, especially since there is no way we can provide examples that cover every single use case under the sun (trust me, there's not).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71122930
https://github.com/broadinstitute/gatk/issues/143#issuecomment-71232186:251,Usability,clear,clearly,251,"I also prefer the use of flags over special values, because of the case where the special value is accidentally triggered: maybe that value is obviously out of a reasonable range to us but not an unsophisticated user. I worry about complaints like ""I clearly set the value, but it didn't take effect."" At the least, there needs to be something like: `WARN: disabling trim feature due to parameter value -1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-71232186
https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180:16,Testability,test,tests,16,options used in tests: ; --compress; -n ; --simplifyBAM; -L 1; -L unmapped; --readGroup; --sample_name,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180
https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180:44,Usability,simpl,simplifyBAM,44,options used in tests: ; --compress; -n ; --simplifyBAM; -L 1; -L unmapped; --readGroup; --sample_name,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/154#issuecomment-71864180
https://github.com/broadinstitute/gatk/issues/175#issuecomment-94355649:7,Usability,simpl,simple,7,"It's a simple change that I've implemented locally, I need to fix the picard programs that rely on the old behavior though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/175#issuecomment-94355649
https://github.com/broadinstitute/gatk/issues/190#issuecomment-96246722:66,Usability,usab,usable,66,how about the gatk3 diff engine? should be easy to port and seems usable here.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/190#issuecomment-96246722
https://github.com/broadinstitute/gatk/issues/190#issuecomment-127382644:58,Usability,clear,clear,58,"i'm removing my assignment then. the requirements are not clear to me. 'until we're all satisfied' is pretty vague, too vague for alpha i think",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/190#issuecomment-127382644
https://github.com/broadinstitute/gatk/issues/193#issuecomment-76831428:14,Usability,simpl,simple,14,"the design is simple - filters should filter, not blow up. There could be a tool that takes a set of filters and blows up if they fail. But the filter's job is to filter not blow up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-76831428
https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438:627,Testability,log,logic,627,"+1MOn Sat, Feb 28, 2015 at 1:14 PM, ldgauthier notifications@github.com wrote: It would be wonderful to be able to use SelectVariants with a query like -select ""AF > 0.1"" on a VCF containing multiallelics and have it filter multiallelics by the allele with the highest AF. (Possibly conversely for ""AF < X""queries. Right now it crashes unless you use a crazy JEXL or pull out the multiallelics. Maybe we could make a maxAF/minAF in htsjdk/JEXLmap.java which equals AF for biallelics?. Internally, it might be nice to have a Map with the AF (or AC) for each allele for the SelectVariants issue and to simplify some of the crazy logic already in VariantAnnotator to deal with different allele ordering. As part of this task, we should also make 100% sure that allele ordering is preserved so that AF/AC array ordering is preserved during VC reading/writing/manipulation. —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438
https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438:600,Usability,simpl,simplify,600,"+1MOn Sat, Feb 28, 2015 at 1:14 PM, ldgauthier notifications@github.com wrote: It would be wonderful to be able to use SelectVariants with a query like -select ""AF > 0.1"" on a VCF containing multiallelics and have it filter multiallelics by the allele with the highest AF. (Possibly conversely for ""AF < X""queries. Right now it crashes unless you use a crazy JEXL or pull out the multiallelics. Maybe we could make a maxAF/minAF in htsjdk/JEXLmap.java which equals AF for biallelics?. Internally, it might be nice to have a Map with the AF (or AC) for each allele for the SelectVariants issue and to simplify some of the crazy logic already in VariantAnnotator to deal with different allele ordering. As part of this task, we should also make 100% sure that allele ordering is preserved so that AF/AC array ordering is preserved during VC reading/writing/manipulation. —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76543438
https://github.com/broadinstitute/gatk/issues/242#issuecomment-76735788:191,Usability,simpl,simple,191,"It gives us the ability to easily aggregate records across multiple FeatureInputs, and (potentially, if we wanted) to retrieve records by type rather than by source. . FeatureInput should be simple, since (due to the way the argument-parsing system works) it must be initialized by a constructor that takes a single String (the argument value).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76735788
https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:171,Integrability,inject,injection,171,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266
https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:171,Security,inject,injection,171,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266
https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:65,Usability,clear,clearly,65,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266
https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391:188,Availability,down,downgrade,188,"I have a few lines of code that dynamically sets the log4j level for command line tools to match the existing VERBOSITY arg, It seems to work in simple testing so I don't think we need to downgrade to do it. Let me know if you want the code, or if you haven't started you can reassign this to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391
https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391:152,Testability,test,testing,152,"I have a few lines of code that dynamically sets the log4j level for command line tools to match the existing VERBOSITY arg, It seems to work in simple testing so I don't think we need to downgrade to do it. Let me know if you want the code, or if you haven't started you can reassign this to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391
https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391:145,Usability,simpl,simple,145,"I have a few lines of code that dynamically sets the log4j level for command line tools to match the existing VERBOSITY arg, It seems to work in simple testing so I don't think we need to downgrade to do it. Let me know if you want the code, or if you haven't started you can reassign this to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391
https://github.com/broadinstitute/gatk/pull/258#issuecomment-78085268:301,Usability,simpl,simplicity,301,"I was hoping that by hardcoding the standard set of covariates, we'd end up with something a bit cleaner -- but it looks like there is still a fair bit of ugliness here (eg., the persistence of things like `getOptionalCovariatesStartIndex()`). Do you feel that we've gained enough in code reduction / simplicity to justify the loss of flexibility?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/258#issuecomment-78085268
https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660:135,Testability,test,tests,135,"Initial commit by Mark DP in 2009: . > simpleComplement function() in BaseUtils. Generic framework for clipp…; > …ing reads along with tests. Support for Q score based clipping, sequence-specific clipping (not1), and clipping of ranges of bases (cycles 1-5, 10-15 for example). Can write out clipped bases as Ns, quality scores as 0s, or in the future will support softclipping the bases themselves. https://github.com/broadinstitute/gsa-unstable/commit/d6385e0d884cbd80c34e16e848297c3694f85a5a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660
https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660:39,Usability,simpl,simpleComplement,39,"Initial commit by Mark DP in 2009: . > simpleComplement function() in BaseUtils. Generic framework for clipp…; > …ing reads along with tests. Support for Q score based clipping, sequence-specific clipping (not1), and clipping of ranges of bases (cycles 1-5, 10-15 for example). Can write out clipped bases as Ns, quality scores as 0s, or in the future will support softclipping the bases themselves. https://github.com/broadinstitute/gsa-unstable/commit/d6385e0d884cbd80c34e16e848297c3694f85a5a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-95101660
https://github.com/broadinstitute/gatk/issues/265#issuecomment-99306834:27,Usability,simpl,simply,27,"This can be described more simply as:. Ensure that intervals in GVCF traversals are END tag aware, so all reference blocks are included correctly. GATK3 considers start position only, so some reference blocks are missed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/265#issuecomment-99306834
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551:24,Usability,clear,cleared,24,"Thanks for getting this cleared up. ; OK, what next? I'll check with colleagues who may be aware this 'feature'. Perhaps the case can be made more clearly by a group of users, including visible labs working on human evolutionary genomics. . I don't know the CA genomics community well, but my shallow poling suggests most are happily unaware that SNPs near indels will often be assigned lower quality than they might.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551:147,Usability,clear,clearly,147,"Thanks for getting this cleared up. ; OK, what next? I'll check with colleagues who may be aware this 'feature'. Perhaps the case can be made more clearly by a group of users, including visible labs working on human evolutionary genomics. . I don't know the CA genomics community well, but my shallow poling suggests most are happily unaware that SNPs near indels will often be assigned lower quality than they might.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279249883:379,Usability,clear,cleared,379,"This is probably affecting some of the GWAS studies but in subtle ways that; haven't popped up yet. I'm cc'ing Andrea in the hopes that he has some time; to think about the issue. I'd need some uninterrupted time to work out the; details and that's hard to come by at the moment. On Feb 11, 2017 12:21 AM, ""chlangley"" <notifications@github.com> wrote:. > Thanks for getting this cleared up.; > OK, what next? I'll check with colleagues who may be aware this 'feature'.; > Perhaps the case can be made more clearly by a group of users, including; > visible labs working on human evolutionary genomics.; >; > I don't know the CA genomics community well, but my shallow poling; > suggests most are happily unaware that SNPs near indels will often be; > assigned lower quality than they might.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdNaqeg_h2KxcxGULyoiSO3D8EY9eks5rbUVogaJpZM4DrC8o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279249883
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279249883:506,Usability,clear,clearly,506,"This is probably affecting some of the GWAS studies but in subtle ways that; haven't popped up yet. I'm cc'ing Andrea in the hopes that he has some time; to think about the issue. I'd need some uninterrupted time to work out the; details and that's hard to come by at the moment. On Feb 11, 2017 12:21 AM, ""chlangley"" <notifications@github.com> wrote:. > Thanks for getting this cleared up.; > OK, what next? I'll check with colleagues who may be aware this 'feature'.; > Perhaps the case can be made more clearly by a group of users, including; > visible labs working on human evolutionary genomics.; >; > I don't know the CA genomics community well, but my shallow poling; > suggests most are happily unaware that SNPs near indels will often be; > assigned lower quality than they might.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdNaqeg_h2KxcxGULyoiSO3D8EY9eks5rbUVogaJpZM4DrC8o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279249883
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279274088:656,Usability,clear,cleared,656,"Hi Laura, hope you are enjoying your maternity leave!; Unfortunately i will not have time to look into this, since I’m writing up a paper.; cheers,. > On Feb 12, 2017, at 4:17 PM, Laura Gauthier <gauthier@broadinstitute.org> wrote:; > ; > This is probably affecting some of the GWAS studies but in subtle ways that haven't popped up yet. I'm cc'ing Andrea in the hopes that he has some time to think about the issue. I'd need some uninterrupted time to work out the details and that's hard to come by at the moment.; > ; > On Feb 11, 2017 12:21 AM, ""chlangley"" <notifications@github.com <mailto:notifications@github.com>> wrote:; > Thanks for getting this cleared up.; > OK, what next? I'll check with colleagues who may be aware this 'feature'. Perhaps the case can be made more clearly by a group of users, including visible labs working on human evolutionary genomics.; > ; > I don't know the CA genomics community well, but my shallow poling suggests most are happily unaware that SNPs near indels will often be assigned lower quality than they might.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AGRhdNaqeg_h2KxcxGULyoiSO3D8EY9eks5rbUVogaJpZM4DrC8o>.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279274088
https://github.com/broadinstitute/gatk/issues/269#issuecomment-279274088:780,Usability,clear,clearly,780,"Hi Laura, hope you are enjoying your maternity leave!; Unfortunately i will not have time to look into this, since I’m writing up a paper.; cheers,. > On Feb 12, 2017, at 4:17 PM, Laura Gauthier <gauthier@broadinstitute.org> wrote:; > ; > This is probably affecting some of the GWAS studies but in subtle ways that haven't popped up yet. I'm cc'ing Andrea in the hopes that he has some time to think about the issue. I'd need some uninterrupted time to work out the details and that's hard to come by at the moment.; > ; > On Feb 11, 2017 12:21 AM, ""chlangley"" <notifications@github.com <mailto:notifications@github.com>> wrote:; > Thanks for getting this cleared up.; > OK, what next? I'll check with colleagues who may be aware this 'feature'. Perhaps the case can be made more clearly by a group of users, including visible labs working on human evolutionary genomics.; > ; > I don't know the CA genomics community well, but my shallow poling suggests most are happily unaware that SNPs near indels will often be assigned lower quality than they might.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/269#issuecomment-279122551>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AGRhdNaqeg_h2KxcxGULyoiSO3D8EY9eks5rbUVogaJpZM4DrC8o>.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279274088
https://github.com/broadinstitute/gatk/issues/269#issuecomment-283549213:92,Usability,feedback,feedback,92,"@chlangley One thing we could potentially do to attract attention to this issue and solicit feedback from the community would be to feature it on the GATK blog. If you were to write a concise case study detailing the impact of the problem on your results, others may be motivated to look at their own results, and if it causes problems there, add their voices to yours. We're willing to bring this to public attention, we just don't have the bandwidth to do the legwork.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-283549213
https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436:844,Integrability,rout,route,844,"Hello Geraldine:. > On 1/Mar/2017, at 7:56 PM, Geraldine Van der Auwera <notifications@github.com> wrote:; > ; > @chlangley One thing we could potentially do to attract attention to this issue and solicit feedback from the community would be to feature it on the GATK blog. If you were to write a concise case study detailing the impact of the problem on your results, others may be motivated to look at their own results, and if it causes problems there, add their voices to yours. We're willing to bring this to public attention, we just don't have the bandwidth to do the legwork. I started to work on this a bit and found myself blocked. . At this point I have a simple question: The GATK blog is separate from the forum (?). When I am on the blog page I can’t seem to find a button to submit a new post. I must be missing something or the route to blog posting is only via the forum?. Sorry to bother you with such mundane question. Cheers,; Chuck",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436
https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436:205,Usability,feedback,feedback,205,"Hello Geraldine:. > On 1/Mar/2017, at 7:56 PM, Geraldine Van der Auwera <notifications@github.com> wrote:; > ; > @chlangley One thing we could potentially do to attract attention to this issue and solicit feedback from the community would be to feature it on the GATK blog. If you were to write a concise case study detailing the impact of the problem on your results, others may be motivated to look at their own results, and if it causes problems there, add their voices to yours. We're willing to bring this to public attention, we just don't have the bandwidth to do the legwork. I started to work on this a bit and found myself blocked. . At this point I have a simple question: The GATK blog is separate from the forum (?). When I am on the blog page I can’t seem to find a button to submit a new post. I must be missing something or the route to blog posting is only via the forum?. Sorry to bother you with such mundane question. Cheers,; Chuck",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436
https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436:667,Usability,simpl,simple,667,"Hello Geraldine:. > On 1/Mar/2017, at 7:56 PM, Geraldine Van der Auwera <notifications@github.com> wrote:; > ; > @chlangley One thing we could potentially do to attract attention to this issue and solicit feedback from the community would be to feature it on the GATK blog. If you were to write a concise case study detailing the impact of the problem on your results, others may be motivated to look at their own results, and if it causes problems there, add their voices to yours. We're willing to bring this to public attention, we just don't have the bandwidth to do the legwork. I started to work on this a bit and found myself blocked. . At this point I have a simple question: The GATK blog is separate from the forum (?). When I am on the blog page I can’t seem to find a button to submit a new post. I must be missing something or the route to blog posting is only via the forum?. Sorry to bother you with such mundane question. Cheers,; Chuck",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287541436
https://github.com/broadinstitute/gatk/issues/269#issuecomment-287546670:230,Usability,clear,clear,230,"Hi Chuck, the GATK blog is set up to only accept posts from admins or moderators on the forum (or my team). If you're willing to write something up, we would do it as a guest post, where I would post the text on your behalf (with clear attribution to you). If you'd like to share a draft with us the easiest way to do it is through a google doc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287546670
https://github.com/broadinstitute/gatk/issues/269#issuecomment-287551239:354,Usability,clear,clear,354,"Sounds good. Thanks,; Chuck. > On Mar 18, 2017, at 06:33, Geraldine Van der Auwera <notifications@github.com> wrote:; > ; > Hi Chuck, the GATK blog is set up to only accept posts from admins or moderators on the forum (or my team). If you're willing to write something up, we would do it as a guest post, where I would post the text on your behalf (with clear attribution to you). If you'd like to share a draft with us the easiest way to do it is through a google doc.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-287551239
https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043:256,Deployability,pipeline,pipeline,256,"I think we have a good idea of what side inputs are for and when we would need them now. . My understanding is that side inputs are appropriate to use when you have a fixed object or set of objects which must be provided as a whole to a task or tasks in a pipeline. If these things can be known at pipeline creation and are inexpensive to generate, it's possible to simply pass the objects as parameters in the pipeline creation. However, if the object is generated as part of the pipeline, then it must be passed as a side input instead. . @wbrockman Is my understanding correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043
https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043:298,Deployability,pipeline,pipeline,298,"I think we have a good idea of what side inputs are for and when we would need them now. . My understanding is that side inputs are appropriate to use when you have a fixed object or set of objects which must be provided as a whole to a task or tasks in a pipeline. If these things can be known at pipeline creation and are inexpensive to generate, it's possible to simply pass the objects as parameters in the pipeline creation. However, if the object is generated as part of the pipeline, then it must be passed as a side input instead. . @wbrockman Is my understanding correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043
https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043:411,Deployability,pipeline,pipeline,411,"I think we have a good idea of what side inputs are for and when we would need them now. . My understanding is that side inputs are appropriate to use when you have a fixed object or set of objects which must be provided as a whole to a task or tasks in a pipeline. If these things can be known at pipeline creation and are inexpensive to generate, it's possible to simply pass the objects as parameters in the pipeline creation. However, if the object is generated as part of the pipeline, then it must be passed as a side input instead. . @wbrockman Is my understanding correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043
https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043:481,Deployability,pipeline,pipeline,481,"I think we have a good idea of what side inputs are for and when we would need them now. . My understanding is that side inputs are appropriate to use when you have a fixed object or set of objects which must be provided as a whole to a task or tasks in a pipeline. If these things can be known at pipeline creation and are inexpensive to generate, it's possible to simply pass the objects as parameters in the pipeline creation. However, if the object is generated as part of the pipeline, then it must be passed as a side input instead. . @wbrockman Is my understanding correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043
https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043:366,Usability,simpl,simply,366,"I think we have a good idea of what side inputs are for and when we would need them now. . My understanding is that side inputs are appropriate to use when you have a fixed object or set of objects which must be provided as a whole to a task or tasks in a pipeline. If these things can be known at pipeline creation and are inexpensive to generate, it's possible to simply pass the objects as parameters in the pipeline creation. However, if the object is generated as part of the pipeline, then it must be passed as a side input instead. . @wbrockman Is my understanding correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/282#issuecomment-94354043
https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:155,Performance,perform,perform,155,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026
https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:54,Usability,simpl,simple,54,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026
https://github.com/broadinstitute/gatk/issues/305#issuecomment-79219917:63,Usability,clear,clear,63,"what `merge()` returns is a `Locatable` though, so it would be clear that it isn't a `SamRecord` or a `VariantContext`. Merge is a bit of a strange one, because you would need to return some concrete implementation. I guess the most pure thing to do would be to return an anonymous implementation of Locatable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79219917
https://github.com/broadinstitute/gatk/issues/312#issuecomment-82511749:52,Usability,clear,clear,52,I think that the name `Transformer` makes it pretty clear that it mutates its input. Returning a brand new read upon each transformation would introduce non-trivial overhead for no good reason (unless we're going to make reads immutable across the board).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82511749
https://github.com/broadinstitute/gatk/issues/312#issuecomment-82514530:59,Usability,clear,clear,59,Then they should be functions from `read -> ()` to make it clear that they operate via side effects I think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82514530
https://github.com/broadinstitute/gatk/issues/317#issuecomment-185798929:148,Usability,clear,clear,148,"It's a problem because we have to work with formats that support zero-length intervals (eg., BED). We need to talk this issue through and come to a clear decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/317#issuecomment-185798929
https://github.com/broadinstitute/gatk/issues/322#issuecomment-94592372:86,Usability,guid,guide,86,"For reference, doc from the GATK3 best practices (https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_bqsr_AnalyzeCovariates.php). ```; # Generate the first pass recalibration table file.; java -jar GenomeAnalysisTK.jar \; -T BaseRecalibrator \; -R myreference.fasta \; -I myinput.bam \; -knownSites bundle/my-trusted-snps.vcf \ # optional but recommendable; -knownSites bundle/my-trusted-indels.vcf \ # optional but recommendable; ... other options; -o firstpass.table. # Generate the second pass recalibration table file.; java -jar GenomeAnalysisTK.jar \; -T BaseRecalibrator \; -BQSR firstpass.table \; -R myreference.fasta \; -I myinput.bam \; -knownSites bundle/my-trusted-snps.vcf \; -knownSites bundle/my-trusted-indels.vcf \; ... other options \; -o secondpass.table. # Finally generate the plots and also keep a copy of the csv (optional).; java -jar GenomeAnalysisTK.jar \; -T AnalyzeCovariates \; -R myrefernce.fasta \; -before firstpass.table \; -after secondpass.table \; -csv BQSR.csv \ # optional; -plots BQSR.pdf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/322#issuecomment-94592372
https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,Energy Efficiency,adapt,adapt,86,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420
https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,Modifiability,adapt,adapt,86,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420
https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:357,Security,access,access,357,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420
https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:146,Usability,simpl,simple,146,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420
https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:521,Usability,simpl,simple,521,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420
https://github.com/broadinstitute/gatk/issues/335#issuecomment-116846893:14,Usability,feedback,feedback,14,"After getting feedback from @vruano on pull request [45](https://github.com/broadinstitute/hellbender-protected/pull/45) in hellbender-protected and speaking with @droazen and @lbergelson, it seems that LocusWalker should be implemented instead. For the time being we will move forward with SamLocusIterator, but I'll file a separate issue in hellbender-protected to remind us to switch over to LocusWalker once it is implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-116846893
https://github.com/broadinstitute/gatk/pull/345#issuecomment-101324830:34,Usability,clear,clear,34,"Am un-assigning this, as it's not clear that we want to merge this at this time. Feel free to re-assign if you'd like to see this merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/345#issuecomment-101324830
https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762:67,Availability,error,error,67,I can confirm that the fix works for me: I now see a user-friendly error message. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762
https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762:73,Integrability,message,message,73,I can confirm that the fix works for me: I now see a user-friendly error message. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762
https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762:53,Usability,user-friendly,user-friendly,53,I can confirm that the fix works for me: I now see a user-friendly error message. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762
https://github.com/broadinstitute/gatk/issues/364#issuecomment-94807857:23,Usability,simpl,simple,23,"perfect. Maybe it's as simple as switching all HashSets to LinkedHashSet; (and same for maps). On Tue, Apr 21, 2015 at 10:02 AM, Matt Sooknah notifications@github.com; wrote:. > Ah yes, I remember this now - they haven't been failing in Picard per se,; > it's just when we tried compiling under Java 8. The info in that issue; > should make the fix much easier, if it's indeed the same thing.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/364#issuecomment-94805095; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/364#issuecomment-94807857
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93779237:32,Usability,feedback,feedback,32,"@nh13 in particular should give feedback on whether these guidelines are useful, or if additional clarity is needed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93779237
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93779237:58,Usability,guid,guidelines,58,"@nh13 in particular should give feedback on whether these guidelines are useful, or if additional clarity is needed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93779237
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346:275,Testability,test,tests,275,"> Perhaps we want that the author certifies it is in a usable state for its intended use?. It's good that the author certifies it, but we are going to be tearing the tools apart and rebuilding them without necessarily understanding the details of the output. We need to have tests that will tell us if we've broken them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346:55,Usability,usab,usable,55,"> Perhaps we want that the author certifies it is in a usable state for its intended use?. It's good that the author certifies it, but we are going to be tearing the tools apart and rebuilding them without necessarily understanding the details of the output. We need to have tests that will tell us if we've broken them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93804346
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764:78,Testability,test,tests,78,"Certainly the author can't just ""certify"" that the tool works without writing tests to prove it on an ongoing basis as the tool is modified -- I don't think that was what @nh13 meant (but please correct me if I misunderstood). I read that as ""if tests pass, the author certifies that the tool is in a usable state for its intended use(s)""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764:246,Testability,test,tests,246,"Certainly the author can't just ""certify"" that the tool works without writing tests to prove it on an ongoing basis as the tool is modified -- I don't think that was what @nh13 meant (but please correct me if I misunderstood). I read that as ""if tests pass, the author certifies that the tool is in a usable state for its intended use(s)""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764
https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764:301,Usability,usab,usable,301,"Certainly the author can't just ""certify"" that the tool works without writing tests to prove it on an ongoing basis as the tool is modified -- I don't think that was what @nh13 meant (but please correct me if I misunderstood). I read that as ""if tests pass, the author certifies that the tool is in a usable state for its intended use(s)""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382#issuecomment-93806764
https://github.com/broadinstitute/gatk/pull/383#issuecomment-101324906:34,Usability,clear,clear,34,"Am un-assigning this, as it's not clear that we want to merge this at this time. Feel free to re-assign if you'd like to see this merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/383#issuecomment-101324906
https://github.com/broadinstitute/gatk/pull/415#issuecomment-94565460:214,Usability,guid,guidelines,214,[![Coverage Status](https://coveralls.io/builds/2381417/badge)](https://coveralls.io/builds/2381417). Coverage increased (+0.01%) to 70.37% when pulling **ddae3d1f9ca34832ab52d445f25c2e2cde2fafc3 on akiezun-README-guidelines** into **eff7d0f596502c4425366fad1985e6a1d5ad0542 on master**.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/415#issuecomment-94565460
https://github.com/broadinstitute/gatk/pull/416#issuecomment-99142930:132,Usability,undo,undocumented,132,"I don't think this branch satisfies https://github.com/broadinstitute/hellbender/issues/372 at all. There are still methods with 4+ undocumented parameters, and you eliminated what seemed a useful abstraction (`TruthSensitivityMetric`) in favor of passing around a raw array of doubles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/416#issuecomment-99142930
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:399,Deployability,integrat,integration,399,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:399,Integrability,integrat,integration,399,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:70,Security,validat,validation,70,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:304,Security,validat,validation,304,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:690,Security,validat,validation,690,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:163,Testability,test,test,163,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:244,Testability,assert,assertSamsEqual,244,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:411,Testability,test,test,411,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:435,Testability,test,tests,435,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:547,Testability,test,test,547,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:589,Testability,test,test,589,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:10,Usability,guid,guidance,10,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266
https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320:103,Security,sanitiz,sanitize,103,"For repeated operators (whether xIyI or xMyM), I think GATK3 has/had a function to simplify cigars to ""sanitize"" that situation. In terms of desired behavior, we don't want to filter out the reads, we want to transform them to be processable without difficulty. I think xIyD should be considered valid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320
https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320:83,Usability,simpl,simplify,83,"For repeated operators (whether xIyI or xMyM), I think GATK3 has/had a function to simplify cigars to ""sanitize"" that situation. In terms of desired behavior, we don't want to filter out the reads, we want to transform them to be processable without difficulty. I think xIyD should be considered valid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320
https://github.com/broadinstitute/gatk/pull/432#issuecomment-97531671:24,Usability,clear,clearer,24,"@vruano This is so much clearer now. Much better to write your own search function that does what you want than to use the existing one with an invalid comparator and rely on the implementation to stay the same. . The only issues left I think are to create a comparator in SimpleInterval and just use that, instead of having a lot of them scattered all over the place. I think they're all compatible with doing the same thing sorting first by contig, then start, then end. That and using the new SimpleInterval(Locatable) constructor when appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/432#issuecomment-97531671
https://github.com/broadinstitute/gatk/pull/435#issuecomment-95712825:96,Usability,simpl,simply,96,"Hi Nick, looks good. A few comments: many if not all of the warnings could be fixed rather than simply suppressed. Fixing warnings is always preferred to suppressing them. Can you try fixing them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/435#issuecomment-95712825
https://github.com/broadinstitute/gatk/pull/440#issuecomment-96059672:421,Usability,simpl,simple,421,"@akiezun Could you take a look at this? . It's adding a mechanism for automatically switching in the appropriate argument collection based on whether a tool overrides `requiresReference` and it's ilk. It plays nicely with help and command line parsing in a way that it didn't before. . Unfortunately, it adds a bunch of really repetitive code. I think it's worth the extra Optional/Required classes, since they're pretty simple and they ideally shouldn't have to be touched at all. (Although if we end up making complicated additions to them it might be worth a revisit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/440#issuecomment-96059672
https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999:24,Availability,error,error,24,That would be a clearer error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999
https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999:30,Integrability,message,message,30,That would be a clearer error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999
https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999:16,Usability,clear,clearer,16,That would be a clearer error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999
https://github.com/broadinstitute/gatk/issues/489#issuecomment-119218019:25,Deployability,release,release,25,closing. `invalid source release: 1.8` is pretty clear,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-119218019
https://github.com/broadinstitute/gatk/issues/489#issuecomment-119218019:49,Usability,clear,clear,49,closing. `invalid source release: 1.8` is pretty clear,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-119218019
https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449:121,Deployability,release,release,121,"Disagree. For sophisticated Java developers like us, it's clear enough. But will the average GATK user know that `source release 1.8` refers to **Java 8**, and that they may need to set their Java default version **manually** even after installing Java 8? Would like to hear from @vdauwera on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449
https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449:237,Deployability,install,installing,237,"Disagree. For sophisticated Java developers like us, it's clear enough. But will the average GATK user know that `source release 1.8` refers to **Java 8**, and that they may need to set their Java default version **manually** even after installing Java 8? Would like to hear from @vdauwera on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449
https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449:58,Usability,clear,clear,58,"Disagree. For sophisticated Java developers like us, it's clear enough. But will the average GATK user know that `source release 1.8` refers to **Java 8**, and that they may need to set their Java default version **manually** even after installing Java 8? Would like to hear from @vdauwera on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-119227449
https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:404,Integrability,interface,interface,404,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247
https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:568,Modifiability,extend,extends,568,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247
https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:609,Modifiability,extend,extends,609,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247
https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:1012,Testability,test,testing,1012,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247
https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:507,Usability,simpl,simply,507,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:20,Deployability,update,updated,20,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:126,Deployability,integrat,integration,126,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:126,Integrability,integrat,integration,126,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:62,Testability,test,test,62,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:138,Testability,test,test,138,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201:10,Usability,feedback,feedback,10,"Addressed feedback, updated so it can write reports and added test that checks the exact same reports as the BaseRecalibrator integration test (they pass).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101783201
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101807825:33,Deployability,update,update,33,Thank you for the feedback! Will update style.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101807825
https://github.com/broadinstitute/gatk/pull/511#issuecomment-101807825:18,Usability,feedback,feedback,18,Thank you for the feedback! Will update style.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101807825
https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315:26,Testability,test,test,26,"That would not be a valid test, since it wouldn't be testing the way the code actually handles invalid intervals. All we want to know is that we throw when we encounter an invalid interval. Did I mention that this very simple change is urgently needed by many branches?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315
https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315:53,Testability,test,testing,53,"That would not be a valid test, since it wouldn't be testing the way the code actually handles invalid intervals. All we want to know is that we throw when we encounter an invalid interval. Did I mention that this very simple change is urgently needed by many branches?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315
https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315:219,Usability,simpl,simple,219,"That would not be a valid test, since it wouldn't be testing the way the code actually handles invalid intervals. All we want to know is that we throw when we encounter an invalid interval. Did I mention that this very simple change is urgently needed by many branches?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104401315
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677904:18,Usability,feedback,feedback,18,Thank you for the feedback! I applied it all. Back to @droazen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677904
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686:59,Testability,test,tests,59,"Applied feedback, rebased, squashed. Merge pending passing tests. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686:8,Usability,feedback,feedback,8,"Applied feedback, rebased, squashed. Merge pending passing tests. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694686
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694945:201,Usability,feedback,feedback,201,"@pgrosu, would you like to try your hand at writing a minimal repro for the bug? This would be useful, and this doesn't require signing anything. You could then submit it as a bug report (or to SO for feedback first).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107694945
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2200,Availability,error,error,2200,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:4857,Availability,down,down,4857,"cle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Done initializing engine; 17:51:03.123 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Shutting down engine; [June 1, 2015 5:51:03 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1077936128; Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.collect.Sets.newConcurrentHashSet()Ljava/util/Set;; at com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:426); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:77); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); ```. I don't have a billing-enabled GCS account, so I did this test to see if I could make it run with my `client-secrets.json` file. Any g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:441,Integrability,interface,interfaces,441,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:863,Integrability,synchroniz,synchronize,863,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:1333,Integrability,interface,interface,1333,"serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase fro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:721,Performance,perform,performing,721,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2222,Performance,perform,performing,2222,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2278,Performance,perform,performed,2278,"d I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:404,Security,hash,hash,404,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2176,Testability,test,test,2176,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2704,Testability,test,test,2704,"ltSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_qu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2768,Testability,test,test,2768,"ds();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2891,Testability,test,test,2891,"igs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3239,Testability,test,test,3239,"orming something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.jso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3302,Testability,test,test,3302,"t https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3497,Testability,test,test,3497,"2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitut",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:3629,Testability,test,test,3629,"version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8/./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true --input ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf --reference ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --run_without_dbsnp_potentially_ruining_quality false --solid_recal_mode SET_Q_ZERO --solid_nocall_strategy THROW_EXCEPTION --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_default_quality -1 --insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:5787,Testability,test,test,5787,"-insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Done initializing engine; 17:51:03.123 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Shutting down engine; [June 1, 2015 5:51:03 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1077936128; Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.collect.Sets.newConcurrentHashSet()Ljava/util/Set;; at com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:426); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:77); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); ```. I don't have a billing-enabled GCS account, so I did this test to see if I could make it run with my `client-secrets.json` file. Any guidance would be appreciated. Thank you,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:5862,Usability,guid,guidance,5862,"-insertions_default_quality 45 --deletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --interval_set_rule UNION --interval_padding 0 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_less_than 6 --useOriginalQualities false --defaultBaseQualities -1 --runner LOCAL --client_secret client-secrets.json --help false --version false --VERBOSITY INFO --QUIET false; [June 1, 2015 5:51:02 PM EDT] Executing as pgrosu@eofe5 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:version-unknown-SNAPSHOT JdkDeflater; 17:51:02.969 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Initializing engine; 17:51:02.970 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Done initializing engine; 17:51:03.123 [main] INFO org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow - Shutting down engine; [June 1, 2015 5:51:03 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1077936128; Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.collect.Sets.newConcurrentHashSet()Ljava/util/Set;; at com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:426); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:77); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); ```. I don't have a billing-enabled GCS account, so I did this test to see if I could make it run with my `client-secrets.json` file. Any guidance would be appreciated. Thank you,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499
https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579:204,Modifiability,rewrite,rewrite,204,"@lbergelson You're right, it would be easier to read that way, but it leaves a dangling ""Optional Arguments"" string in the output even when there are no optional arguments. If you're still not sold I can rewrite it to iterate once to count the optional args, but this is a cheap, simple way to get the right output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579
https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579:280,Usability,simpl,simple,280,"@lbergelson You're right, it would be easier to read that way, but it leaves a dangling ""Optional Arguments"" string in the output even when there are no optional arguments. If you're still not sold I can rewrite it to iterate once to count the optional args, but this is a cheap, simple way to get the right output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:232,Availability,error,errors,232,"I did some more work on the broadcast approach to see how feasible it would be, and found that Spark Dataflow made two unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:314,Deployability,pipeline,pipeline,314,"I did some more work on the broadcast approach to see how feasible it would be, and found that Spark Dataflow made two unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:475,Deployability,pipeline,pipelines,475,"I did some more work on the broadcast approach to see how feasible it would be, and found that Spark Dataflow made two unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:1918,Deployability,pipeline,pipeline,1918," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:2064,Deployability,pipeline,pipeline,2064," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:1148,Energy Efficiency,efficient,efficient,1148," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:1158,Integrability,protocol,protocol,1158," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:1189,Modifiability,variab,variables,1189," unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tree/da_read_pipeline? Is that at a point where I could try with pipeline on Spark, or should I wait until it's merged?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:641,Usability,simpl,simply,641,"I did some more work on the broadcast approach to see how feasible it would be, and found that Spark Dataflow made two unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:247,Availability,error,errors,247,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:295,Availability,error,errors,295,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:417,Availability,error,error,417,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1386,Availability,error,errors,1386,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1749,Availability,redundant,redundant,1749,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1749,Safety,redund,redundant,1749,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:823,Security,validat,validation,823,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1375,Security,validat,validation,1375,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:276,Testability,test,tests,276,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:678,Testability,test,tests,678,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:723,Testability,test,tests,723,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:955,Testability,test,test,955,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1425,Testability,test,tests,1425,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1486,Testability,test,test,1486,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1566,Usability,simpl,simple,1566,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:240,Availability,error,errors,240,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:346,Availability,error,errors,346,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:406,Availability,error,error,406,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:496,Availability,error,error,496,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:506,Availability,error,error,506,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:1224,Availability,error,errors,1224,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:1281,Availability,down,downside,1281,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:934,Deployability,release,released,934,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:1213,Security,validat,validation,1213,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:1348,Security,validat,validator,1348,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392:84,Usability,clear,clearer,84,"Your interpretation sounds right, although I wish the language in the SAM spec were clearer - something like ""if 0x1 is unset, fields 0x2, 0x8, 0x20, 0x40, and 0x80 have no meaning and are ignored by the tools"". SAMRecord.IsValid() returns errors not only for 0x8(mate unmapped)/unpaired read, but also for the other four fields, so all of these errors would need to be removed. IsValid() also triggers an error when an unpaired read has RNEXT set, but the spec. ; doesn't appear to exclude this error. No error is triggered for the unpaired/PNEXT case. . So I agree that it looks like IsValid() should be changed to align with the spec. But I can also imagine potential pitfalls of leaving the GenomicsConverter code the way it is. The code adds spurious information to the bam that might cause problems with legacy versions of the tools. I don't know what the plans are for the state of the existing tool distribution once gatk4 is released. Is it possible that bam files produced in the cloud could make their way into gatk3 workflows, maybe via the sharing of bams between groups? If this happens, then unpaired reads processed in the cloud, with their mate unpaired flags set by the converter, could trigger validation errors when they are fed to the legacy tools. Is there a downside to altering the converter code as well as modifying the ; validator (I don't know what altering google packages entails...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114510392
https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160:31,Testability,test,test,31,"Cleaning the bam (and the sam) test files is almost done - it's taken some time because some have been like mini debugging exercises in themselves, followed by the capturing and substitution of the new expected output files for the tests. There are a few issues that still remain, however, and unfortunately I am out of time - I'm headed to an overseas meeting on Monday and will be out for two weeks. I had hoped to finish before the trip, but my BMC Bioinformatics paper came through so I had to spend time on proofs etc.. I will resume ASAP after I get back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160
https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160:232,Testability,test,tests,232,"Cleaning the bam (and the sam) test files is almost done - it's taken some time because some have been like mini debugging exercises in themselves, followed by the capturing and substitution of the new expected output files for the tests. There are a few issues that still remain, however, and unfortunately I am out of time - I'm headed to an overseas meeting on Monday and will be out for two weeks. I had hoped to finish before the trip, but my BMC Bioinformatics paper came through so I had to spend time on proofs etc.. I will resume ASAP after I get back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160
https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160:532,Usability,resume,resume,532,"Cleaning the bam (and the sam) test files is almost done - it's taken some time because some have been like mini debugging exercises in themselves, followed by the capturing and substitution of the new expected output files for the tests. There are a few issues that still remain, however, and unfortunately I am out of time - I'm headed to an overseas meeting on Monday and will be out for two weeks. I had hoped to finish before the trip, but my BMC Bioinformatics paper came through so I had to spend time on proofs etc.. I will resume ASAP after I get back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-122571160
https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502:132,Integrability,message,message,132,"Sorry, I wasn't very clear: Spark doesn't return the user exception to the driver even as the 'cause' exception (only the exception message is preserved). So it won't be possible to do the unwrapping in the same way at the moment. I agree that #551 will help catch regressions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502
https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502:21,Usability,clear,clear,21,"Sorry, I wasn't very clear: Spark doesn't return the user exception to the driver even as the 'cause' exception (only the exception message is preserved). So it won't be possible to do the unwrapping in the same way at the moment. I agree that #551 will help catch regressions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/574#issuecomment-113196502
https://github.com/broadinstitute/gatk/issues/588#issuecomment-308850737:285,Usability,simpl,simple,285,"@vdauwera - I think that makes sense. We've been brainstorming ideas for how a user would actually input the filter strings and there seem to be a few options. - JEXL; - it's already in use elsewhere and we can use JEXL functions at the command-line to specify ""hasAtLeast(5,""D"")"" for simple filters, but it seems like it would get clunky with increasing filter complexity; - Regular Expressions; - they're fairly universal, but it would be hard to match numerical values and can be confusing/exhausting to write correctly; - Modified regular expressions, for example:; - ^D matches any number of deletions at the start; - DMD matches any number of deletions followed by any number of (mis-)matches, followed by any number of deletions; - ^<5SM>=4D$ matches less than 5 soft clipped bases at the start of the cigar, followed by any number of (mis-)matches, followed by at least 4 deletions at the end of the cigar; - Command-line options passed into the filter, for example:; - --hasAtLeast 5 D --startsWith S --endsWith M. We can also implement some combination of these. What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-308850737
https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:103,Energy Efficiency,power,power,103,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640
https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:121,Safety,avoid,avoid,121,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640
https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:77,Usability,usab,usability,77,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640
https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:97,Availability,error,error-submitting-a-cloud-dataflow-job,97,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863
https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:182,Performance,load,loading,182,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863
https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:32,Usability,learn,learned,32,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:108,Testability,test,test,108,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:177,Testability,test,test,177,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:311,Testability,log,logging,311,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:352,Testability,log,logging,352,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:439,Testability,test,test,439,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:491,Testability,log,logging,491,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:506,Testability,test,test,506,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930:263,Usability,clear,clear,263,"@cmnbroad Sorry for the very long delay before reviewing this. I think this is a good solution. I think the test is a bit weird at the moment, but it's a weird thing to try and test. . 1: Could you make `setLoggingLevel` a static method in `Utils`, and give it a clear comment saying that it changes the global logging level (it does change the global logging level yes? Do I misunderstand what's happening? ) . 2: Could you then move the test to `UtilsUnitTest` run through all 4 levels of logging in the test, and then reset it to the initial state (if that's possible..) . Thanks for figuring this out and sorry for delay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122420930
https://github.com/broadinstitute/gatk/issues/610#issuecomment-118360200:325,Usability,guid,guides,325,"Thanks for reporting this @lbergelson. I've raised it internally at Cloudera, so we'll see if it can be fixed. I also had a hunt around to see if it would be possible to suppress this on the Gradle side, but I couldn't see anything. It's possible for Maven, but I don't think this applies to Gradle:. http://maven.apache.org/guides/mini/guide-http-settings.html#Ignoring_Cookies",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/610#issuecomment-118360200
https://github.com/broadinstitute/gatk/issues/610#issuecomment-118360200:337,Usability,guid,guide-http-settings,337,"Thanks for reporting this @lbergelson. I've raised it internally at Cloudera, so we'll see if it can be fixed. I also had a hunt around to see if it would be possible to suppress this on the Gradle side, but I couldn't see anything. It's possible for Maven, but I don't think this applies to Gradle:. http://maven.apache.org/guides/mini/guide-http-settings.html#Ignoring_Cookies",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/610#issuecomment-118360200
https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252:709,Availability,avail,available,709,"Posting at the suggetion of shlee. There's discussion about what parts of VariantEval will be ported to GATK4 or whether Picard's partially overlapping tool CollectVariantCallingMetrics will take this over. I want to at least make you aware that we've developed a tool we're calling VariantQC, which is built in GATK3 and runs VariantEval internally to generate data stratified in various ways to make HTML QC reports, sorta like FASTQC or MultiQC (https://github.com/bbimber/gatk-protected/releases). An example report is here: https://prime-seq.ohsu.edu/_webdav/Internal/Bimber/Public/%40files/VariantQC_Example.html. Our goal was always to port this to GATK4, polish it up, and then make it more generally available. Much of what this tool does is take the pre-built tables/reports from VariantEval and put them into HTML, but we also wrote some custom stratifications to bin data by filter, etc. Like this thread notes, VariantEval has a lot of features not in picard, and honestly we dont use many of them. However, the extensibility of Stratifier/VariantEvaluator are pretty important for us. . We realize this is prioritized against all the GATK4 features; however, 1) how set are plans about migration of VariantEval/merge w/ picard and 2) if most of VariantEval isnt going to be ported, can we pick it up in our repo? We could also potentially offer some assistance in porting the tool because we have a vested interest; however, unless the task is defined as porting VariantEval as close as possible to as-is (not that this is critical, but it's the simplest thing for the outsider to do), it would need some discussion around exactly what's planned.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252
https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252:491,Deployability,release,releases,491,"Posting at the suggetion of shlee. There's discussion about what parts of VariantEval will be ported to GATK4 or whether Picard's partially overlapping tool CollectVariantCallingMetrics will take this over. I want to at least make you aware that we've developed a tool we're calling VariantQC, which is built in GATK3 and runs VariantEval internally to generate data stratified in various ways to make HTML QC reports, sorta like FASTQC or MultiQC (https://github.com/bbimber/gatk-protected/releases). An example report is here: https://prime-seq.ohsu.edu/_webdav/Internal/Bimber/Public/%40files/VariantQC_Example.html. Our goal was always to port this to GATK4, polish it up, and then make it more generally available. Much of what this tool does is take the pre-built tables/reports from VariantEval and put them into HTML, but we also wrote some custom stratifications to bin data by filter, etc. Like this thread notes, VariantEval has a lot of features not in picard, and honestly we dont use many of them. However, the extensibility of Stratifier/VariantEvaluator are pretty important for us. . We realize this is prioritized against all the GATK4 features; however, 1) how set are plans about migration of VariantEval/merge w/ picard and 2) if most of VariantEval isnt going to be ported, can we pick it up in our repo? We could also potentially offer some assistance in porting the tool because we have a vested interest; however, unless the task is defined as porting VariantEval as close as possible to as-is (not that this is critical, but it's the simplest thing for the outsider to do), it would need some discussion around exactly what's planned.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252
https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252:1560,Usability,simpl,simplest,1560,"Posting at the suggetion of shlee. There's discussion about what parts of VariantEval will be ported to GATK4 or whether Picard's partially overlapping tool CollectVariantCallingMetrics will take this over. I want to at least make you aware that we've developed a tool we're calling VariantQC, which is built in GATK3 and runs VariantEval internally to generate data stratified in various ways to make HTML QC reports, sorta like FASTQC or MultiQC (https://github.com/bbimber/gatk-protected/releases). An example report is here: https://prime-seq.ohsu.edu/_webdav/Internal/Bimber/Public/%40files/VariantQC_Example.html. Our goal was always to port this to GATK4, polish it up, and then make it more generally available. Much of what this tool does is take the pre-built tables/reports from VariantEval and put them into HTML, but we also wrote some custom stratifications to bin data by filter, etc. Like this thread notes, VariantEval has a lot of features not in picard, and honestly we dont use many of them. However, the extensibility of Stratifier/VariantEvaluator are pretty important for us. . We realize this is prioritized against all the GATK4 features; however, 1) how set are plans about migration of VariantEval/merge w/ picard and 2) if most of VariantEval isnt going to be ported, can we pick it up in our repo? We could also potentially offer some assistance in porting the tool because we have a vested interest; however, unless the task is defined as porting VariantEval as close as possible to as-is (not that this is critical, but it's the simplest thing for the outsider to do), it would need some discussion around exactly what's planned.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799:145,Deployability,release,release,145,"Apologies @bbimber -- your efforts to port this tool to GATK4 are much appreciated. Our team has been extremely busy with the lead up to the 4.0 release, which is why we haven't been as responsive lately. I'll have someone take a look at the test data in question to see if it can be publicly shared.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799:242,Testability,test,test,242,"Apologies @bbimber -- your efforts to port this tool to GATK4 are much appreciated. Our team has been extremely busy with the lead up to the 4.0 release, which is why we haven't been as responsive lately. I'll have someone take a look at the test data in question to see if it can be publicly shared.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799:186,Usability,responsiv,responsive,186,"Apologies @bbimber -- your efforts to port this tool to GATK4 are much appreciated. Our team has been extremely busy with the lead up to the 4.0 release, which is why we haven't been as responsive lately. I'll have someone take a look at the test data in question to see if it can be publicly shared.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358029799
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120:98,Testability,test,test,98,"I fully understand, and realize this isnt a priority for the group. Nonetheless, just getting the test data seems like it should be a simple thing if at all possible. i dont know the full reasoning behind why the GATK3 test data are not public, but I have no need to share it beyond myself if that makes this easier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120:219,Testability,test,test,219,"I fully understand, and realize this isnt a priority for the group. Nonetheless, just getting the test data seems like it should be a simple thing if at all possible. i dont know the full reasoning behind why the GATK3 test data are not public, but I have no need to share it beyond myself if that makes this easier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120:134,Usability,simpl,simple,134,"I fully understand, and realize this isnt a priority for the group. Nonetheless, just getting the test data seems like it should be a simple thing if at all possible. i dont know the full reasoning behind why the GATK3 test data are not public, but I have no need to share it beyond myself if that makes this easier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358031120
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795:236,Deployability,update,update,236,"@bbimber Unfortunately it's not so simple -- some of the GATK3 test data cannot be shared externally at all due to, eg., IRB restrictions. Someone will have to look at the test data in question to make sure that it can be shared. We'll update you once we've done this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795:63,Testability,test,test,63,"@bbimber Unfortunately it's not so simple -- some of the GATK3 test data cannot be shared externally at all due to, eg., IRB restrictions. Someone will have to look at the test data in question to make sure that it can be shared. We'll update you once we've done this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795:172,Testability,test,test,172,"@bbimber Unfortunately it's not so simple -- some of the GATK3 test data cannot be shared externally at all due to, eg., IRB restrictions. Someone will have to look at the test data in question to make sure that it can be shared. We'll update you once we've done this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795
https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795:35,Usability,simpl,simple,35,"@bbimber Unfortunately it's not so simple -- some of the GATK3 test data cannot be shared externally at all due to, eg., IRB restrictions. Someone will have to look at the test data in question to make sure that it can be shared. We'll update you once we've done this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358032795
https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611:190,Testability,benchmark,benchmarking,190,"Update: we have a PoC impl. working with sharded writing and simple indexing implementation in https://github.com/googlegenomics/dataflow-java/tree/sharded-bam-writer , need to do some more benchmarking and merge to main branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611
https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611:61,Usability,simpl,simple,61,"Update: we have a PoC impl. working with sharded writing and simple indexing implementation in https://github.com/googlegenomics/dataflow-java/tree/sharded-bam-writer , need to do some more benchmarking and merge to main branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/621#issuecomment-132294611
https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505:129,Testability,test,test,129,"Thanks for all the feedback Adam. I got a bunch of the metrics code written; today and hopefully once I have that I can actually test this code and port; the other tests. I will merge that PR into this one and fix these changes; and get back to you. On Thu, Jul 16, 2015 at 9:04 PM, Adam Kiezun notifications@github.com; wrote:. > Assigned #631 https://github.com/broadinstitute/hellbender/pull/631 to; > @tovanadler https://github.com/tovanadler.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/631#event-358132720.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505
https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505:164,Testability,test,tests,164,"Thanks for all the feedback Adam. I got a bunch of the metrics code written; today and hopefully once I have that I can actually test this code and port; the other tests. I will merge that PR into this one and fix these changes; and get back to you. On Thu, Jul 16, 2015 at 9:04 PM, Adam Kiezun notifications@github.com; wrote:. > Assigned #631 https://github.com/broadinstitute/hellbender/pull/631 to; > @tovanadler https://github.com/tovanadler.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/631#event-358132720.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505
https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505:19,Usability,feedback,feedback,19,"Thanks for all the feedback Adam. I got a bunch of the metrics code written; today and hopefully once I have that I can actually test this code and port; the other tests. I will merge that PR into this one and fix these changes; and get back to you. On Thu, Jul 16, 2015 at 9:04 PM, Adam Kiezun notifications@github.com; wrote:. > Assigned #631 https://github.com/broadinstitute/hellbender/pull/631 to; > @tovanadler https://github.com/tovanadler.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/631#event-358132720.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/631#issuecomment-122149505
https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449:344,Availability,error,error,344,"To clarify this ticket: in `GATKTool.initializeReads()`, just check `readArguments.getReadFiles()` for files ending with a cram extension (should see if there's a canonical method in htsjdk for checking whether a file is cram) -- if you find any and we don't have a reference according to `hasReference()`, throw a `UserException` with a clear error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449
https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449:350,Integrability,message,message,350,"To clarify this ticket: in `GATKTool.initializeReads()`, just check `readArguments.getReadFiles()` for files ending with a cram extension (should see if there's a canonical method in htsjdk for checking whether a file is cram) -- if you find any and we don't have a reference according to `hasReference()`, throw a `UserException` with a clear error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449
https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449:338,Usability,clear,clear,338,"To clarify this ticket: in `GATKTool.initializeReads()`, just check `readArguments.getReadFiles()` for files ending with a cram extension (should see if there's a canonical method in htsjdk for checking whether a file is cram) -- if you find any and we don't have a reference according to `hasReference()`, throw a `UserException` with a clear error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:279,Availability,error,error,279,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:545,Deployability,integrat,integration,545,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:285,Integrability,message,message,285,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:545,Integrability,integrat,integration,545,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:468,Security,validat,validation,468,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:305,Testability,test,test,305,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:557,Testability,test,test,557,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:538,Usability,simpl,simple,538,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:190,Availability,down,down,190,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:919,Availability,down,down,919,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1294,Availability,down,downsides,1294,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:49,Integrability,interface,interface,49,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:200,Integrability,rout,route,200,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:265,Integrability,interface,interface,265,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:307,Integrability,interface,interface,307,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:330,Integrability,interface,interfaces,330,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1645,Integrability,interface,interface,1645,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:341,Modifiability,extend,extend,341,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:373,Safety,unsafe,unsafe,373,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1175,Testability,test,test,1175,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1465,Testability,test,test,1465,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:1537,Testability,test,test,1537,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:466,Usability,clear,clear,466,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661
https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902:7,Testability,test,tests,7,simple tests added in #884,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902
https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902:0,Usability,simpl,simple,0,simple tests added in #884,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/709#issuecomment-160836902
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071:317,Testability,log,logically,317,"Since we never actually look to see if something IS an optical duplicate and only care about the total number, we could just output a single annotation on one read in the best read pair with the number of optical duplicates found for that set of reads. It would make the code simpler but maybe not make as much sense logically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071:276,Usability,simpl,simpler,276,"Since we never actually look to see if something IS an optical duplicate and only care about the total number, we could just output a single annotation on one read in the best read pair with the number of optical duplicates found for that set of reads. It would make the code simpler but maybe not make as much sense logically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126755071
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:463,Integrability,depend,depend,463,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:144,Modifiability,inherit,inherited,144,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:205,Modifiability,refactor,refactoring,205,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:368,Testability,test,test,368,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:595,Testability,test,tests,595,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:760,Usability,simpl,simplify,760,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958
https://github.com/broadinstitute/gatk/pull/782#issuecomment-128095847:27,Deployability,update,update,27,@tovanadler Thanks for the update. It's clearer this way. Do you know how the Combine.perKey is implemented? Will it scale? I'm afraid it's going to try and pulldown all metrics in a library to a single node and then iterate through them all.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/782#issuecomment-128095847
https://github.com/broadinstitute/gatk/pull/782#issuecomment-128095847:40,Usability,clear,clearer,40,@tovanadler Thanks for the update. It's clearer this way. Do you know how the Combine.perKey is implemented? Will it scale? I'm afraid it's going to try and pulldown all metrics in a library to a single node and then iterate through them all.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/782#issuecomment-128095847
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:85,Deployability,integrat,integration,85,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:85,Integrability,integrat,integration,85,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:73,Modifiability,rewrite,rewrite,73,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:172,Security,access,access,172,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:97,Testability,test,tests,97,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:268,Testability,test,tests,268,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:568,Testability,test,tests,568,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:1117,Testability,test,test,1117,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:1331,Testability,test,testFileWithoutInfoLineInHeaderWithOverride,1331,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:395,Usability,feedback,feedback,395,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027
https://github.com/broadinstitute/gatk/pull/792#issuecomment-128807445:22,Usability,feedback,feedback,22,"Gave a bit of initial feedback -- will do a full review next week, and answer all of the questions you posted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128807445
https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850:136,Testability,test,tests,136,Thanks @jean-philippe-martin! I've addressed your other feedback points and submitted a new pull request against the main repo (so that tests are run): https://github.com/broadinstitute/hellbender/pull/827. I'm closing this one now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850
https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850:56,Usability,feedback,feedback,56,Thanks @jean-philippe-martin! I've addressed your other feedback points and submitted a new pull request against the main repo (so that tests are run): https://github.com/broadinstitute/hellbender/pull/827. I'm closing this one now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-131862850
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:373,Availability,error,errors,373,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:457,Availability,error,errors,457,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:1100,Availability,error,errors,1100,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:257,Security,validat,validations,257,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:571,Testability,test,test,571,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:897,Testability,test,test,897,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:951,Testability,test,test,951,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:196,Usability,learn,learning,196,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:1205,Usability,clear,clear,1205,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051
https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:17,Availability,failure,failures,17,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122
https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:11,Testability,test,tests,11,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122
https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:45,Testability,test,test,45,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122
https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:77,Testability,test,test,77,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122
https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122:32,Usability,simpl,simple,32,"Fixing the tests failures. Some simple ones (test groups now ""_todo"", rename test inputs on git too). One of them is weird, and only reproduces with the command line... after many minutes of running...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132282122
https://github.com/broadinstitute/gatk/pull/827#issuecomment-132560424:38,Usability,feedback,feedback,38,"I rebased on master and addressed the feedback. I've added comments (prefixed with ""Hellbender"") in the htsjdk code to make it very clear where the changes are. I'll look at the changes needed in htsjdk next to address #831.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/827#issuecomment-132560424
https://github.com/broadinstitute/gatk/pull/827#issuecomment-132560424:132,Usability,clear,clear,132,"I rebased on master and addressed the feedback. I've added comments (prefixed with ""Hellbender"") in the htsjdk code to make it very clear where the changes are. I'll look at the changes needed in htsjdk next to address #831.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/827#issuecomment-132560424
https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966:37,Deployability,update,updated,37,"Applied feedback, reproduced bug and updated our description (#650), submitted bug report (https://github.com/google/google-http-java-client/issues/297), squashed. Merging once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966
https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966:177,Testability,test,tests,177,"Applied feedback, reproduced bug and updated our description (#650), submitted bug report (https://github.com/google/google-http-java-client/issues/297), squashed. Merging once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966
https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966:8,Usability,feedback,feedback,8,"Applied feedback, reproduced bug and updated our description (#650), submitted bug report (https://github.com/google/google-http-java-client/issues/297), squashed. Merging once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/835#issuecomment-132698966
https://github.com/broadinstitute/gatk/pull/844#issuecomment-142767158:0,Usability,simpl,simply,0,simply rebasing does not do it. back to @lbergelson for advice,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/844#issuecomment-142767158
https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842:181,Testability,test,tests,181,"Overall this looks good to me. I've added a few comments inline. Note that I haven't reviewed for style particularly, or consistency with the existing codebase. > 5) There are unit tests for all code except for the skeleton itself. This could be as simple as a Spark variant of `ReadsPreprocessingPipelineIntegrationTest`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842
https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842:249,Usability,simpl,simple,249,"Overall this looks good to me. I've added a few comments inline. Note that I haven't reviewed for style particularly, or consistency with the existing codebase. > 5) There are unit tests for all code except for the skeleton itself. This could be as simple as a Spark variant of `ReadsPreprocessingPipelineIntegrationTest`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/850#issuecomment-134164842
https://github.com/broadinstitute/gatk/pull/869#issuecomment-135849587:8,Usability,feedback,feedback,8,"Applied feedback, let me know if it's OK now!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135849587
https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333:148,Testability,test,tests,148,"Good catches, thank you! I've switched to using paging as you recommend since the code's a big shorter and simpler that way. I'll squash&merge once tests pass, unless you object.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333
https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333:107,Usability,simpl,simpler,107,"Good catches, thank you! I've switched to using paging as you recommend since the code's a big shorter and simpler that way. I'll squash&merge once tests pass, unless you object.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135875333
https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148:766,Deployability,patch,patch,766,"Returning false instead of throwing when data is missing in the `GoogleGenomicsReadToGATKReadAdapter` is misleading -- we don't know the answer to the question the client is asking in such cases, so returning false is not correct behavior. If these fields are actually missing in the underlying reads we really do want to blow up with an exception on access, as it means the read object is not usable by us (and the query that produced these incomplete objects likely needs to be modified). Could you restore the previous versions of these methods (`isSecondaryAlignment()`, `isDuplicate()`, etc.) before I review?. As for the Google read converters, could you open a separate ticket with your description of the inconsistencies so we can decide whether to submit a patch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148
https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148:351,Security,access,access,351,"Returning false instead of throwing when data is missing in the `GoogleGenomicsReadToGATKReadAdapter` is misleading -- we don't know the answer to the question the client is asking in such cases, so returning false is not correct behavior. If these fields are actually missing in the underlying reads we really do want to blow up with an exception on access, as it means the read object is not usable by us (and the query that produced these incomplete objects likely needs to be modified). Could you restore the previous versions of these methods (`isSecondaryAlignment()`, `isDuplicate()`, etc.) before I review?. As for the Google read converters, could you open a separate ticket with your description of the inconsistencies so we can decide whether to submit a patch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148
https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148:394,Usability,usab,usable,394,"Returning false instead of throwing when data is missing in the `GoogleGenomicsReadToGATKReadAdapter` is misleading -- we don't know the answer to the question the client is asking in such cases, so returning false is not correct behavior. If these fields are actually missing in the underlying reads we really do want to blow up with an exception on access, as it means the read object is not usable by us (and the query that produced these incomplete objects likely needs to be modified). Could you restore the previous versions of these methods (`isSecondaryAlignment()`, `isDuplicate()`, etc.) before I review?. As for the Google read converters, could you open a separate ticket with your description of the inconsistencies so we can decide whether to submit a patch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871#issuecomment-136771148
https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282:556,Testability,test,tests,556,"@tomwhite @davidadamsphd could you take a look and see if it's in the right direction?. Some notes:; - This is only impl'd for pure Spark at the moment. Gonna work on adding support for dataflow as well.; - It was easier to remove the `final` modifier on the `GATKRead` impl rather than reimplement all the methods and simply pass them through. Let me know if that's ok.; - Should we target Parquet IO only in the context of writing to Hadoop? Or should I make sure it works anytime a local/single bam file is being written?; - Definitely need to add more tests. One thing that's annoying is that the cleanup for `readsSinkParquetTest` doesn't seem to happen.; - Registered `AlignmentRecord` with the `GATKRegistrator`, but there aren't any tests that exercise it. There's a touch of scala-to-java ugliness there, so let me know if we should just reimplement the `AvroSerializer` class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282
https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282:741,Testability,test,tests,741,"@tomwhite @davidadamsphd could you take a look and see if it's in the right direction?. Some notes:; - This is only impl'd for pure Spark at the moment. Gonna work on adding support for dataflow as well.; - It was easier to remove the `final` modifier on the `GATKRead` impl rather than reimplement all the methods and simply pass them through. Let me know if that's ok.; - Should we target Parquet IO only in the context of writing to Hadoop? Or should I make sure it works anytime a local/single bam file is being written?; - Definitely need to add more tests. One thing that's annoying is that the cleanup for `readsSinkParquetTest` doesn't seem to happen.; - Registered `AlignmentRecord` with the `GATKRegistrator`, but there aren't any tests that exercise it. There's a touch of scala-to-java ugliness there, so let me know if we should just reimplement the `AvroSerializer` class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282
https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282:319,Usability,simpl,simply,319,"@tomwhite @davidadamsphd could you take a look and see if it's in the right direction?. Some notes:; - This is only impl'd for pure Spark at the moment. Gonna work on adding support for dataflow as well.; - It was easier to remove the `final` modifier on the `GATKRead` impl rather than reimplement all the methods and simply pass them through. Let me know if that's ok.; - Should we target Parquet IO only in the context of writing to Hadoop? Or should I make sure it works anytime a local/single bam file is being written?; - Definitely need to add more tests. One thing that's annoying is that the cleanup for `readsSinkParquetTest` doesn't seem to happen.; - Registered `AlignmentRecord` with the `GATKRegistrator`, but there aren't any tests that exercise it. There's a touch of scala-to-java ugliness there, so let me know if we should just reimplement the `AvroSerializer` class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/888#issuecomment-139423282
https://github.com/broadinstitute/gatk/pull/890#issuecomment-183170746:113,Usability,clear,clear,113,"as discussed with @droazen, I'm not touching this PR until he's done with his experiment. Reassigning to make it clear that I'm not working on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/890#issuecomment-183170746
https://github.com/broadinstitute/gatk/pull/894#issuecomment-142445038:33,Usability,undo,undone,33,It appears the merge process has undone a bunch of the work. Working on fixing that now :-(,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/894#issuecomment-142445038
https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644:522,Deployability,patch,patch,522,"ADAM has no header. I realize I'm only coming into the game rather late, but would it be possible to ditch `SAMRecord`? Since you're already using Google `Read`-backed data as well, instead of writing against an interface (`GATKRead`), you could simply come up with your own, more-awesome concrete data structure. (Perhaps even an Avro `SpecificRecord` a la `AlignmentRecord`). In cases where ADAM wants a header back, at the moment it actually runs an aggregation across all the reads to rebuild it. (I'm trying to add a patch that allows you to specify a header, though, because it's breaking a hellbender test for reading/writing parquet.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644
https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644:212,Integrability,interface,interface,212,"ADAM has no header. I realize I'm only coming into the game rather late, but would it be possible to ditch `SAMRecord`? Since you're already using Google `Read`-backed data as well, instead of writing against an interface (`GATKRead`), you could simply come up with your own, more-awesome concrete data structure. (Perhaps even an Avro `SpecificRecord` a la `AlignmentRecord`). In cases where ADAM wants a header back, at the moment it actually runs an aggregation across all the reads to rebuild it. (I'm trying to add a patch that allows you to specify a header, though, because it's breaking a hellbender test for reading/writing parquet.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644
https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644:608,Testability,test,test,608,"ADAM has no header. I realize I'm only coming into the game rather late, but would it be possible to ditch `SAMRecord`? Since you're already using Google `Read`-backed data as well, instead of writing against an interface (`GATKRead`), you could simply come up with your own, more-awesome concrete data structure. (Perhaps even an Avro `SpecificRecord` a la `AlignmentRecord`). In cases where ADAM wants a header back, at the moment it actually runs an aggregation across all the reads to rebuild it. (I'm trying to add a patch that allows you to specify a header, though, because it's breaking a hellbender test for reading/writing parquet.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644
https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644:246,Usability,simpl,simply,246,"ADAM has no header. I realize I'm only coming into the game rather late, but would it be possible to ditch `SAMRecord`? Since you're already using Google `Read`-backed data as well, instead of writing against an interface (`GATKRead`), you could simply come up with your own, more-awesome concrete data structure. (Perhaps even an Avro `SpecificRecord` a la `AlignmentRecord`). In cases where ADAM wants a header back, at the moment it actually runs an aggregation across all the reads to rebuild it. (I'm trying to add a patch that allows you to specify a header, though, because it's breaking a hellbender test for reading/writing parquet.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-140990644
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1163,Availability,error,errors,1163,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1237,Availability,error,error,1237,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1402,Availability,error,error,1402,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:203,Integrability,interface,interface,203,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1243,Integrability,message,message,1243,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:159,Modifiability,refactor,refactored,159,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1186,Performance,perform,perform,1186,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:871,Usability,user experience,user experience,871,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1431,Usability,simpl,simple,1431,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:146,Energy Efficiency,efficient,efficient,146,"on **data representation**:. @laserson: yes, I think it makes total sense to eventually move to a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:292,Energy Efficiency,efficient,efficient,292,"on **data representation**:. @laserson: yes, I think it makes total sense to eventually move to a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:682,Energy Efficiency,efficient,efficient,682,"on **data representation**:. @laserson: yes, I think it makes total sense to eventually move to a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:1585,Energy Efficiency,power,power,1585," a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header (unless I happen to know that the ""black box"" won't call any of the header-requiring methods).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:1502,Integrability,interface,interface,1502," a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header (unless I happen to know that the ""black box"" won't call any of the header-requiring methods).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:1495,Usability,simpl,simple,1495," a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header (unless I happen to know that the ""black box"" won't call any of the header-requiring methods).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141293657:244,Usability,simpl,simple,244,"I'll wait for @laserson and @tomwhite to get a chance to chime in, but it looks like we have plan. Supposing that it takes @cmnbroad two weeks (which seems like a reasonable estimate to me). We should we do in the mean time? Is there something simple we can do (that's hacky but correct) to hold us over until the real fix is in?; @tomwhite's suggestion of stripping the header before shuffles, then adding it back after sounds OK to me (and should be easy to clean up).; Other thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141293657
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:612,Deployability,update,update,612,"Since you asked, I have a couple of thoughts:. First, I don't know if allowing the SAMRecord header to be set to null ; something that was intended to be widely used, or whether it was an ; oversight in the API or done to solve some particular corner case. If ; many SAMRecord methods appear to be broken if the header is null, then ; perhaps this isn't something that was intended for wide use. Second, it sounds like the suggestion is that headerless SAMRecords ; would now be widely used, and thus a common thing that people writing ; code against htsjdk need to anticipate.; So if you go this way you should update the SAMRecord documentation to ; clearly indicate that SAMRecords can be in either a headerless or ; non-headerless (headerful?) state; and indicate how each API function is affected by this. If certain ; methods behave differently, then people writing code against SAMRecord ; need to anticipate this; and existing code may need to be updated. In other words, headerless ; SAMRecords should become ""part of the spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:955,Deployability,update,updated,955,"Since you asked, I have a couple of thoughts:. First, I don't know if allowing the SAMRecord header to be set to null ; something that was intended to be widely used, or whether it was an ; oversight in the API or done to solve some particular corner case. If ; many SAMRecord methods appear to be broken if the header is null, then ; perhaps this isn't something that was intended for wide use. Second, it sounds like the suggestion is that headerless SAMRecords ; would now be widely used, and thus a common thing that people writing ; code against htsjdk need to anticipate.; So if you go this way you should update the SAMRecord documentation to ; clearly indicate that SAMRecords can be in either a headerless or ; non-headerless (headerful?) state; and indicate how each API function is affected by this. If certain ; methods behave differently, then people writing code against SAMRecord ; need to anticipate this; and existing code may need to be updated. In other words, headerless ; SAMRecords should become ""part of the spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:3283,Deployability,patch,patched,3283,"the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers do require that a header be present in the ; > records, I believe).; > ; > I've created #903 ; > https://github.com/broadinstitute/hellbender/issues/903 to make the ; > necessary changes in htsjdk, and assigned it to @cmnbroad ; > https://github.com/cmnbroad. He said he could get to it early next ; > week. What do you guys think of this approach to the problem?; > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-141218134.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:1398,Energy Efficiency,efficient,efficiently,1398," wide use. Second, it sounds like the suggestion is that headerless SAMRecords ; would now be widely used, and thus a common thing that people writing ; code against htsjdk need to anticipate.; So if you go this way you should update the SAMRecord documentation to ; clearly indicate that SAMRecords can be in either a headerless or ; non-headerless (headerful?) state; and indicate how each API function is affected by this. If certain ; methods behave differently, then people writing code against SAMRecord ; need to anticipate this; and existing code may need to be updated. In other words, headerless ; SAMRecords should become ""part of the spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2515,Energy Efficiency,efficient,efficient,2515,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:1773,Integrability,depend,depends,1773,"; and indicate how each API function is affected by this. If certain ; methods behave differently, then people writing code against SAMRecord ; need to anticipate this; and existing code may need to be updated. In other words, headerless ; SAMRecords should become ""part of the spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, dr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2038,Integrability,depend,depend,2038,"e spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2594,Modifiability,portab,portable,2594,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2261,Security,hash,hash,2261,"en't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter int",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:3315,Testability,test,tests,3315,"the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers do require that a header be present in the ; > records, I believe).; > ; > I've created #903 ; > https://github.com/broadinstitute/hellbender/issues/903 to make the ; > necessary changes in htsjdk, and assigned it to @cmnbroad ; > https://github.com/cmnbroad. He said he could get to it early next ; > week. What do you guys think of this approach to the problem?; > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-141218134.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:652,Usability,clear,clearly,652,"Since you asked, I have a couple of thoughts:. First, I don't know if allowing the SAMRecord header to be set to null ; something that was intended to be widely used, or whether it was an ; oversight in the API or done to solve some particular corner case. If ; many SAMRecord methods appear to be broken if the header is null, then ; perhaps this isn't something that was intended for wide use. Second, it sounds like the suggestion is that headerless SAMRecords ; would now be widely used, and thus a common thing that people writing ; code against htsjdk need to anticipate.; So if you go this way you should update the SAMRecord documentation to ; clearly indicate that SAMRecords can be in either a headerless or ; non-headerless (headerful?) state; and indicate how each API function is affected by this. If certain ; methods behave differently, then people writing code against SAMRecord ; need to anticipate this; and existing code may need to be updated. In other words, headerless ; SAMRecords should become ""part of the spec"". Third, although I don't know in detail about the different execution ; environments you are trying to support, there is a general strategy that ; I haven't seen discussed in these threads.; Perhaps it's impractical, but I'll mention it anyway. It seems like ; another approach would be to create (internal to the implementation) a ; ""header tag"" that could be efficiently serialized; and passed as part of the SAMRecord when you need to distribute it. The ; header tag could be used by the receiver to reattach the SAMRecord to ; its header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109:396,Availability,down,downstream,396,"@bhandsaker Thanks for chiming in with your thoughts/concerns. Under this proposal, the various classes in htsjdk that read and return `SAMRecords` (eg., `SAMReader` & co.) would continue to put the header inside of the records, so we would not be imposing an additional burden on direct clients of htsjdk to check for null headers any more than they do currently. The only difference is that if downstream consumers of `SAMRecords` (like hellbender) choose to strip the header from the records, there would be an explicit contract governing the behavior of headerless `SAMRecords` (as opposed to the status quo, in which the header may be null but behavior is totally undocumented and in some cases inconsistent -- eg., the reference name and index in a headerless `SAMRecord` can get out-of-sync in some cases). . In addition to documenting/clarifying the behavior of headerless `SAMRecords` and fixing any consistency-related bugs we find when operating without a header, we would also make an effort to document when a class in htsjdk that consumes `SAMRecords` requires that a header be present in the records (such as the various writer classes). Does this sound reasonable? It's actually a much more conservative proposal than it may have initially sounded :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109:523,Integrability,contract,contract,523,"@bhandsaker Thanks for chiming in with your thoughts/concerns. Under this proposal, the various classes in htsjdk that read and return `SAMRecords` (eg., `SAMReader` & co.) would continue to put the header inside of the records, so we would not be imposing an additional burden on direct clients of htsjdk to check for null headers any more than they do currently. The only difference is that if downstream consumers of `SAMRecords` (like hellbender) choose to strip the header from the records, there would be an explicit contract governing the behavior of headerless `SAMRecords` (as opposed to the status quo, in which the header may be null but behavior is totally undocumented and in some cases inconsistent -- eg., the reference name and index in a headerless `SAMRecord` can get out-of-sync in some cases). . In addition to documenting/clarifying the behavior of headerless `SAMRecords` and fixing any consistency-related bugs we find when operating without a header, we would also make an effort to document when a class in htsjdk that consumes `SAMRecords` requires that a header be present in the records (such as the various writer classes). Does this sound reasonable? It's actually a much more conservative proposal than it may have initially sounded :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109:669,Usability,undo,undocumented,669,"@bhandsaker Thanks for chiming in with your thoughts/concerns. Under this proposal, the various classes in htsjdk that read and return `SAMRecords` (eg., `SAMReader` & co.) would continue to put the header inside of the records, so we would not be imposing an additional burden on direct clients of htsjdk to check for null headers any more than they do currently. The only difference is that if downstream consumers of `SAMRecords` (like hellbender) choose to strip the header from the records, there would be an explicit contract governing the behavior of headerless `SAMRecords` (as opposed to the status quo, in which the header may be null but behavior is totally undocumented and in some cases inconsistent -- eg., the reference name and index in a headerless `SAMRecord` can get out-of-sync in some cases). . In addition to documenting/clarifying the behavior of headerless `SAMRecords` and fixing any consistency-related bugs we find when operating without a header, we would also make an effort to document when a class in htsjdk that consumes `SAMRecords` requires that a header be present in the records (such as the various writer classes). Does this sound reasonable? It's actually a much more conservative proposal than it may have initially sounded :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142020109
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:1365,Availability,down,downstream,1365," operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an effort to document ; > when a class in htsjdk that consumes |SAMRecords| requires that a ; > header be present in the records (such as the various writer classes).; > ; > Does this sound reasonable? It's actually a much more conservative ; > proposal than it may have initially sounded :); > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-142020109.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:645,Energy Efficiency,efficient,efficient,645,"Thanks for the explanation. It isn't clear to me that the SAMRecord API was ever intended to support ; headerless records (except maybe in very rare corner cases). I don't really know the scope of hellbender. If it is just for internal ; DSDE tools development, then I guess it doesn't matter.; If you ever want to leverage code/libraries from elsewhere, then those ; would have to be ""headerless-aware"", I guess. For example, a common operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:730,Energy Efficiency,efficient,efficient,730,"Thanks for the explanation. It isn't clear to me that the SAMRecord API was ever intended to support ; headerless records (except maybe in very rare corner cases). I don't really know the scope of hellbender. If it is just for internal ; DSDE tools development, then I guess it doesn't matter.; If you ever want to leverage code/libraries from elsewhere, then those ; would have to be ""headerless-aware"", I guess. For example, a common operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:1496,Integrability,contract,contract,1496," operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an effort to document ; > when a class in htsjdk that consumes |SAMRecords| requires that a ; > header be present in the records (such as the various writer classes).; > ; > Does this sound reasonable? It's actually a much more conservative ; > proposal than it may have initially sounded :); > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-142020109.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:37,Usability,clear,clear,37,"Thanks for the explanation. It isn't clear to me that the SAMRecord API was ever intended to support ; headerless records (except maybe in very rare corner cases). I don't really know the scope of hellbender. If it is just for internal ; DSDE tools development, then I guess it doesn't matter.; If you ever want to leverage code/libraries from elsewhere, then those ; would have to be ""headerless-aware"", I guess. For example, a common operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910:1654,Usability,undo,undocumented,1654," operation in Genome STRiP is to ask for the sample ; associated with a read. This requires the header, I would think.; Anyway, my main point was just to stimulate thinking about the value of ; implementing an efficient way to transmit the headers.; It also seems to me that this generalizes to efficient patterns to ; transmit any widely shared data (that is referenced by many serialized ; individual data items) out-of-band. -Bob. On 9/21/15 11:42 AM, droazen wrote:. > @bhandsaker https://github.com/bhandsaker Thanks for chiming in with ; > your thoughts/concerns.; > ; > Under this proposal, the various classes in htsjdk that read and ; > return |SAMRecords| (eg., |SAMReader| & co.) would continue to put the ; > header inside of the records, so we would not be imposing an ; > additional burden on direct clients of htsjdk to check for null ; > headers any more than they do currently. The only difference is that ; > if downstream consumers of |SAMRecords| (like hellbender) choose to ; > strip the header from the records, there would be an explicit contract ; > governing the behavior of headerless |SAMRecords| (as opposed to the ; > status quo, in which the header may be null but behavior is totally ; > undocumented and in some cases inconsistent -- eg., the reference name ; > and index in a headerless |SAMRecord| can get out-of-sync in some cases).; > ; > In additional to documenting/clarifying the behavior of headerless ; > |SAMRecords| and fixing any consistency-related bugs we find when ; > operating without a header, we would also make an effort to document ; > when a class in htsjdk that consumes |SAMRecords| requires that a ; > header be present in the records (such as the various writer classes).; > ; > Does this sound reasonable? It's actually a much more conservative ; > proposal than it may have initially sounded :); > ; > —; > Reply to this email directly or view it on GitHub ; > https://github.com/broadinstitute/hellbender/issues/900#issuecomment-142020109.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142402910
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:524,Modifiability,refactor,refactoring,524,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1113,Modifiability,refactor,refactored,1113,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1325,Security,expose,exposed,1325,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1669,Security,expose,exposed,1669,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:350,Testability,test,tests,350,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:37,Usability,guid,guide,37,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1715,Usability,usab,usable,1715,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080:55,Modifiability,refactor,refactor,55,"Done with my review. Thanks for doing this much-needed refactor! The BaseRecal stuff looks sound, but I have some other feedback that we should discuss/address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080
https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080:120,Usability,feedback,feedback,120,"Done with my review. Thanks for doing this much-needed refactor! The BaseRecal stuff looks sound, but I have some other feedback that we should discuss/address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080
https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469:182,Integrability,wrap,wrapper,182,"@akiezun I have switched a lot so far; currently I'm stuck on `GenotypeLikelihoodCalculator`, which relies on `GenotypeLikelihoods` in htsjdk, which is log10. I could write a simple wrapper class to present a natural log interface. Is there a better solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469
https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469:221,Integrability,interface,interface,221,"@akiezun I have switched a lot so far; currently I'm stuck on `GenotypeLikelihoodCalculator`, which relies on `GenotypeLikelihoods` in htsjdk, which is log10. I could write a simple wrapper class to present a natural log interface. Is there a better solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469
https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469:217,Testability,log,log,217,"@akiezun I have switched a lot so far; currently I'm stuck on `GenotypeLikelihoodCalculator`, which relies on `GenotypeLikelihoods` in htsjdk, which is log10. I could write a simple wrapper class to present a natural log interface. Is there a better solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469
https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469:175,Usability,simpl,simple,175,"@akiezun I have switched a lot so far; currently I'm stuck on `GenotypeLikelihoodCalculator`, which relies on `GenotypeLikelihoods` in htsjdk, which is log10. I could write a simple wrapper class to present a natural log interface. Is there a better solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/917#issuecomment-149321469
https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716:44,Integrability,depend,depend,44,clearing the milestone - our alpha does not depend on this pull req.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716
https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716:0,Usability,clear,clearing,0,clearing the milestone - our alpha does not depend on this pull req.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/936#issuecomment-151705716
https://github.com/broadinstitute/gatk/issues/945#issuecomment-145615665:191,Usability,clear,clear,191,"For the most part I expect people use the precompiled jar for production; work, so I'm not too worried about that. Just make sure to have a TL;DR; line at the top of the readme that makes it clear what this is. On Mon, Oct 5, 2015 at 1:16 PM, Adam Kiezun notifications@github.com; wrote:. > sad but necessary; > @vdauwera https://github.com/vdauwera will buy us a plush hellbender; > ; > BTW, @vdauwera https://github.com/vdauwera will people be confused when; > they find the broadinstitute/gatk repository and think this is the 3.x; > branch that they can use for production? I bet they will - what should we; > do about this then?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/945#issuecomment-145602883; > . ## . Geraldine A. Van der Auwera, Ph.D.; Group leader, Comms & Support; Data Science & Data Engineering; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/945#issuecomment-145615665
https://github.com/broadinstitute/gatk/issues/951#issuecomment-169148886:44,Usability,clear,clearly,44,"The javadoc for `AddContextDataToReadSpark` clearly mentions that unmapped reads are filtered out, so it hopefully shouldn't be too much of a surprise. But I agree that ideally this transform should function as an ""outer join"" and not filter unmapped reads.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/951#issuecomment-169148886
https://github.com/broadinstitute/gatk/issues/960#issuecomment-158207631:115,Usability,guid,guide,115,"This is a beta ticket -- for alpha we will just recommend conventions that promote composability when we write the guide in https://github.com/broadinstitute/gatk/issues/1016. For beta, we should probably create a Transform abstraction a la dataflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/960#issuecomment-158207631
https://github.com/broadinstitute/gatk/issues/964#issuecomment-164045940:113,Usability,guid,guide,113,I think we've satisfied this one for alpha purposes with our recent README changes. We can write a more in-depth guide for beta.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/964#issuecomment-164045940
https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:140,Performance,optimiz,optimizations,140,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446
https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:211,Usability,simpl,simpler,211,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446
https://github.com/broadinstitute/gatk/issues/974#issuecomment-149692146:57,Usability,simpl,simple,57,AFAIK the GATK3 one used multithreading etc. Can we do a simple one just for readwalkers that does not use multithreading?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/974#issuecomment-149692146
https://github.com/broadinstitute/gatk/pull/980#issuecomment-147751416:19,Usability,feedback,feedback,19,"I've addressed the feedback so far - except for @davidadamsphd's last comment about checking in a sharded BAM. @lbergelson let me know if you'd like me to do that; also, what do you think about the overwriting behaviour?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/980#issuecomment-147751416
https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:105,Performance,optimiz,optimization,105,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889
https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:77,Usability,feedback,feedback,77,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889
https://github.com/broadinstitute/gatk/pull/987#issuecomment-147777938:173,Usability,learn,learn,173,"I tried to run it locally with inputs in GCS and the program crashed with a; null pointer exception. The same command worked fine when running on the; cluster. I'm happy to learn that copying the inputs locally is a workaround. I'll start on the rebasing journey. I'll post a note once I'm done so; someone can do the code review. On Tue, Oct 13, 2015 at 6:19 AM, Tom White notifications@github.com wrote:. > @jean-philippe-martin https://github.com/jean-philippe-martin, that's; > accurate.; > ; > BTW ReadsSparkSink should work fine locally (it can write local files, see; > ReadsSparkSinkUnitTest). What was the problem that you hit?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/987#issuecomment-147711651.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147777938
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:202,Deployability,integrat,integration,202,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:543,Deployability,integrat,integration,543,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:202,Integrability,integrat,integration,202,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:543,Integrability,integrat,integration,543,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:95,Testability,test,test,95,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:191,Testability,test,testing,191,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:214,Testability,test,tests,214,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:236,Testability,test,tests,236,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:339,Testability,test,tested,339,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:385,Testability,test,tests,385,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:452,Testability,test,testing,452,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:485,Testability,test,tests,485,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:555,Testability,test,tests,555,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490:311,Usability,clear,clear,311,"Overall, this looks fine, just a few minor comments. Two broader questions/issues:; - Does the test data exercise all of the code paths and edge cases?; - In general, putting the majority of testing in integration tests instead of unit tests is bad pattern. It have several bad consequences (1) it becomes less clear which cases are being tested (2) it's slower than just running unit tests and (3) it makes it unhelpful to (perhaps someday) move to a testing framework that only runs tests relevant to the code directly affected (because all integration tests must be run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1013#issuecomment-149599490
https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:58,Availability,down,down,58,"Next I set out to determine whether hellbender is slowing down on the larger interval simply because there is more data / a longer traversal, or because it's slower at processing the `1:1-10000000` interval than the `1:10000000-20000000` interval. And surprisingly, it appears that the latter is the case:. Time to process the `1:1-10000000` interval across two runs:. ```; GATK3: 5m25.983s 5m31.913s; HB: 6m2.156s 5m59.804s; ```. (Recall that HB was ~5% faster than GATK3 at processing the `1:10000000-20000000` interval). Moreover, our newly-installed progress meter shows that the rate at which we process records is unusually low at the start of the `1:1-10000000` interval, but is consistent throughout the processing of the `1:10000000-20000000` interval:. HB processing rate over 1:1-10000000:. ```; 14:22:19.520 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:22:29.522 INFO ProgressMeter - 1:769026 0.2 133000 797920.2; 14:22:39.531 INFO ProgressMeter - 1:1066133 0.3 298000 893553.2; 14:22:49.544 INFO ProgressMeter - 1:1389358 0.5 471000 941247.0; 14:22:59.572 INFO ProgressMeter - 1:1695902 0.7 636000 952785.2; 14:23:09.601 INFO ProgressMeter - 1:1961884 0.8 808000 968031.8; 14:23:19.636 INFO ProgressMeter - 1:2264803 1.0 985000 983099.3; 14:23:29.637 INFO ProgressMeter - 1:2583326 1.2 1162000 994352.2; 14:23:39.694 INFO ProgressMeter - 1:2817177 1.3 1297000 970638.9; 14:23:49.705 INFO ProgressMeter - 1:3095124 1.5 1467000 975993.8; 14:23:59.726 INFO ProgressMeter - 1:3372416 1.7 1637000 980190.6; 14:24:09.734 INFO ProgressMeter - 1:3678706 1.8 1810000 985355.8; 14:24:19.777 INFO ProgressMeter - 1:4087198 2.0 1984000 989880.0; 14:24:29.813 INFO ProgressMeter - 1:4341518 2.2 2165000 996983.7; 14:24:39.822 INFO ProgressMeter - 1:4598153 2.3 2350000 1004975.0; 14:24:49.834 INFO ProgressMeter - 1:4859664 2.5 2530000 1009892.7; 14:24:59.838 INFO ProgressMeter - 1:5103960 2.7 2712000 1014982.7; 14:25:09.887 INFO ProgressMeter - 1:5341742 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236
https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:4391,Availability,down,down,4391,r - 1:7202715 4.0 4191000 1045250.1; 14:26:30.135 INFO ProgressMeter - 1:7440529 4.2 4368000 1045747.5; 14:26:40.182 INFO ProgressMeter - 1:7704929 4.3 4546000 1046412.6; 14:26:50.246 INFO ProgressMeter - 1:7943120 4.5 4721000 1046297.7; 14:27:00.269 INFO ProgressMeter - 1:8191836 4.7 4903000 1047839.9; 14:27:10.295 INFO ProgressMeter - 1:8443081 4.8 5076000 1047407.8; 14:27:20.317 INFO ProgressMeter - 1:8693600 5.0 5267000 1050608.9; 14:27:30.363 INFO ProgressMeter - 1:8947934 5.2 5462000 1054294.3; 14:27:40.417 INFO ProgressMeter - 1:9199287 5.3 5639000 1054360.3; 14:27:50.420 INFO ProgressMeter - 1:9454308 5.5 5811000 1053671.8; 14:28:00.434 INFO ProgressMeter - 1:9724703 5.7 5994000 1054928.8; 14:28:10.442 INFO ProgressMeter - 1:9997608 5.8 6184000 1057329.0; ```. Whereas for the `1:10000000-20000000` interval we reach our max records/minute rate from the beginning and sustain it throughout:. ```; 11:59:25.779 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 11:59:35.800 INFO ProgressMeter - 1:10238878 0.2 179000 1071856.3; 11:59:45.815 INFO ProgressMeter - 1:10479871 0.3 365000 1093032.5; 11:59:55.820 INFO ProgressMeter - 1:10734140 0.5 543000 1084517.8; 12:00:05.862 INFO ProgressMeter - 1:11009535 0.7 720000 1077763.6; 12:00:15.906 INFO ProgressMeter - 1:11266264 0.8 906000 1084445.5; 12:00:25.975 INFO ProgressMeter - 1:11519786 1.0 1088000 1084457.4; 12:00:36.005 INFO ProgressMeter - 1:11779127 1.2 1264000 1079941.9; 12:00:46.045 INFO ProgressMeter - 1:12038732 1.3 1440000 1076420.9; 12:00:56.072 INFO ProgressMeter - 1:12294624 1.5 1617000 1074501.9; 12:01:06.118 INFO ProgressMeter - 1:12547820 1.7 1818000 1087125.5; 12:01:16.130 INFO ProgressMeter - 1:12796451 1.8 2002000 1088526.6; 12:01:26.168 INFO ProgressMeter - 1:13176986 2.0 2170000 1081494.2; ....; etc.; ```. I can only conclude that there's something about the start of chromosome 1 that is slowing HB down. Next step is to profile HB during traversal of that region.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236
https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:544,Deployability,install,installed,544,"Next I set out to determine whether hellbender is slowing down on the larger interval simply because there is more data / a longer traversal, or because it's slower at processing the `1:1-10000000` interval than the `1:10000000-20000000` interval. And surprisingly, it appears that the latter is the case:. Time to process the `1:1-10000000` interval across two runs:. ```; GATK3: 5m25.983s 5m31.913s; HB: 6m2.156s 5m59.804s; ```. (Recall that HB was ~5% faster than GATK3 at processing the `1:10000000-20000000` interval). Moreover, our newly-installed progress meter shows that the rate at which we process records is unusually low at the start of the `1:1-10000000` interval, but is consistent throughout the processing of the `1:10000000-20000000` interval:. HB processing rate over 1:1-10000000:. ```; 14:22:19.520 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:22:29.522 INFO ProgressMeter - 1:769026 0.2 133000 797920.2; 14:22:39.531 INFO ProgressMeter - 1:1066133 0.3 298000 893553.2; 14:22:49.544 INFO ProgressMeter - 1:1389358 0.5 471000 941247.0; 14:22:59.572 INFO ProgressMeter - 1:1695902 0.7 636000 952785.2; 14:23:09.601 INFO ProgressMeter - 1:1961884 0.8 808000 968031.8; 14:23:19.636 INFO ProgressMeter - 1:2264803 1.0 985000 983099.3; 14:23:29.637 INFO ProgressMeter - 1:2583326 1.2 1162000 994352.2; 14:23:39.694 INFO ProgressMeter - 1:2817177 1.3 1297000 970638.9; 14:23:49.705 INFO ProgressMeter - 1:3095124 1.5 1467000 975993.8; 14:23:59.726 INFO ProgressMeter - 1:3372416 1.7 1637000 980190.6; 14:24:09.734 INFO ProgressMeter - 1:3678706 1.8 1810000 985355.8; 14:24:19.777 INFO ProgressMeter - 1:4087198 2.0 1984000 989880.0; 14:24:29.813 INFO ProgressMeter - 1:4341518 2.2 2165000 996983.7; 14:24:39.822 INFO ProgressMeter - 1:4598153 2.3 2350000 1004975.0; 14:24:49.834 INFO ProgressMeter - 1:4859664 2.5 2530000 1009892.7; 14:24:59.838 INFO ProgressMeter - 1:5103960 2.7 2712000 1014982.7; 14:25:09.887 INFO ProgressMeter - 1:5341742 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236
https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:563,Energy Efficiency,meter,meter,563,"Next I set out to determine whether hellbender is slowing down on the larger interval simply because there is more data / a longer traversal, or because it's slower at processing the `1:1-10000000` interval than the `1:10000000-20000000` interval. And surprisingly, it appears that the latter is the case:. Time to process the `1:1-10000000` interval across two runs:. ```; GATK3: 5m25.983s 5m31.913s; HB: 6m2.156s 5m59.804s; ```. (Recall that HB was ~5% faster than GATK3 at processing the `1:10000000-20000000` interval). Moreover, our newly-installed progress meter shows that the rate at which we process records is unusually low at the start of the `1:1-10000000` interval, but is consistent throughout the processing of the `1:10000000-20000000` interval:. HB processing rate over 1:1-10000000:. ```; 14:22:19.520 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:22:29.522 INFO ProgressMeter - 1:769026 0.2 133000 797920.2; 14:22:39.531 INFO ProgressMeter - 1:1066133 0.3 298000 893553.2; 14:22:49.544 INFO ProgressMeter - 1:1389358 0.5 471000 941247.0; 14:22:59.572 INFO ProgressMeter - 1:1695902 0.7 636000 952785.2; 14:23:09.601 INFO ProgressMeter - 1:1961884 0.8 808000 968031.8; 14:23:19.636 INFO ProgressMeter - 1:2264803 1.0 985000 983099.3; 14:23:29.637 INFO ProgressMeter - 1:2583326 1.2 1162000 994352.2; 14:23:39.694 INFO ProgressMeter - 1:2817177 1.3 1297000 970638.9; 14:23:49.705 INFO ProgressMeter - 1:3095124 1.5 1467000 975993.8; 14:23:59.726 INFO ProgressMeter - 1:3372416 1.7 1637000 980190.6; 14:24:09.734 INFO ProgressMeter - 1:3678706 1.8 1810000 985355.8; 14:24:19.777 INFO ProgressMeter - 1:4087198 2.0 1984000 989880.0; 14:24:29.813 INFO ProgressMeter - 1:4341518 2.2 2165000 996983.7; 14:24:39.822 INFO ProgressMeter - 1:4598153 2.3 2350000 1004975.0; 14:24:49.834 INFO ProgressMeter - 1:4859664 2.5 2530000 1009892.7; 14:24:59.838 INFO ProgressMeter - 1:5103960 2.7 2712000 1014982.7; 14:25:09.887 INFO ProgressMeter - 1:5341742 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236
https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:86,Usability,simpl,simply,86,"Next I set out to determine whether hellbender is slowing down on the larger interval simply because there is more data / a longer traversal, or because it's slower at processing the `1:1-10000000` interval than the `1:10000000-20000000` interval. And surprisingly, it appears that the latter is the case:. Time to process the `1:1-10000000` interval across two runs:. ```; GATK3: 5m25.983s 5m31.913s; HB: 6m2.156s 5m59.804s; ```. (Recall that HB was ~5% faster than GATK3 at processing the `1:10000000-20000000` interval). Moreover, our newly-installed progress meter shows that the rate at which we process records is unusually low at the start of the `1:1-10000000` interval, but is consistent throughout the processing of the `1:10000000-20000000` interval:. HB processing rate over 1:1-10000000:. ```; 14:22:19.520 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:22:29.522 INFO ProgressMeter - 1:769026 0.2 133000 797920.2; 14:22:39.531 INFO ProgressMeter - 1:1066133 0.3 298000 893553.2; 14:22:49.544 INFO ProgressMeter - 1:1389358 0.5 471000 941247.0; 14:22:59.572 INFO ProgressMeter - 1:1695902 0.7 636000 952785.2; 14:23:09.601 INFO ProgressMeter - 1:1961884 0.8 808000 968031.8; 14:23:19.636 INFO ProgressMeter - 1:2264803 1.0 985000 983099.3; 14:23:29.637 INFO ProgressMeter - 1:2583326 1.2 1162000 994352.2; 14:23:39.694 INFO ProgressMeter - 1:2817177 1.3 1297000 970638.9; 14:23:49.705 INFO ProgressMeter - 1:3095124 1.5 1467000 975993.8; 14:23:59.726 INFO ProgressMeter - 1:3372416 1.7 1637000 980190.6; 14:24:09.734 INFO ProgressMeter - 1:3678706 1.8 1810000 985355.8; 14:24:19.777 INFO ProgressMeter - 1:4087198 2.0 1984000 989880.0; 14:24:29.813 INFO ProgressMeter - 1:4341518 2.2 2165000 996983.7; 14:24:39.822 INFO ProgressMeter - 1:4598153 2.3 2350000 1004975.0; 14:24:49.834 INFO ProgressMeter - 1:4859664 2.5 2530000 1009892.7; 14:24:59.838 INFO ProgressMeter - 1:5103960 2.7 2712000 1014982.7; 14:25:09.887 INFO ProgressMeter - 1:5341742 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236
https://github.com/broadinstitute/gatk/issues/1035#issuecomment-150248706:23,Usability,clear,clear,23,Here's a profile. It's clear that all time goes into reading and writing and almost no overhead comes from the engine. Closing this - we win and no obvious problems in the profile.; ![image](https://cloud.githubusercontent.com/assets/1993519/10668338/da596c10-78a9-11e5-8aa0-b6c0ceddad08.png),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1035#issuecomment-150248706
https://github.com/broadinstitute/gatk/issues/1036#issuecomment-150240722:19,Usability,clear,clear,19,"on profiling, it's clear that the engine adds almost no overhead on top of htsjdk iterators - see screenshot from jprofiler; ![image](https://cloud.githubusercontent.com/assets/1993519/10667806/52a4f7e6-78a7-11e5-9ad2-b11d97d4647f.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1036#issuecomment-150240722
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151661949:73,Usability,guid,guide,73,"FYI I'm planning a ""GATK 4 Alpha"" category for the forum + documentation guide to host the user-facing documentation, so feel free to keep the developer docs in the readme -- or we'll add a note in the readme pointing to the forum/guide pages.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151661949
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151661949:231,Usability,guid,guide,231,"FYI I'm planning a ""GATK 4 Alpha"" category for the forum + documentation guide to host the user-facing documentation, so feel free to keep the developer docs in the readme -- or we'll add a note in the readme pointing to the forum/guide pages.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151661949
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151706338:25,Usability,simpl,simpler,25,@vdauwera wouldn't it be simpler to have those docs on the github page?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151706338
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151707594:527,Deployability,update,update,527,"I prefer to host the docs in the forum for the following reasons:; - We want people to use the GATK website and forum as a one-stop shop for all GATK needs, not have to go to Github for some things, both for convenience and as a matter of branding;; - Many end-users don't know/understand Github;; - In the forum we can easily host multiple documents in a way that's intuitive to navigate;; - We can easily render the docs as webpages within the GATK website, which many end-users prefer;; - Forum docs are easy for my team to update or tweak at a moment's notice;; - Users can comment directly on the documents, or create new discussion threads, and it's easier for us to answer them if all is in the same place. ; - If we need to open a github issue ticket (for bug report, feature request etc) we can do it directly from the forum discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151707594
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151707594:367,Usability,intuit,intuitive,367,"I prefer to host the docs in the forum for the following reasons:; - We want people to use the GATK website and forum as a one-stop shop for all GATK needs, not have to go to Github for some things, both for convenience and as a matter of branding;; - Many end-users don't know/understand Github;; - In the forum we can easily host multiple documents in a way that's intuitive to navigate;; - We can easily render the docs as webpages within the GATK website, which many end-users prefer;; - Forum docs are easy for my team to update or tweak at a moment's notice;; - Users can comment directly on the documents, or create new discussion threads, and it's easier for us to answer them if all is in the same place. ; - If we need to open a github issue ticket (for bug report, feature request etc) we can do it directly from the forum discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151707594
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151874932:106,Usability,clear,clearer,106,"For what its worth, I know I always find it difficult to find things on the GATK forums. A wiki is a much clearer way for me to navigate static information. It's also easy for everyone to edit and has a clear history.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151874932
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151874932:203,Usability,clear,clear,203,"For what its worth, I know I always find it difficult to find things on the GATK forums. A wiki is a much clearer way for me to navigate static information. It's also easy for everyone to edit and has a clear history.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151874932
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:56,Availability,error,error,56,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:333,Performance,queue,queue,333,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099
https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:175,Usability,feedback,feedback,175,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099
https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427:94,Performance,optimiz,optimizations,94,"That's correct, @akiezun.; However, it's not just stripping out that code. There are a ton of optimizations that can then be made to the code to simplify it afterwards. These classes were made very bulky to accommodate the indel calibration, and we should really remove that bulk. I can help with that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427
https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427:145,Usability,simpl,simplify,145,"That's correct, @akiezun.; However, it's not just stripping out that code. There are a ton of optimizations that can then be made to the code to simplify it afterwards. These classes were made very bulky to accommodate the indel calibration, and we should really remove that bulk. I can help with that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427
https://github.com/broadinstitute/gatk/pull/1068#issuecomment-152042390:32,Usability,simpl,simplify,32,"+1, lgtm! looks great to delete/simplify so much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1068#issuecomment-152042390
https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:94,Integrability,wrap,wrap,94,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633
https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:515,Safety,detect,detect,515,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633
https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:222,Usability,simpl,simple,222,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633
https://github.com/broadinstitute/gatk/issues/1091#issuecomment-156428631:1499,Usability,clear,clear,1499,"oks for the start of a BAM record within the BGZF block boundary. The latter is the part that uses heuristics to find the record start and for which we’ve fixed some bugs (https://github.com/HadoopGenomics/Hadoop-BAM/pull/25, https://github.com/HadoopGenomics/Hadoop-BAM/pull/29). The guessing algorithm is discussed in some detail in http://bioinformatics.oxfordjournals.org/content/suppl/2013/05/10/bts054.DC1/supplement.pdf. Hadoop-BAM has the concept of a “splitting” index file (with extension .splitting-bai). It contains the virtual file offsets for every nth read in a BAM (where n is 4096 by default). The input format code then uses the index to get the next read (or at least the next one in the index) that starts after the HDFS block boundary for the split. As far as I can tell, the code for creating splitting index files isn’t easy to run on files in HDFS (SplittingBAMIndexer runs on local filesystem files), so it may be that splitting index files are not widely used. . The standard BAM index (.bai) is not used by Hadoop-BAM for generating splits. ADAM has IndexedBamInputFormat (see https://github.com/bigdatagenomics/adam/pull/732 and https://github.com/bigdatagenomics/adam/issues/787), which seems to be focused on reading a subset of a file, since it doesn’t respect HDFS locality (mappers would not run on the machine hosting the input data). It's not clear to me how we could use .bai files to get an even distribution of splits, and to get decent locality. Note that Matti Niemenmaa's thesis on Hadoop-BAM states. > Note that BAM and BCF can be, and commonly are, indexed. As the; > index contains the precise positions of many records in the file, one might; > think that it could be used for aligning splits accurately. Unfortunately; > their indexing schemes are not suitable for the task, due to at least all of; > the following reasons... And the reasons are listed on p46 of https://aaltodoc.aalto.fi/bitstream/handle/123456789/11886/master_niemenmaa_matti_2013.pdf.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1091#issuecomment-156428631
https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155500559:27,Usability,simpl,simple,27,"Woohoo, knew it would be a simple fix! :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155500559
https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156166699:403,Usability,clear,clear,403,"@droazen :+1: for requiring a BAM index (once we verify that it indeed solves all of our problems). @jean-philippe-martin said exactly the same thing yesterday to me. So, I think for GATK 4 that's probably the right solution. It's not a problem of block boundaries. It's a problem of finding the first record within a BGZF block. A record from the previous block _can_ trail onto the next block with no clear indication of how much is left. That requires ""guessing"" for where the next record starts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156166699
https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278:108,Availability,down,down,108,"Very funny! Closing, since this is clearly meant as a joke. Let's discuss after alpha ways to actually slim down our dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278
https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278:117,Integrability,depend,dependencies,117,"Very funny! Closing, since this is clearly meant as a joke. Let's discuss after alpha ways to actually slim down our dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278
https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278:35,Usability,clear,clearly,35,"Very funny! Closing, since this is clearly meant as a joke. Let's discuss after alpha ways to actually slim down our dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1120#issuecomment-157435278
https://github.com/broadinstitute/gatk/pull/1126#issuecomment-157404178:12,Usability,clear,clearly,12,@lbergelson clearly volunteered to review this pull req,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1126#issuecomment-157404178
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:482,Deployability,pipeline,pipelines,482,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:646,Deployability,pipeline,pipeline,646,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:122,Integrability,wrap,wrapper,122,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:186,Integrability,contract,contract,186,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:450,Performance,perform,performance,450,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:861,Safety,avoid,avoid,861,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:566,Usability,simpl,simplifies,566,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856
https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039:46,Testability,test,tests,46,"I've addressed @davidadamsphd's feedback. The tests were passing on Friday, but now the build is failing due to https://github.com/broadinstitute/gatk/pull/1185, so that should be merged before this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039
https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039:32,Usability,feedback,feedback,32,"I've addressed @davidadamsphd's feedback. The tests were passing on Friday, but now the build is failing due to https://github.com/broadinstitute/gatk/pull/1185, so that should be merged before this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158912039
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:100,Safety,detect,detect,100,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:191,Safety,detect,detect,191,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:543,Usability,simpl,simpler,543,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163301007:40,Usability,simpl,simply,40,"That would work, yet I don't see why we simply can focus only in codecs that return the right type. Seems that would be the same amount of work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163301007
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163305134:37,Availability,failure,failureMessage,37,"A new annotation with features like `failureMessage` and `force=true` is a lot more complex and more work than just prioritizing type `X` for a `FeatureInput<X>`, if that will work just as well. See the chat room slogan ""hellbender instinct shall be to simplify and rip stuff out"" :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163305134
https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163305134:253,Usability,simpl,simplify,253,"A new annotation with features like `failureMessage` and `force=true` is a lot more complex and more work than just prioritizing type `X` for a `FeatureInput<X>`, if that will work just as well. See the chat room slogan ""hellbender instinct shall be to simplify and rip stuff out"" :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163305134
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:490,Deployability,install,installed,490,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:69,Testability,test,test,69,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:132,Testability,test,test,132,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:174,Testability,test,test,174,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:193,Testability,test,test,193,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:233,Testability,test,test,233,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:298,Testability,test,tests,298,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:555,Testability,log,logs,555,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676:565,Usability,usab,usable,565,"I don't know how to accomplish that with gradle. Is just keeping the test jvm going all we need? Or does the ui shutdown after each test? We could add an infinitely running ""test"" in a special test group to If we want to be keep the test jvm open. Alternatively if we really need to be able to run tests and then view the UI afterwards we could put together something using https://github.com/hammerlab/spree. It's a minor pain to set up, I had weird ruby packaging problems getting meteor installed, but it solves the problem of ""how do we collect spark logs in a usable way"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1193#issuecomment-159679676
https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606:232,Integrability,depend,depend,232,"So the long term timeline is fairly up in the air. You can check out the alpha and beta milestones for some idea of what's been prioritized. Alpha milestone is due for completion ~this week. Beta is much more up in the air and will depend at least in part on feedback from user. . Incidentally, if you're interested in CNV calling take a look at https://github.com/broadinstitute/gatk-protected/ which has some tools for CNV calling built on this engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606
https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606:259,Usability,feedback,feedback,259,"So the long term timeline is fairly up in the air. You can check out the alpha and beta milestones for some idea of what's been prioritized. Alpha milestone is due for completion ~this week. Beta is much more up in the air and will depend at least in part on feedback from user. . Incidentally, if you're interested in CNV calling take a look at https://github.com/broadinstitute/gatk-protected/ which has some tools for CNV calling built on this engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160717606
https://github.com/broadinstitute/gatk/pull/1207#issuecomment-160844574:99,Usability,clear,clearly,99,See my minor correction/suggestion. Should follow up with a task to explain the two-repo structure clearly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1207#issuecomment-160844574
https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954:180,Integrability,wrap,wraps,180,"I don't think that will work as the key needs to be `GATKRead` to take advantage of the `SAMRecordToGATKReadAdapterSerializer`. How about writing a new `Comparator<GATKRead>` that wraps a `SAMRecordCoordinateComparator`? That should be pretty simple and won't require a new serializer. BTW minor correction: `ReadSparkSink` operates on `JavaPairRDD<GATKRead, Void>` (not `JavaPairRDD<GATKRead, SAMRecordWritable>`) at the moment - the values are null so as to not duplicate the amount of data going through the shuffle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954
https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954:243,Usability,simpl,simple,243,"I don't think that will work as the key needs to be `GATKRead` to take advantage of the `SAMRecordToGATKReadAdapterSerializer`. How about writing a new `Comparator<GATKRead>` that wraps a `SAMRecordCoordinateComparator`? That should be pretty simple and won't require a new serializer. BTW minor correction: `ReadSparkSink` operates on `JavaPairRDD<GATKRead, Void>` (not `JavaPairRDD<GATKRead, SAMRecordWritable>`) at the moment - the values are null so as to not duplicate the amount of data going through the shuffle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162024954
https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168:295,Testability,test,tests,295,"In the interest of getting this merged, I've addressed the remaining blocking issue via documentation and naming: tool is now named `ConvertHeaderlessHadoopBamShardToBam`, and the docs make it clear what kinds of Hadoop bam shards it should be used with, and which it shouldn't. Will merge once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168
https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168:193,Usability,clear,clear,193,"In the interest of getting this merged, I've addressed the remaining blocking issue via documentation and naming: tool is now named `ConvertHeaderlessHadoopBamShardToBam`, and the docs make it clear what kinds of Hadoop bam shards it should be used with, and which it shouldn't. Will merge once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1278#issuecomment-221101168
https://github.com/broadinstitute/gatk/issues/1286#issuecomment-163307090:78,Usability,clear,clear,78,"I think the general format `gatk-launch Tool toolArgs -- sparkArgs` is pretty clear. Would this be solved by just making the terminology consistent (eg., change `Tool` to `GATKTool` etc.)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1286#issuecomment-163307090
https://github.com/broadinstitute/gatk/issues/1291#issuecomment-163309365:47,Usability,clear,clearly,47,"The spark tools all end in `*Spark` and are in clearly marked groups that begin with `Spark*`, which seems pretty clear to me :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291#issuecomment-163309365
https://github.com/broadinstitute/gatk/issues/1291#issuecomment-163309365:114,Usability,clear,clear,114,"The spark tools all end in `*Spark` and are in clearly marked groups that begin with `Spark*`, which seems pretty clear to me :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291#issuecomment-163309365
https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166677912:2,Deployability,release,released,2,I released a snapshot. A signed artifact will follow (1.0.0). Feel free to send me feedbacks BEFORE.; https://oss.sonatype.org/content/repositories/snapshots/com/github/jsr203hadoop/jsr203hadoop/0.0.1-SNAPSHOT/. Final artifact will follow these names:. ``` xml; <groupId>com.github.jsr203hadoop</groupId>; <artifactId>jsr203hadoop</artifactId>; <version>0.0.1-SNAPSHOT</version>; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166677912
https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166677912:83,Usability,feedback,feedbacks,83,I released a snapshot. A signed artifact will follow (1.0.0). Feel free to send me feedbacks BEFORE.; https://oss.sonatype.org/content/repositories/snapshots/com/github/jsr203hadoop/jsr203hadoop/0.0.1-SNAPSHOT/. Final artifact will follow these names:. ``` xml; <groupId>com.github.jsr203hadoop</groupId>; <artifactId>jsr203hadoop</artifactId>; <version>0.0.1-SNAPSHOT</version>; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166677912
https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166679112:28,Deployability,release,release,28,@damiencarol Thanks for the release! . I don't know if we'll be able to give you feedback about this until after the holidays. I believe @tomwhite is on vacation and he's the one most involved with hadoop-bam.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166679112
https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166679112:81,Usability,feedback,feedback,81,@damiencarol Thanks for the release! . I don't know if we'll be able to give you feedback about this until after the holidays. I believe @tomwhite is on vacation and he's the one most involved with hadoop-bam.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-166679112
https://github.com/broadinstitute/gatk/issues/1329#issuecomment-163672338:272,Usability,clear,clear,272,"I would prefer to not require an extra `--sparkMaster` argument to run a Spark tool locally -- ie, `/gatk-launch CountReadsSpark -I flag_stat.bam` should run locally by default. I think we should keep the name `sparkRunner` for symmetry with `sparkMaster`, and to make it clear it's a Spark argument and not a tool argument, but we should rename `SUBMIT` to `SPARK`, and `DIRECT` to `LOCAL`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1329#issuecomment-163672338
https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249:2,Deployability,patch,patched,2,I patched this in joptSimple in https://github.com/pholser/jopt-simple/pull/89. This can be enabled by upgrading to the 5.0.1-beta build or waiting for a stable release. Unclear on the time lines for stable release. I suspect if we really need it we can ask for a 4.10 release and the maintainer would likely be willing to create one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249
https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249:161,Deployability,release,release,161,I patched this in joptSimple in https://github.com/pholser/jopt-simple/pull/89. This can be enabled by upgrading to the 5.0.1-beta build or waiting for a stable release. Unclear on the time lines for stable release. I suspect if we really need it we can ask for a 4.10 release and the maintainer would likely be willing to create one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249
https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249:207,Deployability,release,release,207,I patched this in joptSimple in https://github.com/pholser/jopt-simple/pull/89. This can be enabled by upgrading to the 5.0.1-beta build or waiting for a stable release. Unclear on the time lines for stable release. I suspect if we really need it we can ask for a 4.10 release and the maintainer would likely be willing to create one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249
https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249:269,Deployability,release,release,269,I patched this in joptSimple in https://github.com/pholser/jopt-simple/pull/89. This can be enabled by upgrading to the 5.0.1-beta build or waiting for a stable release. Unclear on the time lines for stable release. I suspect if we really need it we can ask for a 4.10 release and the maintainer would likely be willing to create one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249
https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249:64,Usability,simpl,simple,64,I patched this in joptSimple in https://github.com/pholser/jopt-simple/pull/89. This can be enabled by upgrading to the 5.0.1-beta build or waiting for a stable release. Unclear on the time lines for stable release. I suspect if we really need it we can ask for a 4.10 release and the maintainer would likely be willing to create one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1347#issuecomment-178650249
https://github.com/broadinstitute/gatk/issues/1357#issuecomment-164069571:294,Usability,clear,clear,294,"Non-Spark walker implementations don't need an explicit --runner argument at all (and shouldn't, as we want them to be as easy to run as their GATK3 counterparts). I vote no on the `--sparkRunner` -> `--runner` rename, as I think our users are mostly unfamiliar with Spark, and it's good to be clear about what is a Spark argument and what is a tool argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1357#issuecomment-164069571
https://github.com/broadinstitute/gatk/pull/1373#issuecomment-166668206:60,Usability,usab,usable,60,closing this. we'll move the code when it's more mature and usable by germline cnvs,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1373#issuecomment-166668206
https://github.com/broadinstitute/gatk/pull/1378#issuecomment-169100605:89,Usability,simpl,simple,89,"@akiezun This should be fixed now. I think we're hitting a bug in git-lfs, but I found a simple workaround so once it passes I'm going to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1378#issuecomment-169100605
https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440:281,Testability,log,logic,281,@vdauwera The latest game plan for this ticket (which is being addressed in #2021) is to have SplitNCigarReads create a supplementary alignment from each split and soft clip the bases that align elsewhere. This allows the bam to be more usable by other tools (like BQSR) since the logic to handle supplementary reads already exists. This means that we need an additional argument to HaplotypeCaller so that allows using supplementary alignments (See #2043). . My question is how can we document that SplitNCigarReads is not recommended for use with the changes to make the reads supplementary until #2043 is resolved? Is it possible to make the tool hidden in GATK4 or give it some tag that makes it clear it isn't ready for use yet?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440
https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440:237,Usability,usab,usable,237,@vdauwera The latest game plan for this ticket (which is being addressed in #2021) is to have SplitNCigarReads create a supplementary alignment from each split and soft clip the bases that align elsewhere. This allows the bam to be more usable by other tools (like BQSR) since the logic to handle supplementary reads already exists. This means that we need an additional argument to HaplotypeCaller so that allows using supplementary alignments (See #2043). . My question is how can we document that SplitNCigarReads is not recommended for use with the changes to make the reads supplementary until #2043 is resolved? Is it possible to make the tool hidden in GATK4 or give it some tag that makes it clear it isn't ready for use yet?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440
https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440:700,Usability,clear,clear,700,@vdauwera The latest game plan for this ticket (which is being addressed in #2021) is to have SplitNCigarReads create a supplementary alignment from each split and soft clip the bases that align elsewhere. This allows the bam to be more usable by other tools (like BQSR) since the logic to handle supplementary reads already exists. This means that we need an additional argument to HaplotypeCaller so that allows using supplementary alignments (See #2043). . My question is how can we document that SplitNCigarReads is not recommended for use with the changes to make the reads supplementary until #2043 is resolved? Is it possible to make the tool hidden in GATK4 or give it some tag that makes it clear it isn't ready for use yet?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-235062440
https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556:16,Testability,test,tests,16,@lbergelson the tests are not running on [gatk-jenkins.broadinstitute.org](url) so it's not usable yet. What remains to be done?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556
https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556:92,Usability,usab,usable,92,@lbergelson the tests are not running on [gatk-jenkins.broadinstitute.org](url) so it's not usable yet. What remains to be done?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400#issuecomment-199324556
https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:206,Safety,predict,predicted,206,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133
https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:1881,Testability,log,log,1881,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133
https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:1756,Usability,simpl,simple,1756,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133
https://github.com/broadinstitute/gatk/issues/1426#issuecomment-170688961:137,Usability,guid,guidance,137,"@davidadamsphd Have a look at this one and let me know whether you think it's doable as a 20% project. @tomwhite should be able to offer guidance on how to implement this, since he contributed the reference version of this in https://github.com/samtools/htsjdk/pull/308",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426#issuecomment-170688961
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907:405,Safety,avoid,avoid,405,"First-pass review complete -- back to @tomwhite. Many of my suggestions center around pushing arguments and functionality up into `GATKSparkTool` as much as possible, even if they're not applicable to every tool, as we ideally want to spare tool authors from having to manually manage these low-level Spark parameters when they don't want/need to, and we also want to enforce consistency across tools and avoid duplicated boilerplate code. At the same time, there should be clear mechanisms for tools to override the defaults when they have to (eg., overridable methods in `GATKSparkTool`), as I'm not sure whether tools like BQSR are going to be happy with the new 128 MB default input split size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907:474,Usability,clear,clear,474,"First-pass review complete -- back to @tomwhite. Many of my suggestions center around pushing arguments and functionality up into `GATKSparkTool` as much as possible, even if they're not applicable to every tool, as we ideally want to spare tool authors from having to manually manage these low-level Spark parameters when they don't want/need to, and we also want to enforce consistency across tools and avoid duplicated boilerplate code. At the same time, there should be clear mechanisms for tools to override the defaults when they have to (eg., overridable methods in `GATKSparkTool`), as I'm not sure whether tools like BQSR are going to be happy with the new 128 MB default input split size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156:48,Testability,test,test,48,"I added the split size option back, and wrote a test for `dirSize`. All feedback should have been addressed now. Back to @droazen.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156:72,Usability,feedback,feedback,72,"I added the split size option back, and wrote a test for `dirSize`. All feedback should have been addressed now. Back to @droazen.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-173633156
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593:107,Testability,test,tests,107,Thanks for the review @droazen. I've addressed all your feedback in the latest commit. I'll merge once the tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593
https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593:56,Usability,feedback,feedback,56,Thanks for the review @droazen. I've addressed all your feedback in the latest commit. I'll merge once the tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-175651593
https://github.com/broadinstitute/gatk/pull/1442#issuecomment-183170796:113,Usability,clear,clear,113,"as discussed with @droazen, I'm not touching this PR until he's done with his experiment. Reassigning to make it clear that I'm not working on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1442#issuecomment-183170796
https://github.com/broadinstitute/gatk/issues/1460#issuecomment-178321797:36,Usability,guid,guidance,36,"Fair enough. So could you then give guidance on how much it would require as a function of number of read groups? Since we're scrapping indels, the context covariate is just previous base.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-178321797
https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180910563:201,Usability,simpl,simple,201,"@akiezun I believe that BAQ was only introduced into BQSR to handle indels... which we are dropping. I think you should tie out to GATK3 with BAQ included, remove BAQ completely, and then it should be simple to show that the BAQ-less results are barely any different (esp. if we enable binning). (@yfarjoun on cc)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180910563
https://github.com/broadinstitute/gatk/issues/1460#issuecomment-198037375:150,Usability,clear,clearly,150,BaseRecalibrator with 3GB heap (file size 100GB). This is a snapshot after ~1 hour of runtime. Check out the differences in the y axis here. GATK3 is clearly struggling while Hellbender is doing fine:. GATK3 heap profile after GC (each red triangle is a full GC); ![image](https://cloud.githubusercontent.com/assets/1993519/13857812/dce2bcb8-ec51-11e5-9fb2-9745e86c8e24.png). GATK4 heap profile after GC:; ![image](https://cloud.githubusercontent.com/assets/1993519/13857765/b4525934-ec51-11e5-95aa-4135c05ef850.png),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-198037375
https://github.com/broadinstitute/gatk/issues/1460#issuecomment-198379664:0,Usability,clear,clearly,0,"clearly, it's not working properly. @lbergelson what's the status?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-198379664
https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178662601:249,Usability,simpl,simpler,249,"If there is sufficient demand, we could implement a `LocusWalker` class for GATK4, particularly as the key classes from GATK3 were recently ported as part of https://github.com/broadinstitute/gatk/pull/1442. Adding new walker types in GATK4 is much simpler than it was in GATK3, so this would actually be a fairly easy thing to add.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178662601
https://github.com/broadinstitute/gatk/pull/1471#issuecomment-183170831:113,Usability,clear,clear,113,"as discussed with @droazen, I'm not touching this PR until he's done with his experiment. Reassigning to make it clear that I'm not working on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1471#issuecomment-183170831
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2509,Availability,down,down,2509,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:355,Deployability,pipeline,pipelines,355,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:411,Deployability,update,updates,411,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1334,Deployability,install,install,1334,"itute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1483,Deployability,pipeline,pipelines,1483,"SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO Prin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2599,Deployability,pipeline,pipelines,2599,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:159,Performance,load,loading,159,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:391,Performance,perform,performed,391,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2182,Performance,load,load,2182,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2729,Performance,load,load,2729,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:61,Security,access,access,61,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:120,Testability,test,testing,120,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:151,Testability,test,test,151,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:437,Testability,test,test,437,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:512,Testability,test,test,512,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:791,Testability,test,test,791,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1242,Testability,test,test,1242,"b.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1260,Testability,test,test,1260,"b/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1378,Testability,test,test,1378,"itute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1396,Testability,test,test,1396,"erformed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not se",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1516,Testability,test,test,1516,"tils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shuttin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:1533,Testability,test,test,1533,"e local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [Februar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:144,Usability,simpl,simple,144,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857
https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:35,Performance,optimiz,optimization,35,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065
https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:17,Usability,clear,clearly,17,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065
https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881:72,Testability,test,tests,72,done. moved getSpanningInterval to IntervalUtils and simplified + added tests for it. Moved IntervalUtilsUnitTest to the right package. No empty string in ctor. back to @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881
https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881:53,Usability,simpl,simplified,53,done. moved getSpanningInterval to IntervalUtils and simplified + added tests for it. Moved IntervalUtilsUnitTest to the right package. No empty string in ctor. back to @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-186019881
https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186431033:359,Usability,intuit,intuitive,359,"@gspowley The directory layout seems a bit strange to me. I would have naturally put it into `src/main/cpp`, but I guess gradle considers it to be a separate component that doesn't belong in main? Do you think it would be more or less confusing to move it? There's definitely something to be said for leaving it in the default location, but it seems very non-intuitive to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186431033
https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186434234:70,Usability,intuit,intuitive,70,"@lbergelson I agree, gradle's default location for native code is non-intuitive and a little ugly. I left the code in the default location so we could discuss where you want to keep it. I'm OK with moving it to `src/main/cpp`. I'll look into removing the `build/classes/main` hardcoding. Also, I'll look into a way to convince gradle to use /usr/bin/gcc-4.8 without using sudo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186434234
https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188414763:50,Usability,learn,learn,50,"jbwa was just a proof-of-concept and a pretext to learn JNI. The algorithm was built on the ""simple"" API for BWA, but the internal 'mem' algorithm is much more complicated and the last time (2013) I looked at it, it was not easy to use it as a library (e.g. too many things in the `main`). I remember people at http://cloudgene.uibk.ac.at/ used it. . > FYI, I try to use jbwa in a MapReduce approach. If you interested, I can; > keep u posted.; > We implemented a framework for the execution of MapReduce jobs; > graphically and therefore new use cases are always nice",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188414763
https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188414763:93,Usability,simpl,simple,93,"jbwa was just a proof-of-concept and a pretext to learn JNI. The algorithm was built on the ""simple"" API for BWA, but the internal 'mem' algorithm is much more complicated and the last time (2013) I looked at it, it was not easy to use it as a library (e.g. too many things in the `main`). I remember people at http://cloudgene.uibk.ac.at/ used it. . > FYI, I try to use jbwa in a MapReduce approach. If you interested, I can; > keep u posted.; > We implemented a framework for the execution of MapReduce jobs; > graphically and therefore new use cases are always nice",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188414763
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2346,Deployability,integrat,integration,2346,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2503,Deployability,update,update,2503,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2154,Integrability,interface,interface,2154,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2314,Integrability,interface,interfaces,2314,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2346,Integrability,integrat,integration,2346,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1629,Modifiability,extend,extend,1629,"windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you thi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:388,Performance,perform,perform,388,"I was looking `ReadWindowWalker` and I found it very interesting for iterate over windows, but although it is similar I still think that is not solving the same problem as the `SlidingWindowWalker` for two reasons:; 1. `ReadWindowWalker` requires reads to construct the `ReadWindow`, and it is not general for both reads and variants. My main idea behind the `SlidingWindowWalker` was to perform operation over windows along the genome (or requested intervals) for any kind of source provided to the walker.; 2. On the other hand, the approach to generate the sliding windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2524,Safety,avoid,avoid,2524,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2530,Safety,redund,redundancy,2530,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1854,Usability,simpl,simplest,1854,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:184,Performance,perform,performance,184,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:813,Usability,simpl,simple,813,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784:522,Deployability,update,update,522,"@droazen, I was thinking about changing to the more general implementation described in the first part of the discussion, because I think it will be more useful for other API clients. Because I would like to make it as much efficient as possible, I would like to know if using `ReadWindow` instead of `ReadsContext` will be better, and use a similar approach as the `ReadWindowWalker` for construct the windows. I will wait to address your comments to your feedback about this, to close this PR and open a new one or just update this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784:224,Energy Efficiency,efficient,efficient,224,"@droazen, I was thinking about changing to the more general implementation described in the first part of the discussion, because I think it will be more useful for other API clients. Because I would like to make it as much efficient as possible, I would like to know if using `ReadWindow` instead of `ReadsContext` will be better, and use a similar approach as the `ReadWindowWalker` for construct the windows. I will wait to address your comments to your feedback about this, to close this PR and open a new one or just update this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784
https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784:457,Usability,feedback,feedback,457,"@droazen, I was thinking about changing to the more general implementation described in the first part of the discussion, because I think it will be more useful for other API clients. Because I would like to make it as much efficient as possible, I would like to know if using `ReadWindow` instead of `ReadsContext` will be better, and use a similar approach as the `ReadWindowWalker` for construct the windows. I will wait to address your comments to your feedback about this, to close this PR and open a new one or just update this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-205412784
https://github.com/broadinstitute/gatk/pull/1529#issuecomment-191304706:88,Usability,learn,learn,88,"No problem. I'd rather take styling issues ironed out from the beginning, and in fact I learn quite a lot about GATK/Picard and Java in the process.; Thank you very much for your patience!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529#issuecomment-191304706
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:37,Availability,mask,mask,37,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:91,Availability,mask,mask,91,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:153,Availability,mask,masked,153,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:269,Availability,mask,mask,269,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:687,Availability,mask,mask,687,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:993,Availability,mask,mask,993,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610:359,Usability,simpl,simpler,359,"@akiezun The goal is to use a genome mask, like for example Heng's low complexity sequence mask, which has many small intervals from the reference to be masked out. To that end I've tried using both the -XL exclude regions arg and -L with a 'complement' version of the mask. I was running on custom 10X tools in one of my branches but the problem exists with simpler tools like PrintReadsSpark. . For example running like this on a bam subset of 1MB of the reference is fine:. ```; time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -- --sparkRunner LOCAL; real 0m49.678s; user 5m10.396s; sys 0m14.556s; ```. But add the complemented mask:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -L /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.complement.bed -- -- sparkRunner LOCAL; ```. And it's still running 35+ minutes later. Similar results observed using the original mask and XL argument:. ```; $ time /humgen/gsa-scr1/cwhelan/GATK/gatk/gatk-launch PrintReadsSpark --input test_in.bam -O test_out.bam -XL /humgen/gsa-hpprojects/dev/cwhelan/gatk-sv/hs37d5LCRs.bed -- --sparkRunner LOCAL; ```. Let me know if I can provide more details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209056610
https://github.com/broadinstitute/gatk/issues/1537#issuecomment-447900242:11,Usability,learn,learned,11,Have since learned external tools can separate out mixed-type records into biallelic records.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1537#issuecomment-447900242
https://github.com/broadinstitute/gatk/issues/1537#issuecomment-583977235:13,Usability,learn,learned,13,"> Have since learned external tools can separate out mixed-type records into biallelic records. Hi, I am also facing the same issue, would you mind to share what type of external tools you used for the separation?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1537#issuecomment-583977235
https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209:189,Testability,test,test,189,@cmnbroad Would you mind profiling `setHeaderStrict()` against `setHeader()` when you get a chance to determine whether the former is more costly than the latter? We've seen a spike in our test suite runtimes today and I want to rule this out as a potential cause. A simple loop that calls each method on each read in a bam + the unix time command should suffice.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209
https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209:267,Usability,simpl,simple,267,@cmnbroad Would you mind profiling `setHeaderStrict()` against `setHeader()` when you get a chance to determine whether the former is more costly than the latter? We've seen a spike in our test suite runtimes today and I want to rule this out as a potential cause. A simple loop that calls each method on each read in a bam + the unix time command should suffice.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191873209
https://github.com/broadinstitute/gatk/issues/1558#issuecomment-231765000:29,Usability,simpl,simple,29,This ticket should be fairly simple once https://github.com/broadinstitute/gatk/issues/1988 is implemented.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1558#issuecomment-231765000
https://github.com/broadinstitute/gatk/pull/1566#issuecomment-196365569:67,Usability,simpl,simpler,67,@SHuang-Broad done with review for now. I think it would get a lot simpler if you can simplify that list-of-maps data structure. Let's talk in person about it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1566#issuecomment-196365569
https://github.com/broadinstitute/gatk/pull/1566#issuecomment-196365569:86,Usability,simpl,simplify,86,@SHuang-Broad done with review for now. I think it would get a lot simpler if you can simplify that list-of-maps data structure. Let's talk in person about it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1566#issuecomment-196365569
https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227219320:223,Usability,simpl,simply,223,"you mean, for spark? Up to you - we'll only be comparing results between runs on your setup anyway. For my experiments I used 10 nodes of 16 CPUs each and stuff runs in <10-15 minutes then. If you go for fewer cores, it'll simply take longer - not a big deal here because the requirement is that those run overnight",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227219320
https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738:51,Security,access,accessed,51,"Ok -- caveat for all -- objects in bucket that are accessed via simple API Key need to have: User:allUsers:reader ACL permissions. if you need more complex access control, we'll have to support the ""secretFile"" attribute in `gcloud dataproc` -- not just the apiKey.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738
https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738:156,Security,access,access,156,"Ok -- caveat for all -- objects in bucket that are accessed via simple API Key need to have: User:allUsers:reader ACL permissions. if you need more complex access control, we'll have to support the ""secretFile"" attribute in `gcloud dataproc` -- not just the apiKey.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738
https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738:64,Usability,simpl,simple,64,"Ok -- caveat for all -- objects in bucket that are accessed via simple API Key need to have: User:allUsers:reader ACL permissions. if you need more complex access control, we'll have to support the ""secretFile"" attribute in `gcloud dataproc` -- not just the apiKey.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738
https://github.com/broadinstitute/gatk/issues/1610#issuecomment-211932533:180,Deployability,install,installed,180,"1. For spark - cloud dataproc works well; 2. For non-spark, the simplest setup seems to create a master-only dataproc cluster because it comes with a bunch of software already pre-installed. . Maybe that's all we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1610#issuecomment-211932533
https://github.com/broadinstitute/gatk/issues/1610#issuecomment-211932533:64,Usability,simpl,simplest,64,"1. For spark - cloud dataproc works well; 2. For non-spark, the simplest setup seems to create a master-only dataproc cluster because it comes with a bunch of software already pre-installed. . Maybe that's all we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1610#issuecomment-211932533
https://github.com/broadinstitute/gatk/pull/1621#issuecomment-201330553:10,Usability,clear,clearer,10,:+1: Much clearer now I think. Squash and merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1621#issuecomment-201330553
https://github.com/broadinstitute/gatk/issues/1644#issuecomment-288515251:71,Usability,clear,clear,71,"Re-assigning to @tomwhite as a possible future project, since he has a clear idea of how this could be implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-288515251
https://github.com/broadinstitute/gatk/pull/1662#issuecomment-207071663:274,Usability,clear,clear,274,"Not sure how to get Travis to take up your branch again, so I made a pull request (#1691); https://github.com/broadinstitute/gatk/pull/1691. even though I don't actually intend to merge this yet (it should stay in a separate branch because it changes the repository). To be clear I don't intend to merge it into _main_ yet, but it makes perfect sense to merge it into the _gcs-nio_ branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1662#issuecomment-207071663
https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204227646:261,Usability,clear,clearly,261,"thanks @gspowley I'll measure it again tomorrow. I see the KMP code has an array allocation - I wonder if this accounts for the relatively small difference between string and KMP. I'll also try again with my mud-room implementation and compare. Also, my Mac is clearly slower than what you're running on :) I'll try David's suggestion too and limit to active regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204227646
https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206324138:489,Energy Efficiency,efficient,efficient,489,"Can you describe more precisely what you mean by ""processing"" here?. On Wednesday, April 6, 2016, Geraldine Van der Auwera <; notifications@github.com> wrote:. > GATK3 is very slow when processing references with large numbers of; > contigs, such as draft genomes. In the past this mostly affected microbial; > genomes so we didn't do anything about it, but now the Hg38 has a lot more; > contigs so we have to make sure that's not going to be a problem with; > GATK4.; > ; > To be clear, efficient processing of reference genomes with thousands of; > contigs is a must-have.; > ; > Efficient processing of e.g. microbial draft genomes with tens of; > thousands of contigs is a nice-to-have. More than that is just crazy talk.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1688. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206324138
https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206324138:482,Usability,clear,clear,482,"Can you describe more precisely what you mean by ""processing"" here?. On Wednesday, April 6, 2016, Geraldine Van der Auwera <; notifications@github.com> wrote:. > GATK3 is very slow when processing references with large numbers of; > contigs, such as draft genomes. In the past this mostly affected microbial; > genomes so we didn't do anything about it, but now the Hg38 has a lot more; > contigs so we have to make sure that's not going to be a problem with; > GATK4.; > ; > To be clear, efficient processing of reference genomes with thousands of; > contigs is a must-have.; > ; > Efficient processing of e.g. microbial draft genomes with tens of; > thousands of contigs is a nice-to-have. More than that is just crazy talk.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1688. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206324138
https://github.com/broadinstitute/gatk/issues/1689#issuecomment-234084913:207,Usability,simpl,simple,207,"There is. Gatk-launch should be handling --files for both yarn and dataproc. I think James found some bug with gatk-launch and multiple files passed in, can't remember exactly what it was but it should be a simple fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1689#issuecomment-234084913
https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015:157,Testability,test,test,157,"In the course of investigating https://github.com/broadinstitute/gsa-unstable/issues/1409 a few things came up that I want to document for when it's time to test the new model:; - Throw away the spanning deletions alleles! They shouldn't affect the QUAL anymore since we won't be using the independent alleles approximation, but I don't want them mucking with the site type and choice of prior (SNP vs INDEL); - For the purposes of QD for VQSR, we should be using AS_QD very shortly, in which case the choice of prior for mixed sites will be clear because we're evaluating per-allele; - For the purposes of QUAL for emission, at mixed sites I'm in favor of continuing to apply the SNP prior in accordance with the doctrine of maximal sensitivity until VQSR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015
https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015:542,Usability,clear,clear,542,"In the course of investigating https://github.com/broadinstitute/gsa-unstable/issues/1409 a few things came up that I want to document for when it's time to test the new model:; - Throw away the spanning deletions alleles! They shouldn't affect the QUAL anymore since we won't be using the independent alleles approximation, but I don't want them mucking with the site type and choice of prior (SNP vs INDEL); - For the purposes of QD for VQSR, we should be using AS_QD very shortly, in which case the choice of prior for mixed sites will be clear because we're evaluating per-allele; - For the purposes of QUAL for emission, at mixed sites I'm in favor of continuing to apply the SNP prior in accordance with the doctrine of maximal sensitivity until VQSR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697#issuecomment-230464015
https://github.com/broadinstitute/gatk/issues/1697#issuecomment-242794037:46,Usability,clear,clear,46,> the choice of prior for mixed sites will be clear because we're evaluating per-allele. @ldgauthier I don't recall whether we've discussed this but the new model does handle mixed sites very naturally by assigning separate indel/SNP pseudocountsto indel/SNP alleles in the Dirichlet prior on allele frequencies.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697#issuecomment-242794037
https://github.com/broadinstitute/gatk/issues/1702#issuecomment-212155764:136,Usability,clear,clear,136,"Reopening. It was hard to see due to the in-house cluster being used by multiple people. On a private cluster on GSC, the difference is clear - MarkDuplicatesSpark goes from 7.4 minutes to 6.6-6.7 minutes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-212155764
https://github.com/broadinstitute/gatk/issues/1705#issuecomment-213567101:54,Usability,clear,clear,54,"Multiple badges should be fine, provided we include a clear label for each one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1705#issuecomment-213567101
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:891,Deployability,update,update,891,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:417,Energy Efficiency,adapt,adapted,417,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1712,Energy Efficiency,meter,meter,1712,"he superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1802,Energy Efficiency,meter,meter,1802,"he superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1660,Integrability,rout,routines,1660,"ollapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:373,Modifiability,extend,extend,373,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:417,Modifiability,adapt,adapted,417,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2073,Modifiability,inherit,inheritance,2073,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2193,Modifiability,inherit,inheriting,2193,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2432,Modifiability,inherit,inherit,2432,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2542,Modifiability,evolve,evolve,2542,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:100,Performance,perform,performance,100,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1359,Performance,tune,tuned,1359,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1369,Performance,perform,performance,1369,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1255,Security,expose,expose,1255,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:275,Usability,simpl,simplified,275,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:634,Usability,usab,usability,634,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:933,Usability,clear,clear,933,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2317,Usability,simpl,simplicity,2317,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2728,Usability,clear,clearly,2728,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2740,Usability,simpl,simply,2740,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210833987:37,Usability,simpl,simpler,37,"I agree that it is better to keep it simpler. I was proposing this before looking the latest commits in the HC branch. I will work on this walker without thinking about other cases, but I would like to keep the idea of padding and slide over intervals instead of the genome from the begining. It will be useful for the things that I have in mind. Should I close this PR until I implement everything, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210833987
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:104,Deployability,integrat,integration,104,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:104,Integrability,integrat,integration,104,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:116,Testability,test,test,116,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163:492,Usability,feedback,feedback,492,"I finished the implementation for the draft `SlidingWindowWalker` (I should implement an example and an integration test, but I would like to wait till some issues are solved). made a ""TODO"" about the way in which the intervals are constructed, because I will need a that `ReadShard` have a way to construct a shard without `ReadSource` (either null or empty source), just in case that the implemented `SlidingWindowWalker` does not require reads. @droazen, could you review and give me some feedback about this, because this class is important for other parts of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-215710163
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004:43,Availability,error,error,43,"Sorry @droazen, the previous commit had an error in the tests. I'm rebasing/squashing to make a clear PR and when all check pass (except CLOUD), you can review if you have time. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004:56,Testability,test,tests,56,"Sorry @droazen, the previous commit had an error in the tests. I'm rebasing/squashing to make a clear PR and when all check pass (except CLOUD), you can review if you have time. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004
https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004:96,Usability,clear,clear,96,"Sorry @droazen, the previous commit had an error in the tests. I'm rebasing/squashing to make a clear PR and when all check pass (except CLOUD), you can review if you have time. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212591053:333,Usability,simpl,simple,333,"@akiezun Do we really need both `onTraversalDone()` and `onTraversalSuccess()`? This could be a major source of confusion. Is there any tool that overrides `onTraversalSuccess()` but not `onTraversalDone()`? Maybe we should just rename `onTraversalSuccess()` back to `onTraversalDone()`, put it in the finally block, and keep things simple?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212591053
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212597490:51,Usability,clear,clear,51,"If we do keep both methods, we should make it VERY clear in the docs that `onTraversalDone()` should be for closing resources, and `onTraversalSucess()` should be for producing final outputs, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212597490
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212609203:18,Usability,clear,clearer,18,"Maybe it would be clearer to rename `onTraversalDone()` -> `cleanup()` It seems useful to offer both options, but it is a bit confusing still.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212609203
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:294,Deployability,update,update,294,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:261,Safety,avoid,avoid,261,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504
https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:321,Usability,clear,clear,321,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:848,Deployability,update,updates,848,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:314,Integrability,depend,depend,314,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:891,Integrability,depend,depend,891,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:934,Integrability,depend,depend,934,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:990,Integrability,depend,depend,990,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1101,Integrability,interface,interface,1101,"ergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolder",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1650,Integrability,interface,interface,1650," burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. /**; * Real compute kernel; */; void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);. /**; * Print final profiling information from native code. ; */; default void close() { jniClose(); }. void jniClose();; }; ```. and a class that implements those as native methods",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:265,Modifiability,extend,extend,265,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1680,Modifiability,extend,extends,1680," burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. /**; * Real compute kernel; */; void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);. /**; * Print final profiling information from native code. ; */; default void close() { jniClose(); }. void jniClose();; }; ```. and a class that implements those as native methods",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1130,Security,expose,expose,1130,"e PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIRead",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:666,Testability,test,testing,666,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:1079,Testability,test,tests,1079,"ergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolder",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:3253,Testability,test,tests,3253,"rom reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class type of JNIReadDataHolderClass; * @param haplotypeDataHolderClass class type of JNIHaplotypeDataHolderClass; */; void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. /**; * Real compute kernel; */; void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);. /**; * Print final profiling information from native code. ; */; default void close() { jniClose(); }. void jniClose();; }; ```. and a class that implements those as native methods . ```; public class AVXNativePairHMMKernel implements NativePairHMMKernel{. @Override; native void jniInitializeClassFields(Class<JNIReadDataHolderClass> readDataHolderClass, Class<JNIHaplotypeDataHolderClass> haplotypeDataHolderClass);. @Override; native void jniComputeLikelihoods(int numReads, int numHaplotypes, JNIReadDataHolderClass[] readDataArray,; JNIHaplotypeDataHolderClass[] haplotypeDataArray, double[] likelihoodArray, int maxNumThreadsToUse);; @Override; native void jniClose();; }; ```. The PPC repo will implement `NativePairHMMKernel` as well and have its own tests",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:653,Usability,simpl,simplify,653,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733:58,Energy Efficiency,power,powerdev,58,"@droazen [OSU Open Source Lab](http://osuosl.org/services/powerdev) provides the POWER8 cluster for open-source projects. Is it usable for your testing on PPC? Many open-source projects are using it. . With the source tree in a single repo that I propose, changes that are specific to AVX will be made only for the files under ""avx/"" directory, which are not used for building the PPC binary. For example, build.gradle will specify ""avx/"" when building the binary on x86_64 (""power8/"" on ppc64le). If the files under ""common/"" are changed (e.g., the package name is renamed from hellbender to gatk4), the changes should work on PPC if the tests don't fail on x86_64.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733:144,Testability,test,testing,144,"@droazen [OSU Open Source Lab](http://osuosl.org/services/powerdev) provides the POWER8 cluster for open-source projects. Is it usable for your testing on PPC? Many open-source projects are using it. . With the source tree in a single repo that I propose, changes that are specific to AVX will be made only for the files under ""avx/"" directory, which are not used for building the PPC binary. For example, build.gradle will specify ""avx/"" when building the binary on x86_64 (""power8/"" on ppc64le). If the files under ""common/"" are changed (e.g., the package name is renamed from hellbender to gatk4), the changes should work on PPC if the tests don't fail on x86_64.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733:639,Testability,test,tests,639,"@droazen [OSU Open Source Lab](http://osuosl.org/services/powerdev) provides the POWER8 cluster for open-source projects. Is it usable for your testing on PPC? Many open-source projects are using it. . With the source tree in a single repo that I propose, changes that are specific to AVX will be made only for the files under ""avx/"" directory, which are not used for building the PPC binary. For example, build.gradle will specify ""avx/"" when building the binary on x86_64 (""power8/"" on ppc64le). If the files under ""common/"" are changed (e.g., the package name is renamed from hellbender to gatk4), the changes should work on PPC if the tests don't fail on x86_64.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733
https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733:128,Usability,usab,usable,128,"@droazen [OSU Open Source Lab](http://osuosl.org/services/powerdev) provides the POWER8 cluster for open-source projects. Is it usable for your testing on PPC? Many open-source projects are using it. . With the source tree in a single repo that I propose, changes that are specific to AVX will be made only for the files under ""avx/"" directory, which are not used for building the PPC binary. For example, build.gradle will specify ""avx/"" when building the binary on x86_64 (""power8/"" on ppc64le). If the files under ""common/"" are changed (e.g., the package name is renamed from hellbender to gatk4), the changes should work on PPC if the tests don't fail on x86_64.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215324733
https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215795846:84,Deployability,install,install,84,@tomwhite this looks fine to me as a start for us to experiment with but I wish the install was simpler and so we should either include building of jbwa as part of our build or (preferable) use a pre-built version from maven central. wdyt?. back to @tomwhite,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215795846
https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215795846:96,Usability,simpl,simpler,96,@tomwhite this looks fine to me as a start for us to experiment with but I wish the install was simpler and so we should either include building of jbwa as part of our build or (preferable) use a pre-built version from maven central. wdyt?. back to @tomwhite,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215795846
https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:128,Performance,perform,performance,128,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978
https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:144,Usability,usab,usability,144,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978
https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213057081:70,Deployability,patch,patch,70,"Thanks @lbergelson and @droazen. Could it be possible to add a simple patch to add all the reads to the returned `AlignmentContext`, instead of just add the `ReadPileup` from just one covered sample?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213057081
https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213057081:63,Usability,simpl,simple,63,"Thanks @lbergelson and @droazen. Could it be possible to add a simple patch to add all the reads to the returned `AlignmentContext`, instead of just add the `ReadPileup` from just one covered sample?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213057081
https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514:17,Deployability,patch,patch,17,"I added a simple patch to fix this undocumented behaviour (#1757). Nevertheless, I'm working in an abstraction to include multi-sample support instead of include all the reads without differentiation in the pileup.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514
https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514:10,Usability,simpl,simple,10,"I added a simple patch to fix this undocumented behaviour (#1757). Nevertheless, I'm working in an abstraction to include multi-sample support instead of include all the reads without differentiation in the pileup.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514
https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514:35,Usability,undo,undocumented,35,"I added a simple patch to fix this undocumented behaviour (#1757). Nevertheless, I'm working in an abstraction to include multi-sample support instead of include all the reads without differentiation in the pileup.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752#issuecomment-213277514
https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356:115,Testability,test,test,115,"It's not clear to me what code path you're going through when using a `gs://` URI for the input bam in your second test. `CountReadsSpark` calls `GATKSparkTool.getReads()` which calls `JavaSparkContext.newAPIHadoopFile()`, but the question is how Hadoop-BAM handles your `gs://` URI. In other parts of the GATK (eg., `ReferenceTwoBitSource`) we call into `BucketUtils.openFile()`, which handles GCS URIs directly by calling into `GcsUtil.open()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356
https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356:9,Usability,clear,clear,9,"It's not clear to me what code path you're going through when using a `gs://` URI for the input bam in your second test. `CountReadsSpark` calls `GATKSparkTool.getReads()` which calls `JavaSparkContext.newAPIHadoopFile()`, but the question is how Hadoop-BAM handles your `gs://` URI. In other parts of the GATK (eg., `ReferenceTwoBitSource`) we call into `BucketUtils.openFile()`, which handles GCS URIs directly by calling into `GcsUtil.open()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213153356
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:246,Performance,load,loaded,246,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:363,Performance,load,loaded,363,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:462,Performance,load,loads,462,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:144,Testability,test,tests,144,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:160,Testability,test,testng,160,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:173,Testability,test,test,173,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:283,Testability,test,test,283,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:428,Testability,test,test,428,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:504,Testability,test,test,504,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:570,Testability,test,tests,570,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:61,Usability,clear,cleared,61,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701
https://github.com/broadinstitute/gatk/issues/1771#issuecomment-224300621:33,Usability,clear,clear,33,"Fine either way, as long as it's clear what remains to be done",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-224300621
https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716:43,Modifiability,enhance,enhancement,43,@lbergelson can you review - it's a simple enhancement to the CompareBaseQualities tool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716
https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716:36,Usability,simpl,simple,36,@lbergelson can you review - it's a simple enhancement to the CompareBaseQualities tool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716
https://github.com/broadinstitute/gatk/pull/1774#issuecomment-214595508:107,Deployability,patch,patch,107,"@droazen, I opened this new PR for the handling of multiple-samples in the ReadPileup. It is a very simple patch, instead of some implementation based on caching the spliting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-214595508
https://github.com/broadinstitute/gatk/pull/1774#issuecomment-214595508:100,Usability,simpl,simple,100,"@droazen, I opened this new PR for the handling of multiple-samples in the ReadPileup. It is a very simple patch, instead of some implementation based on caching the spliting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-214595508
https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141:64,Testability,test,testing,64,"Removed the `splitContextByReadGroup()`, simplified methods and testing exception thrown by `splitBySample()`. Back to you for review again, @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141
https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141:41,Usability,simpl,simplified,41,"Removed the `splitContextByReadGroup()`, simplified methods and testing exception thrown by `splitBySample()`. Back to you for review again, @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1774#issuecomment-219648141
https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:1057,Availability,repair,repairing,1057,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210
https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:653,Deployability,update,update,653,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210
https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:295,Safety,avoid,avoid,295,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210
https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:1210,Usability,simpl,simple,1210,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210
https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820:106,Integrability,interface,interfaces,106,"To summarize current state of discussions - we're going to have 3 repos, as originally planned (1 for the interfaces and 2 for Intel and IBM implementations, respectively). There will be a bit code duplication but many other aspects (some technical, some organizational) are massively simplified by such architecture. . @droazen @lbergelson @gspowley @paolonarvaez @frank-y-liu @t-ogasawara - please use this ticket to comment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820
https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820:285,Usability,simpl,simplified,285,"To summarize current state of discussions - we're going to have 3 repos, as originally planned (1 for the interfaces and 2 for Intel and IBM implementations, respectively). There will be a bit code duplication but many other aspects (some technical, some organizational) are massively simplified by such architecture. . @droazen @lbergelson @gspowley @paolonarvaez @frank-y-liu @t-ogasawara - please use this ticket to comment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-216545820
https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217072086:10,Usability,clear,clear,10,It is not clear for me what 2 separate repos can improve. Could you elaborate on that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217072086
https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:336,Performance,optimiz,optimization,336,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202
https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:15,Testability,test,test,15,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202
https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:359,Usability,simpl,simple,359,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202
https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:14,Performance,scalab,scalability,14,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496
https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:342,Performance,scalab,scalability,342,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496
https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:1501,Usability,clear,clearly,1501,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496
https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:245,Integrability,depend,depends,245,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823
https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:78,Performance,cache,cache,78,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823
https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:62,Usability,simpl,simply,62,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823
https://github.com/broadinstitute/gatk/issues/1812#issuecomment-287792231:55,Usability,usab,usable,55,This is currently in development at Intel -- should be usable within a quarter.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1812#issuecomment-287792231
https://github.com/broadinstitute/gatk/pull/1814#issuecomment-218598366:50,Usability,simpl,simple,50,"@droazen please review this one, should be pretty simple :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1814#issuecomment-218598366
https://github.com/broadinstitute/gatk/pull/1827#issuecomment-223698253:20,Usability,feedback,feedback,20,"@akiezun I've given feedback on it in person. @lbergelson may want to have a glance over it, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827#issuecomment-223698253
https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220021626:100,Usability,simpl,simple,100,"Because it is related with the `LocusWalker` pull request, could you review, @droazen? It is a very simple PR, but I will need it in my software. Thank you in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220021626
https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813:28,Testability,test,test,28,"I implemented a very simple test for tracking Ns. Is it enough, @akiezun?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813
https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813:21,Usability,simpl,simple,21,"I implemented a very simple test for tracking Ns. Is it enough, @akiezun?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-220032813
https://github.com/broadinstitute/gatk/pull/1833#issuecomment-222350655:46,Usability,simpl,simple,46,"I need this for my own work, and it is a very simple change that does not affect the default behaviour. @droazen can you review?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833#issuecomment-222350655
https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:8,Deployability,integrat,integration,8,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158
https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:8,Integrability,integrat,integration,8,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158
https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:20,Testability,test,tests,20,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158
https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158:30,Usability,simpl,simple,30,"I added integration tests for simple output and including features or verbose. While doing it, I realized that GATK 3.5 included some filters that wasn't included here, and that indels weren't tracked, so I changed also the code to fit the previous implementation. Back to you @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-221651158
https://github.com/broadinstitute/gatk/pull/1840#issuecomment-223696878:150,Usability,usab,usable,150,"First-pass review complete, back to @akiezun. This is a bit messy/spotty, with large blocks of missing GATK3 functionality, some annotations not in a usable state (eg., `AS_QualByDepth`), lots of TODOs in the code, and the branch is not currently compiling. We can probably get it merged in as a work-in-progress, but we should make sure we have tickets to capture the remaining work to be done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1840#issuecomment-223696878
https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508:26,Testability,test,test,26,"I've added the additional test you requested, and confirmed that it passes. The `SkipExceptions` are there to skip JBWA tests on platforms for which we don't have a build of the library -- I've extracted a `skipJBWATestOnUnsupportedPlatforms()` method to make this clearer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508
https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508:120,Testability,test,tests,120,"I've added the additional test you requested, and confirmed that it passes. The `SkipExceptions` are there to skip JBWA tests on platforms for which we don't have a build of the library -- I've extracted a `skipJBWATestOnUnsupportedPlatforms()` method to make this clearer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508
https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508:265,Usability,clear,clearer,265,"I've added the additional test you requested, and confirmed that it passes. The `SkipExceptions` are there to skip JBWA tests on platforms for which we don't have a build of the library -- I've extracted a `skipJBWATestOnUnsupportedPlatforms()` method to make this clearer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220757508
https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:70,Modifiability,refactor,refactoring,70,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049
https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:327,Safety,avoid,avoid,327,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049
https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:395,Usability,simpl,simplified,395,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049
https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242806423:14,Usability,simpl,simplified,14,@lbergelson I simplified the subsetting and removed the BitSet. Back to you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242806423
https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:324,Integrability,interface,interface,324,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518
https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:541,Integrability,interface,interface,541,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518
https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:1608,Security,expose,expose,1608,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518
https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:623,Usability,simpl,simplify,623,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518
https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:77,Modifiability,refactor,refactoring,77,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394
https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:170,Safety,avoid,avoid,170,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394
https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:163,Usability,simpl,simply,163,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:440,Availability,error,error,440,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:553,Availability,error,errors,553,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:150,Deployability,update,updated,150,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:477,Deployability,integrat,integration,477,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:477,Integrability,integrat,integration,477,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:369,Testability,test,tests,369,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:489,Testability,test,tests,489,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179:299,Usability,simpl,simplification,299,"@magicDGS Review complete for now. Looks good but I have some nitpicks. I think they're almost all due to it being ancient gatk3 code that no one has updated in a long time. I'd recommend dropping the deprecated formats and only supporting mpilup single sample format which should allow for massive simplification of both the Codec and the Feature. . We need some unit tests for the codec itself since it has a bunch of different potential error cases, and we should have some integration tests for the tool that show it correctly failing on cases with errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224417179
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331:284,Deployability,update,update,284,"Thanks, @lbergelson. I was also thinkinhg that this code is mostly deprecated, but I wanted to ported as is for the first pass review. I just need to support the new mpileup version (unique sample, because if not it is more difficult), because the consensus one is deprecated. I will update the codec and add some tests for it. In addition, ~~I was thinking to create a list of `PileupElement` inside the feature to make easier to compare the internal pileup, but with ""reads"" of one base-pair.~~ Update to this: `PileupElement`is difficult to generate without including `GATKRead` simple implementation, and I think that it is not worthy. On the other hand, I will improve the walker itself. I will tell you when I finished with the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331:314,Testability,test,tests,314,"Thanks, @lbergelson. I was also thinkinhg that this code is mostly deprecated, but I wanted to ported as is for the first pass review. I just need to support the new mpileup version (unique sample, because if not it is more difficult), because the consensus one is deprecated. I will update the codec and add some tests for it. In addition, ~~I was thinking to create a list of `PileupElement` inside the feature to make easier to compare the internal pileup, but with ""reads"" of one base-pair.~~ Update to this: `PileupElement`is difficult to generate without including `GATKRead` simple implementation, and I think that it is not worthy. On the other hand, I will improve the walker itself. I will tell you when I finished with the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331:582,Usability,simpl,simple,582,"Thanks, @lbergelson. I was also thinkinhg that this code is mostly deprecated, but I wanted to ported as is for the first pass review. I just need to support the new mpileup version (unique sample, because if not it is more difficult), because the consensus one is deprecated. I will update the codec and add some tests for it. In addition, ~~I was thinking to create a list of `PileupElement` inside the feature to make easier to compare the internal pileup, but with ""reads"" of one base-pair.~~ Update to this: `PileupElement`is difficult to generate without including `GATKRead` simple implementation, and I think that it is not worthy. On the other hand, I will improve the walker itself. I will tell you when I finished with the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224419331
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233:126,Testability,test,tests,126,"Ready for second pass review, @lbergelson. Now the implementation is much more simple than the previous one, and I added unit tests for the codec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233
https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233:79,Usability,simpl,simple,79,"Ready for second pass review, @lbergelson. Now the implementation is much more simple than the previous one, and I added unit tests for the codec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862#issuecomment-224670233
https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222236240:118,Usability,simpl,simple,118,That's exactly what I came here to do! I checked if Picard's FixMateInformation would fix it but it doesn't look that simple.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222236240
https://github.com/broadinstitute/gatk/issues/1865#issuecomment-226005625:10,Usability,simpl,simple,10,"Since the simple fix is merged for now, can we close this ticket and open a new one if the speed ends up being a problem?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1865#issuecomment-226005625
https://github.com/broadinstitute/gatk/pull/1867#issuecomment-223078285:140,Usability,simpl,simply,140,"FWIW I poked around a bit with the deflater and the main issue is that if we pass the deflater as a parameter then the default is specified simply by a constructor call and not by a run-time constant (also, the deflater can only be created once the compression level is known which is very late) and so there's no longer a central place where everyone gets their customizable deflaters from. I have a solution that I can submit for review in the next few days (make `DeflaterFactory` a proper factory with an overridable `makeDeflater` method, make it settable on writerFactory and also and provide a default one that can be set/queried in `BlockCompressedOutputStream.getDeflaterFactory` - that way one can set the default once and for all for everyone and also selectively use different deflaters for different writers is one so desires).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1867#issuecomment-223078285
https://github.com/broadinstitute/gatk/issues/1868#issuecomment-223043508:52,Usability,simpl,simply,52,"To be fair, @vruano suggested the solution and I am simply writing code to try to make the allele-trimming work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868#issuecomment-223043508
https://github.com/broadinstitute/gatk/issues/1869#issuecomment-381080555:87,Usability,learn,learning,87,"Modeling PCR stutter is better done with Valentin's approach, or in BQSR, or with deep learning. Making pairHMM fancier is not the answer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869#issuecomment-381080555
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:274,Availability,down,downsampling,274,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:363,Availability,down,downsampling,363,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:453,Availability,down,downsampling,453,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:379,Performance,perform,performed,379,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:430,Performance,perform,performing,430,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:556,Usability,feedback,feedback,556,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006
https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225997121:187,Integrability,depend,depends,187,"@magicDGS I could certainly do that (split out an OptionalReadFilterArgumentCollection), though we'd then need to add a ""requiresReadFilters"" method to determine which to use. I guess it depends on how common that case would be. An simple alternative would be to just override makeReadFilter and reject any command line filter requests or do any custom filter handling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225997121
https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225997121:232,Usability,simpl,simple,232,"@magicDGS I could certainly do that (split out an OptionalReadFilterArgumentCollection), though we'd then need to add a ""requiresReadFilters"" method to determine which to use. I guess it depends on how common that case would be. An simple alternative would be to just override makeReadFilter and reject any command line filter requests or do any custom filter handling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225997121
https://github.com/broadinstitute/gatk/issues/1914#issuecomment-227452491:106,Usability,guid,guidance,106,"Yes, agreed @akiezun -- and as mentioned at group meeting this week, we can meet with you guys to provide guidance in setting this up in a way that GATK can easily use after we meet our June deadlines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1914#issuecomment-227452491
https://github.com/broadinstitute/gatk/issues/1926#issuecomment-227537537:48,Usability,learn,learning,48,For @jamesemery -- this would be a good one for learning about Spark serialization,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926#issuecomment-227537537
https://github.com/broadinstitute/gatk/issues/1928#issuecomment-227512681:24,Usability,clear,clearer,24,duplicate (but probably clearer wording) of #175,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1928#issuecomment-227512681
https://github.com/broadinstitute/gatk/pull/1939#issuecomment-228442132:74,Usability,learn,learning,74,"@tedsharpe , please feel free to review as well. This to me is more about learning than reviewing, though I'll try hard.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1939#issuecomment-228442132
https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124:59,Deployability,pipeline,pipeline,59,"@tomwhite We host our internal WDLs for the best practices pipeline in the dsde-pipelines repository, and we're starting to put public versions in the public repo https://github.com/broadinstitute/wdl. @vdauwera recommends that we put any WDLs we write for GATK4 in https://github.com/broadinstitute/wdl as well, but within a directory that clearly marks them as experimental/unsupported.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124
https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124:80,Deployability,pipeline,pipelines,80,"@tomwhite We host our internal WDLs for the best practices pipeline in the dsde-pipelines repository, and we're starting to put public versions in the public repo https://github.com/broadinstitute/wdl. @vdauwera recommends that we put any WDLs we write for GATK4 in https://github.com/broadinstitute/wdl as well, but within a directory that clearly marks them as experimental/unsupported.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124
https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124:341,Usability,clear,clearly,341,"@tomwhite We host our internal WDLs for the best practices pipeline in the dsde-pipelines repository, and we're starting to put public versions in the public repo https://github.com/broadinstitute/wdl. @vdauwera recommends that we put any WDLs we write for GATK4 in https://github.com/broadinstitute/wdl as well, but within a directory that clearly marks them as experimental/unsupported.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1974#issuecomment-231770124
https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260:51,Performance,perform,performance,51,"done with my review. small edits, 1 bug, and a few performance questions. I'd like to see a simple perf run of HC with and without those changes. All those streams in math-heavy tight loops make me concerned a bit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260
https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260:92,Usability,simpl,simple,92,"done with my review. small edits, 1 bug, and a few performance questions. I'd like to see a simple perf run of HC with and without those changes. All those streams in math-heavy tight loops make me concerned a bit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:447,Availability,down,download,447,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:51,Deployability,install,install,51,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:417,Deployability,install,install,417,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:235,Modifiability,variab,variable,235,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:636,Modifiability,config,config,636,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:655,Modifiability,config,config,655,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:809,Modifiability,variab,variable,809,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:825,Modifiability,config,config,825,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1080,Modifiability,variab,variable,1080,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1391,Performance,load,load,1391,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1006,Usability,simpl,simply,1006,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495
https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:1057,Performance,perform,performance,1057,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040
https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:805,Testability,test,test,805,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040
https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:198,Usability,simpl,simply,198,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040
https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941:40,Testability,log,logical,40,"I've added some non-trivial (code-wise, logical wise very simple) code to single out slow assemblies more obviously. That is, generating another txt file collecting the runtime for those slow ones.; Example here ; /user/shuang/experiments/NA12878_PCR-_30X; /user/shuang/experiments/NA12878_PCR-_30X/assembly_betterLogging_longOnes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941
https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941:58,Usability,simpl,simple,58,"I've added some non-trivial (code-wise, logical wise very simple) code to single out slow assemblies more obviously. That is, generating another txt file collecting the runtime for those slow ones.; Example here ; /user/shuang/experiments/NA12878_PCR-_30X; /user/shuang/experiments/NA12878_PCR-_30X/assembly_betterLogging_longOnes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-245295941
https://github.com/broadinstitute/gatk/pull/2016#issuecomment-233708552:22,Usability,simpl,simple,22,not much code here so simple comments only. Absence of another `ReadShard` is weird and more comments would be useful. LGTM,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2016#issuecomment-233708552
https://github.com/broadinstitute/gatk/pull/2016#issuecomment-234018888:52,Integrability,interface,interface,52,"As discussed in person, extract a simple `Shard<T>` interface here to be more compatible with the work done in the `SlidingWindowWalker` branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2016#issuecomment-234018888
https://github.com/broadinstitute/gatk/pull/2016#issuecomment-234018888:34,Usability,simpl,simple,34,"As discussed in person, extract a simple `Shard<T>` interface here to be more compatible with the work done in the `SlidingWindowWalker` branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2016#issuecomment-234018888
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-234558814:276,Usability,simpl,simple,276,"> First off, SOAPsnp has the same equations we do, lacking the 2x factor on het genotypes: http://genome.cshlp.org/content/19/6/1124.full#sec-14. True, but because they consider only calling of single-sample diploid SNPs they can absorb the combinatorial factor in their very simple prior. I'm not saying they _do_ this (they don't really justify their priors), but I'm saying that in principle they could. And for all I know maybe we do the same in the exact model. In the new AF model, however, the most natural place for this factor is in the genotype likelihoods, not in the prior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-234558814
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-234686510:10,Usability,simpl,simplifying,10,"Maybe I'm simplifying things too much, but can't the equation in the paper be just rewritten as follows and thus the 2 will get eliminated:. ![paternal-maternal-phasing](https://cloud.githubusercontent.com/assets/6555937/17099330/b068c106-5235-11e6-83a1-718903d0012a.png). The above shows both orders that the phasing can happen assuming each phase representation is independent of the other, where each can appear with equal probability averaging to the unphased likelihood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-234686510
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:869,Performance,perform,performed,869,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:152,Security,validat,validation,152,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:801,Security,validat,validation,801,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:619,Testability,test,tests,619,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:764,Testability,test,test,764,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:812,Testability,test,tests,812,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:942,Testability,test,tests,942,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:612,Usability,simpl,simple,612,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221
https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263:5,Testability,test,tests,5,such tests mostly exist in VectorPairHMMUnitTest that @gspowley wrote. I think we could just move them there and add a simple java implementation of `computeLikelihoods` that is pretty much a copy of code from `LoglessPairHMM`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263
https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263:119,Usability,simpl,simple,119,such tests mostly exist in VectorPairHMMUnitTest that @gspowley wrote. I think we could just move them there and add a simple java implementation of `computeLikelihoods` that is pretty much a copy of code from `LoglessPairHMM`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234598263
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:160,Availability,down,downside,160,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:131,Energy Efficiency,adapt,adapter,131,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:189,Energy Efficiency,adapt,adapter,189,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:131,Integrability,adapter,adapter,131,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:189,Integrability,adapter,adapter,189,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:252,Integrability,interface,interface,252,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:131,Modifiability,adapt,adapter,131,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:189,Modifiability,adapt,adapter,189,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:79,Performance,cache,cached,79,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:397,Performance,perform,performance,397,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:210,Safety,avoid,avoid,210,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:365,Usability,clear,clear,365,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,Energy Efficiency,adapt,adapter,134,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:29,Integrability,interface,interface,29,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,Integrability,adapter,adapter,134,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,Modifiability,adapt,adapter,134,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:102,Performance,cache,cache,102,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:259,Performance,cache,cached,259,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:375,Testability,log,logic,375,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:362,Usability,simpl,simplify,362,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:123,Energy Efficiency,adapt,adapter,123,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:262,Energy Efficiency,adapt,adapter,262,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:323,Energy Efficiency,adapt,adapter,323,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:109,Integrability,wrap,wrapped,109,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:123,Integrability,adapter,adapter,123,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:173,Integrability,contract,contract,173,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:243,Integrability,wrap,wrapping,243,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:262,Integrability,adapter,adapter,262,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:306,Integrability,wrap,wrapped,306,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:323,Integrability,adapter,adapter,323,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:123,Modifiability,adapt,adapter,123,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:262,Modifiability,adapt,adapter,262,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:323,Modifiability,adapt,adapter,323,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:151,Usability,clear,clear,151,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718
https://github.com/broadinstitute/gatk/pull/2042#issuecomment-237349722:26,Usability,simpl,simple,26,Back to @jamesemery for a simple change,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2042#issuecomment-237349722
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:278,Modifiability,refactor,refactor,278,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:591,Modifiability,variab,variable,591,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:43,Usability,simpl,simple,43,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:140,Modifiability,refactor,refactoring,140,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:88,Security,validat,validate,88,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525
https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:20,Usability,simpl,simply,20,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525
https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240524115:97,Usability,clear,clear,97,Address PR comments to some extent. @SHuang-Broad take a look and let me know if things are more clear/better structured to you now. I think that renaming the `AssembledBreakpoint` class makes things at least a little bit less confusing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240524115
https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:88,Modifiability,plugin,plugin,88,"Thanks for the feedback, @cmnbroad.  @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652
https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:15,Usability,feedback,feedback,15,"Thanks for the feedback, @cmnbroad.  @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652
https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637:4285,Deployability,update,update,4285,"vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <0%> (-18.009%)` | `28% <0%> (ø)` | |; | ... and [96 more](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=footer). Last update [a85e0ff...1d6ce76](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637
https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637:2651,Testability,test,test,2651,N0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `91.667% <100%> (ø)` | `6 <2> (+1)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `91.772% <80%> (-0.795%)` | `77 <3> (+3)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637
https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637:4051,Usability,learn,learn,4051,"vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <0%> (-18.009%)` | `28% <0%> (ø)` | |; | ... and [96 more](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=footer). Last update [a85e0ff...1d6ce76](https://codecov.io/gh/broadinstitute/gatk/pull/2085?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-290039637
https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881149:163,Usability,clear,clearer,163,@droazen one comment. I think we should consider the naming of our jars in the final packages though. I think a folder with `gatk.jar` and `gatk-for-spark.jar` is clearer to users than `gatk-shadow.jar` and `gatk-spark.jar`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881149
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388:19,Testability,test,tests,19,A couple unrelated tests failing; hopefully a rebase will clear those up.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388:58,Usability,clear,clear,58,A couple unrelated tests failing; hopefully a rebase will clear those up.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-240491388
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:364,Energy Efficiency,power,power,364,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:482,Energy Efficiency,power,powerful,482,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:111,Performance,perform,perform,111,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127
https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:851,Usability,clear,clear,851,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127
https://github.com/broadinstitute/gatk/pull/2101#issuecomment-248730919:13,Usability,clear,clear,13,"OK, so to be clear - should I press the ""merge"" button?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2101#issuecomment-248730919
https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191:85,Deployability,update,update,85,"Intriguing. Thanks for the good example. To get around this it looks like we need to update NIO to allow it to be in a special ""broken"" state where the CloudStorageFileSystemProvider allows itself to be constructed even without credentials, failing later when we ask it to do anything. I think this is possible, but the change would have to be in gcloud-java-nio itself. The additional state is a bit counter-intuitive (usually allocation-is-initialization) but it seems worthwhile in this case. I'll get the ball rolling over at gcloud-java-nio.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191
https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191:514,Deployability,rolling,rolling,514,"Intriguing. Thanks for the good example. To get around this it looks like we need to update NIO to allow it to be in a special ""broken"" state where the CloudStorageFileSystemProvider allows itself to be constructed even without credentials, failing later when we ask it to do anything. I think this is possible, but the change would have to be in gcloud-java-nio itself. The additional state is a bit counter-intuitive (usually allocation-is-initialization) but it seems worthwhile in this case. I'll get the ball rolling over at gcloud-java-nio.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191
https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191:409,Usability,intuit,intuitive,409,"Intriguing. Thanks for the good example. To get around this it looks like we need to update NIO to allow it to be in a special ""broken"" state where the CloudStorageFileSystemProvider allows itself to be constructed even without credentials, failing later when we ask it to do anything. I think this is possible, but the change would have to be in gcloud-java-nio itself. The additional state is a bit counter-intuitive (usually allocation-is-initialization) but it seems worthwhile in this case. I'll get the ball rolling over at gcloud-java-nio.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241813191
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:685,Integrability,interface,interface,685,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:101,Safety,avoid,avoid,101,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:127,Testability,test,test,127,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:222,Testability,test,testing,222,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:446,Testability,test,testing,446,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:16,Usability,feedback,feedback,16,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889
https://github.com/broadinstitute/gatk/issues/2127#issuecomment-494961720:37,Usability,clear,clearly,37,Checking in from the future: we have clearly been failing this as I'm finding `time gatkPrintReads --help` takes ~2.5-3 seconds.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2127#issuecomment-494961720
https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672:150,Availability,avail,available,150,"Yes, but... It looks to me as if the index files, which appear to be in the master node's Linux file system in this failing example, are probably not available to the worker nodes. You'd have to copy each of the 5 index files to each of the workers, putting them in the same location on each. The same problem would occur with the new version: The single-image index file will still need to be available to all workers. You could distribute this file with:; ```--conf spark.yarn.dist.files=<location of the image file>```; which will copy it from your local machine to all workers each time you run the program. This isn't optimal, because it's pretty large. So, instead, you could copy it to a fixed path, identical on each worker, once up front, and then run your alignment jobs to your heart's content. The new version is a little simpler, because there's just one index file, but otherwise suffers from the same issue: bwa mem only knows how to deal with ordinary file system files -- not HDFS, not GCS -- and so the file must be copied to each worker machine in the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672
https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672:394,Availability,avail,available,394,"Yes, but... It looks to me as if the index files, which appear to be in the master node's Linux file system in this failing example, are probably not available to the worker nodes. You'd have to copy each of the 5 index files to each of the workers, putting them in the same location on each. The same problem would occur with the new version: The single-image index file will still need to be available to all workers. You could distribute this file with:; ```--conf spark.yarn.dist.files=<location of the image file>```; which will copy it from your local machine to all workers each time you run the program. This isn't optimal, because it's pretty large. So, instead, you could copy it to a fixed path, identical on each worker, once up front, and then run your alignment jobs to your heart's content. The new version is a little simpler, because there's just one index file, but otherwise suffers from the same issue: bwa mem only knows how to deal with ordinary file system files -- not HDFS, not GCS -- and so the file must be copied to each worker machine in the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672
https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672:834,Usability,simpl,simpler,834,"Yes, but... It looks to me as if the index files, which appear to be in the master node's Linux file system in this failing example, are probably not available to the worker nodes. You'd have to copy each of the 5 index files to each of the workers, putting them in the same location on each. The same problem would occur with the new version: The single-image index file will still need to be available to all workers. You could distribute this file with:; ```--conf spark.yarn.dist.files=<location of the image file>```; which will copy it from your local machine to all workers each time you run the program. This isn't optimal, because it's pretty large. So, instead, you could copy it to a fixed path, identical on each worker, once up front, and then run your alignment jobs to your heart's content. The new version is a little simpler, because there's just one index file, but otherwise suffers from the same issue: bwa mem only knows how to deal with ordinary file system files -- not HDFS, not GCS -- and so the file must be copied to each worker machine in the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-288545672
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:683,Integrability,interface,interface,683,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:600,Testability,log,logging,600,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:640,Testability,log,logging,640,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:770,Testability,log,log,770,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:1006,Testability,log,logger,1006,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:676,Usability,simpl,simple,676,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062:1282,Usability,simpl,simple,1282,"I tried to do it, and I'm afraid that it won't be trivial as I expected: because it is a facade, there is not accesibility to `Logger.setLogLevel()` and this is required to set the verbosity level in the command line. After explore a bit the code, it seems that [`LoggingUtils`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java) is the only place where a concrete implementation should be used. My suggestion is to move this class to a package that could be excluded by the backend user (because it contains methods to change the logging of log4j, I suggest `org.apache.logging.log4j`), which implements a simple interface/abstract class `org.broadinstitute.hellbender.utils.LoggingUtils` to set the log level (LoggingUtils.setLoggingLevel(final Log.LogLevel verbosity)`. The default implementation (that could be used by final users callid`super.setLoggingLevel(final Log.LogLevel verbosity)`) could setup the htsjdk and the java.util.logger.Logger. This implementation requires to change the `CommandLineProgram` to have a setter for the `LoggingUtils` to use, that could be set in `Main` (as in my PR for improve the extensibility of this class). The only pronblem is that it requires to be initialize with a simple implementation class of `LoggingUtils`, which should use the default. I think that this design does not break the behaviour of GATK, but introduce more complexity in the code. If you think that this is worthy, I could implement it today. @lbergelson, I'm not able to run Spark tools in a cluster yet, neither in gcloud dataproc, sorry. I'll wait for your answers on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259073062
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:557,Deployability,integrat,integration,557,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:382,Integrability,depend,dependency,382,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:480,Integrability,depend,dependencies,480,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:557,Integrability,integrat,integration,557,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:728,Performance,load,load,728,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:99,Testability,log,logging,99,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:172,Testability,log,logging,172,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:258,Testability,log,logging,258,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:330,Testability,log,logging,330,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:882,Testability,log,logback,882,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:1136,Testability,log,logback,1136,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:73,Usability,simpl,simpler,73,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054
https://github.com/broadinstitute/gatk/issues/2177#issuecomment-288761221:15,Usability,feedback,feedback,15,"Thanks for the feedback, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2177#issuecomment-288761221
https://github.com/broadinstitute/gatk/pull/2181#issuecomment-267126100:173,Usability,simpl,simple,173,"@kdatta Is this example code still current? If not, should we close this? We do eventually want some TileDB example code checked in, but ideally it would take the form of a simple, runnable GATK tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2181#issuecomment-267126100
https://github.com/broadinstitute/gatk/pull/2182#issuecomment-251455000:4,Deployability,update,updates,4,"The updates based on code review are done, but I need feedback on couple of questions above before I can finalize this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2182#issuecomment-251455000
https://github.com/broadinstitute/gatk/pull/2182#issuecomment-251455000:54,Usability,feedback,feedback,54,"The updates based on code review are done, but I need feedback on couple of questions above before I can finalize this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2182#issuecomment-251455000
https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107:2622,Integrability,interface,interface,2622," current = Allele.create(e.getBase());; }; pralm.add(e, current, DEFAULT_FAKE_LIKELIHOOD);; }; return pralm;; }; ```. The solution that I found after looking at the class was this one, that it's very complicated:. ``` java; public static ReadLikelihoods<Allele> flatPerReadAlleleLikelihoodsFromPileup(final ReadPileup pileup, final Allele refAllele, final SAMFileHeader header) {; final Set<Allele> alleleSet = new TreeSet<Allele>();; final Map<String, List<GATKRead>> reads = new HashMap<>();; final byte ref = refAllele.getBases()[0];; alleleSet.add(refAllele);; for (final PileupElement e : pileup) {; if (e.isDeletion()) {; alleleSet.add(Allele.SPAN_DEL);; } else if (e.getBase() == ref) {; alleleSet.add(refAllele);; } else {; alleleSet.add(Allele.create(e.getBase()));; }; final String sample = ReadUtils.getSampleName(e.getRead(), header);; List<GATKRead> list = reads.getOrDefault(sample, null);; if(list == null) {; list = new ArrayList<>();; reads.put(sample, list);; }; list.add(e.getRead());; }; final ReadLikelihoods<Allele> likelihoods = new ReadLikelihoods<>(new IndexedSampleList(reads.keySet()), new IndexedAlleleList<Allele>(alleleSet), reads);; for(final PileupElement e: pileup) {; final String sample = ReadUtils.getSampleName(e.getRead(), header);; final LikelihoodMatrix<Allele> l = likelihoods.sampleMatrix(likelihoods.indexOfSample(sample));; final int alleleIndex;; if (e.isDeletion()) {; alleleIndex = likelihoods.indexOfAllele(Allele.SPAN_DEL);; } else if (e.getBase() != ref) {; alleleIndex = likelihoods.indexOfAllele(Allele.create(e.getBase());; } else {; alleleIndex = likelihoods.indexOfReference();; }. l.set(alleleIndex, l.indexOfRead(e.getRead()), DEFAULT_FAKE_LIKELIHOOD);; }; return likelihoods;; }; ```. This example is very simple, but in my case what I need its to assign an unique likelihood to each read after calling the variant for that read. I want to use the variant annotation engine for annotate this likelihood map because it is using this interface.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107
https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107:167,Usability,simpl,simple,167,"Thanks a lot @davidbenjamin! One of the questions that I have is how to add a new read to the `ReadLikelihoods` for a new allele that is found while iterating. A very simple example will be:. ``` java; public static PerReadAlleleLikelihoodMap flatPerReadAlleleLikelihoodsFromPileup(final ReadPileup pileup, final Allele refAllele) {; final PerReadAlleleLikelihoodMap pralm = new PerReadAlleleLikelihoodMap();; final byte ref = refAllele.getBases()[0];; for (final PileupElement e : pileup) {; final Allele current;; if (e.isDeletion()) {; current = Allele.SPAN_DEL;; } else if (e.getBase() == ref) {; current = refAllele;; } else {; current = Allele.create(e.getBase());; }; pralm.add(e, current, DEFAULT_FAKE_LIKELIHOOD);; }; return pralm;; }; ```. The solution that I found after looking at the class was this one, that it's very complicated:. ``` java; public static ReadLikelihoods<Allele> flatPerReadAlleleLikelihoodsFromPileup(final ReadPileup pileup, final Allele refAllele, final SAMFileHeader header) {; final Set<Allele> alleleSet = new TreeSet<Allele>();; final Map<String, List<GATKRead>> reads = new HashMap<>();; final byte ref = refAllele.getBases()[0];; alleleSet.add(refAllele);; for (final PileupElement e : pileup) {; if (e.isDeletion()) {; alleleSet.add(Allele.SPAN_DEL);; } else if (e.getBase() == ref) {; alleleSet.add(refAllele);; } else {; alleleSet.add(Allele.create(e.getBase()));; }; final String sample = ReadUtils.getSampleName(e.getRead(), header);; List<GATKRead> list = reads.getOrDefault(sample, null);; if(list == null) {; list = new ArrayList<>();; reads.put(sample, list);; }; list.add(e.getRead());; }; final ReadLikelihoods<Allele> likelihoods = new ReadLikelihoods<>(new IndexedSampleList(reads.keySet()), new IndexedAlleleList<Allele>(alleleSet), reads);; for(final PileupElement e: pileup) {; final String sample = ReadUtils.getSampleName(e.getRead(), header);; final LikelihoodMatrix<Allele> l = likelihoods.sampleMatrix(likelihoods.indexOfSample(sample));; f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107
https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107:2396,Usability,simpl,simple,2396," current = Allele.create(e.getBase());; }; pralm.add(e, current, DEFAULT_FAKE_LIKELIHOOD);; }; return pralm;; }; ```. The solution that I found after looking at the class was this one, that it's very complicated:. ``` java; public static ReadLikelihoods<Allele> flatPerReadAlleleLikelihoodsFromPileup(final ReadPileup pileup, final Allele refAllele, final SAMFileHeader header) {; final Set<Allele> alleleSet = new TreeSet<Allele>();; final Map<String, List<GATKRead>> reads = new HashMap<>();; final byte ref = refAllele.getBases()[0];; alleleSet.add(refAllele);; for (final PileupElement e : pileup) {; if (e.isDeletion()) {; alleleSet.add(Allele.SPAN_DEL);; } else if (e.getBase() == ref) {; alleleSet.add(refAllele);; } else {; alleleSet.add(Allele.create(e.getBase()));; }; final String sample = ReadUtils.getSampleName(e.getRead(), header);; List<GATKRead> list = reads.getOrDefault(sample, null);; if(list == null) {; list = new ArrayList<>();; reads.put(sample, list);; }; list.add(e.getRead());; }; final ReadLikelihoods<Allele> likelihoods = new ReadLikelihoods<>(new IndexedSampleList(reads.keySet()), new IndexedAlleleList<Allele>(alleleSet), reads);; for(final PileupElement e: pileup) {; final String sample = ReadUtils.getSampleName(e.getRead(), header);; final LikelihoodMatrix<Allele> l = likelihoods.sampleMatrix(likelihoods.indexOfSample(sample));; final int alleleIndex;; if (e.isDeletion()) {; alleleIndex = likelihoods.indexOfAllele(Allele.SPAN_DEL);; } else if (e.getBase() != ref) {; alleleIndex = likelihoods.indexOfAllele(Allele.create(e.getBase());; } else {; alleleIndex = likelihoods.indexOfReference();; }. l.set(alleleIndex, l.indexOfRead(e.getRead()), DEFAULT_FAKE_LIKELIHOOD);; }; return likelihoods;; }; ```. This example is very simple, but in my case what I need its to assign an unique likelihood to each read after calling the variant for that read. I want to use the variant annotation engine for annotate this likelihood map because it is using this interface.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-249930107
https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250233112:462,Usability,feedback,feedback,462,"I like the first idea the most, not adding it for a concrete sample. The point of the methods that I described is added the likelihood for a single sample, and default likelihood to the others. Probably will be better defined as following: `add(String sample, GATKRead read, Allele allele, double likelihood, double defaultLikelihood)`. Do you think that you can implement the `Visitor` before removing the `PerReadAlleleLikelihoodMap`? Thanks a lot for all the feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250233112
https://github.com/broadinstitute/gatk/pull/2185#issuecomment-255604622:167,Usability,simpl,simply,167,"@magicDGS This PR is necessary for my work on Mutect2, but I'm out on Monday and Tuesday anyway. If #2154 is merged before Wednesday then all is fine; otherwise I can simply re-instate `PerReadAlleleLikelihoodMap` in this PR and delete it in a later PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-255604622
https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843:528,Availability,down,down,528,"I've made some improvements to this PR, including:; - Made it easier to use the `joinOverlapping` method by making the function you supply only have to worry about one interval (shard) at a time. This simplifies the callers code, so PileupSpark (for example) is now shorter.; - Added some documentation. I've also used the same technique to improve `AddContextDataToReadSpark` so that references are filled in on a per shard basis, rather than per read. In tests on a 6.6GB file I managed to get BaseRecalibratorSpark's runtime down from 10.61 minutes to 3.73 minutes, which is over 60% faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843
https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843:457,Testability,test,tests,457,"I've made some improvements to this PR, including:; - Made it easier to use the `joinOverlapping` method by making the function you supply only have to worry about one interval (shard) at a time. This simplifies the callers code, so PileupSpark (for example) is now shorter.; - Added some documentation. I've also used the same technique to improve `AddContextDataToReadSpark` so that references are filled in on a per shard basis, rather than per read. In tests on a 6.6GB file I managed to get BaseRecalibratorSpark's runtime down from 10.61 minutes to 3.73 minutes, which is over 60% faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843
https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843:201,Usability,simpl,simplifies,201,"I've made some improvements to this PR, including:; - Made it easier to use the `joinOverlapping` method by making the function you supply only have to worry about one interval (shard) at a time. This simplifies the callers code, so PileupSpark (for example) is now shorter.; - Added some documentation. I've also used the same technique to improve `AddContextDataToReadSpark` so that references are filled in on a per shard basis, rather than per read. In tests on a 6.6GB file I managed to get BaseRecalibratorSpark's runtime down from 10.61 minutes to 3.73 minutes, which is over 60% faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843
https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260:723,Testability,log,logger,723,"@droazen thanks for the review. I have now addressed all of your feedback, with the main changes being; - Revert the change to ShardBoundary that uses the padded interval as its interval, and use an anonymous class instead.; - Add a new join strategy (OVERLAPS_PARTITIONER) so that running BQSR can be done using the old way still.; - Add a check for overly long read sizes. If exceeded the job will fail with an exception.; - Be more conservative about the partition end point by using the maximum read length, rather than just the length of the read that happens the start the next partition.; - Rather than making a field in FeatureManager transient, do a better job of reinstating the field that is not serializable (a logger).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260
https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260:65,Usability,feedback,feedback,65,"@droazen thanks for the review. I have now addressed all of your feedback, with the main changes being; - Revert the change to ShardBoundary that uses the padded interval as its interval, and use an anonymous class instead.; - Add a new join strategy (OVERLAPS_PARTITIONER) so that running BQSR can be done using the old way still.; - Add a check for overly long read sizes. If exceeded the job will fail with an exception.; - Be more conservative about the partition end point by using the maximum read length, rather than just the length of the read that happens the start the next partition.; - Rather than making a field in FeatureManager transient, do a better job of reinstating the field that is not serializable (a logger).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-257368260
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261775822:15,Usability,feedback,feedback,15,Thanks for the feedback @cmnbroad! I could wait til #2218 is accepted to continue working and check that nothing is broked...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261775822
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:168,Modifiability,extend,extended,168,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:230,Testability,test,tests,230,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:114,Usability,simpl,simple,114,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:21,Availability,down,down,21,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:339,Testability,test,tests,339,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:540,Testability,test,test,540,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:765,Testability,test,tests,765,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313
https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:209,Usability,simpl,simpler,209,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313
https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:202,Availability,repair,repair,202,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574
https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:160,Deployability,update,updated,160,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574
https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:315,Security,expose,exposed,315,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574
https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:177,Usability,clear,clear,177,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574
https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:57,Modifiability,extend,extend,57,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430
https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:167,Modifiability,extend,extending,167,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430
https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:121,Usability,simpl,simplest,121,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430
https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:511,Usability,feedback,feedback,511,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430
https://github.com/broadinstitute/gatk/issues/2208#issuecomment-253618598:55,Deployability,release,release,55,We'll need a simple build system + README + documented release procedure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2208#issuecomment-253618598
https://github.com/broadinstitute/gatk/issues/2208#issuecomment-253618598:13,Usability,simpl,simple,13,We'll need a simple build system + README + documented release procedure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2208#issuecomment-253618598
https://github.com/broadinstitute/gatk/issues/2209#issuecomment-253618941:12,Usability,clear,clear,12,Should have clear instructions in README on how to use the docs system from another project (likely via one's build file).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2209#issuecomment-253618941
https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398:980,Testability,log,logic,980,"Here's a prototype showing one way of doing this: https://github.com/broadinstitute/gatk/commit/3cf7f9078b6aedcb87a4ca77c2aa8e84e7e5fe99. The idea is that `GATKTool` has a `-spark` flag, so you can run a tool in Spark mode. The parallel `GATKSparkTool` hierarchy would disappear, and tool writers would optionally be able to support Spark as an alternative to the regular walker, by providing the relevant code in the same tool class. As an example, the `FlagStat` implementation looks like this:. ``` java; @Override; public void traverse() {; if (sparkArgs.useSpark) {; sum = getReadsRdd().aggregate(new FlagStatus(), FlagStatus::add, FlagStatus::merge);; } else {; sum = getReadsStream().collect(FlagStatus::new, FlagStatus::add, FlagStatus::merge);; }; }; ```. Note that the walker version uses the Java 8 Streams API, while the Spark version uses the RDD API. While these are different APIs, many of the concepts are similar, so it should be possible to share the underlying logic by encapsulating it in classes, in much the same way as is done by the `FlagStatus` class for `FlagStat`. It would also be possible to keep the existing `apply` method for walker versions of tools that simply iterate over each record. Removing the parallel `GATKSparkTool` hierarchy has a number of benefits, such as reducing duplication, making it easier for tool writers to add Spark support, and making it easier to share tests. Thoughts? /cc @DR, @lbergelson, @cmnbroad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398
https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398:1411,Testability,test,tests,1411,"Here's a prototype showing one way of doing this: https://github.com/broadinstitute/gatk/commit/3cf7f9078b6aedcb87a4ca77c2aa8e84e7e5fe99. The idea is that `GATKTool` has a `-spark` flag, so you can run a tool in Spark mode. The parallel `GATKSparkTool` hierarchy would disappear, and tool writers would optionally be able to support Spark as an alternative to the regular walker, by providing the relevant code in the same tool class. As an example, the `FlagStat` implementation looks like this:. ``` java; @Override; public void traverse() {; if (sparkArgs.useSpark) {; sum = getReadsRdd().aggregate(new FlagStatus(), FlagStatus::add, FlagStatus::merge);; } else {; sum = getReadsStream().collect(FlagStatus::new, FlagStatus::add, FlagStatus::merge);; }; }; ```. Note that the walker version uses the Java 8 Streams API, while the Spark version uses the RDD API. While these are different APIs, many of the concepts are similar, so it should be possible to share the underlying logic by encapsulating it in classes, in much the same way as is done by the `FlagStatus` class for `FlagStat`. It would also be possible to keep the existing `apply` method for walker versions of tools that simply iterate over each record. Removing the parallel `GATKSparkTool` hierarchy has a number of benefits, such as reducing duplication, making it easier for tool writers to add Spark support, and making it easier to share tests. Thoughts? /cc @DR, @lbergelson, @cmnbroad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398
https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398:1188,Usability,simpl,simply,1188,"Here's a prototype showing one way of doing this: https://github.com/broadinstitute/gatk/commit/3cf7f9078b6aedcb87a4ca77c2aa8e84e7e5fe99. The idea is that `GATKTool` has a `-spark` flag, so you can run a tool in Spark mode. The parallel `GATKSparkTool` hierarchy would disappear, and tool writers would optionally be able to support Spark as an alternative to the regular walker, by providing the relevant code in the same tool class. As an example, the `FlagStat` implementation looks like this:. ``` java; @Override; public void traverse() {; if (sparkArgs.useSpark) {; sum = getReadsRdd().aggregate(new FlagStatus(), FlagStatus::add, FlagStatus::merge);; } else {; sum = getReadsStream().collect(FlagStatus::new, FlagStatus::add, FlagStatus::merge);; }; }; ```. Note that the walker version uses the Java 8 Streams API, while the Spark version uses the RDD API. While these are different APIs, many of the concepts are similar, so it should be possible to share the underlying logic by encapsulating it in classes, in much the same way as is done by the `FlagStatus` class for `FlagStat`. It would also be possible to keep the existing `apply` method for walker versions of tools that simply iterate over each record. Removing the parallel `GATKSparkTool` hierarchy has a number of benefits, such as reducing duplication, making it easier for tool writers to add Spark support, and making it easier to share tests. Thoughts? /cc @DR, @lbergelson, @cmnbroad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254180398
https://github.com/broadinstitute/gatk/issues/2219#issuecomment-254623625:179,Usability,usab,usable,179,"@mwalker174 If the reads in your bam are failing `WellFormedReadFilter`, then the GATK (generally speaking) can't handle them. Reads must pass at least that filter in order to be usable by GATK. Are you able to modify the bam to add read groups, etc., to allow the bam to pass the filter? If not, it is theoretically possible to disable `WellFormedReadFilter` using the `--disableReadFilter` argument, but I don't recommend it...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219#issuecomment-254623625
https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314:155,Deployability,update,updated,155,"I've now improved the naming of the parameter tot specify the Spark submit command (now it's `--sparkSubmitCommand`), to address @lbergelson's feedback. I updated to the latest shaded google-cloud-nio artifact, and it works with Spark 2 on a cluster. However, the `GcsNioIntegrationTest` fails due to the `javax` package (and subpackages) being shaded (these packages should not be shaded since Java provides these classes). So I'm afraid we'll need another release to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314
https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314:458,Deployability,release,release,458,"I've now improved the naming of the parameter tot specify the Spark submit command (now it's `--sparkSubmitCommand`), to address @lbergelson's feedback. I updated to the latest shaded google-cloud-nio artifact, and it works with Spark 2 on a cluster. However, the `GcsNioIntegrationTest` fails due to the `javax` package (and subpackages) being shaded (these packages should not be shaded since Java provides these classes). So I'm afraid we'll need another release to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314
https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314:143,Usability,feedback,feedback,143,"I've now improved the naming of the parameter tot specify the Spark submit command (now it's `--sparkSubmitCommand`), to address @lbergelson's feedback. I updated to the latest shaded google-cloud-nio artifact, and it works with Spark 2 on a cluster. However, the `GcsNioIntegrationTest` fails due to the `javax` package (and subpackages) being shaded (these packages should not be shaded since Java provides these classes). So I'm afraid we'll need another release to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257582314
https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:144,Security,access,access,144,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338
https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:59,Testability,test,testing,59,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338
https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:90,Usability,clear,clearly,90,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338
https://github.com/broadinstitute/gatk/pull/2232#issuecomment-256020212:260,Usability,feedback,feedback,260,"But if your implementation is a `VariantWalker` and #2223 is implemented, this will clash. By the way, I don't know if there is any plan to include all the tools from Picard, but it seems that the behaviour will be replicated if so, no?. Thanks anyway for the feedback, close the PR in case you think that it is not necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-256020212
https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389:336,Availability,error,error,336,"I had a look to the other branch, @droazen, and I think that it is more functional than this one:; - Check if the input already have a sequence dictionary, and only updates if `--replace` is provided. The version in this PR just overrides the dictionary.; - Check if all the variants agree with the new sequence dictionary, throwing an error if the contig is not present or the variant falls outside the chromosome range. This version does not account at all for that.; - It is a `VariantWalker`, and thus the code is simplest. But the pitfall of this is that if #2223 is implemented, that class will require a dictionary for the input as a `GATKTool`. I'm not sure how that is going to be done, but I guess that it will introduce problems in the class implemented by @cmnbroad. I think that the other version is more complete and I like it more because it is more concern about putative problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389
https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389:165,Deployability,update,updates,165,"I had a look to the other branch, @droazen, and I think that it is more functional than this one:; - Check if the input already have a sequence dictionary, and only updates if `--replace` is provided. The version in this PR just overrides the dictionary.; - Check if all the variants agree with the new sequence dictionary, throwing an error if the contig is not present or the variant falls outside the chromosome range. This version does not account at all for that.; - It is a `VariantWalker`, and thus the code is simplest. But the pitfall of this is that if #2223 is implemented, that class will require a dictionary for the input as a `GATKTool`. I'm not sure how that is going to be done, but I guess that it will introduce problems in the class implemented by @cmnbroad. I think that the other version is more complete and I like it more because it is more concern about putative problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389
https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389:518,Usability,simpl,simplest,518,"I had a look to the other branch, @droazen, and I think that it is more functional than this one:; - Check if the input already have a sequence dictionary, and only updates if `--replace` is provided. The version in this PR just overrides the dictionary.; - Check if all the variants agree with the new sequence dictionary, throwing an error if the contig is not present or the variant falls outside the chromosome range. This version does not account at all for that.; - It is a `VariantWalker`, and thus the code is simplest. But the pitfall of this is that if #2223 is implemented, that class will require a dictionary for the input as a `GATKTool`. I'm not sure how that is going to be done, but I guess that it will introduce problems in the class implemented by @cmnbroad. I think that the other version is more complete and I like it more because it is more concern about putative problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389
https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869:43,Testability,test,test,43,@davidbenjamin Can you craft a simple unit test for these annotations to make sure this stays fixed?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869
https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869:31,Usability,simpl,simple,31,@davidbenjamin Can you craft a simple unit test for these annotations to make sure this stays fixed?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257574869
https://github.com/broadinstitute/gatk/pull/2243#issuecomment-268280066:55,Usability,simpl,simple,55,"Can you have a look to this, @cmnbroad? This is a very simple PR and it is already reviewed in Picard...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-268280066
https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271648869:15,Usability,simpl,simple,15,@magicDGS very simple comments 👍 when addressed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271648869
https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593:20,Testability,test,test,20,Added a very simple test for sorting iterator. Back to you @droazen.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593
https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593:13,Usability,simpl,simple,13,Added a very simple test for sorting iterator. Back to you @droazen.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2249#issuecomment-267139593
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:15,Deployability,release,release,15,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:109,Deployability,release,release,109,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:167,Performance,perform,performance,167,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:59,Testability,test,test-drive,59,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:155,Usability,feedback,feedback,155,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797:63,Testability,test,tested,63,"@lh3 We agree about the low-cov joint calling and have already tested it on 191 low-coverage (roughly 3-4x) samples from 1kg. The calls were identical except for one site with qual just above 30 that used to be just below, but this difference is basically arbitrary. If we simply add 15 to the qual threshold (as we should, because the new qual is systematically more permissive due to learning a minor allele fraction that may be greater than the average genome-wide heterozygosity), results are completely identical.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797:273,Usability,simpl,simply,273,"@lh3 We agree about the low-cov joint calling and have already tested it on 191 low-coverage (roughly 3-4x) samples from 1kg. The calls were identical except for one site with qual just above 30 that used to be just below, but this difference is basically arbitrary. If we simply add 15 to the qual threshold (as we should, because the new qual is systematically more permissive due to learning a minor allele fraction that may be greater than the average genome-wide heterozygosity), results are completely identical.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797:386,Usability,learn,learning,386,"@lh3 We agree about the low-cov joint calling and have already tested it on 191 low-coverage (roughly 3-4x) samples from 1kg. The calls were identical except for one site with qual just above 30 that used to be just below, but this difference is basically arbitrary. If we simply add 15 to the qual threshold (as we should, because the new qual is systematically more permissive due to learning a minor allele fraction that may be greater than the average genome-wide heterozygosity), results are completely identical.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258436797
https://github.com/broadinstitute/gatk/issues/2255#issuecomment-272495952:14,Usability,feedback,feedback,14,@vdauwera any feedback from GATK 3.7 users? @droazen how's the Palantir tie-out doing?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-272495952
https://github.com/broadinstitute/gatk/pull/2256#issuecomment-267098362:62,Usability,feedback,feedback,62,"Thanks for the review @lbergelson. I've addressed most of the feedback, but still have a few more to do.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2256#issuecomment-267098362
https://github.com/broadinstitute/gatk/pull/2256#issuecomment-267551963:32,Usability,feedback,feedback,32,"@lbergelson I've addressed your feedback now, so back to you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2256#issuecomment-267551963
https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373:52,Deployability,update,updated,52,@cwhelan thanks for the clear up on the tests! Just updated with some fix and the output is now the same as master up to some formatting changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373
https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373:40,Testability,test,tests,40,@cwhelan thanks for the clear up on the tests! Just updated with some fix and the output is now the same as master up to some formatting changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373
https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373:24,Usability,clear,clear,24,@cwhelan thanks for the clear up on the tests! Just updated with some fix and the output is now the same as master up to some formatting changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-261665373
https://github.com/broadinstitute/gatk/pull/2258#issuecomment-262379027:213,Usability,simpl,simply,213,"@cwhelan , the tool, because of the Spark 2.0 bug, is not workable. But all changes has been reflected in the branch `sh_rewind_refactor`, which has the same output as branch `sh_master_formatter` which itself is simply a change of output format from the master version of the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258#issuecomment-262379027
https://github.com/broadinstitute/gatk/pull/2260#issuecomment-268280390:15,Usability,simpl,simple,15,"This is a very simple path, @lbergelson. Could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260#issuecomment-268280390
https://github.com/broadinstitute/gatk/issues/2267#issuecomment-266453955:85,Usability,simpl,simple,85,"Great! I can't remember if I wrote up the design requirements somewhere, but they're simple. The only non-obvious thing is to take a ""high-confidence"" interval list where we assume anything not in the truth is a false positive (as in NIST GiaB). Might also be useful for DREAM, but I don't remember the details of how that truth data is represented.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-266453955
https://github.com/broadinstitute/gatk/issues/2269#issuecomment-278370291:473,Deployability,update,updated,473,"There is an existing method in GATKTool called getHeaderForSAMWriter that creates and populates the PG record, and it's called by GATKTool.createSAMWriter, so we already do this for BAMs that are created that way (which excludes the Picard tools). We should probably fix MarkDuplicates though. HaplotypeCaller and Mutect2 use HaplotypBAMWriter/SAMFileDestination/HaplotypeBAMDestination, all of which live in gatk, but create their own writers directly, so they need to be updated (and could probably be simplified a bit). We need a similar method for vcf header lines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269#issuecomment-278370291
https://github.com/broadinstitute/gatk/issues/2269#issuecomment-278370291:504,Usability,simpl,simplified,504,"There is an existing method in GATKTool called getHeaderForSAMWriter that creates and populates the PG record, and it's called by GATKTool.createSAMWriter, so we already do this for BAMs that are created that way (which excludes the Picard tools). We should probably fix MarkDuplicates though. HaplotypeCaller and Mutect2 use HaplotypBAMWriter/SAMFileDestination/HaplotypeBAMDestination, all of which live in gatk, but create their own writers directly, so they need to be updated (and could probably be simplified a bit). We need a similar method for vcf header lines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269#issuecomment-278370291
https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268280677:22,Deployability,patch,patch,22,"This is a very simple patch, @cmnbroad. Could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268280677
https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268280677:15,Usability,simpl,simple,15,"This is a very simple patch, @cmnbroad. Could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268280677
https://github.com/broadinstitute/gatk/pull/2282#issuecomment-271587711:99,Usability,clear,clear,99,"@magicDGS Yes, it was the additional commits beyond 6d1cbf5 that I was referring to (and it wasn't clear to me whether **all** of the code review requests were in that commit, or if some were distributed amongst the other commits with the additional changes). In general I think once we start a review, the only changes should be based the code review requests. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-271587711
https://github.com/broadinstitute/gatk/pull/2282#issuecomment-274960068:312,Usability,simpl,simplifying,312,"Hi @magicDGS. After looking a bit more at this PR , and talking with others, I think we should put this branch on hold for a bit. We plan to introduce a new class soon that will serve as a common currency for input specifiers, and we'll probably want to revisit these constructors then. Hopefully that will be a simplifying change. In the meantime, I'd prefer not to introduce more overloads. We'd like to put this on hold until we make progress on that, and then perhaps resurrect parts of this branch as needed. My apologies for the churn.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-274960068
https://github.com/broadinstitute/gatk/pull/2282#issuecomment-275080208:15,Usability,feedback,feedback,15,"Thanks for the feedback @cmnbroad. I would like to have something more consistent for my own tools, independently on if I'm using `Path` or `File, but it could wait.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-275080208
https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:106,Performance,perform,performance,106,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713
https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:34,Testability,test,tests,34,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713
https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:118,Testability,test,tests,118,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713
https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:353,Testability,test,tests,353,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713
https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:375,Usability,clear,clearly,375,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713
https://github.com/broadinstitute/gatk/issues/2291#issuecomment-272664743:71,Deployability,release,release,71,"@vdauwera -- just to be clear, this is only in GATK 4 M2, which is pre-release (but we're working as fast as we can).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2291#issuecomment-272664743
https://github.com/broadinstitute/gatk/issues/2291#issuecomment-272664743:24,Usability,clear,clear,24,"@vdauwera -- just to be clear, this is only in GATK 4 M2, which is pre-release (but we're working as fast as we can).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2291#issuecomment-272664743
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726:43,Testability,test,tests,43,"@davidbernick Since you set up the jenkins tests for us, would you be able to help us out with this ticket? We've gotten weary of catching Spark regressions post-merge with jenkins, and want to set up fast dataproc-based tests in travis that run on every pull request, and just run simple Spark tools like PrintReadsSpark to try to catch at least the most basic kinds of breakage before a branch is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726:221,Testability,test,tests,221,"@davidbernick Since you set up the jenkins tests for us, would you be able to help us out with this ticket? We've gotten weary of catching Spark regressions post-merge with jenkins, and want to set up fast dataproc-based tests in travis that run on every pull request, and just run simple Spark tools like PrintReadsSpark to try to catch at least the most basic kinds of breakage before a branch is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726:282,Usability,simpl,simple,282,"@davidbernick Since you set up the jenkins tests for us, would you be able to help us out with this ticket? We've gotten weary of catching Spark regressions post-merge with jenkins, and want to set up fast dataproc-based tests in travis that run on every pull request, and just run simple Spark tools like PrintReadsSpark to try to catch at least the most basic kinds of breakage before a branch is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-286240726
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:254,Availability,down,down,254,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:4,Testability,test,tests,4,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:406,Testability,test,tests,406,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:550,Testability,test,tests,550,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:605,Testability,test,tests,605,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:451,Usability,simpl,simply,451,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886
https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:272,Deployability,integrat,integrated,272,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998
https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:204,Energy Efficiency,efficient,efficient,204,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998
https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:272,Integrability,integrat,integrated,272,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998
https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:138,Testability,test,tested,138,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998
https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998:375,Usability,clear,clear,375,"If you're interested in BWASpark tool I might wait a bit. There are a lot of issues with it as it currently stands, it's one of the least tested tools we have. We have someone working on a different more efficient implementation of the bwa bindings that may eventually be integrated into mainline gatk, so we've sort of stopped most development on BWASparkEngine until we're clear on the direction that the new work is going to take.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119998
https://github.com/broadinstitute/gatk/pull/2306#issuecomment-267026554:103,Usability,simpl,simple,103,"@lbergelson that seems to be a separate bug, since this just reverts some commits. There's obviously a simple workaround here too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-267026554
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:319,Availability,error,error,319,"@meganshand Thanks for making the code better! If and only if you care to beautify it further while you're at it, I noticed a few other things:. * Using `addExact` instead of `+` is overkill, since the overflow it protects against can only occur if we have a read depth of 2,147,483,648. In any case, it just throws an error. If we wanted to be super-scrupulous, we would put an `if` statement in the `FisherExactTest` code to switch to an asymptotic approximation, but, like I said, overkill. * This class has it's own `apply` method which replicates `MathUtils::applyToArray`. * Similarly, it's `range` method should be deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1960,Safety,avoid,avoided,1960,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:2063,Security,validat,validateArg,2063,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:2138,Security,validat,validateArg,2138,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1212,Testability,log,log-space,1212,"while you're at it, I noticed a few other things:. * Using `addExact` instead of `+` is overkill, since the overflow it protects against can only occur if we have a read depth of 2,147,483,648. In any case, it just throws an error. If we wanted to be super-scrupulous, we would put an `if` statement in the `FisherExactTest` code to switch to an asymptotic approximation, but, like I said, overkill. * This class has it's own `apply` method which replicates `MathUtils::applyToArray`. * Similarly, it's `range` method should be deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1329,Testability,log,log-space,1329,"ld put an `if` statement in the `FisherExactTest` code to switch to an asymptotic approximation, but, like I said, overkill. * This class has it's own `apply` method which replicates `MathUtils::applyToArray`. * Similarly, it's `range` method should be deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1625,Testability,log,log-,1625,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1736,Testability,log,log-,1736,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1925,Usability,simpl,simpler,1925,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:1172,Energy Efficiency,reduce,reduce,1172,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:153,Testability,log,logProbability,153,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:250,Testability,log,logBinomial,250,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:358,Testability,log,logBinomial,358,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:382,Testability,log,logGamma,382,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:400,Testability,log,logGamma,400,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:416,Testability,log,log,416,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:519,Testability,test,test,519,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:555,Testability,test,test,555,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:622,Testability,test,test,622,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:853,Testability,log,logBionomial,853,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:867,Testability,log,logGamma,867,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:877,Testability,log,log,877,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:1242,Testability,test,test,1242,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:1658,Testability,test,test,1658,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212:769,Usability,simpl,simply,769,"I had a look at the source code of [HypergeometricDistribution](HypergeometricDistribution). If I am right, we are doing the following. We are invoking `logProbability()` for all possible `x[0][0]`. For a table with large numbers, we have to compute logBinomial for many iterations (see line 202–222 in the HypergeometricDistribution source code). Typically logBinomial calls three logGamma and each logGamma calls `log()` twice. This involves lots of computation and is not the fastest way to implement Fisher's exact test. A faster way to implement the test takes the advantage of two observations. 1) When carrying the test, we are calling hypergeo(i,m+n,m,k), hypergeo(i+1,m+n,m,k), ... in order, and we can derive hypergeo(i+1,m+n,m,k) from hypergeo(i,m+n,m,k) by simply multiplying a number. This will be much faster than doing the full hypergeo->logBionomial->logGamma->log computation for each `i`. 2) For a large table, often when `i` is sufficiently smaller or larger than `x[0][0]`, the hypergeo probability is small enough to be ignored from the sum. It is not necessary to calculate hypergeo for the full range of `lo<=i<hi`. This trick can also dramatically reduce the number of iterations for large tables. htslib has a [exact test implementation](https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/kfunc.c#L201) that considers the two observations. I understand that the time spent on the `FisherExactTest` class probably won't show up at all in a profiler. I am not requesting to improve the implementation now. Just let you know the tricks. In addition, when we use this class for other purposes, a fast exact test may become a good thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266289212
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114:596,Energy Efficiency,reduce,reduce,596,"@davidbenjamin I made the changes you requested, plus some additional cleanup. Since this function takes a `normalizedTable` it only ever actually sees tables whose sums are less than 400. The smallest p-value we'd expect given that we can't have entries that are larger than 400 is around 1e-120. Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from `HypergeometricDistribution`. We also don't need `relErr`. . Also given this, I didn't make the changes @lh3 described, although this would clearly be a good way to reduce the computation needed for calculating the p-value with larger tables. . If you think it would be useful to keep these numerical stability features, I can add them back in, but removing them feels more readable to me given that we are only calculating small tables.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114:343,Testability,log,log,343,"@davidbenjamin I made the changes you requested, plus some additional cleanup. Since this function takes a `normalizedTable` it only ever actually sees tables whose sums are less than 400. The smallest p-value we'd expect given that we can't have entries that are larger than 400 is around 1e-120. Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from `HypergeometricDistribution`. We also don't need `relErr`. . Also given this, I didn't make the changes @lh3 described, although this would clearly be a good way to reduce the computation needed for calculating the p-value with larger tables. . If you think it would be useful to keep these numerical stability features, I can add them back in, but removing them feels more readable to me given that we are only calculating small tables.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114:571,Usability,clear,clearly,571,"@davidbenjamin I made the changes you requested, plus some additional cleanup. Since this function takes a `normalizedTable` it only ever actually sees tables whose sums are less than 400. The smallest p-value we'd expect given that we can't have entries that are larger than 400 is around 1e-120. Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from `HypergeometricDistribution`. We also don't need `relErr`. . Also given this, I didn't make the changes @lh3 described, although this would clearly be a good way to reduce the computation needed for calculating the p-value with larger tables. . If you think it would be useful to keep these numerical stability features, I can add them back in, but removing them feels more readable to me given that we are only calculating small tables.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266828114
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:164,Energy Efficiency,reduce,reduced,164,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:143,Performance,perform,performance,143,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486
https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:18,Usability,feedback,feedback,18,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486
https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027:28,Testability,test,test,28,It does in this very simple test I ran on dataproc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027
https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027:21,Usability,simpl,simple,21,It does in this very simple test I ran on dataproc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-266636027
https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267113820:189,Usability,clear,clear,189,"@droazen We can keep this on ice until after the tie outs are complete, it's not a problem. . @ronlevine This looks good to me but we'll hold off of merging until @droazen gives us the all-clear.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267113820
https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267118800:2835,Deployability,update,update,2835,"c=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (ø)` | :x: |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.903% <33.333%> (ø)` | `32 <0> (ø)` | :x: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2314?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2314?src=pr&el=footer). Last update [5d2f859...ed0b8ca](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267118800
https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267118800:2601,Usability,learn,learn,2601,"c=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (ø)` | :x: |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.903% <33.333%> (ø)` | `32 <0> (ø)` | :x: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2314?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2314?src=pr&el=footer). Last update [5d2f859...ed0b8ca](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...ed0b8cac3375023f23d5c0bd8a31ee155d707dae?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267118800
https://github.com/broadinstitute/gatk/issues/2315#issuecomment-278732586:15,Usability,feedback,feedback,15,Thanks for the feedback!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-278732586
https://github.com/broadinstitute/gatk/issues/2315#issuecomment-286425107:22,Usability,feedback,feedback,22,Great! Thanks for the feedback @magicDGS.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-286425107
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:393,Availability,error,error,393,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:468,Availability,error,errors,468,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:495,Availability,fault,fault,495,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:507,Availability,error,errors,507,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:538,Availability,fault,fault,538,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:761,Availability,error,errors,761,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:810,Availability,error,error,810,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:657,Energy Efficiency,efficient,efficiently,657,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:399,Integrability,message,message,399,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:747,Usability,clear,clearly,747,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:141,Availability,down,downstream,141,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:356,Availability,error,errors,356,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:387,Availability,fault,fault,387,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:572,Availability,error,error,572,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:796,Integrability,message,message,796,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854:438,Usability,usab,usable,438,"Unfortunately I don't think it's practical to try to enforce that ""only the arg parser can throw `CommandLineException`"" -- there's too much downstream code and too many tools that do so already, so it would be a bit painful to treat it as a bug. Instead I think what we should do is:. * Make sure that barclay uses a separate exception class for internal errors that are not the user's fault. This internal exception class should not be usable outside of barclay (perhaps we can make it package-private?).; * Catch `CommandLineException` in GATK and present it as a user error (output should say ""A USER ERROR HAS OCCURRED"").; * Move the `printDecoratedUserExceptionMessage()` call for caught `CommandLineExceptions` from `CommandLineProgram.parseArgs()` to `Main.mainEntry()`, to ensure that a message always gets printed for `CommandLineException`. As @cmnbroad said, this will have to wait until after the holiday break (most of us are going to be away until early January).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268828854
https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705:1280,Deployability,pipeline,pipelines,1280,el=desc) will **increase** coverage by `0.064%`.; > The diff coverage is `96%`. ```diff; @@ Coverage Diff @@; ## master #2327 +/- ##; ===============================================; + Coverage 76.136% 76.201% +0.064% ; - Complexity 10787 10810 +23 ; ===============================================; Files 748 750 +2 ; Lines 39378 39417 +39 ; Branches 6857 6858 +1 ; ===============================================; + Hits 29981 30036 +55 ; + Misses 6791 6775 -16 ; Partials 2606 2606; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2327?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../org/broadinstitute/hellbender/tools/FlagStat.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9GbGFnU3RhdC5qYXZh) | `76.404% <ø> (ø)` | `3 <ø> (ø)` | :x: |; | [...lbender/tools/spark/pipelines/CountBasesSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQ291bnRCYXNlc1NwYXJrLmphdmE=) | `90% <ø> (ø)` | `5 <ø> (ø)` | :x: |; | [...rg/broadinstitute/hellbender/tools/CountReads.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Db3VudFJlYWRzLmphdmE=) | `100% <ø> (ø)` | `3 <ø> (ø)` | :x: |; | [...s/metrics/CollectBaseDistributionByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9Db2xsZW,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705
https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705:5070,Deployability,update,update,5070,"b9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9MaWJyYXJ5UmVhZEZpbHRlci5qYXZh) | `100% <ø> (ø)` | `4 <ø> (ø)` | :x: |; | [...institute/hellbender/tools/picard/sam/SortSam.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9waWNhcmQvc2FtL1NvcnRTYW0uamF2YQ==) | `94.118% <ø> (ø)` | `3 <ø> (ø)` | :x: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `90.323% <ø> (ø)` | `12 <ø> (ø)` | :x: |; | [...org/broadinstitute/hellbender/tools/ClipReads.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9DbGlwUmVhZHMuamF2YQ==) | `90.385% <ø> (ø)` | `35 <ø> (ø)` | :x: |; | ... and [81 more](https://codecov.io/gh/broadinstitute/gatk/pull/2327/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2327?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2327?src=pr&el=footer). Last update [10b16a6...d4483e8](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705
https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705:4836,Usability,learn,learn,4836,"b9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9MaWJyYXJ5UmVhZEZpbHRlci5qYXZh) | `100% <ø> (ø)` | `4 <ø> (ø)` | :x: |; | [...institute/hellbender/tools/picard/sam/SortSam.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9waWNhcmQvc2FtL1NvcnRTYW0uamF2YQ==) | `94.118% <ø> (ø)` | `3 <ø> (ø)` | :x: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `90.323% <ø> (ø)` | `12 <ø> (ø)` | :x: |; | [...org/broadinstitute/hellbender/tools/ClipReads.java](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9DbGlwUmVhZHMuamF2YQ==) | `90.385% <ø> (ø)` | `35 <ø> (ø)` | :x: |; | ... and [81 more](https://codecov.io/gh/broadinstitute/gatk/pull/2327/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2327?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2327?src=pr&el=footer). Last update [10b16a6...d4483e8](https://codecov.io/gh/broadinstitute/gatk/compare/10b16a671dc2e153dbc92a16a72bdbf88eaa5ccd...d4483e8cf8d2e50e125c5340556b3eb49abb9636?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-268877705
https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:294,Energy Efficiency,reduce,reduce,294,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164
https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:243,Safety,avoid,avoid,243,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164
https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:440,Usability,simpl,simple,440,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164
https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039:184,Safety,avoid,avoid,184,"Thanks a lot for all your feedback about this @lbergelson and @droazen. From my side this could be close now, although it may be useful to have some of this information in the Wiki to avoid confusion. Thank you very much again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039
https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039:26,Usability,feedback,feedback,26,"Thanks a lot for all your feedback about this @lbergelson and @droazen. From my side this could be close now, although it may be useful to have some of this information in the Wiki to avoid confusion. Thank you very much again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039
https://github.com/broadinstitute/gatk/pull/2350#issuecomment-274847005:3159,Deployability,update,update,3159,"32...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `64.286% <85.714%> (-7.666%)` | `28 <12> (+1)` | |; | [...roadinstitute/hellbender/utils/SimpleInterval.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9TaW1wbGVJbnRlcnZhbC5qYXZh) | `94.048% <ø> (-1.19%)` | `46% <ø> (-1%)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <ø> (+1.429%)` | `24% <ø> (+1%)` | :white_check_mark: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <ø> (+3.333%)` | `10% <ø> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2350?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2350?src=pr&el=footer). Last update [fcd103c...a28ecfd](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-274847005
https://github.com/broadinstitute/gatk/pull/2350#issuecomment-274847005:2925,Usability,learn,learn,2925,"32...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `64.286% <85.714%> (-7.666%)` | `28 <12> (+1)` | |; | [...roadinstitute/hellbender/utils/SimpleInterval.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9TaW1wbGVJbnRlcnZhbC5qYXZh) | `94.048% <ø> (-1.19%)` | `46% <ø> (-1%)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <ø> (+1.429%)` | `24% <ø> (+1%)` | :white_check_mark: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <ø> (+3.333%)` | `10% <ø> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2350?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2350?src=pr&el=footer). Last update [fcd103c...a28ecfd](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...a28ecfd6f409451b9ecf1b8ca6da1e803462c50e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-274847005
https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274518246:15,Usability,simpl,simple,15,This is a very simple PR. Maybe @droazen or @lbergelson could have a quick look? Thank you very much.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274518246
https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:278,Modifiability,plugin,plugin,278,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983
https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:530,Modifiability,plugin,plugins,530,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983
https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:357,Security,expose,expose,357,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983
https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:128,Usability,simpl,simpler,128,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:61,Availability,error,error,61,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:238,Modifiability,flexible,flexible,238,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:484,Modifiability,plugin,plugin,484,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:540,Modifiability,extend,extending,540,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:674,Modifiability,plugin,plugin,674,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:322,Safety,avoid,avoid,322,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:78,Testability,test,tests,78,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:650,Usability,simpl,simple,650,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:44,Modifiability,plugin,plugin,44,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:51,Usability,usab,usable,51,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278340959:43,Usability,simpl,simpler,43,"I prefer to keep this one open, it will be simpler to rebase and include the changes. Should the `common = true` changes being included in the other or here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278340959
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306:4406,Deployability,update,update,4406,"haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `48.837% <0%> (-24.774%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | ... and [27 more](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=footer). Last update [5d2f859...7a651b7](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306:2484,Testability,test,test,2484,bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.6% <94.737%> (+0.116%)` | `50 <9> (+1)` | :arrow_up: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `48.837% <0%> (-24.774%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306
https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306:4172,Usability,learn,learn,4172,"haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `48.837% <0%> (-24.774%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | ... and [27 more](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=footer). Last update [5d2f859...7a651b7](https://codecov.io/gh/broadinstitute/gatk/pull/2355?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-287020306
https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276023533:139,Usability,simpl,simpler,139,"@cmnbroad I have a question regarding `getDefaultInstances()`. Why the return type is a `List<Object>` and not a `List<T>`. That will make simpler for me using it, if not maybe we require the changes in my PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276023533
https://github.com/broadinstitute/gatk/pull/2366#issuecomment-276527211:2012,Deployability,update,update,2012,"el=desc) will **decrease** coverage by `-<.001%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2366 +/- ##; ===============================================; - Coverage 76.201% 76.201% -<.001% ; - Complexity 10808 10812 +4 ; ===============================================; Files 750 750 ; Lines 39417 39421 +4 ; Branches 6858 6859 +1 ; ===============================================; + Hits 30036 30039 +3 ; Misses 6775 6775 ; - Partials 2606 2607 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <ø> (ø)` | :x: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `92.568% <75%> (-0.488%)` | `74 <2> (+3)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=footer). Last update [f45f6a5...75c14f4](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-276527211
https://github.com/broadinstitute/gatk/pull/2366#issuecomment-276527211:1778,Usability,learn,learn,1778,"el=desc) will **decrease** coverage by `-<.001%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2366 +/- ##; ===============================================; - Coverage 76.201% 76.201% -<.001% ; - Complexity 10808 10812 +4 ; ===============================================; Files 750 750 ; Lines 39417 39421 +4 ; Branches 6858 6859 +1 ; ===============================================; + Hits 30036 30039 +3 ; Misses 6775 6775 ; - Partials 2606 2607 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <ø> (ø)` | :x: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `92.568% <75%> (-0.488%)` | `74 <2> (+3)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2366?src=pr&el=footer). Last update [f45f6a5...75c14f4](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...75c14f4c17c957aa969a69a94c966fad3d5c8f1d?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-276527211
https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872:3153,Deployability,pipeline,pipelines,3153,36d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmsuamF2YQ==) | `66.667% <57.143%> (-11.905%)` | `4 <1> (+4)` | |; | [...ute/hellbender/utils/bwa/BwaMemAlignmentUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtQWxpZ25tZW50VXRpbHMuamF2YQ==) | `77.193% <77.193%> (ø)` | `16 <16> (?)` | |; | [...ute/hellbender/utils/bwa/BwaMemIndexSingleton.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtSW5kZXhTaW5nbGV0b24uamF2YQ==) | `82.353% <82.353%> (ø)` | `8 <8> (?)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `88% <86.667%> (-12%)` | `7 <2> (+7)` | |; | [...ute/hellbender/tools/spark/bwa/BwaSparkEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `89.286% <88%> (+18.697%)` | `5 <5> (+5)` | :white_check_mark: |; | [...itute/hellbender/tools/spark/sv/ContigAligner.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872
https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872:5165,Deployability,update,update,5165,"lbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `88% <86.667%> (-12%)` | `7 <2> (+7)` | |; | [...ute/hellbender/tools/spark/bwa/BwaSparkEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `89.286% <88%> (+18.697%)` | `5 <5> (+5)` | :white_check_mark: |; | [...itute/hellbender/tools/spark/sv/ContigAligner.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Db250aWdBbGlnbmVyLmphdmE=) | `88.462% <88.889%> (+6.643%)` | `8 <4> (+8)` | :white_check_mark: |; | [...g/broadinstitute/hellbender/utils/NativeUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXRpdmVVdGlscy5qYXZh) | `25% <ø> (-43.75%)` | `3% <ø> (+3%)` | |; | ... and [96 more](https://codecov.io/gh/broadinstitute/gatk/pull/2367/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2367?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2367?src=pr&el=footer). Last update [9d82097...975121e](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872
https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872:4931,Usability,learn,learn,4931,"lbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `88% <86.667%> (-12%)` | `7 <2> (+7)` | |; | [...ute/hellbender/tools/spark/bwa/BwaSparkEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `89.286% <88%> (+18.697%)` | `5 <5> (+5)` | :white_check_mark: |; | [...itute/hellbender/tools/spark/sv/ContigAligner.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Db250aWdBbGlnbmVyLmphdmE=) | `88.462% <88.889%> (+6.643%)` | `8 <4> (+8)` | :white_check_mark: |; | [...g/broadinstitute/hellbender/utils/NativeUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXRpdmVVdGlscy5qYXZh) | `25% <ø> (-43.75%)` | `3% <ø> (+3%)` | |; | ... and [96 more](https://codecov.io/gh/broadinstitute/gatk/pull/2367/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2367?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2367?src=pr&el=footer). Last update [9d82097...975121e](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...975121efa109472c00cd8aa9b03359647b71749b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-276460872
https://github.com/broadinstitute/gatk/pull/2376#issuecomment-276436132:4808,Deployability,update,update,4808,"EFkamFjZW5jeVJlZmVyZW5jZUxvY2F0aW9ucy5qYXZh) | `90.377% <85.714%> (ø)` | `55 <0> (ø)` | :x: |; | [...lbender/tools/spark/sv/SVVariantConsensusCall.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNDYWxsLmphdmE=) | `85.556% <89.474%> (-1.33%)` | `21 <0> (-14)` | |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `64.706% <90.741%> (+12.941%)` | `32 <31> (+19)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |; | ... and [1 more](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=footer). Last update [92cb860...3ac3c99](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2376#issuecomment-276436132
https://github.com/broadinstitute/gatk/pull/2376#issuecomment-276436132:4574,Usability,learn,learn,4574,"EFkamFjZW5jeVJlZmVyZW5jZUxvY2F0aW9ucy5qYXZh) | `90.377% <85.714%> (ø)` | `55 <0> (ø)` | :x: |; | [...lbender/tools/spark/sv/SVVariantConsensusCall.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNDYWxsLmphdmE=) | `85.556% <89.474%> (-1.33%)` | `21 <0> (-14)` | |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `64.706% <90.741%> (+12.941%)` | `32 <31> (+19)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |; | ... and [1 more](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2376?src=pr&el=footer). Last update [92cb860...3ac3c99](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...3ac3c99977729d83c38f42bd372cece0a16df996?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2376#issuecomment-276436132
https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318:95,Security,validat,validation,95,"@magicDGS Yes, I think it would be much simpler if we had one PR with all of the fixes for the validation rules (and related help issues). The extensibility changes we've been discussing should be a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318
https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318:40,Usability,simpl,simpler,40,"@magicDGS Yes, I think it would be much simpler if we had one PR with all of the fixes for the validation rules (and related help issues). The extensibility changes we've been discussing should be a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318
https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501:1508,Deployability,update,update,1508,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=h1) Report; > Merging [#2378](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9d82097641f160e00fa1ef4236d9bcdccbfa38b0?src=pr&el=desc) will **not impact** coverage. ```diff; @@ Coverage Diff @@; ## master #2378 +/- ##; =========================================; Coverage 76.378% 76.378% ; =========================================; Files 748 748 ; Lines 39315 39315 ; Branches 6847 6847 ; =========================================; Hits 30028 30028 ; Misses 6693 6693 ; Partials 2594 2594; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...roadinstitute/hellbender/utils/tsv/TableUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvVGFibGVVdGlscy5qYXZh) | `85% <ø> (ø)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=footer). Last update [9d82097...30b7f8d](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501
https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501:1274,Usability,learn,learn,1274,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=h1) Report; > Merging [#2378](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9d82097641f160e00fa1ef4236d9bcdccbfa38b0?src=pr&el=desc) will **not impact** coverage. ```diff; @@ Coverage Diff @@; ## master #2378 +/- ##; =========================================; Coverage 76.378% 76.378% ; =========================================; Files 748 748 ; Lines 39315 39315 ; Branches 6847 6847 ; =========================================; Hits 30028 30028 ; Misses 6693 6693 ; Partials 2594 2594; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...roadinstitute/hellbender/utils/tsv/TableUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvVGFibGVVdGlscy5qYXZh) | `85% <ø> (ø)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2378?src=pr&el=footer). Last update [9d82097...30b7f8d](https://codecov.io/gh/broadinstitute/gatk/compare/9d82097641f160e00fa1ef4236d9bcdccbfa38b0...30b7f8dc5646d760cd7d8b42513538b30f803d33?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2378#issuecomment-276484501
https://github.com/broadinstitute/gatk/pull/2379#issuecomment-276752112:2304,Deployability,update,update,2304,"+1 ; =============================================; + Hits 30028 30036 +8 ; + Misses 6693 6689 -4 ; + Partials 2594 2592 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...adinstitute/hellbender/tools/spark/sv/SVUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlV0aWxzLmphdmE=) | `32.184% <ø> (-0.757%)` | :x: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `76.389% <ø> (+2.083%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `82.707% <ø> (+3.759%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=footer). Last update [8a42977...20a2c01](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2379#issuecomment-276752112
https://github.com/broadinstitute/gatk/pull/2379#issuecomment-276752112:2070,Usability,learn,learn,2070,"+1 ; =============================================; + Hits 30028 30036 +8 ; + Misses 6693 6689 -4 ; + Partials 2594 2592 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...adinstitute/hellbender/tools/spark/sv/SVUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlV0aWxzLmphdmE=) | `32.184% <ø> (-0.757%)` | :x: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `76.389% <ø> (+2.083%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `82.707% <ø> (+3.759%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2379?src=pr&el=footer). Last update [8a42977...20a2c01](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...20a2c012125731780810c4f8a0075be745b2925a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2379#issuecomment-276752112
https://github.com/broadinstitute/gatk/pull/2381#issuecomment-276803321:3530,Deployability,update,update,3530,"ee#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mZXJtaS9GZXJtaUxpdGVBc3NlbWJsZXIuamF2YQ==) | `80.645% <80.645%> (ø)` | `8 <8> (?)` | |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `92.308% <ø> (+0.447%)` | `28% <ø> (+28%)` | :white_check_mark: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `87.097% <ø> (+0.986%)` | `59% <ø> (+59%)` | :white_check_mark: |; | [...adinstitute/hellbender/tools/spark/sv/SVUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlV0aWxzLmphdmE=) | `38.462% <ø> (+5.52%)` | `12% <ø> (+12%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2381?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2381?src=pr&el=footer). Last update [8a42977...d6fb1ba](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2381#issuecomment-276803321
https://github.com/broadinstitute/gatk/pull/2381#issuecomment-276803321:3296,Usability,learn,learn,3296,"ee#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mZXJtaS9GZXJtaUxpdGVBc3NlbWJsZXIuamF2YQ==) | `80.645% <80.645%> (ø)` | `8 <8> (?)` | |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `92.308% <ø> (+0.447%)` | `28% <ø> (+28%)` | :white_check_mark: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `87.097% <ø> (+0.986%)` | `59% <ø> (+59%)` | :white_check_mark: |; | [...adinstitute/hellbender/tools/spark/sv/SVUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlV0aWxzLmphdmE=) | `38.462% <ø> (+5.52%)` | `12% <ø> (+12%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2381?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2381?src=pr&el=footer). Last update [8a42977...d6fb1ba](https://codecov.io/gh/broadinstitute/gatk/compare/8a42977d248c4257e4fcbf2f69e21ab787ba3866...d6fb1ba347fbb8042e8473870fbffec02e211349?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2381#issuecomment-276803321
https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892:5133,Deployability,update,update,5133,"WRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <ø> (-62.264%)` | `8% <ø> (+8%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <ø> (-60.87%)` | `2% <ø> (+2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `44.444% <ø> (-29.861%)` | `28% <ø> (+28%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <ø> (-23.729%)` | `0% <ø> (ø)` | |; | ... and [15 more](https://codecov.io/gh/broadinstitute/gatk/pull/2385/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2385?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2385?src=pr&el=footer). Last update [14f73e2...b1802b2](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892
https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892:3578,Testability,test,test,3578,9f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvQXNzZW1ibHlSZWdpb25XYWxrZXJTcGFyay5qYXZh) | `0% <ø> (-90.566%)` | `0% <ø> (ø)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <ø> (-66.667%)` | `0% <ø> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <ø> (-62.264%)` | `8% <ø> (+8%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <ø> (-60.87%)` | `2% <ø> (+2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `44.444% <ø> (-29.861%)` | `28% <ø> (+28%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892
https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892:4899,Usability,learn,learn,4899,"WRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <ø> (-62.264%)` | `8% <ø> (+8%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <ø> (-60.87%)` | `2% <ø> (+2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `44.444% <ø> (-29.861%)` | `28% <ø> (+28%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <ø> (-23.729%)` | `0% <ø> (ø)` | |; | ... and [15 more](https://codecov.io/gh/broadinstitute/gatk/pull/2385/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2385?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2385?src=pr&el=footer). Last update [14f73e2...b1802b2](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...b1802b27996e3b0ee8a1b4a035a8ac78282b8666?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-279409892
https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231:1661,Deployability,update,update,1661,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=h1) Report; > Merging [#2387](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/14f73e217970a1c53092dee88c409f8a6cdb6e87?src=pr&el=desc) will **increase** coverage by `-0.002%`. ```diff; @@ Coverage Diff @@; ## master #2387 +/- ##; ===============================================; - Coverage 76.379% 76.377% -0.002% ; - Complexity 0 10849 +10849 ; ===============================================; Files 748 748 ; Lines 39325 39347 +22 ; Branches 6849 6851 +2 ; ===============================================; + Hits 30036 30052 +16 ; - Misses 6695 6703 +8 ; + Partials 2594 2592 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `78.065% <54.167%> (-0.883%)` | `20 <ø> (+20)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=footer). Last update [14f73e2...ce8d93c](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231
https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231:1427,Usability,learn,learn,1427,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=h1) Report; > Merging [#2387](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/14f73e217970a1c53092dee88c409f8a6cdb6e87?src=pr&el=desc) will **increase** coverage by `-0.002%`. ```diff; @@ Coverage Diff @@; ## master #2387 +/- ##; ===============================================; - Coverage 76.379% 76.377% -0.002% ; - Complexity 0 10849 +10849 ; ===============================================; Files 748 748 ; Lines 39325 39347 +22 ; Branches 6849 6851 +2 ; ===============================================; + Hits 30036 30052 +16 ; - Misses 6695 6703 +8 ; + Partials 2594 2592 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `78.065% <54.167%> (-0.883%)` | `20 <ø> (+20)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2387?src=pr&el=footer). Last update [14f73e2...ce8d93c](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ce8d93ca83ab90330776d2f46fea691af347349d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2387#issuecomment-277112231
https://github.com/broadinstitute/gatk/pull/2388#issuecomment-277262598:2019,Deployability,update,update,2019,") will **increase** coverage by `-0.005%`. ```diff; @@ Coverage Diff @@; ## master #2388 +/- ##; ===============================================; - Coverage 76.379% 76.374% -0.005% ; - Complexity 0 10845 +10845 ; ===============================================; Files 748 748 ; Lines 39325 39325 ; Branches 6849 6849 ; ===============================================; - Hits 30036 30034 -2 ; - Misses 6695 6697 +2 ; Partials 2594 2594; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.593% <ø> (-1.476%)` | `45 <ø> (+45)` | |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `91.667% <100%> (-0.194%)` | `24 <1> (+24)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=footer). Last update [14f73e2...ca6c34e](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388#issuecomment-277262598
https://github.com/broadinstitute/gatk/pull/2388#issuecomment-277262598:1785,Usability,learn,learn,1785,") will **increase** coverage by `-0.005%`. ```diff; @@ Coverage Diff @@; ## master #2388 +/- ##; ===============================================; - Coverage 76.379% 76.374% -0.005% ; - Complexity 0 10845 +10845 ; ===============================================; Files 748 748 ; Lines 39325 39325 ; Branches 6849 6849 ; ===============================================; - Hits 30036 30034 -2 ; - Misses 6695 6697 +2 ; Partials 2594 2594; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.593% <ø> (-1.476%)` | `45 <ø> (+45)` | |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `91.667% <100%> (-0.194%)` | `24 <1> (+24)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2388?src=pr&el=footer). Last update [14f73e2...ca6c34e](https://codecov.io/gh/broadinstitute/gatk/compare/14f73e217970a1c53092dee88c409f8a6cdb6e87...ca6c34e559073d30d05b624da48cfcbfd53f160a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388#issuecomment-277262598
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277253978:67,Usability,simpl,simple,67,"@lbergelson and I will review in person later today, and make easy/simple changes ourselves to save on back-and-forth -- if we have non-trivial questions or changes needed we'll send back to you for a second round.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277253978
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:23,Deployability,update,update,23,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:125,Integrability,rout,routine,125,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:187,Integrability,rout,routine,187,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:416,Integrability,rout,routine,416,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:57,Testability,test,tests,57,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:81,Testability,assert,assertVariantContextsAreEqual,81,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:762,Testability,test,tests,762,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277:842,Usability,feedback,feedback,842,"@kdatta Thanks for the update -- we do need to get these tests passing with our `assertVariantContextsAreEqual()` comparison routine, rather than your diffing tool, but we can relax this routine to be agnostic to allele ordering. About the `MIN_DP` issue: can you explain what was causing it? Why was it working with a `VCFCodec` and not a `BCF2Codec`? I still don't understand. Does it work now with our comparison routine and using a `BCF2Codec` internally, or does it still require a `VCFCodec` to pass? . And what was the ""ExcessHet problem"" (don't see a description of it above)? Was it just the ""no combination operation"" warning? What did the output look like before and after the fix for this annotation?. @cmnbroad has volunteered to have a look at the tests in this branch this afternoon, so we should be able to give you some more feedback soon!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293978277
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293989145:143,Usability,simpl,simply,143,"@kdatta Thanks for the clarification! One more question: when a combine operation is not defined for a particular attribute, is that attribute simply dropped by GenomicsDB? Ie., a client who reads VariantContexts from GenomicsDB will not see the attribute at all in the returned VariantContexts, even though it was present in the original GVCF inputs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293989145
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:373,Deployability,integrat,integration,373,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:373,Integrability,integrat,integration,373,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:292,Performance,load,load,292,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:526,Performance,load,load,526,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:670,Performance,load,load,670,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:1156,Performance,load,load,1156,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:93,Testability,test,tests,93,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:132,Testability,test,tests,132,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:323,Testability,test,tests,323,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:385,Testability,test,tests,385,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:416,Testability,test,tests,416,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:878,Testability,test,tests,878,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:916,Usability,simpl,simplest,916,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:21,Deployability,update,updated,21,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:48,Performance,cache,cached,48,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:38,Usability,clear,clear,38,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:23,Deployability,integrat,integration,23,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:23,Integrability,integrat,integration,23,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:77,Performance,cache,cache,77,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:35,Testability,test,tests,35,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:167,Testability,test,tests,167,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:64,Usability,clear,clearing,64,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829
https://github.com/broadinstitute/gatk/issues/2390#issuecomment-277346980:406,Usability,simpl,simplify,406,"@lbergelson @droazen Ah, SV has two arguments for samples: ""sample_name"" takes sample names, and ""sample_files"" takes filea containing samples, but it will take *any* files, regardless of extension. The whole thing is also true of ""exclude_samples"". We could eliminate the file-based arguments, but the result would only work with "".list"" files. If we can break backward command line compatibility, we can simplify SV, remove two arguments, and remove the SampleUtils and SampleUtilsUnitTest file-reading code. We just need to decide.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2390#issuecomment-277346980
https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:186,Testability,test,test,186,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501
https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:209,Testability,test,test,209,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501
https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:89,Usability,clear,clear,89,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501
https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278096683:3939,Deployability,update,update,3939,"cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZU1hbmFnZXIuamF2YQ==) | `86.592% <ø> (+1.025%)` | `78% <ø> (+34%)` | :white_check_mark: |; | [...tute/hellbender/engine/MultiVariantDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTXVsdGlWYXJpYW50RGF0YVNvdXJjZS5qYXZh) | `84.106% <ø> (+2.001%)` | `52% <ø> (+18%)` | :white_check_mark: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `72.105% <ø> (+2.54%)` | `4% <ø> (ø)` | :x: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `65.704% <ø> (+8.146%)` | `88% <ø> (+45%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2391?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2391?src=pr&el=footer). Last update [6f9de16...7247260](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278096683
https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278096683:3705,Usability,learn,learn,3705,"cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZU1hbmFnZXIuamF2YQ==) | `86.592% <ø> (+1.025%)` | `78% <ø> (+34%)` | :white_check_mark: |; | [...tute/hellbender/engine/MultiVariantDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTXVsdGlWYXJpYW50RGF0YVNvdXJjZS5qYXZh) | `84.106% <ø> (+2.001%)` | `52% <ø> (+18%)` | :white_check_mark: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `72.105% <ø> (+2.54%)` | `4% <ø> (ø)` | :x: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `65.704% <ø> (+8.146%)` | `88% <ø> (+45%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2391?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2391?src=pr&el=footer). Last update [6f9de16...7247260](https://codecov.io/gh/broadinstitute/gatk/compare/6f9de16d16eff4fe9d02dc9c6c9884d768c3cc43...7247260205cf7fa54b8785c87a919f1951151789?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278096683
https://github.com/broadinstitute/gatk/pull/2392#issuecomment-277394788:1999,Deployability,update,update,1999,"588a4?src=pr&el=desc) will **increase** coverage by `-0.015%`. ```diff; @@ Coverage Diff @@; ## master #2392 +/- ##; ===============================================; - Coverage 76.157% 76.141% -0.015% ; + Complexity 10823 10820 -3 ; ===============================================; Files 748 748 ; Lines 39361 39361 ; Branches 6855 6855 ; ===============================================; - Hits 29976 29970 -6 ; - Misses 6798 6801 +3 ; - Partials 2587 2590 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `74.839% <ø> (-3.226%)` | `18% <ø> (-2%)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <ø> (-1.429%)` | `23% <ø> (-1%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=footer). Last update [99e0b84...aeeaff8](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2392#issuecomment-277394788
https://github.com/broadinstitute/gatk/pull/2392#issuecomment-277394788:1765,Usability,learn,learn,1765,"588a4?src=pr&el=desc) will **increase** coverage by `-0.015%`. ```diff; @@ Coverage Diff @@; ## master #2392 +/- ##; ===============================================; - Coverage 76.157% 76.141% -0.015% ; + Complexity 10823 10820 -3 ; ===============================================; Files 748 748 ; Lines 39361 39361 ; Branches 6855 6855 ; ===============================================; - Hits 29976 29970 -6 ; - Misses 6798 6801 +3 ; - Partials 2587 2590 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `74.839% <ø> (-3.226%)` | `18% <ø> (-2%)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <ø> (-1.429%)` | `23% <ø> (-1%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2392?src=pr&el=footer). Last update [99e0b84...aeeaff8](https://codecov.io/gh/broadinstitute/gatk/compare/99e0b84c600d27cbe0a3016de0fe969f69b588a4...aeeaff8b188fb8b5f487a54ab615b0cbc3d0118d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2392#issuecomment-277394788
https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:61,Deployability,continuous,continuous,61,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090
https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:96,Integrability,wrap,wrapper,96,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090
https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:72,Testability,test,tests,72,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090
https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:161,Testability,test,tests,161,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090
https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090:25,Usability,usab,usable,25,"Merging this now to have usable VCF NIO support in master -- continuous tests to prove that the wrapper is applied will be added in a separate PR, but my ad-hoc tests on the latest version of this branch suggest it's working fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2393#issuecomment-277697090
https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747:2389,Deployability,update,update,2389,"29978 29989 +11 ; + Misses 6802 6791 -11 ; Partials 2592 2592; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <ø> (ø)` | `37 <ø> (ø)` | :x: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+1%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `79.747% <ø> (+6.329%)` | `22% <ø> (+4%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=footer). Last update [3c10554...efe544d](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747
https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747:905,Testability,test,test,905,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=h1) Report; > Merging [#2396](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/3c10554709a4f254300a3d38f24216c42da5913c?src=pr&el=desc) will **increase** coverage by `0.028%`. ```diff; @@ Coverage Diff @@; ## master #2396 +/- ##; ==============================================; + Coverage 76.14% 76.168% +0.028% ; - Complexity 10824 10829 +5 ; ==============================================; Files 748 748 ; Lines 39372 39372 ; Branches 6856 6856 ; ==============================================; + Hits 29978 29989 +11 ; + Misses 6802 6791 -11 ; Partials 2592 2592; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <ø> (ø)` | `37 <ø> (ø)` | :x: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+1%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `79.747% <ø> (+6.329%)` | `22% <ø> (+4%)` | :white_check_mark: |. --,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747
https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747:2155,Usability,learn,learn,2155,"29978 29989 +11 ; + Misses 6802 6791 -11 ; Partials 2592 2592; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <ø> (ø)` | `37 <ø> (ø)` | :x: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+1%)` | :white_check_mark: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `79.747% <ø> (+6.329%)` | `22% <ø> (+4%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2396?src=pr&el=footer). Last update [3c10554...efe544d](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...efe544dd515af5f5f25f6c73e8d54726fceca914?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2396#issuecomment-278174747
https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658:2019,Deployability,pipeline,pipelines,2019,472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mcmFnbWVudHMvRnJhZ21lbnRDb2xsZWN0aW9uLmphdmE=) | `57.143% <ø> (-4.762%)` | `9% <ø> (-4%)` | |; | [...lines/metrics/InsertSizeMetricsCollectorSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9JbnNlcnRTaXplTWV0cmljc0NvbGxlY3RvclNwYXJrLmphdmE=) | `86.364% <ø> (-4.545%)` | `7% <ø> (-1%)` | |; | [...oadinstitute/hellbender/engine/FeatureContext.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZUNvbnRleHQuamF2YQ==) | `72.973% <ø> (-2.703%)` | `26% <ø> (-1%)` | |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `90.625% <ø> (-1.042%)` | `10% <ø> (ø)` | |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `86.111% <ø> (-0.926%)` | `39% <ø> (-1%)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvY,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658
https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658:5124,Deployability,update,update,5124,"vYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `73.109% <ø> (-0.84%)` | `26% <ø> (ø)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <ø> (-0.694%)` | `36% <ø> (-1%)` | |; | [...org/broadinstitute/hellbender/utils/GenomeLoc.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2MuamF2YQ==) | `67.797% <ø> (-0.565%)` | `85% <ø> (-1%)` | |; | [...lbender/tools/walkers/vqsr/VariantDataManager.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVmFyaWFudERhdGFNYW5hZ2VyLmphdmE=) | `66.228% <ø> (-0.439%)` | `78% <ø> (-1%)` | |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2399/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2399?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2399?src=pr&el=footer). Last update [3c10554...9d80a51](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658
https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658:4890,Usability,learn,learn,4890,"vYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `73.109% <ø> (-0.84%)` | `26% <ø> (ø)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <ø> (-0.694%)` | `36% <ø> (-1%)` | |; | [...org/broadinstitute/hellbender/utils/GenomeLoc.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2MuamF2YQ==) | `67.797% <ø> (-0.565%)` | `85% <ø> (-1%)` | |; | [...lbender/tools/walkers/vqsr/VariantDataManager.java](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVmFyaWFudERhdGFNYW5hZ2VyLmphdmE=) | `66.228% <ø> (-0.439%)` | `78% <ø> (-1%)` | |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2399/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2399?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2399?src=pr&el=footer). Last update [3c10554...9d80a51](https://codecov.io/gh/broadinstitute/gatk/compare/3c10554709a4f254300a3d38f24216c42da5913c...9d80a51aa17f77bfca830472bdc4923b98151771?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278182658
https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649:3504,Deployability,pipeline,pipelines,3504,k/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...g/broadinstitute/hellbender/utils/NativeUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXRpdmVVdGlscy5qYXZh) | `25% <0%> (-43.75%)` | `3% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `76.471% <0%> (-23.529%)` | `4% <0%> (ø)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649
https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649:5149,Deployability,update,update,5149,"nVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `76.471% <0%> (-23.529%)` | `4% <0%> (ø)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | ... and [24 more](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=footer). Last update [51360c7...51285dc](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649
https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649:2463,Testability,test,test,2463,f0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.484% <86.538%> (-1.183%)` | `49 <26> (+4)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...g/broadinstitute/hellbender/utils/NativeUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXRpdmVVdGlscy5qYXZh) | `25% <0%> (-43.75%)` | `3% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649
https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649:4915,Usability,learn,learn,4915,"nVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `76.471% <0%> (-23.529%)` | `4% <0%> (ø)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | ... and [24 more](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2401?src=pr&el=footer). Last update [51360c7...51285dc](https://codecov.io/gh/broadinstitute/gatk/compare/51360c7357f47f1ce602e0a682aab3e37047440c...51285dcfa305e66b4af0c3e4a6c76376d6faeba9?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-279424649
https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175:1633,Deployability,update,update,1633,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=h1) Report; > Merging [#2403](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/30365e7bea2d081204a11e7d916026cb3494961f?src=pr&el=desc) will **increase** coverage by `0.003%`. ```diff; @@ Coverage Diff @@; ## master #2403 +/- ##; ===============================================; + Coverage 76.133% 76.135% +0.003% ; - Complexity 10785 10786 +1 ; ===============================================; Files 748 748 ; Lines 39372 39372 ; Branches 6856 6856 ; ===============================================; + Hits 29975 29976 +1 ; Misses 6791 6791 ; + Partials 2606 2605 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <ø> (+1.429%)` | `24% <ø> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=footer). Last update [30365e7...a51febd](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175
https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175:1399,Usability,learn,learn,1399,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=h1) Report; > Merging [#2403](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/30365e7bea2d081204a11e7d916026cb3494961f?src=pr&el=desc) will **increase** coverage by `0.003%`. ```diff; @@ Coverage Diff @@; ## master #2403 +/- ##; ===============================================; + Coverage 76.133% 76.135% +0.003% ; - Complexity 10785 10786 +1 ; ===============================================; Files 748 748 ; Lines 39372 39372 ; Branches 6856 6856 ; ===============================================; + Hits 29975 29976 +1 ; Misses 6791 6791 ; + Partials 2606 2605 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <ø> (+1.429%)` | `24% <ø> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2403?src=pr&el=footer). Last update [30365e7...a51febd](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...a51febdea00d0e15069996b5b8a492587d6d220b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2403#issuecomment-279082175
https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842:3944,Deployability,pipeline,pipelines,3944,ee#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `73.95% <ø> (+0.84%)` | `26% <ø> (ø)` | :x: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+2%)` | :white_check_mark: |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `87.037% <ø> (+0.926%)` | `40% <ø> (+1%)` | :white_check_mark: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `91.667% <ø> (+1.042%)` | `10% <ø> (ø)` | :x: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `74.194% <ø> (+1.075%)` | `26% <ø> (+1%)` | :white_check_mark: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2404/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=continue).; > **Leg,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842
https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842:5220,Deployability,update,update,5220,"> (+2%)` | :white_check_mark: |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `87.037% <ø> (+0.926%)` | `40% <ø> (+1%)` | :white_check_mark: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `91.667% <ø> (+1.042%)` | `10% <ø> (ø)` | :x: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `74.194% <ø> (+1.075%)` | `26% <ø> (+1%)` | :white_check_mark: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2404/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=footer). Last update [30365e7...09a6f24](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842
https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842:4362,Testability,test,test,4362,"4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `77.477% <ø> (+0.901%)` | `38% <ø> (+2%)` | :white_check_mark: |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `87.037% <ø> (+0.926%)` | `40% <ø> (+1%)` | :white_check_mark: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `91.667% <ø> (+1.042%)` | `10% <ø> (ø)` | :x: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `74.194% <ø> (+1.075%)` | `26% <ø> (+1%)` | :white_check_mark: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2404/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=footer). Last update [30365e7...09a6f24](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842
https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842:4986,Usability,learn,learn,4986,"> (+2%)` | :white_check_mark: |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `87.037% <ø> (+0.926%)` | `40% <ø> (+1%)` | :white_check_mark: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `91.667% <ø> (+1.042%)` | `10% <ø> (ø)` | :x: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `74.194% <ø> (+1.075%)` | `26% <ø> (+1%)` | :white_check_mark: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2404/changes?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2404?src=pr&el=footer). Last update [30365e7...09a6f24](https://codecov.io/gh/broadinstitute/gatk/compare/30365e7bea2d081204a11e7d916026cb3494961f...09a6f249352cd17f9214fccb5a4b3ac2c31f2cce?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279082842
https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441:1668,Deployability,update,update,1668,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=h1) Report; > Merging [#2407](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/f45f6a52d69fbf01541099cf737a0fc5391d584e?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2407 +/- ##; ===============================================; + Coverage 76.201% 76.206% +0.005% ; - Complexity 10808 10812 +4 ; ===============================================; Files 750 750 ; Lines 39417 39417 ; Branches 6858 6858 ; ===============================================; + Hits 30036 30038 +2 ; + Misses 6775 6773 -2 ; Partials 2606 2606; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <ø> (+1.587%)` | `61% <ø> (+2%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=footer). Last update [f45f6a5...9d14cf8](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441
https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441:1434,Usability,learn,learn,1434,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=h1) Report; > Merging [#2407](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/f45f6a52d69fbf01541099cf737a0fc5391d584e?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2407 +/- ##; ===============================================; + Coverage 76.201% 76.206% +0.005% ; - Complexity 10808 10812 +4 ; ===============================================; Files 750 750 ; Lines 39417 39417 ; Branches 6858 6858 ; ===============================================; + Hits 30036 30038 +2 ; + Misses 6775 6773 -2 ; Partials 2606 2606; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <ø> (+1.587%)` | `61% <ø> (+2%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2407?src=pr&el=footer). Last update [f45f6a5...9d14cf8](https://codecov.io/gh/broadinstitute/gatk/compare/f45f6a52d69fbf01541099cf737a0fc5391d584e...9d14cf8831c2f51e6ca75d560343f35411b15c5b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2407#issuecomment-279825441
https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503:232,Availability,error,error-reference,232,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@a49f0b3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2411 +/- ##; ==========================================; Coverage ? 76.206% ; Complexity ? 10814 ; ==========================================; Files ? 750 ; Lines ? 39421 ; Branches ? 6859 ; ==========================================; Hits ? 30041 ; Misses ? 6773 ; Partials ? 2607; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=footer). Last update [a49f0b3...00efddd](https://codecov.io/gh/broadinstitute/gatk/compare/a49f0b30b69eb3de3263cc976f976cd528721cc5...00efddd232b43006ad4f33e51d9387f507efe6ae?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503
https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503:1028,Deployability,update,update,1028,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@a49f0b3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2411 +/- ##; ==========================================; Coverage ? 76.206% ; Complexity ? 10814 ; ==========================================; Files ? 750 ; Lines ? 39421 ; Branches ? 6859 ; ==========================================; Hits ? 30041 ; Misses ? 6773 ; Partials ? 2607; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=footer). Last update [a49f0b3...00efddd](https://codecov.io/gh/broadinstitute/gatk/compare/a49f0b30b69eb3de3263cc976f976cd528721cc5...00efddd232b43006ad4f33e51d9387f507efe6ae?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503
https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503:180,Usability,learn,learn,180,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@a49f0b3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2411 +/- ##; ==========================================; Coverage ? 76.206% ; Complexity ? 10814 ; ==========================================; Files ? 750 ; Lines ? 39421 ; Branches ? 6859 ; ==========================================; Hits ? 30041 ; Misses ? 6773 ; Partials ? 2607; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=footer). Last update [a49f0b3...00efddd](https://codecov.io/gh/broadinstitute/gatk/compare/a49f0b30b69eb3de3263cc976f976cd528721cc5...00efddd232b43006ad4f33e51d9387f507efe6ae?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503
https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503:794,Usability,learn,learn,794,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@a49f0b3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2411 +/- ##; ==========================================; Coverage ? 76.206% ; Complexity ? 10814 ; ==========================================; Files ? 750 ; Lines ? 39421 ; Branches ? 6859 ; ==========================================; Hits ? 30041 ; Misses ? 6773 ; Partials ? 2607; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2411?src=pr&el=footer). Last update [a49f0b3...00efddd](https://codecov.io/gh/broadinstitute/gatk/compare/a49f0b30b69eb3de3263cc976f976cd528721cc5...00efddd232b43006ad4f33e51d9387f507efe6ae?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2411#issuecomment-280715503
https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:77,Availability,error,error,77,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286
https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:83,Integrability,message,message,83,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286
https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:309,Security,access,access,309,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286
https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:102,Usability,user-friendly,user-friendly,102,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286
https://github.com/broadinstitute/gatk/pull/2416#issuecomment-281483092:2059,Deployability,update,update,2059,"`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2416 +/- ##; ===============================================; - Coverage 76.224% 76.218% -0.006% ; + Complexity 10820 10819 -1 ; ===============================================; Files 750 750 ; Lines 39422 39420 -2 ; Branches 6883 6883 ; ===============================================; - Hits 30049 30045 -4 ; - Misses 6755 6757 +2 ; Partials 2618 2618; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...alkers/genotyper/afcalc/CustomAFPriorProvider.java](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ3VzdG9tQUZQcmlvclByb3ZpZGVyLmphdmE=) | `94.444% <ø> (-0.556%)` | `6 <ø> (-1)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <ø> (-3.333%)` | `10% <ø> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=footer). Last update [75f6331...3f2a04a](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2416#issuecomment-281483092
https://github.com/broadinstitute/gatk/pull/2416#issuecomment-281483092:1825,Usability,learn,learn,1825,"`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2416 +/- ##; ===============================================; - Coverage 76.224% 76.218% -0.006% ; + Complexity 10820 10819 -1 ; ===============================================; Files 750 750 ; Lines 39422 39420 -2 ; Branches 6883 6883 ; ===============================================; - Hits 30049 30045 -4 ; - Misses 6755 6757 +2 ; Partials 2618 2618; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...alkers/genotyper/afcalc/CustomAFPriorProvider.java](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ3VzdG9tQUZQcmlvclByb3ZpZGVyLmphdmE=) | `94.444% <ø> (-0.556%)` | `6 <ø> (-1)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <ø> (-3.333%)` | `10% <ø> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2416?src=pr&el=footer). Last update [75f6331...3f2a04a](https://codecov.io/gh/broadinstitute/gatk/compare/75f633135798145079ddb32c7dc2e884d47de4b3...3f2a04aa9723a86271120755e6be8945ff103532?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2416#issuecomment-281483092
https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264:5186,Deployability,update,update,5186," <0%> (+16%)` | :white_check_mark: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <0%> (+0.508%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `97.826% <0%> (+1.159%)` | `16% <0%> (+3%)` | :white_check_mark: |; | [...ute/hellbender/tools/spark/sv/AlignmentRegion.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbm1lbnRSZWdpb24uamF2YQ==) | `65.493% <0%> (+4.203%)` | `22% <0%> (+8%)` | :white_check_mark: |; | ... and [5 more](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=footer). Last update [fcd103c...475cd13](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264
https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264:4952,Usability,learn,learn,4952," <0%> (+16%)` | :white_check_mark: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <0%> (+0.508%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `97.826% <0%> (+1.159%)` | `16% <0%> (+3%)` | :white_check_mark: |; | [...ute/hellbender/tools/spark/sv/AlignmentRegion.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbm1lbnRSZWdpb24uamF2YQ==) | `65.493% <0%> (+4.203%)` | `22% <0%> (+8%)` | :white_check_mark: |; | ... and [5 more](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2417?src=pr&el=footer). Last update [fcd103c...475cd13](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...475cd13e0c19561a3569b7816f06ba5a52dabe77?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-281527264
https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285121959:35,Usability,feedback,feedback,35,"I already addressed the reviewer's feedback, so this should be assigned to @droazen or @lbergelson.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285121959
https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285175500:276,Usability,user-friendly,user-friendly,276,"Ah, ok. I've filed a new issue for the documentation of -DSTACK_TRACE_ON_USEREXCEPTION (#2445). . StorageException is different from UserException in that it doesn't have the user-relevant context. Like IOExceptions, we normally catch StorageException and transform them into user-friendly UserExceptions (as this PR does). Because of this lack of context, I don't think there's much ""special"" we can do about them (printing them out fully, as we do now, is the best I can think of). If you have an idea though I'm open to suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285175500
https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091:1543,Deployability,pipeline,pipelines,1543,================================; Files 769 769 ; Lines 40058 40137 +79 ; Branches 6979 6995 +16 ; ==============================================; + Hits 30438 30506 +68 ; - Misses 6981 6990 +9 ; - Partials 2639 2641 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../hellbender/tools/spark/BaseRecalibratorSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmsuamF2YQ==) | `86.957% <0%> (-8.282%)` | `9 <0> (+1)` | |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `74.766% <100%> (+1.657%)` | `23 <1> (-3)` | :arrow_down: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `93.103% <100%> (+1.103%)` | `8 <1> (+1)` | :arrow_up: |; | [...adinstitute/hellbender/utils/spark/SparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zcGFyay9TcGFya1V0aWxzLmphdmE=) | `71.154% <63.158%> (-4.604%)` | `9 <5> (+5)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `95.313% <0%> (+1.563%)` | `22% <0%> (+1%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091
https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091:3377,Deployability,update,update,3377,"dsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `93.103% <100%> (+1.103%)` | `8 <1> (+1)` | :arrow_up: |; | [...adinstitute/hellbender/utils/spark/SparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zcGFyay9TcGFya1V0aWxzLmphdmE=) | `71.154% <63.158%> (-4.604%)` | `9 <5> (+5)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `95.313% <0%> (+1.563%)` | `22% <0%> (+1%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.909% <0%> (+3.831%)` | `46% <0%> (+11%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=footer). Last update [2ecdef4...71a1b94](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091
https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091:3143,Usability,learn,learn,3143,"dsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `93.103% <100%> (+1.103%)` | `8 <1> (+1)` | :arrow_up: |; | [...adinstitute/hellbender/utils/spark/SparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zcGFyay9TcGFya1V0aWxzLmphdmE=) | `71.154% <63.158%> (-4.604%)` | `9 <5> (+5)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `95.313% <0%> (+1.563%)` | `22% <0%> (+1%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.909% <0%> (+3.831%)` | `46% <0%> (+11%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=footer). Last update [2ecdef4...71a1b94](https://codecov.io/gh/broadinstitute/gatk/pull/2419?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2419#issuecomment-293289091
https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687:3198,Deployability,update,update,3198,"a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.694% <0%> (+2.083%)` | `36% <0%> (ø)` | :x: |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `90.083% <0%> (+4.132%)` | `57% <0%> (+2%)` | :white_check_mark: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `90.476% <0%> (+4.762%)` | `8% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=footer). Last update [5211285...cab0d17](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687
https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687:2435,Testability,test,test,2435,"...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.694% <0%> (+2.083%)` | `36% <0%> (ø)` | :x: |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `90.083% <0%> (+4.132%)` | `57% <0%> (+2%)` | :white_check_mark: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `90.476% <0%> (+4.762%)` | `8% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=footer). Last update [5211285...cab0d17](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687
https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687:2964,Usability,learn,learn,2964,"a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.694% <0%> (+2.083%)` | `36% <0%> (ø)` | :x: |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `90.083% <0%> (+4.132%)` | `57% <0%> (+2%)` | :white_check_mark: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `90.476% <0%> (+4.762%)` | `8% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2423?src=pr&el=footer). Last update [5211285...cab0d17](https://codecov.io/gh/broadinstitute/gatk/compare/521128573b0d1a01ee60725c2b84e4a4f6a12fa3...cab0d179986f7f7587e0e005a7b8e54d20168a65?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-282342687
https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284076588:773,Deployability,update,updated,773,"Thanks JP. This is really Interesting. Unfortunately I think the vcf slice is the major motivating use case. How; large was that vcf? Do you think there's anything we can do to get some; speedup with NIO for small files when we only have 1 core? I'm not totally; clear on how data transfer over a network interacts with thread waiting.; If we are receiving data over the internet does that need cpu time or is; that handled asynchronously by the network card? I.e. if we're prefetching; in on thread, can that thread be asleep or is it consuming cpu time the; whole time a transfer is in progress?. I suspect that the immediate next question people are going to have is ""4; cores are inefficient, 1 core is slow, how about 2 cores..."". I'm curious about async and vcf. The updated slides show vcf with async on; being ~40% slower than with async off. That's; setting use_async_io_write_tribble on / off? It looks like we should just; disable it if we're on a single core, but by default we have it on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284076588
https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284076588:263,Usability,clear,clear,263,"Thanks JP. This is really Interesting. Unfortunately I think the vcf slice is the major motivating use case. How; large was that vcf? Do you think there's anything we can do to get some; speedup with NIO for small files when we only have 1 core? I'm not totally; clear on how data transfer over a network interacts with thread waiting.; If we are receiving data over the internet does that need cpu time or is; that handled asynchronously by the network card? I.e. if we're prefetching; in on thread, can that thread be asleep or is it consuming cpu time the; whole time a transfer is in progress?. I suspect that the immediate next question people are going to have is ""4; cores are inefficient, 1 core is slow, how about 2 cores..."". I'm curious about async and vcf. The updated slides show vcf with async on; being ~40% slower than with async off. That's; setting use_async_io_write_tribble on / off? It looks like we should just; disable it if we're on a single core, but by default we have it on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284076588
https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:335,Availability,down,download,335,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470
https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:177,Integrability,protocol,protocols,177,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470
https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:559,Security,access,access,559,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470
https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:695,Security,access,access,695,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470
https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:508,Usability,clear,cleared,508,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470
https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:184,Modifiability,variab,variable,184,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370
https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:376,Usability,clear,clear,376,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370
https://github.com/broadinstitute/gatk/pull/2427#issuecomment-282851101:2077,Deployability,update,update,2077,"rage is `100%`. ```diff; @@ Coverage Diff @@; ## master #2427 +/- ##; ===============================================; + Coverage 76.218% 76.221% +0.003% ; - Complexity 10819 10821 +2 ; ===============================================; Files 750 750 ; Lines 39420 39421 +1 ; Branches 6883 6883 ; ===============================================; + Hits 30045 30047 +2 ; Misses 6757 6757 ; + Partials 2618 2617 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `83.607% <100%> (+0.273%)` | `31 <4> (+1)` | :white_check_mark: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <0%> (+0.508%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=footer). Last update [fcd103c...fc95362](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2427#issuecomment-282851101
https://github.com/broadinstitute/gatk/pull/2427#issuecomment-282851101:1843,Usability,learn,learn,1843,"rage is `100%`. ```diff; @@ Coverage Diff @@; ## master #2427 +/- ##; ===============================================; + Coverage 76.218% 76.221% +0.003% ; - Complexity 10819 10821 +2 ; ===============================================; Files 750 750 ; Lines 39420 39421 +1 ; Branches 6883 6883 ; ===============================================; + Hits 30045 30047 +2 ; Misses 6757 6757 ; + Partials 2618 2617 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `83.607% <100%> (+0.273%)` | `31 <4> (+1)` | :white_check_mark: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <0%> (+0.508%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2427?src=pr&el=footer). Last update [fcd103c...fc95362](https://codecov.io/gh/broadinstitute/gatk/compare/fcd103c48afd0443512e1c490ea487278abe0332...fc95362d5a29cc5738032c43aa922b491b6accf5?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2427#issuecomment-282851101
https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250:232,Availability,error,error-reference,232,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@dc15e61`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2431 +/- ##; ==========================================; Coverage ? 42.757% ; Complexity ? 5801 ; ==========================================; Files ? 750 ; Lines ? 39425 ; Branches ? 6885 ; ==========================================; Hits ? 16857 ; Misses ? 20600 ; Partials ? 1968; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `86.667% <100%> (ø)` | `11 <0> (?)` | |; | [...dinstitute/hellbender/utils/report/GATKReport.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydC5qYXZh) | `40.196% <66.667%> (ø)` | `16 <0> (?)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=footer). Last update [dc15e61...0d5d1b1](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250
https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250:1912,Deployability,update,update,1912,"rn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2431 +/- ##; ==========================================; Coverage ? 42.757% ; Complexity ? 5801 ; ==========================================; Files ? 750 ; Lines ? 39425 ; Branches ? 6885 ; ==========================================; Hits ? 16857 ; Misses ? 20600 ; Partials ? 1968; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `86.667% <100%> (ø)` | `11 <0> (?)` | |; | [...dinstitute/hellbender/utils/report/GATKReport.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydC5qYXZh) | `40.196% <66.667%> (ø)` | `16 <0> (?)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=footer). Last update [dc15e61...0d5d1b1](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250
https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250:180,Usability,learn,learn,180,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@dc15e61`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2431 +/- ##; ==========================================; Coverage ? 42.757% ; Complexity ? 5801 ; ==========================================; Files ? 750 ; Lines ? 39425 ; Branches ? 6885 ; ==========================================; Hits ? 16857 ; Misses ? 20600 ; Partials ? 1968; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `86.667% <100%> (ø)` | `11 <0> (?)` | |; | [...dinstitute/hellbender/utils/report/GATKReport.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydC5qYXZh) | `40.196% <66.667%> (ø)` | `16 <0> (?)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=footer). Last update [dc15e61...0d5d1b1](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250
https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250:1678,Usability,learn,learn,1678,"rn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #2431 +/- ##; ==========================================; Coverage ? 42.757% ; Complexity ? 5801 ; ==========================================; Files ? 750 ; Lines ? 39425 ; Branches ? 6885 ; ==========================================; Hits ? 16857 ; Misses ? 20600 ; Partials ? 1968; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/recalibration/covariates/ReadGroupCovariate.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL2NvdmFyaWF0ZXMvUmVhZEdyb3VwQ292YXJpYXRlLmphdmE=) | `86.667% <100%> (ø)` | `11 <0> (?)` | |; | [...dinstitute/hellbender/utils/report/GATKReport.java](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZXBvcnQvR0FUS1JlcG9ydC5qYXZh) | `40.196% <66.667%> (ø)` | `16 <0> (?)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2431?src=pr&el=footer). Last update [dc15e61...0d5d1b1](https://codecov.io/gh/broadinstitute/gatk/compare/dc15e61a728ccd4e61b139332e48c05b57e4e88c...0d5d1b12d611725c80fa572e5ffba3bf8ea45ee4?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2431#issuecomment-283404250
https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:5140,Deployability,update,update,5140,"Rlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=footer). Last update [92cb860...6737d16](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034
https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:2083,Testability,test,test,2083,749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9NYXBwaW5nUXVhbGl0eVJlYWRGaWx0ZXIuamF2YQ==) | `100% <100%> (ø)` | `5 <5> (+2)` | :white_check_mark: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-31.944%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034
https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:3953,Testability,test,test,3953,f869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/do,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034
https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034:4906,Usability,learn,learn,4906,"Rlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2433?src=pr&el=footer). Last update [92cb860...6737d16](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...6737d16d1f0749554cafe9f8cf869fac1fcede0c?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2433#issuecomment-283613034
https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466:1645,Deployability,update,update,1645,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=h1) Report; > Merging [#2435](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/92cb86051b59acb6b18115135a5b5db99b617d22?src=pr&el=desc) will **decrease** coverage by `-0.008%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2435 +/- ##; ===============================================; - Coverage 76.231% 76.223% -0.008% ; Complexity 10822 10822 ; ===============================================; Files 750 750 ; Lines 39425 39425 ; Branches 6885 6885 ; ===============================================; - Hits 30054 30051 -3 ; - Misses 6754 6757 +3 ; Partials 2617 2617; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=footer). Last update [92cb860...f615b91](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466
https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466:1411,Usability,learn,learn,1411,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=h1) Report; > Merging [#2435](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/92cb86051b59acb6b18115135a5b5db99b617d22?src=pr&el=desc) will **decrease** coverage by `-0.008%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2435 +/- ##; ===============================================; - Coverage 76.231% 76.223% -0.008% ; Complexity 10822 10822 ; ===============================================; Files 750 750 ; Lines 39425 39425 ; Branches 6885 6885 ; ===============================================; - Hits 30054 30051 -3 ; - Misses 6754 6757 +3 ; Partials 2617 2617; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2435?src=pr&el=footer). Last update [92cb860...f615b91](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f615b91329aaa84fff4fb4c22660820e2ed0dcb0?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-284289466
https://github.com/broadinstitute/gatk/pull/2440#issuecomment-284888102:2401,Deployability,update,update,2401,"==========================; + Hits 30054 30059 +5 ; - Misses 6754 6759 +5 ; - Partials 2617 2618 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/tsv/TableReader.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvVGFibGVSZWFkZXIuamF2YQ==) | `75% <71.429%> (-1.543%)` | `35 <3> (+2)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=footer). Last update [92cb860...f53692e](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2440#issuecomment-284888102
https://github.com/broadinstitute/gatk/pull/2440#issuecomment-284888102:2167,Usability,learn,learn,2167,"==========================; + Hits 30054 30059 +5 ; - Misses 6754 6759 +5 ; - Partials 2617 2618 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/tsv/TableReader.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvVGFibGVSZWFkZXIuamF2YQ==) | `75% <71.429%> (-1.543%)` | `35 <3> (+2)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.611% <0%> (-2.083%)` | `36% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2440?src=pr&el=footer). Last update [92cb860...f53692e](https://codecov.io/gh/broadinstitute/gatk/compare/92cb86051b59acb6b18115135a5b5db99b617d22...f53692e637725d29f957ffdec11a96d8caeb74f5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2440#issuecomment-284888102
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:391,Availability,failure,failure,391,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:445,Modifiability,plugin,plugins,445,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:891,Modifiability,variab,variable,891,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:1033,Modifiability,variab,variables,1033,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:419,Security,access,access,419,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:534,Usability,user experience,user experience,534,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394
https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285180830:4305,Deployability,update,update,4305,"#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9XaW5kb3dTb3J0ZXIuamF2YQ==) | `100% <100%> (ø)` | `5 <5> (?)` | |; | [...bender/tools/spark/sv/AlignedAssemblyOrExcuse.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbmVkQXNzZW1ibHlPckV4Y3VzZS5qYXZh) | `11.299% <11.299%> (ø)` | `4 <4> (?)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <17.742%> (-18.009%)` | `28 <1> (ø)` | |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `81.463% <40%> (-2.293%)` | `24 <0> (ø)` | |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `82.278% <44.231%> (-6.409%)` | `22 <1> (ø)` | |; | ... and [24 more](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=footer). Last update [f91f7ac...553ba12](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285180830
https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285180830:4071,Usability,learn,learn,4071,"#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9XaW5kb3dTb3J0ZXIuamF2YQ==) | `100% <100%> (ø)` | `5 <5> (?)` | |; | [...bender/tools/spark/sv/AlignedAssemblyOrExcuse.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbmVkQXNzZW1ibHlPckV4Y3VzZS5qYXZh) | `11.299% <11.299%> (ø)` | `4 <4> (?)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <17.742%> (-18.009%)` | `28 <1> (ø)` | |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `81.463% <40%> (-2.293%)` | `24 <0> (ø)` | |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `82.278% <44.231%> (-6.409%)` | `22 <1> (ø)` | |; | ... and [24 more](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=footer). Last update [f91f7ac...553ba12](https://codecov.io/gh/broadinstitute/gatk/pull/2444?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285180830
https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285411154:414,Usability,clear,clear,414,"On the general comment of this class being too big: I totally agree.; I haven't figured out how to make Java in this semi-functional style pretty. I could certainly pull a mess of nested classes out into top-level classes just to make the file smaller. But most of them are so specific to their use in this program that they wouldn't really be useful outside this context. I'll probably do just that, but it's not clear to me that it makes the program more comprehensible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2444#issuecomment-285411154
https://github.com/broadinstitute/gatk/pull/2447#issuecomment-285197333:4374,Deployability,update,update,4374,"RpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `8% <0%> (+5%)` | |; | [...stitute/hellbender/utils/collections/CountSet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2xsZWN0aW9ucy9Db3VudFNldC5qYXZh) | `31.21% <0%> (-0.403%)` | `22% <0%> (ø)` | |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.198% <0%> (-0.393%)` | `25% <0%> (+3%)` | |; | [...gine/spark/AddContextDataToReadSparkOptimized.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvQWRkQ29udGV4dERhdGFUb1JlYWRTcGFya09wdGltaXplZC5qYXZh) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/sv/GATKSVVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9HQVRLU1ZWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | ... and [92 more](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=footer). Last update [e7c90f1...08af964](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2447#issuecomment-285197333
https://github.com/broadinstitute/gatk/pull/2447#issuecomment-285197333:4140,Usability,learn,learn,4140,"RpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `8% <0%> (+5%)` | |; | [...stitute/hellbender/utils/collections/CountSet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2xsZWN0aW9ucy9Db3VudFNldC5qYXZh) | `31.21% <0%> (-0.403%)` | `22% <0%> (ø)` | |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.198% <0%> (-0.393%)` | `25% <0%> (+3%)` | |; | [...gine/spark/AddContextDataToReadSparkOptimized.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvQWRkQ29udGV4dERhdGFUb1JlYWRTcGFya09wdGltaXplZC5qYXZh) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/sv/GATKSVVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9HQVRLU1ZWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | ... and [92 more](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=footer). Last update [e7c90f1...08af964](https://codecov.io/gh/broadinstitute/gatk/pull/2447?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2447#issuecomment-285197333
https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809:2047,Deployability,update,update,2047," by `-0.002%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2448 +/- ##; ===============================================; - Coverage 76.238% 76.236% -0.002% ; + Complexity 10859 10854 -5 ; ===============================================; Files 751 750 -1 ; Lines 39559 39551 -8 ; Branches 6912 6911 -1 ; ===============================================; - Hits 30159 30152 -7 ; Misses 6780 6780 ; + Partials 2620 2619 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <0%> (-3.333%)` | `10% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=footer). Last update [e7c90f1...23ba83e](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809
https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809:1813,Usability,learn,learn,1813," by `-0.002%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2448 +/- ##; ===============================================; - Coverage 76.238% 76.236% -0.002% ; + Complexity 10859 10854 -5 ; ===============================================; Files 751 750 -1 ; Lines 39559 39551 -8 ; Branches 6912 6911 -1 ; ===============================================; - Hits 30159 30152 -7 ; Misses 6780 6780 ; + Partials 2620 2619 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <0%> (-3.333%)` | `10% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2448?src=pr&el=footer). Last update [e7c90f1...23ba83e](https://codecov.io/gh/broadinstitute/gatk/compare/e7c90f1da2ac17173e56d352fbc4d926f9d2b871...23ba83e98b5b49ad0285a6366e79ad37e70efd0b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2448#issuecomment-285370809
https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285652447:2420,Deployability,update,update,2420,"s 30169 30168 -1 ; + Misses 6771 6768 -3 ; Partials 2620 2620; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <100%> (ø)` | `2 <0> (ø)` | :x: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `84.211% <100%> (-0.164%)` | `53 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `66.316% <33.333%> (+2.03%)` | `28 <4> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=footer). Last update [987e2f9...05211ec](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285652447
https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285652447:2186,Usability,learn,learn,2186,"s 30169 30168 -1 ; + Misses 6771 6768 -3 ; Partials 2620 2620; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <100%> (ø)` | `2 <0> (ø)` | :x: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `84.211% <100%> (-0.164%)` | `53 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `66.316% <33.333%> (+2.03%)` | `28 <4> (ø)` | :x: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2450?src=pr&el=footer). Last update [987e2f9...05211ec](https://codecov.io/gh/broadinstitute/gatk/compare/987e2f98c4f9a97d74488bf37bc902ee25274c83...05211ec9114cfb2886fe17c56fa991603241f50d?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285652447
https://github.com/broadinstitute/gatk/pull/2452#issuecomment-285786533:4510,Deployability,update,update,4510,".267% <0%> (-1.635%)` | `36% <0%> (+4%)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `2% <0%> (ø)` | :arrow_down: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `86.911% <0%> (+0.244%)` | `83% <0%> (+38%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.385% <0%> (+1.774%)` | `49% <0%> (+13%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=footer). Last update [dfa9cf1...5a67eb6](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2452#issuecomment-285786533
https://github.com/broadinstitute/gatk/pull/2452#issuecomment-285786533:4276,Usability,learn,learn,4276,".267% <0%> (-1.635%)` | `36% <0%> (+4%)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `2% <0%> (ø)` | :arrow_down: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `86.911% <0%> (+0.244%)` | `83% <0%> (+38%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.385% <0%> (+1.774%)` | `49% <0%> (+13%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2452?src=pr&el=footer). Last update [dfa9cf1...5a67eb6](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...5a67eb67b78c6fe2a8ccab54b4b257099fa1b3a5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2452#issuecomment-285786533
https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600:2467,Deployability,update,update,2467," Partials 2620 2618 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0 <0> (-5)` | |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `66.917% <85.714%> (+2.211%)` | `38 <6> (+6)` | :white_check_mark: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=footer). Last update [5d2f859...9b319ac](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600
https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600:2233,Usability,learn,learn,2233," Partials 2620 2618 -2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ark/sv/CallVariantsFromAlignedContigsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DYWxsVmFyaWFudHNGcm9tQWxpZ25lZENvbnRpZ3NTQU1TcGFyay5qYXZh) | `0% <0%> (-26.087%)` | `0 <0> (-5)` | |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `66.917% <85.714%> (+2.211%)` | `38 <6> (+6)` | :white_check_mark: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2453?src=pr&el=footer). Last update [5d2f859...9b319ac](https://codecov.io/gh/broadinstitute/gatk/compare/5d2f859db87f60a0f5b5f0ed7f73e39ebae09bec...9b319acdbc3eb6e5d6b26bffcd1ad2b53f40bc3a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2453#issuecomment-285796600
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467:82,Deployability,upgrade,upgrade,82,"Can you have a look to this one, @cmnbroad? It is just a simple change for let me upgrade my dependencies and do not include the NPE in not bounded arguments...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467:93,Integrability,depend,dependencies,93,"Can you have a look to this one, @cmnbroad? It is just a simple change for let me upgrade my dependencies and do not include the NPE in not bounded arguments...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467:57,Usability,simpl,simple,57,"Can you have a look to this one, @cmnbroad? It is just a simple change for let me upgrade my dependencies and do not include the NPE in not bounded arguments...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285858467
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:5105,Deployability,update,update,5105,"Rlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=footer). Last update [dfa9cf1...f539662](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:2048,Testability,test,test,2048,917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `93.258% <100%> (ø)` | `29 <1> (ø)` | :x: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:3918,Testability,test,test,3918,4e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/do,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315
https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315:4871,Usability,learn,learn,4871,"Rlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `70.946% <0%> (-6.757%)` | `18% <0%> (-4%)` | |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2455?src=pr&el=footer). Last update [dfa9cf1...f539662](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...f539662b2a136507a34ea2da64e0445d6df3469d?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455#issuecomment-285859315
https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823:1666,Deployability,update,update,1666,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=h1) Report; > Merging [#2456](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/dfa9cf1a420490285b7be7917082222a07e2b042?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2456 +/- ##; ===============================================; + Coverage 76.254% 76.256% +0.003% ; - Complexity 10861 10862 +1 ; ===============================================; Files 750 750 ; Lines 39556 39556 ; Branches 6914 6914 ; ===============================================; + Hits 30163 30164 +1 ; Misses 6775 6775 ; + Partials 2618 2617 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=footer). Last update [dfa9cf1...988bc45](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823
https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823:1432,Usability,learn,learn,1432,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=h1) Report; > Merging [#2456](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/dfa9cf1a420490285b7be7917082222a07e2b042?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2456 +/- ##; ===============================================; + Coverage 76.254% 76.256% +0.003% ; - Complexity 10861 10862 +1 ; ===============================================; Files 750 750 ; Lines 39556 39556 ; Branches 6914 6914 ; ===============================================; + Hits 30163 30164 +1 ; Misses 6775 6775 ; + Partials 2618 2617 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :white_check_mark: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2456?src=pr&el=footer). Last update [dfa9cf1...988bc45](https://codecov.io/gh/broadinstitute/gatk/compare/dfa9cf1a420490285b7be7917082222a07e2b042...988bc45a8ccfe0ae3884f0c8401015ce053f45bb?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-285972823
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2696,Availability,avail,available,2696,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:1344,Energy Efficiency,reduce,reduceByKey,1344,"en.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it loo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:1466,Energy Efficiency,reduce,reduceByKey,1466,"en.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it loo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:109,Performance,perform,performance,109,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2301,Performance,perform,performing,2301,"Pair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,Performance,optimiz,optimizing,2454,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2580,Performance,cache,cached,2580,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2599,Safety,avoid,avoid,2599,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:383,Testability,test,test-data,383,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:456,Testability,test,test-data,456,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:530,Testability,test,test-data,530,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:622,Testability,test,test-data,622,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:3162,Testability,log,logging,3162,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:3230,Testability,log,logging,3230,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:1684,Usability,simpl,simple,1684,"kRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficie",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:1742,Usability,simpl,simple,1742,"rs 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884
https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:5080,Deployability,update,update,5080,"VsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=footer). Last update [e1e71d7...71c03a3](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894
https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:2408,Testability,test,test,2408,92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZVdhbGtlci5qYXZh) | `89.655% <100%> (+6.897%)` | `9 <2> (+1)` | :arrow_up: |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `43.75% <0%> (-29.861%)` | `27% <0%> (-9%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGlu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894
https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:4278,Testability,test,test,4278,"1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=footer). Last update [e1e71d7...71c03a3](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=footer&el=lastupda",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894
https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894:4846,Usability,learn,learn,4846,"VsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-18.75%)` | `6% <0%> (ø)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `10.169% <0%> (-13.559%)` | `1% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `77.6% <0%> (-9.6%)` | `28% <0%> (-8%)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2467?src=pr&el=footer). Last update [e1e71d7...71c03a3](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...71c03a3e81f2df635e709823e1c1de96a2f5ffb5?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2467#issuecomment-287565894
https://github.com/broadinstitute/gatk/pull/2468#issuecomment-288779580:2043,Deployability,update,update,2043,"rage by `0.008%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2468 +/- ##; ===============================================; + Coverage 76.268% 76.275% +0.008% ; - Complexity 10876 10879 +3 ; ===============================================; Files 752 752 ; Lines 39583 39583 ; Branches 6922 6922 ; ===============================================; + Hits 30189 30192 +3 ; + Misses 6774 6772 -2 ; + Partials 2620 2619 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=footer). Last update [c62914a...4bebcdf](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2468#issuecomment-288779580
https://github.com/broadinstitute/gatk/pull/2468#issuecomment-288779580:1809,Usability,learn,learn,1809,"rage by `0.008%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2468 +/- ##; ===============================================; + Coverage 76.268% 76.275% +0.008% ; - Complexity 10876 10879 +3 ; ===============================================; Files 752 752 ; Lines 39583 39583 ; Branches 6922 6922 ; ===============================================; + Hits 30189 30192 +3 ; + Misses 6774 6772 -2 ; + Partials 2620 2619 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2468?src=pr&el=footer). Last update [c62914a...4bebcdf](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...4bebcdf005e9191206558f09e14f59d87324f1c8?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2468#issuecomment-288779580
https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716:4834,Deployability,update,update,4834,"FBhcnNlci5qYXZh) | `67.476% <0%> (+0.558%)` | `66% <0%> (+28%)` | :arrow_up: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `86.911% <0%> (+1.427%)` | `74% <0%> (+25%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `95.714% <0%> (+2.456%)` | `48% <0%> (+19%)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `87.156% <0%> (+2.945%)` | `59% <0%> (+6%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=footer). Last update [e1e71d7...8e22a8a](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716
https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716:1703,Testability,test,test,1703,======; + Hits 30174 30401 +227 ; - Misses 6767 6830 +63 ; - Partials 2619 2629 +10; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `68.217% <ø> (-5.394%)` | `33 <0> (-3)` | |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `45.267% <0%> (-1.635%)` | `36% <0%> (+4%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (-0.8%)` | `35% <0%> (-1%)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `2% <0%> (ø)` | :arrow_down: |; | [...ute/hellbender/cmdline/StrictBooleanConverter.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbG,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716
https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716:4600,Usability,learn,learn,4600,"FBhcnNlci5qYXZh) | `67.476% <0%> (+0.558%)` | `66% <0%> (+28%)` | :arrow_up: |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `86.911% <0%> (+1.427%)` | `74% <0%> (+25%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `95.714% <0%> (+2.456%)` | `48% <0%> (+19%)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `87.156% <0%> (+2.945%)` | `59% <0%> (+6%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2488?src=pr&el=footer). Last update [e1e71d7...8e22a8a](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...8e22a8a5969d2efc6f49ac272e53e893eb5eb048?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287907716
https://github.com/broadinstitute/gatk/issues/2490#issuecomment-291179390:32,Usability,simpl,simple,32,Whew -- glad this was something simple!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-291179390
https://github.com/broadinstitute/gatk/pull/2491#issuecomment-287865370:2055,Deployability,update,update,2055,"03%`.; > The diff coverage is `63.636%`. ```diff; @@ Coverage Diff @@; ## master #2491 +/- ##; ===============================================; + Coverage 76.274% 76.277% +0.003% ; Complexity 10867 10867 ; ===============================================; Files 750 750 ; Lines 39560 39560 ; Branches 6915 6916 +1 ; ===============================================; + Hits 30174 30175 +1 ; + Misses 6767 6765 -2 ; - Partials 2619 2620 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `77.778% <63.636%> (-2.778%)` | `13 <0> (ø)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=footer). Last update [e1e71d7...76fde41](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2491#issuecomment-287865370
https://github.com/broadinstitute/gatk/pull/2491#issuecomment-287865370:1821,Usability,learn,learn,1821,"03%`.; > The diff coverage is `63.636%`. ```diff; @@ Coverage Diff @@; ## master #2491 +/- ##; ===============================================; + Coverage 76.274% 76.277% +0.003% ; Complexity 10867 10867 ; ===============================================; Files 750 750 ; Lines 39560 39560 ; Branches 6915 6916 +1 ; ===============================================; + Hits 30174 30175 +1 ; + Misses 6767 6765 -2 ; - Partials 2619 2620 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `77.778% <63.636%> (-2.778%)` | `13 <0> (ø)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2491?src=pr&el=footer). Last update [e1e71d7...76fde41](https://codecov.io/gh/broadinstitute/gatk/compare/e1e71d7091ee703e547842d025e92ac698407ff0...76fde41e8aa0dab8fcdd10c875f7b62d9faddd21?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2491#issuecomment-287865370
https://github.com/broadinstitute/gatk/pull/2494#issuecomment-287889612:4362,Deployability,update,update,4362,"RpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `82.857% <59.091%> (-6.234%)` | `6 <3> (+1)` | |; | [...institute/hellbender/tools/spark/bwa/BwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmsuamF2YQ==) | `66.667% <60%> (ø)` | `5 <1> (+1)` | :arrow_up: |; | [...e/hellbender/engine/filters/ReadFilterLibrary.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9SZWFkRmlsdGVyTGlicmFyeS5qYXZh) | `94.048% <66.667%> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `98.413% <0%> (-1.587%)` | `34% <0%> (+20%)` | |; | [...ls/walkers/genotyper/afcalc/ExactAFCalculator.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `9% <0%> (+6%)` | |; | ... and [18 more](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=footer). Last update [88c181d...6a33314](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2494#issuecomment-287889612
https://github.com/broadinstitute/gatk/pull/2494#issuecomment-287889612:4128,Usability,learn,learn,4128,"RpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `82.857% <59.091%> (-6.234%)` | `6 <3> (+1)` | |; | [...institute/hellbender/tools/spark/bwa/BwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmsuamF2YQ==) | `66.667% <60%> (ø)` | `5 <1> (+1)` | :arrow_up: |; | [...e/hellbender/engine/filters/ReadFilterLibrary.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9SZWFkRmlsdGVyTGlicmFyeS5qYXZh) | `94.048% <66.667%> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `98.413% <0%> (-1.587%)` | `34% <0%> (+20%)` | |; | [...ls/walkers/genotyper/afcalc/ExactAFCalculator.java](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `9% <0%> (+6%)` | |; | ... and [18 more](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=footer). Last update [88c181d...6a33314](https://codecov.io/gh/broadinstitute/gatk/pull/2494?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2494#issuecomment-287889612
https://github.com/broadinstitute/gatk/pull/2495#issuecomment-287912625:5269,Deployability,update,update,5269,"hbmRMaW5lUHJvZ3JhbS5qYXZh) | `95.714% <0%> (+2.456%)` | `38% <0%> (+9%)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `87.156% <0%> (+2.945%)` | `66% <0%> (+13%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `70.47% <0%> (+4.154%)` | `46% <0%> (+18%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=footer). Last update [88c181d...298212c](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2495#issuecomment-287912625
https://github.com/broadinstitute/gatk/pull/2495#issuecomment-287912625:5035,Usability,learn,learn,5035,"hbmRMaW5lUHJvZ3JhbS5qYXZh) | `95.714% <0%> (+2.456%)` | `38% <0%> (+9%)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `87.156% <0%> (+2.945%)` | `66% <0%> (+13%)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `70.47% <0%> (+4.154%)` | `46% <0%> (+18%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2495?src=pr&el=footer). Last update [88c181d...298212c](https://codecov.io/gh/broadinstitute/gatk/compare/88c181df462379fed902c8ae35b0ca142e2bd88d...298212c811d06554a7ad7e97e200172feef6496c?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2495#issuecomment-287912625
https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597:2778,Deployability,update,update,2778,"bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <0%> (+0.8%)` | `36% <0%> (+1%)` | :arrow_up: |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+6.401%)` | `14% <0%> (+1%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.385% <0%> (+7.168%)` | `49% <0%> (+16%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=footer). Last update [58cb99e...2a7f196](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597
https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597:1301,Testability,test,test,1301,will **increase** coverage by `0.03%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2500 +/- ##; ==============================================; + Coverage 76.256% 76.287% +0.03% ; - Complexity 10864 10881 +17 ; ==============================================; Files 750 750 ; Lines 39543 39619 +76 ; Branches 6914 6935 +21 ; ==============================================; + Hits 30154 30224 +70 ; - Misses 6772 6774 +2 ; - Partials 2617 2621 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <0%> (+0.8%)` | `36% <0%> (+1%)` | :arrow_up: |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+6.401%)` | `14% <0%> (+1%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlsc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597
https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597:2544,Usability,learn,learn,2544,"bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `87.2% <0%> (+0.8%)` | `36% <0%> (+1%)` | :arrow_up: |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+6.401%)` | `14% <0%> (+1%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `75.385% <0%> (+7.168%)` | `49% <0%> (+16%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2500?src=pr&el=footer). Last update [58cb99e...2a7f196](https://codecov.io/gh/broadinstitute/gatk/compare/58cb99ec6c81917a9ac8ecf52e8fde2bd763850b...2a7f1965dff4d460667e64ead68b52d462b125b6?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2500#issuecomment-288139597
https://github.com/broadinstitute/gatk/issues/2502#issuecomment-288285382:6,Usability,simpl,simply,6,"Nope, simply an ignorance of that part of the VCF spec, and the fact that `INSERTED_SEQUENCE_MAPPINGS` is using `,` for separating fields of a single mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2502#issuecomment-288285382
https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288240771:4359,Deployability,update,update,4359,"S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `8% <0%> (+5%)` | |; | [...stitute/hellbender/utils/collections/CountSet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2xsZWN0aW9ucy9Db3VudFNldC5qYXZh) | `31.21% <0%> (-0.403%)` | `22% <0%> (ø)` | |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.198% <0%> (-0.393%)` | `25% <0%> (+3%)` | |; | [...roadinstitute/hellbender/utils/GenotypeCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZUNvdW50cy5qYXZh) | `100% <0%> (ø)` | `7% <0%> (+3%)` | :arrow_up: |; | [...ols/walkers/genotyper/MinimalGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9NaW5pbWFsR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `27.273% <0%> (ø)` | `4% <0%> (+1%)` | :arrow_up: |; | ... and [58 more](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=footer). Last update [724fbd0...a163be6](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288240771
https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288240771:4125,Usability,learn,learn,4125,"S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `89.474% <0%> (-0.526%)` | `8% <0%> (+5%)` | |; | [...stitute/hellbender/utils/collections/CountSet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2xsZWN0aW9ucy9Db3VudFNldC5qYXZh) | `31.21% <0%> (-0.403%)` | `22% <0%> (ø)` | |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.198% <0%> (-0.393%)` | `25% <0%> (+3%)` | |; | [...roadinstitute/hellbender/utils/GenotypeCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZUNvdW50cy5qYXZh) | `100% <0%> (ø)` | `7% <0%> (+3%)` | :arrow_up: |; | [...ols/walkers/genotyper/MinimalGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9NaW5pbWFsR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `27.273% <0%> (ø)` | `4% <0%> (+1%)` | :arrow_up: |; | ... and [58 more](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=footer). Last update [724fbd0...a163be6](https://codecov.io/gh/broadinstitute/gatk/pull/2506?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288240771
https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288256519:5159,Deployability,update,update,5159,"S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQmFzZVF1YWxpdHlDbGlwUmVhZFRyYW5zZm9ybWVyLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-14%)` | |; | [...llbender/tools/walkers/annotator/TandemRepeat.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9UYW5kZW1SZXBlYXQuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/tools/spark/sv/SvType.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdlR5cGUuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-5%)` | |; | [...tute/hellbender/metrics/SAMRecordAndReference.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9tZXRyaWNzL1NBTVJlY29yZEFuZFJlZmVyZW5jZS5qYXZh) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | ... and [430 more](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=footer). Last update [724fbd0...6b3c7a9](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288256519
https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288256519:4925,Usability,learn,learn,4925,"S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQmFzZVF1YWxpdHlDbGlwUmVhZFRyYW5zZm9ybWVyLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-14%)` | |; | [...llbender/tools/walkers/annotator/TandemRepeat.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9UYW5kZW1SZXBlYXQuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-5%)` | |; | [...oadinstitute/hellbender/tools/spark/sv/SvType.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdlR5cGUuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-5%)` | |; | [...tute/hellbender/metrics/SAMRecordAndReference.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9tZXRyaWNzL1NBTVJlY29yZEFuZFJlZmVyZW5jZS5qYXZh) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | ... and [430 more](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2510?src=pr&el=footer). Last update [724fbd0...6b3c7a9](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...6b3c7a9b6d6dfb45fc64613bccf1a74e85a374fe?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288256519
https://github.com/broadinstitute/gatk/pull/2511#issuecomment-288267091:2034,Deployability,update,update,2034,"se** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2511 +/- ##; ===============================================; - Coverage 76.259% 76.256% -0.003% ; + Complexity 10865 10864 -1 ; ===============================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===============================================; - Hits 30155 30154 -1 ; Misses 6771 6771 ; - Partials 2617 2618 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/walkers/variantutils/VariantsToTable.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYXJpYW50c1RvVGFibGUuamF2YQ==) | `94.083% <ø> (ø)` | `73 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=footer). Last update [724fbd0...1a7a561](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2511#issuecomment-288267091
https://github.com/broadinstitute/gatk/pull/2511#issuecomment-288267091:1800,Usability,learn,learn,1800,"se** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2511 +/- ##; ===============================================; - Coverage 76.259% 76.256% -0.003% ; + Complexity 10865 10864 -1 ; ===============================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===============================================; - Hits 30155 30154 -1 ; Misses 6771 6771 ; - Partials 2617 2618 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/walkers/variantutils/VariantsToTable.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYXJpYW50c1RvVGFibGUuamF2YQ==) | `94.083% <ø> (ø)` | `73 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2511?src=pr&el=footer). Last update [724fbd0...1a7a561](https://codecov.io/gh/broadinstitute/gatk/compare/724fbd08b213454c996815d4ab22ff1ab517921c...1a7a561a7da5603a607f04cd982c62fb8491403b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2511#issuecomment-288267091
https://github.com/broadinstitute/gatk/pull/2512#issuecomment-288291739:2816,Deployability,update,update,2816,"c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9HQVRLU1ZWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/tools/spark/sv/AlignmentRegion.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbm1lbnRSZWdpb24uamF2YQ==) | `63.265% <100%> (+1.16%)` | `16 <2> (ø)` | :arrow_down: |; | [...lbender/tools/spark/sv/SVVariantConsensusCall.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNDYWxsLmphdmE=) | `85.556% <80%> (ø)` | `21 <4> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2512?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2512?src=pr&el=footer). Last update [9c1d1fb...f1380fe](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2512#issuecomment-288291739
https://github.com/broadinstitute/gatk/pull/2512#issuecomment-288291739:2582,Usability,learn,learn,2582,"c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9HQVRLU1ZWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/tools/spark/sv/AlignmentRegion.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbm1lbnRSZWdpb24uamF2YQ==) | `63.265% <100%> (+1.16%)` | `16 <2> (ø)` | :arrow_down: |; | [...lbender/tools/spark/sv/SVVariantConsensusCall.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNDYWxsLmphdmE=) | `85.556% <80%> (ø)` | `21 <4> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2512?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2512?src=pr&el=footer). Last update [9c1d1fb...f1380fe](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...f1380fe7f813931a2eb402867b07fb1c0b0f318c?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2512#issuecomment-288291739
https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418:1104,Deployability,update,update,1104,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=h1) Report; > Merging [#2513](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9c1d1fb2cc1aeb171e01764ee69c1544698e796d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2513 +/- ##; ===========================================; Coverage 76.256% 76.256% ; Complexity 10864 10864 ; ===========================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===========================================; Hits 30154 30154 ; Misses 6771 6771 ; Partials 2618 2618; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=footer). Last update [9c1d1fb...7fc08f1](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...7fc08f1c4ac1def9789665bd56448220d7ba774a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418
https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418:870,Usability,learn,learn,870,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=h1) Report; > Merging [#2513](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/9c1d1fb2cc1aeb171e01764ee69c1544698e796d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2513 +/- ##; ===========================================; Coverage 76.256% 76.256% ; Complexity 10864 10864 ; ===========================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===========================================; Hits 30154 30154 ; Misses 6771 6771 ; Partials 2618 2618; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2513?src=pr&el=footer). Last update [9c1d1fb...7fc08f1](https://codecov.io/gh/broadinstitute/gatk/compare/9c1d1fb2cc1aeb171e01764ee69c1544698e796d...7fc08f1c4ac1def9789665bd56448220d7ba774a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2513#issuecomment-288454418
https://github.com/broadinstitute/gatk/pull/2515#issuecomment-288529799:2161,Deployability,update,update,2161,"iff coverage is `66.667%`. ```diff; @@ Coverage Diff @@; ## master #2515 +/- ##; ===============================================; - Coverage 76.273% 76.268% -0.004% ; - Complexity 10876 10878 +2 ; ===============================================; Files 752 752 ; Lines 39583 39584 +1 ; Branches 6922 6922 ; ===============================================; - Hits 30191 30190 -1 ; - Misses 6772 6774 +2 ; Partials 2620 2620; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `82.813% <66.667%> (-1.314%)` | `30 <0> (-1)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <0%> (-3.333%)` | `10% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=footer). Last update [d40ccc2...d5c85bb](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515#issuecomment-288529799
https://github.com/broadinstitute/gatk/pull/2515#issuecomment-288529799:1927,Usability,learn,learn,1927,"iff coverage is `66.667%`. ```diff; @@ Coverage Diff @@; ## master #2515 +/- ##; ===============================================; - Coverage 76.273% 76.268% -0.004% ; - Complexity 10876 10878 +2 ; ===============================================; Files 752 752 ; Lines 39583 39584 +1 ; Branches 6922 6922 ; ===============================================; - Hits 30191 30190 -1 ; - Misses 6772 6774 +2 ; Partials 2620 2620; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `82.813% <66.667%> (-1.314%)` | `30 <0> (-1)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `66.667% <0%> (-3.333%)` | `10% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=footer). Last update [d40ccc2...d5c85bb](https://codecov.io/gh/broadinstitute/gatk/pull/2515?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515#issuecomment-288529799
https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729:2439,Deployability,update,update,2439," +26 ; Misses 6771 6771 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `66.667% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/FeatureWalker.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZVdhbGtlci5qYXZh) | `86.957% <0%> (-2.699%)` | `9% <0%> (ø)` | |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+9.179%)` | `14% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=footer). Last update [91b41d8...f741a03](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729
https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729:2205,Usability,learn,learn,2205," +26 ; Misses 6771 6771 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `66.667% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/FeatureWalker.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZVdhbGtlci5qYXZh) | `86.957% <0%> (-2.699%)` | `9% <0%> (ø)` | |; | [...ellbender/tools/walkers/annotator/RankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYW5rU3VtVGVzdC5qYXZh) | `86.957% <0%> (+9.179%)` | `14% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2518?src=pr&el=footer). Last update [91b41d8...f741a03](https://codecov.io/gh/broadinstitute/gatk/compare/91b41d8011a1465c637b7548899ff1e7f58f4e40...f741a033e28fe707ade9c9f0ea3fe9a20ecd78fd?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2518#issuecomment-288545729
https://github.com/broadinstitute/gatk/pull/2521#issuecomment-288757656:2076,Deployability,update,update,2076,"erage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2521 +/- ##; ===============================================; + Coverage 76.256% 76.261% +0.005% ; Complexity 10864 10864 ; ===============================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===============================================; + Hits 30154 30156 +2 ; + Misses 6771 6769 -2 ; Partials 2618 2618; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=footer). Last update [7ad3c91...2622598](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2521#issuecomment-288757656
https://github.com/broadinstitute/gatk/pull/2521#issuecomment-288757656:1842,Usability,learn,learn,1842,"erage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2521 +/- ##; ===============================================; + Coverage 76.256% 76.261% +0.005% ; Complexity 10864 10864 ; ===============================================; Files 750 750 ; Lines 39543 39543 ; Branches 6915 6915 ; ===============================================; + Hits 30154 30156 +2 ; + Misses 6771 6769 -2 ; Partials 2618 2618; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2521?src=pr&el=footer). Last update [7ad3c91...2622598](https://codecov.io/gh/broadinstitute/gatk/compare/7ad3c91b96448c4a867451b40b7ce6ae41cef690...2622598137d07ee362c7d98cb67e89862df0276e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2521#issuecomment-288757656
https://github.com/broadinstitute/gatk/issues/2522#issuecomment-288813318:75,Usability,simpl,simpler,75,@droazen I can do it. It will be gratifying to complete a ticket made much simpler by the removal of the old `AFCalculator`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2522#issuecomment-288813318
https://github.com/broadinstitute/gatk/pull/2524#issuecomment-288804022:2417,Deployability,update,update,2417,"=====; + Hits 30163 30192 +29 ; Misses 6772 6772 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (-1.102%)` | `4 <0> (ø)` | |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `98% <96.552%> (-2%)` | `23 <9> (+9)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=footer). Last update [78f4f61...fc03f04](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2524#issuecomment-288804022
https://github.com/broadinstitute/gatk/pull/2524#issuecomment-288804022:2183,Usability,learn,learn,2183,"=====; + Hits 30163 30192 +29 ; Misses 6772 6772 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (-1.102%)` | `4 <0> (ø)` | |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `98% <96.552%> (-2%)` | `23 <9> (+9)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2524?src=pr&el=footer). Last update [78f4f61...fc03f04](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...fc03f04a1347287d8121f018019fc422322b9f2c?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2524#issuecomment-288804022
https://github.com/broadinstitute/gatk/pull/2526#issuecomment-288832482:2381,Deployability,update,update,2381,"==============================; + Hits 30163 30164 +1 ; + Misses 6772 6770 -2 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/BwaMemIndexImageCreator.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Cd2FNZW1JbmRleEltYWdlQ3JlYXRvci5qYXZh) | `71.429% <ø> (ø)` | `2 <0> (?)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=footer). Last update [78f4f61...c122c34](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2526#issuecomment-288832482
https://github.com/broadinstitute/gatk/pull/2526#issuecomment-288832482:2147,Usability,learn,learn,2147,"==============================; + Hits 30163 30164 +1 ; + Misses 6772 6770 -2 ; - Partials 2618 2619 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/BwaMemIndexImageCreator.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Cd2FNZW1JbmRleEltYWdlQ3JlYXRvci5qYXZh) | `71.429% <ø> (ø)` | `2 <0> (?)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `90% <0%> (-1.429%)` | `23% <0%> (-1%)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2526?src=pr&el=footer). Last update [78f4f61...c122c34](https://codecov.io/gh/broadinstitute/gatk/compare/78f4f61435bef501879e9a4cccb9a978fd484a6b...c122c345d26c17d97c759c81b4fcf825dfaecb9e?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2526#issuecomment-288832482
https://github.com/broadinstitute/gatk/pull/2527#issuecomment-288844391:2499,Deployability,update,update,2499,"====================; + Hits 30189 30196 +7 ; - Misses 6774 6802 +28 ; - Partials 2621 2626 +5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `58.479% <0%> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `82.813% <100%> (ø)` | `30 <0> (ø)` | :arrow_down: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <100%> (ø)` | `24 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `77.778% <38.462%> (-10.91%)` | `25 <5> (+3)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=footer). Last update [47d8c52...f2df0f7](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2527#issuecomment-288844391
https://github.com/broadinstitute/gatk/pull/2527#issuecomment-288844391:2265,Usability,learn,learn,2265,"====================; + Hits 30189 30196 +7 ; - Misses 6774 6802 +28 ; - Partials 2621 2626 +5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `58.479% <0%> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...tute/hellbender/tools/spark/sv/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkQ2xhc3NpZmllci5qYXZh) | `82.813% <100%> (ø)` | `30 <0> (ø)` | :arrow_down: |; | [.../hellbender/tools/spark/sv/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `83.756% <100%> (ø)` | `24 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `77.778% <38.462%> (-10.91%)` | `25 <5> (+3)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=footer). Last update [47d8c52...f2df0f7](https://codecov.io/gh/broadinstitute/gatk/pull/2527?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2527#issuecomment-288844391
https://github.com/broadinstitute/gatk/pull/2528#issuecomment-288859643:4067,Deployability,update,update,4067,"l=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `80% <100%> (ø)` | `119 <0> (ø)` | :arrow_down: |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.491% <20%> (-0.411%)` | `32 <0> (ø)` | |; | [...ls/walkers/genotyper/afcalc/ExactAFCalculator.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `81.818% <50%> (-8.182%)` | `6 <3> (+3)` | |; | [...stitute/hellbender/utils/genotyper/AlleleList.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvQWxsZWxlTGlzdC5qYXZh) | `89.744% <0%> (+1.282%)` | `16% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2528?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2528?src=pr&el=footer). Last update [c62914a...cc1b2b9](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2528#issuecomment-288859643
https://github.com/broadinstitute/gatk/pull/2528#issuecomment-288859643:3833,Usability,learn,learn,3833,"l=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `80% <100%> (ø)` | `119 <0> (ø)` | :arrow_down: |; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.491% <20%> (-0.411%)` | `32 <0> (ø)` | |; | [...ls/walkers/genotyper/afcalc/ExactAFCalculator.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvRXhhY3RBRkNhbGN1bGF0b3IuamF2YQ==) | `81.818% <50%> (-8.182%)` | `6 <3> (+3)` | |; | [...stitute/hellbender/utils/genotyper/AlleleList.java](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvQWxsZWxlTGlzdC5qYXZh) | `89.744% <0%> (+1.282%)` | `16% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2528?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2528?src=pr&el=footer). Last update [c62914a...cc1b2b9](https://codecov.io/gh/broadinstitute/gatk/compare/c62914a72df31653f801072d9d5b63ef44ecc248...cc1b2b9e9989a18f36d2014445eadce21bef3373?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2528#issuecomment-288859643
https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454:2217,Deployability,update,update,2217," master #2529 +/- ##; ===============================================; + Coverage 76.266% 76.277% +0.011% ; - Complexity 10877 10879 +2 ; ===============================================; Files 752 752 ; Lines 39584 39586 +2 ; Branches 6922 6923 +1 ; ===============================================; + Hits 30189 30195 +6 ; + Misses 6774 6771 -3 ; + Partials 2621 2620 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/annotator/QualByDepth.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9RdWFsQnlEZXB0aC5qYXZh) | `94.595% <100%> (+0.15%)` | `17 <0> (+1)` | :arrow_up: |; | [...nder/tools/walkers/annotator/DepthPerSampleHC.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9EZXB0aFBlclNhbXBsZUhDLmphdmE=) | `73.913% <100%> (+10.277%)` | `8 <0> (+1)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=footer). Last update [47d8c52...d16a01a](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454
https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454:1983,Usability,learn,learn,1983," master #2529 +/- ##; ===============================================; + Coverage 76.266% 76.277% +0.011% ; - Complexity 10877 10879 +2 ; ===============================================; Files 752 752 ; Lines 39584 39586 +2 ; Branches 6922 6923 +1 ; ===============================================; + Hits 30189 30195 +6 ; + Misses 6774 6771 -3 ; + Partials 2621 2620 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/annotator/QualByDepth.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9RdWFsQnlEZXB0aC5qYXZh) | `94.595% <100%> (+0.15%)` | `17 <0> (+1)` | :arrow_up: |; | [...nder/tools/walkers/annotator/DepthPerSampleHC.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9EZXB0aFBlclNhbXBsZUhDLmphdmE=) | `73.913% <100%> (+10.277%)` | `8 <0> (+1)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=footer). Last update [47d8c52...d16a01a](https://codecov.io/gh/broadinstitute/gatk/pull/2529?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2529#issuecomment-289058454
https://github.com/broadinstitute/gatk/pull/2531#issuecomment-289150335:2236,Deployability,update,update,2236,"#; ===============================================; + Coverage 76.262% 76.279% +0.018% ; - Complexity 10880 10891 +11 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30192 30199 +7 ; + Misses 6776 6768 -8 ; - Partials 2622 2623 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `49.123% <100%> (+2.632%)` | `41 <0> (+9)` | :arrow_up: |; | [...ols/walkers/genotyper/MinimalGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9NaW5pbWFsR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `27.273% <0%> (ø)` | `4% <0%> (+1%)` | :arrow_up: |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `78.175% <0%> (+0.179%)` | `176% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=footer). Last update [a85e0ff...985628d](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2531#issuecomment-289150335
https://github.com/broadinstitute/gatk/pull/2531#issuecomment-289150335:2002,Usability,learn,learn,2002,"#; ===============================================; + Coverage 76.262% 76.279% +0.018% ; - Complexity 10880 10891 +11 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30192 30199 +7 ; + Misses 6776 6768 -8 ; - Partials 2622 2623 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `49.123% <100%> (+2.632%)` | `41 <0> (+9)` | :arrow_up: |; | [...ols/walkers/genotyper/MinimalGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9NaW5pbWFsR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `27.273% <0%> (ø)` | `4% <0%> (+1%)` | :arrow_up: |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `78.175% <0%> (+0.179%)` | `176% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=footer). Last update [a85e0ff...985628d](https://codecov.io/gh/broadinstitute/gatk/pull/2531?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2531#issuecomment-289150335
https://github.com/broadinstitute/gatk/pull/2534#issuecomment-290677995:220,Deployability,release,release,220,"Thanks for your feedback, @droazen. I think that it will be nice to have a better annotator engine for handling what should be on/off in which cases instead of hardcoded them when it is necessary. But I can wait til the release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-290677995
https://github.com/broadinstitute/gatk/pull/2534#issuecomment-290677995:16,Usability,feedback,feedback,16,"Thanks for your feedback, @droazen. I think that it will be nice to have a better annotator engine for handling what should be on/off in which cases instead of hardcoded them when it is necessary. But I can wait til the release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-290677995
https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:4065,Deployability,update,update,4065,"iff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | ... and [7 more](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=footer). Last update [5ccfd00...b1d407f](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549
https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:957,Testability,test,test,957,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=h1) Report; > Merging [#2540](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/5ccfd0097c39b44464692fab1566d850f73aa5c7?src=pr&el=desc) will **increase** coverage by `0.523%`.; > The diff coverage is `82.075%`. ```diff; @@ Coverage Diff @@; ## master #2540 +/- ##; ===============================================; + Coverage 75.865% 76.388% +0.523% ; - Complexity 10839 10918 +79 ; ===============================================; Files 754 755 +1 ; Lines 39552 39653 +101 ; Branches 6907 6925 +18 ; ===============================================; + Hits 30006 30290 +284 ; + Misses 6936 6741 -195 ; - Partials 2610 2622 +12; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `89.474% <100%> (+1.974%)` | `7 <3> (+2)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `71.318% <100%> (+22.481%)` | `34 <1> (+7)` | :arrow_up: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `80.612% <80.612%> (ø)` | `19 <19> (?)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGV,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549
https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:3334,Testability,test,test,3334,"iff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | ... and [7 more](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=footer). Last update [5ccfd00...b1d407f](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549
https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549:3831,Usability,learn,learn,3831,"iff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | ... and [7 more](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=footer). Last update [5ccfd00...b1d407f](https://codecov.io/gh/broadinstitute/gatk/pull/2540?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-290122549
https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890:4428,Deployability,update,update,4428,"lbmRlci91dGlscy9waWxldXAvUGlsZXVwRWxlbWVudC5qYXZh) | `96.04% <0%> (+1.865%)` | `76 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/HashedListTargetCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9IYXNoZWRMaXN0VGFyZ2V0Q29sbGVjdGlvbi5qYXZh) | `90.741% <0%> (+1.65%)` | `43 <0> (ø)` | :arrow_down: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (+2.033%)` | `13 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `89.404% <0%> (+0.588%)` | `68 <0> (ø)` | :arrow_down: |; | [...der/utils/locusiterator/AlignmentStateMachine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9sb2N1c2l0ZXJhdG9yL0FsaWdubWVudFN0YXRlTWFjaGluZS5qYXZh) | `87.879% <0%> (+1.312%)` | `27 <1> (ø)` | :arrow_down: |; | ... and [22 more](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=footer). Last update [62d58c5...cd59cde](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890
https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890:4194,Usability,learn,learn,4194,"lbmRlci91dGlscy9waWxldXAvUGlsZXVwRWxlbWVudC5qYXZh) | `96.04% <0%> (+1.865%)` | `76 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/HashedListTargetCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9IYXNoZWRMaXN0VGFyZ2V0Q29sbGVjdGlvbi5qYXZh) | `90.741% <0%> (+1.65%)` | `43 <0> (ø)` | :arrow_down: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (+2.033%)` | `13 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `89.404% <0%> (+0.588%)` | `68 <0> (ø)` | :arrow_down: |; | [...der/utils/locusiterator/AlignmentStateMachine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9sb2N1c2l0ZXJhdG9yL0FsaWdubWVudFN0YXRlTWFjaGluZS5qYXZh) | `87.879% <0%> (+1.312%)` | `27 <1> (ø)` | :arrow_down: |; | ... and [22 more](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=footer). Last update [62d58c5...cd59cde](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890
https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909:1104,Deployability,update,update,1104,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=h1) Report; > Merging [#2544](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/8b4122cfb8268dcd86cca6bd8d6b3b4b6e1ed5a6?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2544 +/- ##; ===========================================; Coverage 76.282% 76.282% ; Complexity 10892 10892 ; ===========================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===========================================; Hits 30200 30200 ; Misses 6768 6768 ; Partials 2622 2622; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=footer). Last update [8b4122c...df921e4](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909
https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909:870,Usability,learn,learn,870,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=h1) Report; > Merging [#2544](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/8b4122cfb8268dcd86cca6bd8d6b3b4b6e1ed5a6?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2544 +/- ##; ===========================================; Coverage 76.282% 76.282% ; Complexity 10892 10892 ; ===========================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===========================================; Hits 30200 30200 ; Misses 6768 6768 ; Partials 2622 2622; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=footer). Last update [8b4122c...df921e4](https://codecov.io/gh/broadinstitute/gatk/pull/2544?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2544#issuecomment-290236909
https://github.com/broadinstitute/gatk/pull/2546#issuecomment-290509295:2769,Deployability,update,update,2769,"InbreedingCoeff.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9JbmJyZWVkaW5nQ29lZmYuamF2YQ==) | `82.759% <100%> (ø)` | `11 <1> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/GenotypeCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZUNvdW50cy5qYXZh) | `100% <100%> (ø)` | `4 <1> (ø)` | :arrow_down: |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.592% <100%> (ø)` | `22 <2> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/GenotypeUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZVV0aWxzLmphdmE=) | `94.872% <100%> (+2.767%)` | `12 <0> (+3)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=footer). Last update [c8ede6e...c63c08b](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2546#issuecomment-290509295
https://github.com/broadinstitute/gatk/pull/2546#issuecomment-290509295:2535,Usability,learn,learn,2535,"InbreedingCoeff.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9JbmJyZWVkaW5nQ29lZmYuamF2YQ==) | `82.759% <100%> (ø)` | `11 <1> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/GenotypeCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZUNvdW50cy5qYXZh) | `100% <100%> (ø)` | `4 <1> (ø)` | :arrow_down: |; | [.../hellbender/tools/walkers/annotator/ExcessHet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9FeGNlc3NIZXQuamF2YQ==) | `98.592% <100%> (ø)` | `22 <2> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/GenotypeUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vdHlwZVV0aWxzLmphdmE=) | `94.872% <100%> (+2.767%)` | `12 <0> (+3)` | :arrow_up: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=footer). Last update [c8ede6e...c63c08b](https://codecov.io/gh/broadinstitute/gatk/pull/2546?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2546#issuecomment-290509295
https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401:1605,Deployability,update,update,1605,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=h1) Report; > Merging [#2547](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **decrease** coverage by `0.005%`.; > The diff coverage is `90%`. ```diff; @@ Coverage Diff @@; ## master #2547 +/- ##; ===============================================; - Coverage 76.279% 76.275% -0.005% ; + Complexity 10891 10889 -2 ; ===============================================; Files 752 752 ; Lines 39590 39574 -16 ; Branches 6925 6922 -3 ; ===============================================; - Hits 30199 30185 -14 ; + Misses 6768 6767 -1 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.226% <90%> (-2.896%)` | `39 <15> (-2)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=footer). Last update [c8ede6e...24e6497](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401
https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401:1371,Usability,learn,learn,1371,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=h1) Report; > Merging [#2547](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **decrease** coverage by `0.005%`.; > The diff coverage is `90%`. ```diff; @@ Coverage Diff @@; ## master #2547 +/- ##; ===============================================; - Coverage 76.279% 76.275% -0.005% ; + Complexity 10891 10889 -2 ; ===============================================; Files 752 752 ; Lines 39590 39574 -16 ; Branches 6925 6922 -3 ; ===============================================; - Hits 30199 30185 -14 ; + Misses 6768 6767 -1 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.226% <90%> (-2.896%)` | `39 <15> (-2)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=footer). Last update [c8ede6e...24e6497](https://codecov.io/gh/broadinstitute/gatk/pull/2547?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-290296401
https://github.com/broadinstitute/gatk/pull/2548#issuecomment-290305475:1889,Deployability,update,update,1889,"pr&el=h1) Report; > Merging [#2548](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2548 +/- ##; ===============================================; + Coverage 76.279% 76.282% +0.003% ; - Complexity 10891 10893 +2 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30199 30200 +1 ; Misses 6768 6768 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `85.185% <ø> (ø)` | `39 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=footer). Last update [c8ede6e...b79b75d](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2548#issuecomment-290305475
https://github.com/broadinstitute/gatk/pull/2548#issuecomment-290305475:1655,Usability,learn,learn,1655,"pr&el=h1) Report; > Merging [#2548](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2548 +/- ##; ===============================================; + Coverage 76.279% 76.282% +0.003% ; - Complexity 10891 10893 +2 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30199 30200 +1 ; Misses 6768 6768 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `85.185% <ø> (ø)` | `39 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=footer). Last update [c8ede6e...b79b75d](https://codecov.io/gh/broadinstitute/gatk/pull/2548?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2548#issuecomment-290305475
https://github.com/broadinstitute/gatk/pull/2550#issuecomment-290461016:1900,Deployability,update,update,1900,"eport; > Merging [#2550](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #2550 +/- ##; ===============================================; + Coverage 76.279% 76.282% +0.003% ; - Complexity 10891 10892 +1 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30199 30200 +1 ; Misses 6768 6768 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...recalibration/RecalibrationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL1JlY2FsaWJyYXRpb25Bcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `93.827% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=footer). Last update [c8ede6e...f810842](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2550#issuecomment-290461016
https://github.com/broadinstitute/gatk/pull/2550#issuecomment-290461016:1666,Usability,learn,learn,1666,"eport; > Merging [#2550](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/c8ede6ef810a3d9a05c7deb8052e27ca724ce8ba?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #2550 +/- ##; ===============================================; + Coverage 76.279% 76.282% +0.003% ; - Complexity 10891 10892 +1 ; ===============================================; Files 752 752 ; Lines 39590 39590 ; Branches 6925 6925 ; ===============================================; + Hits 30199 30200 +1 ; Misses 6768 6768 ; + Partials 2623 2622 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...recalibration/RecalibrationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL1JlY2FsaWJyYXRpb25Bcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `93.827% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `91.429% <0%> (+1.429%)` | `24% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=footer). Last update [c8ede6e...f810842](https://codecov.io/gh/broadinstitute/gatk/pull/2550?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2550#issuecomment-290461016
https://github.com/broadinstitute/gatk/pull/2556#issuecomment-290787479:3610,Deployability,update,update,3610,"ils/genotyper/SampleList.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvU2FtcGxlTGlzdC5qYXZh) | `75.676% <0%> (ø)` | `8% <0%> (?)` | |; | [...hellbender/tools/walkers/annotator/SampleList.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9TYW1wbGVMaXN0LmphdmE=) | `81.25% <0%> (+1.658%)` | `9% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [.../broadinstitute/hellbender/tools/exome/Sample.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9TYW1wbGUuamF2YQ==) | `100% <0%> (+12.308%)` | `5% <0%> (-21%)` | :arrow_down: |; | [...stitute/hellbender/engine/ReferenceFileSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVmZXJlbmNlRmlsZVNvdXJjZS5qYXZh) | `72.727% <0%> (+15.584%)` | `4% <0%> (-4%)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=footer). Last update [62d58c5...fde9d36](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2556#issuecomment-290787479
https://github.com/broadinstitute/gatk/pull/2556#issuecomment-290787479:3376,Usability,learn,learn,3376,"ils/genotyper/SampleList.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvU2FtcGxlTGlzdC5qYXZh) | `75.676% <0%> (ø)` | `8% <0%> (?)` | |; | [...hellbender/tools/walkers/annotator/SampleList.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9TYW1wbGVMaXN0LmphdmE=) | `81.25% <0%> (+1.658%)` | `9% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |; | [.../broadinstitute/hellbender/tools/exome/Sample.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9TYW1wbGUuamF2YQ==) | `100% <0%> (+12.308%)` | `5% <0%> (-21%)` | :arrow_down: |; | [...stitute/hellbender/engine/ReferenceFileSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVmZXJlbmNlRmlsZVNvdXJjZS5qYXZh) | `72.727% <0%> (+15.584%)` | `4% <0%> (-4%)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=footer). Last update [62d58c5...fde9d36](https://codecov.io/gh/broadinstitute/gatk/pull/2556?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2556#issuecomment-290787479
https://github.com/broadinstitute/gatk/pull/2558#issuecomment-290866339:84,Usability,simpl,simple,84,"Hitting a snag: the md5 output option doesn't seem to work with streams, and so the simple approach of ""just use Path everywhere"" fails because makeSAMWriter in htsjdk doesn't behave identically between a file or an outputStream. Now the question is: is there a fundamental reason for that, or can just add in the md5 feature? At first blush, it seems possible. @droazen, what's your expert opinion?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-290866339
https://github.com/broadinstitute/gatk/pull/2558#issuecomment-340898638:435,Usability,simpl,simplify,435,"@cmnbroad asking:. > I had the same thought as @lbergelson. How does obtaining a Path from the File and delegating to the > Path overload change the semantics ?. OK so I checked again (it's been a long time since I first checked) and now I see that they go to `createCommonSAMWriterFromFactory` which then go to the same place. So it appears this isn't true anymore and the semantics would be the same. With this in mind, then, we can simplify createCommonSAMWriter as well as createSAMWriter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-340898638
https://github.com/broadinstitute/gatk/pull/2559#issuecomment-290845420:4418,Deployability,update,update,4418,"ci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.491% <0%> (-2.632%)` | `32% <0%> (-9%)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `77.996% <0%> (-0.179%)` | `175% <0%> (-1%)` | |; | [...ellbender/tools/walkers/annotator/QualByDepth.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9RdWFsQnlEZXB0aC5qYXZh) | `94.444% <0%> (-0.15%)` | `16% <0%> (-1%)` | |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.484% <0%> (-0.116%)` | `49% <0%> (-1%)` | |; | [...ender/tools/walkers/annotator/InbreedingCoeff.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9JbmJyZWVkaW5nQ29lZmYuamF2YQ==) | `82.759% <0%> (ø)` | `11% <0%> (ø)` | :arrow_down: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=footer). Last update [62d58c5...0492c9c](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2559#issuecomment-290845420
https://github.com/broadinstitute/gatk/pull/2559#issuecomment-290845420:4184,Usability,learn,learn,4184,"ci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `46.491% <0%> (-2.632%)` | `32% <0%> (-9%)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `77.996% <0%> (-0.179%)` | `175% <0%> (-1%)` | |; | [...ellbender/tools/walkers/annotator/QualByDepth.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9RdWFsQnlEZXB0aC5qYXZh) | `94.444% <0%> (-0.15%)` | `16% <0%> (-1%)` | |; | [...ine/GATKPlugin/GATKReadFilterPluginDescriptor.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yLmphdmE=) | `85.484% <0%> (-0.116%)` | `49% <0%> (-1%)` | |; | [...ender/tools/walkers/annotator/InbreedingCoeff.java](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9JbmJyZWVkaW5nQ29lZmYuamF2YQ==) | `82.759% <0%> (ø)` | `11% <0%> (ø)` | :arrow_down: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=footer). Last update [62d58c5...0492c9c](https://codecov.io/gh/broadinstitute/gatk/pull/2559?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2559#issuecomment-290845420
https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:271,Performance,perform,performance,271,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871
https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:335,Testability,benchmark,benchmarking,335,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871
https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:30,Usability,feedback,feedback,30,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871
https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459:1863,Deployability,update,update,1863,"titute/gatk/pull/2566?src=pr&el=h1) Report; > Merging [#2566](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.015%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2566 +/- ##; ==============================================; - Coverage 76.386% 76.37% -0.015% ; + Complexity 10898 10895 -3 ; ==============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ==============================================; - Hits 30212 30206 -6 ; - Misses 6727 6732 +5 ; - Partials 2613 2614 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `85.714% <0%> (-4.762%)` | `7% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `85.95% <0%> (-4.132%)` | `55% <0%> (-2%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=footer). Last update [6859a12...1df1909](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459
https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459:923,Testability,test,test,923,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=h1) Report; > Merging [#2566](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.015%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2566 +/- ##; ==============================================; - Coverage 76.386% 76.37% -0.015% ; + Complexity 10898 10895 -3 ; ==============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ==============================================; - Hits 30212 30206 -6 ; - Misses 6727 6732 +5 ; - Partials 2613 2614 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `85.714% <0%> (-4.762%)` | `7% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `85.95% <0%> (-4.132%)` | `55% <0%> (-2%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=footer). Last update [6859a12...1df1909](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=lastupdated). Read the [comment docs](https://doc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459
https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459:1629,Usability,learn,learn,1629,"titute/gatk/pull/2566?src=pr&el=h1) Report; > Merging [#2566](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.015%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2566 +/- ##; ==============================================; - Coverage 76.386% 76.37% -0.015% ; + Complexity 10898 10895 -3 ; ==============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ==============================================; - Hits 30212 30206 -6 ; - Misses 6727 6732 +5 ; - Partials 2613 2614 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `85.714% <0%> (-4.762%)` | `7% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2NQYXJzZXIuamF2YQ==) | `85.95% <0%> (-4.132%)` | `55% <0%> (-2%)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=footer). Last update [6859a12...1df1909](https://codecov.io/gh/broadinstitute/gatk/pull/2566?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2566#issuecomment-291909459
https://github.com/broadinstitute/gatk/pull/2567#issuecomment-291643361:4387,Deployability,update,update,4387,"sbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclN0cnVjdHVyYWxWYXJpYW50c0Zyb21BbGlnbmVkQ29udGlnc1NBTVNwYXJrLmphdmE=) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [...oadinstitute/hellbender/tools/spark/sv/SvType.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdlR5cGUuamF2YQ==) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/ChimericAlignment.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DaGltZXJpY0FsaWdubWVudC5qYXZh) | `57.831% <33.333%> (ø)` | `25 <1> (ø)` | :arrow_down: |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `66.917% <66.667%> (ø)` | `38 <1> (ø)` | :arrow_down: |; | [...er/tools/spark/sv/SVVariantConsensusDiscovery.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNEaXNjb3ZlcnkuamF2YQ==) | `82.653% <73.913%> (ø)` | `25 <1> (?)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=footer). Last update [d054e7a...4ffa301](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567#issuecomment-291643361
https://github.com/broadinstitute/gatk/pull/2567#issuecomment-291643361:4153,Usability,learn,learn,4153,"sbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclN0cnVjdHVyYWxWYXJpYW50c0Zyb21BbGlnbmVkQ29udGlnc1NBTVNwYXJrLmphdmE=) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [...oadinstitute/hellbender/tools/spark/sv/SvType.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdlR5cGUuamF2YQ==) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/ChimericAlignment.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9DaGltZXJpY0FsaWdubWVudC5qYXZh) | `57.831% <33.333%> (ø)` | `25 <1> (ø)` | :arrow_down: |; | [...bender/tools/spark/sv/AssemblyAlignmentParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9Bc3NlbWJseUFsaWdubWVudFBhcnNlci5qYXZh) | `66.917% <66.667%> (ø)` | `38 <1> (ø)` | :arrow_down: |; | [...er/tools/spark/sv/SVVariantConsensusDiscovery.java](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TVlZhcmlhbnRDb25zZW5zdXNEaXNjb3ZlcnkuamF2YQ==) | `82.653% <73.913%> (ø)` | `25 <1> (?)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=footer). Last update [d054e7a...4ffa301](https://codecov.io/gh/broadinstitute/gatk/pull/2567?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567#issuecomment-291643361
https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495:1583,Deployability,update,update,1583,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=h1) Report; > Merging [#2568](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.008%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #2568 +/- ##; ===============================================; - Coverage 76.386% 76.378% -0.008% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; - Hits 30212 30209 -3 ; - Misses 6727 6730 +3 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `47.807% <0%> (-1.316%)` | `41 <0> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=footer). Last update [6859a12...8066d14](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495
https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495:1349,Usability,learn,learn,1349,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=h1) Report; > Merging [#2568](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **decrease** coverage by `0.008%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #2568 +/- ##; ===============================================; - Coverage 76.386% 76.378% -0.008% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; - Hits 30212 30209 -3 ; - Misses 6727 6730 +3 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/genotyper/GenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `47.807% <0%> (-1.316%)` | `41 <0> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=footer). Last update [6859a12...8066d14](https://codecov.io/gh/broadinstitute/gatk/pull/2568?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2568#issuecomment-291909495
https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451:1583,Deployability,update,update,1583,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=h1) Report; > Merging [#2570](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2570 +/- ##; ===============================================; + Coverage 76.386% 76.391% +0.005% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; + Hits 30212 30214 +2 ; + Misses 6727 6725 -2 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=footer). Last update [6859a12...b9b665a](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451
https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451:1349,Usability,learn,learn,1349,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=h1) Report; > Merging [#2570](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/6859a1202a79c1b123eac73a3f70162c6a90783c?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2570 +/- ##; ===============================================; + Coverage 76.386% 76.391% +0.005% ; Complexity 10898 10898 ; ===============================================; Files 754 754 ; Lines 39552 39552 ; Branches 6907 6907 ; ===============================================; + Hits 30212 30214 +2 ; + Misses 6727 6725 -2 ; Partials 2613 2613; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=footer). Last update [6859a12...b9b665a](https://codecov.io/gh/broadinstitute/gatk/pull/2570?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2570#issuecomment-291915451
https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600:4071,Deployability,update,update,4071,"cmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <0%> (+13.559%)` | `2% <0%> (+1%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=footer). Last update [5ccfd00...8360cbe](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600
https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600:3026,Testability,test,test,3026,"XRhU291cmNlLmphdmE=) | `90.476% <0%> (+1.587%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <0%> (+13.559%)` | `2% <0%> (+1%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600
https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600:3837,Usability,learn,learn,3837,"cmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `66.667% <0%> (+3.252%)` | `4% <0%> (ø)` | :arrow_down: |; | [...g/broadinstitute/hellbender/engine/AuthHolder.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXV0aEhvbGRlci5qYXZh) | `15.254% <0%> (+5.085%)` | `2% <0%> (ø)` | :arrow_down: |; | [...ender/utils/nio/SeekableByteChannelPrefetcher.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uaW8vU2Vla2FibGVCeXRlQ2hhbm5lbFByZWZldGNoZXIuamF2YQ==) | `77.703% <0%> (+6.757%)` | `22% <0%> (+4%)` | :arrow_up: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `86.4% <0%> (+8.8%)` | `35% <0%> (+7%)` | :arrow_up: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `23.729% <0%> (+13.559%)` | `2% <0%> (+1%)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=footer). Last update [5ccfd00...8360cbe](https://codecov.io/gh/broadinstitute/gatk/pull/2573?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2573#issuecomment-291977600
https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941:4334,Deployability,update,update,4334,"f-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-25%)` | `6% <0%> (-1%)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `53.247% <0%> (-18.071%)` | `28% <0%> (-6%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <0%> (-18.009%)` | `28% <0%> (ø)` | |; | ... and [42 more](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=footer). Last update [781db35...13a10e2](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941
https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941:2421,Testability,test,test,2421,3RvckxvZ2xlc3NQYWlySE1NLmphdmE=) | `83.562% <62.5%> (-3.535%)` | `10 <0> (+1)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-80.612%)` | `0% <0%> (-19%)` | |; | [...institute/hellbender/utils/gcs/GATKGCSOptions.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvR0FUS0dDU09wdGlvbnMuamF2YQ==) | `0% <0%> (-66.667%)` | `0% <0%> (ø)` | |; | [...lbender/engine/datasources/ReferenceAPISource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlQVBJU291cmNlLmphdmE=) | `22.013% <0%> (-62.264%)` | `8% <0%> (-26%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-25%)` | `6% <0%> (-1%)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941
https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941:4100,Usability,learn,learn,4100,"f-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-60.87%)` | `2% <0%> (-6%)` | |; | [...llbender/engine/spark/SparkCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `68.75% <0%> (-25%)` | `6% <0%> (-1%)` | |; | [...ender/engine/datasources/ReferenceMultiSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlTXVsdGlTb3VyY2UuamF2YQ==) | `55.556% <0%> (-18.519%)` | `8% <0%> (-1%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `53.247% <0%> (-18.071%)` | `28% <0%> (-6%)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <0%> (-18.009%)` | `28% <0%> (ø)` | |; | ... and [42 more](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=footer). Last update [781db35...13a10e2](https://codecov.io/gh/broadinstitute/gatk/pull/2574?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574#issuecomment-292193941
https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135:1091,Deployability,update,update,1091,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=h1) Report; > Merging [#2576](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/7a3d966f08a205f0961eebf73d89ed8b69be185d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2576 +/- ##; ========================================; Coverage 76.4% 76.4% ; Complexity 10922 10922 ; ========================================; Files 755 755 ; Lines 39674 39674 ; Branches 6927 6927 ; ========================================; Hits 30311 30311 ; Misses 6740 6740 ; Partials 2623 2623; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=footer). Last update [7a3d966...49bbaba](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135
https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135:857,Usability,learn,learn,857,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=h1) Report; > Merging [#2576](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/7a3d966f08a205f0961eebf73d89ed8b69be185d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2576 +/- ##; ========================================; Coverage 76.4% 76.4% ; Complexity 10922 10922 ; ========================================; Files 755 755 ; Lines 39674 39674 ; Branches 6927 6927 ; ========================================; Hits 30311 30311 ; Misses 6740 6740 ; Partials 2623 2623; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=footer). Last update [7a3d966...49bbaba](https://codecov.io/gh/broadinstitute/gatk/pull/2576?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576#issuecomment-292395135
https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716:40,Testability,test,test,40,"That's very strange @cmnbroad -- in the test I did in front of you yesterday, I added only the exclusion above and it worked fine for me. Are you building with `gradle` or `gradlew`?. Recommend we add whatever exclusions are necessary in a separate, simple PR, independent from the GenomicsDB PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716
https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716:250,Usability,simpl,simple,250,"That's very strange @cmnbroad -- in the test I did in front of you yesterday, I added only the exclusion above and it worked fine for me. Are you building with `gradle` or `gradlew`?. Recommend we add whatever exclusions are necessary in a separate, simple PR, independent from the GenomicsDB PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292619716
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:177,Availability,down,down,177,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:201,Deployability,update,update,201,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:20,Performance,cache,caches,20,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:260,Security,firewall,firewall,260,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:306,Security,access,access,306,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:8,Usability,clear,clearing,8,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641
https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127:1552,Deployability,update,update,1552,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=h1) Report; > Merging [#2580](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/d054e7aa910767c9f8d1b1a780435779d389080d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2580 +/- ##; ===========================================; Coverage 76.036% 76.036% ; Complexity 11010 11010 ; ===========================================; Files 768 768 ; Lines 39952 39952 ; Branches 6956 6956 ; ===========================================; Hits 30378 30378 ; Misses 6943 6943 ; Partials 2631 2631; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=footer). Last update [d054e7a...c1d2a60](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127
https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127:1318,Usability,learn,learn,1318,"# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=h1) Report; > Merging [#2580](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/d054e7aa910767c9f8d1b1a780435779d389080d?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2580 +/- ##; ===========================================; Coverage 76.036% 76.036% ; Complexity 11010 11010 ; ===========================================; Files 768 768 ; Lines 39952 39952 ; Branches 6956 6956 ; ===========================================; Hits 30378 30378 ; Misses 6943 6943 ; Partials 2631 2631; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `40.469% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=footer). Last update [d054e7a...c1d2a60](https://codecov.io/gh/broadinstitute/gatk/pull/2580?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2580#issuecomment-292624127
https://github.com/broadinstitute/gatk/pull/2581#issuecomment-292631877:4004,Deployability,update,update,4004,"k/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `86.025% <0%> (+4.561%)` | `30% <0%> (+6%)` | :arrow_up: |; | [...ute/hellbender/tools/spark/bwa/BwaSparkEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `94.805% <0%> (+4.805%)` | `8% <0%> (+3%)` | :arrow_up: |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `89.297% <0%> (+7.018%)` | `33% <0%> (+11%)` | :arrow_up: |; | [...ute/hellbender/utils/bwa/BwaMemAlignmentUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtQWxpZ25tZW50VXRpbHMuamF2YQ==) | `85.507% <0%> (+13.093%)` | `17% <0%> (+1%)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `55.233% <0%> (+14.764%)` | `38% <0%> (+10%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=footer). Last update [d054e7a...8ecb688](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581#issuecomment-292631877
https://github.com/broadinstitute/gatk/pull/2581#issuecomment-292631877:3770,Usability,learn,learn,3770,"k/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `86.025% <0%> (+4.561%)` | `30% <0%> (+6%)` | :arrow_up: |; | [...ute/hellbender/tools/spark/bwa/BwaSparkEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmtFbmdpbmUuamF2YQ==) | `94.805% <0%> (+4.805%)` | `8% <0%> (+3%)` | :arrow_up: |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `89.297% <0%> (+7.018%)` | `33% <0%> (+11%)` | :arrow_up: |; | [...ute/hellbender/utils/bwa/BwaMemAlignmentUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtQWxpZ25tZW50VXRpbHMuamF2YQ==) | `85.507% <0%> (+13.093%)` | `17% <0%> (+1%)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `55.233% <0%> (+14.764%)` | `38% <0%> (+10%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=footer). Last update [d054e7a...8ecb688](https://codecov.io/gh/broadinstitute/gatk/pull/2581?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581#issuecomment-292631877
https://github.com/broadinstitute/gatk/issues/2584#issuecomment-381198194:9,Usability,simpl,simple,9,"One very simple way to do this would be for `GenotypeGVCFs` to simply invoke `CombineGVCFs` directly upon initialization when multiple inputs are specified. We should consider doing this as a quick way of restoring this functionality, as several users have requested this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2584#issuecomment-381198194
https://github.com/broadinstitute/gatk/issues/2584#issuecomment-381198194:63,Usability,simpl,simply,63,"One very simple way to do this would be for `GenotypeGVCFs` to simply invoke `CombineGVCFs` directly upon initialization when multiple inputs are specified. We should consider doing this as a quick way of restoring this functionality, as several users have requested this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2584#issuecomment-381198194
https://github.com/broadinstitute/gatk/issues/2586#issuecomment-297757830:41,Usability,simpl,simple,41,On second thought there is an incredibly simple solution that R uses by including everything past 10e-7 as equal or more extreme.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2586#issuecomment-297757830
https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729:58,Availability,error,error,58,"@cmnbroad I think this proposal is good provided that the error message people get clearly explains what they need to do to resolve things when this happens (eg., explains which dependencies have changed and how to mark the changes as ""ok"")",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729
https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729:64,Integrability,message,message,64,"@cmnbroad I think this proposal is good provided that the error message people get clearly explains what they need to do to resolve things when this happens (eg., explains which dependencies have changed and how to mark the changes as ""ok"")",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729
https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729:178,Integrability,depend,dependencies,178,"@cmnbroad I think this proposal is good provided that the error message people get clearly explains what they need to do to resolve things when this happens (eg., explains which dependencies have changed and how to mark the changes as ""ok"")",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729
https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729:83,Usability,clear,clearly,83,"@cmnbroad I think this proposal is good provided that the error message people get clearly explains what they need to do to resolve things when this happens (eg., explains which dependencies have changed and how to mark the changes as ""ok"")",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2592#issuecomment-300897729
https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293046056:3923,Deployability,update,update,3923,"| [...g/broadinstitute/hellbender/engine/ReadWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZFdhbGtlci5qYXZh) | `100% <0%> (ø)` | `27% <0%> (+13%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `93.411% <0%> (+1.639%)` | `135% <0%> (+58%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.026% <0%> (+1.948%)` | `35% <0%> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `93.75% <0%> (+2.083%)` | `7% <0%> (+1%)` | :arrow_up: |; | [.../broadinstitute/hellbender/engine/LocusWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTG9jdXNXYWxrZXIuamF2YQ==) | `92.188% <0%> (+2.714%)` | `26% <0%> (+12%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=footer). Last update [12c7a2d...7488ed4](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293046056
https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293046056:3689,Usability,learn,learn,3689,"| [...g/broadinstitute/hellbender/engine/ReadWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZFdhbGtlci5qYXZh) | `100% <0%> (ø)` | `27% <0%> (+13%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `93.411% <0%> (+1.639%)` | `135% <0%> (+58%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.026% <0%> (+1.948%)` | `35% <0%> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `93.75% <0%> (+2.083%)` | `7% <0%> (+1%)` | :arrow_up: |; | [.../broadinstitute/hellbender/engine/LocusWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTG9jdXNXYWxrZXIuamF2YQ==) | `92.188% <0%> (+2.714%)` | `26% <0%> (+12%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=footer). Last update [12c7a2d...7488ed4](https://codecov.io/gh/broadinstitute/gatk/pull/2593?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293046056
https://github.com/broadinstitute/gatk/pull/2594#issuecomment-293665515:1864,Deployability,update,update,1864,"itute/gatk/pull/2594?src=pr&el=h1) Report; > Merging [#2594](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/2ecdef4fba1658930c388676be3e388efd67b6a3?src=pr&el=desc) will **increase** coverage by `0.002%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #2594 +/- ##; ===============================================; + Coverage 75.985% 75.987% +0.003% ; - Complexity 11033 11034 +1 ; ===============================================; Files 769 769 ; Lines 40058 40058 ; Branches 6979 6979 ; ===============================================; + Hits 30438 30439 +1 ; Misses 6981 6981 ; + Partials 2639 2638 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/BwaMemIndexImageCreator.java](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Cd2FNZW1JbmRleEltYWdlQ3JlYXRvci5qYXZh) | `71.429% <0%> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `95.313% <0%> (+1.563%)` | `22% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=footer). Last update [2ecdef4...a853f7c](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2594#issuecomment-293665515
https://github.com/broadinstitute/gatk/pull/2594#issuecomment-293665515:1630,Usability,learn,learn,1630,"itute/gatk/pull/2594?src=pr&el=h1) Report; > Merging [#2594](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/2ecdef4fba1658930c388676be3e388efd67b6a3?src=pr&el=desc) will **increase** coverage by `0.002%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #2594 +/- ##; ===============================================; + Coverage 75.985% 75.987% +0.003% ; - Complexity 11033 11034 +1 ; ===============================================; Files 769 769 ; Lines 40058 40058 ; Branches 6979 6979 ; ===============================================; + Hits 30438 30439 +1 ; Misses 6981 6981 ; + Partials 2639 2638 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tute/hellbender/tools/BwaMemIndexImageCreator.java](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Cd2FNZW1JbmRleEltYWdlQ3JlYXRvci5qYXZh) | `71.429% <0%> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `95.313% <0%> (+1.563%)` | `22% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=footer). Last update [2ecdef4...a853f7c](https://codecov.io/gh/broadinstitute/gatk/pull/2594?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2594#issuecomment-293665515
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558:4459,Deployability,update,update,4459,"zcGFyay9zdi9BbGlnbmVkQXNzZW1ibHlPckV4Y3VzZS5qYXZh) | `93.296% <100%> (+81.997%)` | `34 <2> (+30)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `76.994% <78.261%> (+36.525%)` | `44 <1> (+16)` | :arrow_up: |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `90.476% <90.476%> (ø)` | `4 <4> (?)` | |; | [...tructuralVariationDiscoveryArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `95.833% <95.833%> (ø)` | `0 <0> (?)` | |; | [.../main/java/org/broadinstitute/hellbender/Main.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluLmphdmE=) | `50.888% <0%> (-1.183%)` | `23% <0%> (-1%)` | |; | ... and [25 more](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=footer). Last update [bf993d8...dc817a8](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558:956,Testability,test,test,956,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=h1) Report; > Merging [#2595](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bf993d8c6f6925ce6bdb67f50c0e33c6e5bc3336?src=pr&el=desc) will **increase** coverage by `1.133%`.; > The diff coverage is `61.842%`. ```diff; @@ Coverage Diff @@; ## master #2595 +/- ##; ===============================================; + Coverage 75.992% 77.126% +1.133% ; - Complexity 11033 11147 +114 ; ===============================================; Files 769 771 +2 ; Lines 40058 40115 +57 ; Branches 6979 6982 +3 ; ===============================================; + Hits 30441 30939 +498 ; + Misses 6978 6513 -465 ; - Partials 2639 2663 +24; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `89.474% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...sv/DiscoverVariantsFromAlignedSGAContigsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclZhcmlhbnRzRnJvbUFsaWduZWRTR0FDb250aWdzU3BhcmsuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [.../sv/DiscoverVariantsFromContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9EaXNjb3ZlclZhcmlhbnRzRnJvbUNvbnRpZ0FsaWdubWVudHNTcGFyay5qYXZh) | `100% <100%> (ø)` | `7 <1> (?)` | |; | [...stitute/hellbender/tools/spark/sv/SVVCFWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558:4225,Usability,learn,learn,4225,"zcGFyay9zdi9BbGlnbmVkQXNzZW1ibHlPckV4Y3VzZS5qYXZh) | `93.296% <100%> (+81.997%)` | `34 <2> (+30)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `76.994% <78.261%> (+36.525%)` | `44 <1> (+16)` | :arrow_up: |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `90.476% <90.476%> (ø)` | `4 <4> (?)` | |; | [...tructuralVariationDiscoveryArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `95.833% <95.833%> (ø)` | `0 <0> (?)` | |; | [.../main/java/org/broadinstitute/hellbender/Main.java](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluLmphdmE=) | `50.888% <0%> (-1.183%)` | `23% <0%> (-1%)` | |; | ... and [25 more](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=footer). Last update [bf993d8...dc817a8](https://codecov.io/gh/broadinstitute/gatk/pull/2595?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-293918558
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022:603,Availability,down,down,603,"@SHuang-Broad I see. So the conversion to SAM and back when we write the file actually changes the results (or at least their annotations). It makes me a little nervous that in one version of the pipeline the records go through `BwaMemAlignmentUtils.applyAlignment` and in the other they don't, since that method has some complex logic. Right now we have two possible paths:. `AlignedAssemblyOrExcuse -> SAMRecord -> writeToFile -> GATKRead -> AlignmentRegion`. or . `AlignedAssemblyOrExcuse -> AlignmentRegion`. What if we always converted to `SAMRecord`? It's a little more expensive but it would cut down on alternate code paths and conversion code, and IMO would make the code a lot simpler to read if I didn't have to think about which code path I was in. I'm also worried that the different conversions could lead to bugs that will be hard to debug since you have to know the code path that generated them. @tedsharpe what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022:196,Deployability,pipeline,pipeline,196,"@SHuang-Broad I see. So the conversion to SAM and back when we write the file actually changes the results (or at least their annotations). It makes me a little nervous that in one version of the pipeline the records go through `BwaMemAlignmentUtils.applyAlignment` and in the other they don't, since that method has some complex logic. Right now we have two possible paths:. `AlignedAssemblyOrExcuse -> SAMRecord -> writeToFile -> GATKRead -> AlignmentRegion`. or . `AlignedAssemblyOrExcuse -> AlignmentRegion`. What if we always converted to `SAMRecord`? It's a little more expensive but it would cut down on alternate code paths and conversion code, and IMO would make the code a lot simpler to read if I didn't have to think about which code path I was in. I'm also worried that the different conversions could lead to bugs that will be hard to debug since you have to know the code path that generated them. @tedsharpe what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022:330,Testability,log,logic,330,"@SHuang-Broad I see. So the conversion to SAM and back when we write the file actually changes the results (or at least their annotations). It makes me a little nervous that in one version of the pipeline the records go through `BwaMemAlignmentUtils.applyAlignment` and in the other they don't, since that method has some complex logic. Right now we have two possible paths:. `AlignedAssemblyOrExcuse -> SAMRecord -> writeToFile -> GATKRead -> AlignmentRegion`. or . `AlignedAssemblyOrExcuse -> AlignmentRegion`. What if we always converted to `SAMRecord`? It's a little more expensive but it would cut down on alternate code paths and conversion code, and IMO would make the code a lot simpler to read if I didn't have to think about which code path I was in. I'm also worried that the different conversions could lead to bugs that will be hard to debug since you have to know the code path that generated them. @tedsharpe what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022:687,Usability,simpl,simpler,687,"@SHuang-Broad I see. So the conversion to SAM and back when we write the file actually changes the results (or at least their annotations). It makes me a little nervous that in one version of the pipeline the records go through `BwaMemAlignmentUtils.applyAlignment` and in the other they don't, since that method has some complex logic. Right now we have two possible paths:. `AlignedAssemblyOrExcuse -> SAMRecord -> writeToFile -> GATKRead -> AlignmentRegion`. or . `AlignedAssemblyOrExcuse -> AlignmentRegion`. What if we always converted to `SAMRecord`? It's a little more expensive but it would cut down on alternate code paths and conversion code, and IMO would make the code a lot simpler to read if I didn't have to think about which code path I was in. I'm also worried that the different conversions could lead to bugs that will be hard to debug since you have to know the code path that generated them. @tedsharpe what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294168022
https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294180667:215,Usability,simpl,simplify,215,"It would be nice to keep the separate tool around so that it can be used; for stuff like calling variants from de novo assemblies, like I showed at; Tuesday's meeting. But if both of you think it's not necessary to simplify the conversions; I'll defer to you. On Fri, Apr 14, 2017 at 11:59 AM, tedsharpe <notifications@github.com>; wrote:. > The decision about whether to hard clip or soft clip supplementary; > alignments is a flag to bwa mem. All it does is to replace initial and; > final 'S' in the cigar with 'H' instead. The code in applyAlignment; > respects that decision. So if we don't want any hard clipping, that's easy; > enough to do -- we just turn off that flag.; > That's probably the right thing to do since the code on line 89 of the; > AlignmentAssemblyParser always grabs the entire sequence, whether or not; > there's been hard clipping. I'd guess it's likely that there a bugs lurking; > here.; > I don't see any particular need to convert to SAM and then back into an; > AlignmentRegion. We can eliminate the SAM writing entirely once we have a; > single tool, and then there will only be a single path.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294179814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZffa-htFUWK3AckY3g2y2kR14wW-ks5rv5fKgaJpZM4M8xRs>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294180667
https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013:198,Availability,down,downstream,198,"I'm with @davidbenjamin that a camel-case looks clearer, because there are very long names in the GATK-framework that may involve a lot of dashes. Even if the bash-completion will help on this, for downstream projects it can be a nightmare to change this. For instance, I'm not planning to add the bash-completion generation to my toolkit, and I personally find difficult to read long arguments with tons of dashes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013
https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013:48,Usability,clear,clearer,48,"I'm with @davidbenjamin that a camel-case looks clearer, because there are very long names in the GATK-framework that may involve a lot of dashes. Even if the bash-completion will help on this, for downstream projects it can be a nightmare to change this. For instance, I'm not planning to add the bash-completion generation to my toolkit, and I personally find difficult to read long arguments with tons of dashes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013
https://github.com/broadinstitute/gatk/issues/2596#issuecomment-328628951:25,Usability,guid,guidance,25,"@vdauwera Thanks for the guidance. We'll get to work. @takutosato, you were also wondering about this. Here's our answer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-328628951
https://github.com/broadinstitute/gatk/pull/2605#issuecomment-294213579:3358,Deployability,update,update,3358,"der/utils/MannWhitneyU.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYW5uV2hpdG5leVUuamF2YQ==) | `92.793% <92.593%> (+17.237%)` | `48 <48> (+21)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `72.078% <0%> (-1.948%)` | `35% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `93.75% <0%> (-1.563%)` | `21% <0%> (-1%)` | |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `84.104% <0%> (+2.358%)` | `36% <0%> (+11%)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `42.989% <0%> (+4.441%)` | `46% <0%> (+18%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=footer). Last update [c350a09...3b4f53e](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-294213579
https://github.com/broadinstitute/gatk/pull/2605#issuecomment-294213579:3124,Usability,learn,learn,3124,"der/utils/MannWhitneyU.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYW5uV2hpdG5leVUuamF2YQ==) | `92.793% <92.593%> (+17.237%)` | `48 <48> (+21)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `72.078% <0%> (-1.948%)` | `35% <0%> (ø)` | |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `93.75% <0%> (-1.563%)` | `21% <0%> (-1%)` | |; | [...titute/hellbender/tools/spark/sv/ReadMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkTWV0YWRhdGEuamF2YQ==) | `84.104% <0%> (+2.358%)` | `36% <0%> (+11%)` | :arrow_up: |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `42.989% <0%> (+4.441%)` | `46% <0%> (+18%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=footer). Last update [c350a09...3b4f53e](https://codecov.io/gh/broadinstitute/gatk/pull/2605?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-294213579
https://github.com/broadinstitute/gatk/pull/2607#issuecomment-297147327:78,Deployability,pipeline,pipeline,78,"I have no objection to this PR. However, it might be simpler to modify the SV pipeline to optionally produce this data on the fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2607#issuecomment-297147327
https://github.com/broadinstitute/gatk/pull/2607#issuecomment-297147327:53,Usability,simpl,simpler,53,"I have no objection to this PR. However, it might be simpler to modify the SV pipeline to optionally produce this data on the fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2607#issuecomment-297147327
https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400:1464,Availability,down,down,1464,"If Louis says this is allowed in our style guide then you can leave them; in. I didn't realize that. Feel free to drop the hammer on us for any style; violations. On Mon, May 1, 2017 at 1:04 PM, tedsharpe <notifications@github.com> wrote:. > *@tedsharpe* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/; > BreakpointClusterer.java; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>:; >; > > }; >; > - @Override; > - public Iterator<BreakpointEvidence> apply( final BreakpointEvidence evidence ) {; > - if ( evidence.getContigIndex() != currentContig ) {; > - currentContig = evidence.getContigIndex();; > - locMap.clear();; > + public Iterator<BreakpointEvidence> apply( final Iterator<BreakpointEvidence> evidenceItr ) {; > + while ( evidenceItr.hasNext() ) {; > + final BreakpointEvidence evidence = evidenceItr.next();; > + final SVInterval location = evidence.getLocation();; > + final SVIntervalTree.Entry<List<BreakpointEvidence>> entry = evidenceTree.find(location);; > + if ( entry != null ) entry.getValue().add(evidence);; >; > Pretty sure that Louis said that this was one of our departures from; > Google style: single statements following an ""if"", ""else"", or ""else if""; > that fit comfortably on the same line are allowed (but not required) to be; > unbraced.; > Since you prefer braces, I'll change these.; > However, since you've thrown down the gauntlet, I'm going to start nailing; > you guys on very long lines (max line length is supposed to be 100; > characters). So there.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPkQPglpEhCzZqbA17GshZt6t-Dks5r1hCsgaJpZM4NKPYH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400
https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400:43,Usability,guid,guide,43,"If Louis says this is allowed in our style guide then you can leave them; in. I didn't realize that. Feel free to drop the hammer on us for any style; violations. On Mon, May 1, 2017 at 1:04 PM, tedsharpe <notifications@github.com> wrote:. > *@tedsharpe* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/; > BreakpointClusterer.java; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>:; >; > > }; >; > - @Override; > - public Iterator<BreakpointEvidence> apply( final BreakpointEvidence evidence ) {; > - if ( evidence.getContigIndex() != currentContig ) {; > - currentContig = evidence.getContigIndex();; > - locMap.clear();; > + public Iterator<BreakpointEvidence> apply( final Iterator<BreakpointEvidence> evidenceItr ) {; > + while ( evidenceItr.hasNext() ) {; > + final BreakpointEvidence evidence = evidenceItr.next();; > + final SVInterval location = evidence.getLocation();; > + final SVIntervalTree.Entry<List<BreakpointEvidence>> entry = evidenceTree.find(location);; > + if ( entry != null ) entry.getValue().add(evidence);; >; > Pretty sure that Louis said that this was one of our departures from; > Google style: single statements following an ""if"", ""else"", or ""else if""; > that fit comfortably on the same line are allowed (but not required) to be; > unbraced.; > Since you prefer braces, I'll change these.; > However, since you've thrown down the gauntlet, I'm going to start nailing; > you guys on very long lines (max line length is supposed to be 100; > characters). So there.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPkQPglpEhCzZqbA17GshZt6t-Dks5r1hCsgaJpZM4NKPYH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400
https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400:726,Usability,clear,clear,726,"If Louis says this is allowed in our style guide then you can leave them; in. I didn't realize that. Feel free to drop the hammer on us for any style; violations. On Mon, May 1, 2017 at 1:04 PM, tedsharpe <notifications@github.com> wrote:. > *@tedsharpe* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/; > BreakpointClusterer.java; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>:; >; > > }; >; > - @Override; > - public Iterator<BreakpointEvidence> apply( final BreakpointEvidence evidence ) {; > - if ( evidence.getContigIndex() != currentContig ) {; > - currentContig = evidence.getContigIndex();; > - locMap.clear();; > + public Iterator<BreakpointEvidence> apply( final Iterator<BreakpointEvidence> evidenceItr ) {; > + while ( evidenceItr.hasNext() ) {; > + final BreakpointEvidence evidence = evidenceItr.next();; > + final SVInterval location = evidence.getLocation();; > + final SVIntervalTree.Entry<List<BreakpointEvidence>> entry = evidenceTree.find(location);; > + if ( entry != null ) entry.getValue().add(evidence);; >; > Pretty sure that Louis said that this was one of our departures from; > Google style: single statements following an ""if"", ""else"", or ""else if""; > that fit comfortably on the same line are allowed (but not required) to be; > unbraced.; > Since you prefer braces, I'll change these.; > However, since you've thrown down the gauntlet, I'm going to start nailing; > you guys on very long lines (max line length is supposed to be 100; > characters). So there.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPkQPglpEhCzZqbA17GshZt6t-Dks5r1hCsgaJpZM4NKPYH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400
https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108:43,Testability,test,tests,43,@meganshand 1 very minor comment about the tests. 👍 After that. This is awesome to find such a simple solution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108
https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108:95,Usability,simpl,simple,95,@meganshand 1 very minor comment about the tests. 👍 After that. This is awesome to find such a simple solution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2629#issuecomment-297761108
https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693:24,Testability,test,test,24,I've ran this one in my test and it's clear the current code doesn't have the problem of prefetching a prefetcher.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693
https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693:38,Usability,clear,clear,38,I've ran this one in my test and it's clear the current code doesn't have the problem of prefetching a prefetcher.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298468693
https://github.com/broadinstitute/gatk/pull/2646#issuecomment-299292827:55,Availability,down,down,55,@lbergelson @cwhelan @tedsharpe Going to break this PR down into smaller pieces. I will implement the feedback received thus far as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2646#issuecomment-299292827
https://github.com/broadinstitute/gatk/pull/2646#issuecomment-299292827:102,Usability,feedback,feedback,102,@lbergelson @cwhelan @tedsharpe Going to break this PR down into smaller pieces. I will implement the feedback received thus far as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2646#issuecomment-299292827
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397:562,Availability,down,down,562,"Hi @david-wb . I reformatted your comment slightly to make the stack trace more legible, I hope you don't mind. I suspect your intuition about the System.exit(0) is entirely correct. I suspect we haven't noticed it because we typically run in yarn client mode and you're running in cluster mode. . Two questions:; 1. How often does it happen? Can you regularly reproduce it?; 2. Have you examined the output files to make sure they are correct and not truncated? . It looks like we'll probably have to add a check and wait for the spark context to properly shut down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397:127,Usability,intuit,intuition,127,"Hi @david-wb . I reformatted your comment slightly to make the stack trace more legible, I hope you don't mind. I suspect your intuition about the System.exit(0) is entirely correct. I suspect we haven't noticed it because we typically run in yarn client mode and you're running in cluster mode. . Two questions:; 1. How often does it happen? Can you regularly reproduce it?; 2. Have you examined the output files to make sure they are correct and not truncated? . It looks like we'll probably have to add a check and wait for the spark context to properly shut down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:2304,Deployability,pipeline,pipelines,2304,"/05/05 17:03:30 INFO ApplicationMaster: Preparing Local resources; 17/05/05 17:03:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1493961816416_0010_000002; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set(); 17/05/05 17:03:32 INFO ApplicationMaster: Starting the user application in a separate Thread; 17/05/05 17:03:32 INFO ApplicationMaster: Waiting for spark context initialization...; [May 5, 2017 5:03:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hdfs:///output2.bam --input hdfs:///chr1.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [May 5, 2017 5:03:35 PM UTC] Executing as yarn@ip-172-30-0-122 on Linux 4.4.35-33.55.amzn1.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: Version:4.alpha.2-252-gf627ed4-SNAPSHOT; 17/05/05 17:03:35 INFO SparkContext: Running Spark version 2.1.0; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:35 INFO SecurityM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:18234,Deployability,pipeline,pipelines,18234,"on localhost (executor driver) (4/4); 17/05/05 17:03:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 17:03:58 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.370 s; 17/05/05 17:03:58 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 16.702399 s; 17/05/05 17:03:58 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:46483; 17/05/05 17:03:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 17:03:59 INFO MemoryStore: MemoryStore cleared; 17/05/05 17:03:59 INFO BlockManager: BlockManager stopped; 17/05/05 17:03:59 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 17:03:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 17:03:59 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 5:03:59 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 17:03:59 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0010; 17/05/05 17:03:59 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-223a9e8b-0fe9-41f0-8bed-f843978f1882; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-573a9c53-e268-4f3b-8907-1f35e5839788; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:18568,Integrability,message,message,18568,"on localhost (executor driver) (4/4); 17/05/05 17:03:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 17:03:58 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.370 s; 17/05/05 17:03:58 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 16.702399 s; 17/05/05 17:03:58 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:46483; 17/05/05 17:03:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 17:03:59 INFO MemoryStore: MemoryStore cleared; 17/05/05 17:03:59 INFO BlockManager: BlockManager stopped; 17/05/05 17:03:59 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 17:03:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 17:03:59 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 5:03:59 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 17:03:59 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0010; 17/05/05 17:03:59 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-223a9e8b-0fe9-41f0-8bed-f843978f1882; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0010/spark-573a9c53-e268-4f3b-8907-1f35e5839788; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:1835,Security,authenticat,authentication,1835,"d binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/37/__spark_libs__6987413740287883326.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for TERM; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for HUP; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for INT; 17/05/05 17:03:30 INFO ApplicationMaster: Preparing Local resources; 17/05/05 17:03:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1493961816416_0010_000002; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set(); 17/05/05 17:03:32 INFO ApplicationMaster: Starting the user application in a separate Thread; 17/05/05 17:03:32 INFO ApplicationMaster: Waiting for spark context initialization...; [May 5, 2017 5:03:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hdfs:///output2.bam --input hdfs:///chr1.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:3408,Security,authenticat,authentication,3408,"bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [May 5, 2017 5:03:35 PM UTC] Executing as yarn@ip-172-30-0-122 on Linux 4.4.35-33.55.amzn1.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: Version:4.alpha.2-252-gf627ed4-SNAPSHOT; 17/05/05 17:03:35 INFO SparkContext: Running Spark version 2.1.0; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:35 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:35 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set(); 17/05/05 17:03:35 INFO Utils: Successfully started service 'sparkDriver' on port 42358.; 17/05/05 17:03:35 INFO SparkEnv: Registering MapOutputTracker; 17/05/05 17:03:35 INFO SparkEnv: Registering BlockManagerMaster; 17/05/05 17:03:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/05/05 17:03:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/05/05 17:03:35 INFO DiskBlockManager: Created local directory at /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0010/blockmgr-356a706f-2395-4ef6-985a-d3a7d7b01a8a; 17/05/05 17:03:35 INFO DiskBlockManager: Created local directory at /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0010/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:489,Testability,log,log,489,"1. Yes it it reproducible. It happens every time. . 2. No, I have not examined the output files, but I have run the same app on my local machine with ; ```; spark-submit --master local[2] gatk-package-4.alpha.2-252-gf627ed4-SNAPSHOT-spark.jar PrintReadsSpark -I chr1.bam -O output.bam; ```; And the output.bam has nearly the same size in bytes as the output from the yarn cluster, so it doesn't appear to be truncated. The output .bai files have identical sizes. . Here is the full stderr log.; ```; Log Type: stderr; Log Upload Time: Fri May 05 17:04:00 +0000 2017; Log Length: 18471; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/37/__spark_libs__6987413740287883326.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for TERM; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for HUP; 17/05/05 17:03:30 INFO SignalUtils: Registered signal handler for INT; 17/05/05 17:03:30 INFO ApplicationMaster: Preparing Local resources; 17/05/05 17:03:32 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1493961816416_0010_000002; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop; 17/05/05 17:03:32 INFO SecurityManager: Changing view acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: Changing modify acls groups to: ; 17/05/05 17:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users with modify permissions: Set(yar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:5968,Testability,log,log,5968,"fully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34295.; 17/05/05 17:03:36 INFO NettyBlockTransferService: Server created on 172.30.0.122:34295; 17/05/05 17:03:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/05/05 17:03:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.122:34295 with 414.4 MB RAM, BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO BlockManager: external shuffle service port = 7337; 17/05/05 17:03:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.30.0.122, 34295, None); 17/05/05 17:03:36 INFO EventLoggingListener: Logging events to hdfs:///var/log/spark/apps/local-1494003816349; 17/05/05 17:03:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 304.1 KB, free 414.2 MB); 17/05/05 17:03:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.0 KB, free 414.1 MB); 17/05/05 17:03:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.0.122:34295 (size: 26.0 KB, free: 414.4 MB); 17/05/05 17:03:38 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:109; 17/05/05 17:03:38 INFO FileInputFormat: Total input paths to process : 1; 17/05/05 17:03:38 INFO SparkContext: Starting job: sortByKey at ReadsSparkSink.java:244; 17/05/05 17:03:38 INFO DAGScheduler: Got job 0 (sortByKey at ReadsSparkSink.java:244) with 1 output partitions; 17/05/05 17:03:38 INFO DAGScheduler: Final stage: ResultStage 0 (sortByKey at ReadsSparkSink.java:244); 17/05/05 17:03:38 INFO DAGScheduler: Parents of final stage: List(); 17/05/05 17:03:38 INFO DAGScheduler: Missing pa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046:17838,Usability,clear,cleared,17838,"c2.internal:8020/output2.bam.parts/_temporary/0/task_20170505170341_0011_r_000000; 17/05/05 17:03:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1921 bytes result sent to driver; 17/05/05 17:03:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 10369 ms on localhost (executor driver) (4/4); 17/05/05 17:03:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 17:03:58 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.370 s; 17/05/05 17:03:58 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 16.702399 s; 17/05/05 17:03:58 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:46483; 17/05/05 17:03:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 17:03:59 INFO MemoryStore: MemoryStore cleared; 17/05/05 17:03:59 INFO BlockManager: BlockManager stopped; 17/05/05 17:03:59 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 17:03:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 17:03:59 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 5:03:59 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 17:03:59 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 17:03:59 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0010; 17/05/05 17:03:59 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 17:03:59 INFO ShutdownHookManager: Deleting directory /mnt1/yarn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299525046
https://github.com/broadinstitute/gatk/issues/2674#issuecomment-300196976:18,Usability,clear,clear,18,@droazen I have a clear picture of what's happening in GATK 4 `GenotypeGVCFs`. Now I need to trace the code in GATK 3.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2674#issuecomment-300196976
https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060:148,Modifiability,refactor,refactoring,148,@tedsharpe @SHuang-Broad I've tried to address your comments -- want to have a another look? . Due to issues in the class I backed out my usage and refactoring of SATagAlignmentBuilder and SATagAlignment and just went with my own simple little parser.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060
https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060:230,Usability,simpl,simple,230,@tedsharpe @SHuang-Broad I've tried to address your comments -- want to have a another look? . Due to issues in the class I backed out my usage and refactoring of SATagAlignmentBuilder and SATagAlignment and just went with my own simple little parser.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060
https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489:88,Availability,error,error,88,Just as feedback we use gcs nio too in Cromwell and have had to add retries around this error as it has popped up every now and then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489
https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489:8,Usability,feedback,feedback,8,Just as feedback we use gcs nio too in Cromwell and have had to add retries around this error as it has popped up every now and then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489
https://github.com/broadinstitute/gatk/issues/2689#issuecomment-369990259:96,Usability,clear,clear,96,"I've just run into this issue, is it okay to ignore these warnings? I'm new to GATK so it's not clear to me what a combination operation is, and if it has effect on the output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-369990259
https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:85,Security,authenticat,authentication,85,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909
https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:59,Testability,test,tests,59,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909
https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:36,Usability,clear,clear,36,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909
https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380:938,Deployability,release,release,938,"@lbergelson ; I should mention the version of BWA I document is v0.7.15 (https://software.broadinstitute.org/gatk/documentation/article.php?id=8017). The Genomics Platform has also moved on to v0.7.15: ; ```; WMCF9-CB5:Documents shlee$ docker inspect broadinstitute/genomes-in-the-cloud:2.2.5-1486412288; [; {; ""Id"": ""sha256:69ece5bcfc730304ad77e9473c17094328924fc13b2ed3e63b7ac2d4c859a483"",; ""RepoTags"": [; ""broadinstitute/genomes-in-the-cloud:2.2.5-1486412288""; ...; ""Labels"": {; ""GOTC_BGZIP_VER"": ""1.3"",; ""GOTC_BWA_VER"": ""0.7.15.r1140"",; ""GOTC_GATK34_VER"": ""3.4-g3c929b0"",; ""GOTC_GATK35_VER"": ""3.5-0-g36282e4"",; ""GOTC_GATK36_VER"": ""3.6-44-ge7d1cd2"",; ""GOTC_GATK4_VER"": ""4.alpha-249-g7df4044"",; ""GOTC_PICARD_VER"": ""1.1150"",; ""GOTC_SAMTOOLS_VER"": ""1.3.1"",; ""GOTC_SVTOOLKIT_VER"": ""2.00-1650"",; ""GOTC_TABIX_VER"": ""0.2.5_r1005""; ```. If the spark version we offer currently in GATK4 is roughly equivalent to v0.7.13, and this is the latest release in the Apache branch of the BWA repo that is usable by us, then should we ask HL for another Apache release equivalent to v0.7.15?. Note to self: this tool currently is not usable as it requires hacks to the command and also silently drops reads. Needs fixing not documenting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380
https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380:1046,Deployability,release,release,1046,"@lbergelson ; I should mention the version of BWA I document is v0.7.15 (https://software.broadinstitute.org/gatk/documentation/article.php?id=8017). The Genomics Platform has also moved on to v0.7.15: ; ```; WMCF9-CB5:Documents shlee$ docker inspect broadinstitute/genomes-in-the-cloud:2.2.5-1486412288; [; {; ""Id"": ""sha256:69ece5bcfc730304ad77e9473c17094328924fc13b2ed3e63b7ac2d4c859a483"",; ""RepoTags"": [; ""broadinstitute/genomes-in-the-cloud:2.2.5-1486412288""; ...; ""Labels"": {; ""GOTC_BGZIP_VER"": ""1.3"",; ""GOTC_BWA_VER"": ""0.7.15.r1140"",; ""GOTC_GATK34_VER"": ""3.4-g3c929b0"",; ""GOTC_GATK35_VER"": ""3.5-0-g36282e4"",; ""GOTC_GATK36_VER"": ""3.6-44-ge7d1cd2"",; ""GOTC_GATK4_VER"": ""4.alpha-249-g7df4044"",; ""GOTC_PICARD_VER"": ""1.1150"",; ""GOTC_SAMTOOLS_VER"": ""1.3.1"",; ""GOTC_SVTOOLKIT_VER"": ""2.00-1650"",; ""GOTC_TABIX_VER"": ""0.2.5_r1005""; ```. If the spark version we offer currently in GATK4 is roughly equivalent to v0.7.13, and this is the latest release in the Apache branch of the BWA repo that is usable by us, then should we ask HL for another Apache release equivalent to v0.7.15?. Note to self: this tool currently is not usable as it requires hacks to the command and also silently drops reads. Needs fixing not documenting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380
https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380:991,Usability,usab,usable,991,"@lbergelson ; I should mention the version of BWA I document is v0.7.15 (https://software.broadinstitute.org/gatk/documentation/article.php?id=8017). The Genomics Platform has also moved on to v0.7.15: ; ```; WMCF9-CB5:Documents shlee$ docker inspect broadinstitute/genomes-in-the-cloud:2.2.5-1486412288; [; {; ""Id"": ""sha256:69ece5bcfc730304ad77e9473c17094328924fc13b2ed3e63b7ac2d4c859a483"",; ""RepoTags"": [; ""broadinstitute/genomes-in-the-cloud:2.2.5-1486412288""; ...; ""Labels"": {; ""GOTC_BGZIP_VER"": ""1.3"",; ""GOTC_BWA_VER"": ""0.7.15.r1140"",; ""GOTC_GATK34_VER"": ""3.4-g3c929b0"",; ""GOTC_GATK35_VER"": ""3.5-0-g36282e4"",; ""GOTC_GATK36_VER"": ""3.6-44-ge7d1cd2"",; ""GOTC_GATK4_VER"": ""4.alpha-249-g7df4044"",; ""GOTC_PICARD_VER"": ""1.1150"",; ""GOTC_SAMTOOLS_VER"": ""1.3.1"",; ""GOTC_SVTOOLKIT_VER"": ""2.00-1650"",; ""GOTC_TABIX_VER"": ""0.2.5_r1005""; ```. If the spark version we offer currently in GATK4 is roughly equivalent to v0.7.13, and this is the latest release in the Apache branch of the BWA repo that is usable by us, then should we ask HL for another Apache release equivalent to v0.7.15?. Note to self: this tool currently is not usable as it requires hacks to the command and also silently drops reads. Needs fixing not documenting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380
https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380:1119,Usability,usab,usable,1119,"@lbergelson ; I should mention the version of BWA I document is v0.7.15 (https://software.broadinstitute.org/gatk/documentation/article.php?id=8017). The Genomics Platform has also moved on to v0.7.15: ; ```; WMCF9-CB5:Documents shlee$ docker inspect broadinstitute/genomes-in-the-cloud:2.2.5-1486412288; [; {; ""Id"": ""sha256:69ece5bcfc730304ad77e9473c17094328924fc13b2ed3e63b7ac2d4c859a483"",; ""RepoTags"": [; ""broadinstitute/genomes-in-the-cloud:2.2.5-1486412288""; ...; ""Labels"": {; ""GOTC_BGZIP_VER"": ""1.3"",; ""GOTC_BWA_VER"": ""0.7.15.r1140"",; ""GOTC_GATK34_VER"": ""3.4-g3c929b0"",; ""GOTC_GATK35_VER"": ""3.5-0-g36282e4"",; ""GOTC_GATK36_VER"": ""3.6-44-ge7d1cd2"",; ""GOTC_GATK4_VER"": ""4.alpha-249-g7df4044"",; ""GOTC_PICARD_VER"": ""1.1150"",; ""GOTC_SAMTOOLS_VER"": ""1.3.1"",; ""GOTC_SVTOOLKIT_VER"": ""2.00-1650"",; ""GOTC_TABIX_VER"": ""0.2.5_r1005""; ```. If the spark version we offer currently in GATK4 is roughly equivalent to v0.7.13, and this is the latest release in the Apache branch of the BWA repo that is usable by us, then should we ask HL for another Apache release equivalent to v0.7.15?. Note to self: this tool currently is not usable as it requires hacks to the command and also silently drops reads. Needs fixing not documenting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2711#issuecomment-301184380
https://github.com/broadinstitute/gatk/issues/2714#issuecomment-301542931:47,Usability,clear,clear,47,Re-worded the title and description to make it clear that the call in question is happening in the constructor for `GenomicsDBImporter` (`GenomicsDBImporter` line 464),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2714#issuecomment-301542931
https://github.com/broadinstitute/gatk/issues/2746#issuecomment-319178018:56,Usability,simpl,simpler,56,@samuelklee Sounds like a perfect application and a lot simpler than my fix. Hopefully your rapid prototype will be faster too.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-319178018
https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:378,Availability,error,error,378,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226
https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:290,Deployability,patch,patched,290,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226
https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:312,Deployability,release,release,312,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226
https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:62,Testability,test,testing,62,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226
https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226:365,Usability,simpl,simple,365,"Am 90% certain at this point that the jar they were using for testing was built incorrectly. The `CloudStorageReadChannel.class` file in the latest `google-cloud-nio-0.19.0-alpha-shaded.jar` should be 6169 bytes, but their jar has 5401 bytes for that class. And that's the primary class JP patched in the latest release. So I am pretty hopeful that this was just a simple build error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306947226
https://github.com/broadinstitute/gatk/issues/2769#issuecomment-309642761:42,Usability,simpl,simpler,42,Sure. But we'll probably need to use some simpler stylesheets than what the website uses.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2769#issuecomment-309642761
https://github.com/broadinstitute/gatk/pull/2816#issuecomment-306281558:49,Usability,feedback,feedback,49,I've also incorporated @davidbenjamin 's in line feedback. Please let me know if there are additional fixes for the documentation section.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816#issuecomment-306281558
https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260:125,Testability,test,test,125,"I think without a matched normal, there is not much you can do for high purity samples in LOH regions. Flipping the binomial test to filter against the null hypothesis of hom (rather than a null of f = 0.5, as in GetHetCoverage) seems to work well otherwise. Expanding the allele-fraction model to include hom sites is an option, but then you would be guided by the prior. Closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260
https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260:352,Usability,guid,guided,352,"I think without a matched normal, there is not much you can do for high purity samples in LOH regions. Flipping the binomial test to filter against the null hypothesis of hom (rather than a null of f = 0.5, as in GetHetCoverage) seems to work well otherwise. Expanding the allele-fraction model to include hom sites is an option, but then you would be guided by the prior. Closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2856#issuecomment-335586260
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316479500:416,Usability,simpl,simply,416,"TableReader is a dumb-dumb one-record at a time reader so it shouldn't suffer from memory leaks. . In contrast the parser() method uses a incremental ""buffer"" that accumulates the counts until the end when the actual returned table is created... The reason for this is to keep the ReadCountsCollection class constant. So at some point you need at least twice the amount of memory as compare to a solution that would simply use the returned object as the accumulator.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316479500
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:855,Availability,down,down,855,"First cut at a rewrite seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:2932,Availability,down,downside,2932,"pically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This alre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1656,Deployability,pipeline,pipeline,1656,"s took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5840,Deployability,pipeline,pipeline,5840,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1075,Energy Efficiency,efficient,efficient,1075," seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5171,Energy Efficiency,reduce,reduces,5171,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:15,Modifiability,rewrite,rewrite,15,"First cut at a rewrite seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,Performance,perform,performed,1105," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1555,Performance,perform,perform,1555,"which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 wr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1812,Performance,perform,perform,1812,"y ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1915,Performance,perform,performed,1915,"VD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:2412,Performance,perform,performing,2412,"e to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give bet",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,Performance,perform,performance,3383," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:4673,Performance,perform,perform,4673,"s doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5096,Performance,perform,performed,5096,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5811,Performance,perform,performance,5811,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5610,Usability,simpl,simple,5610,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1345,Availability,down,down,1345,"c.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible for hypersegmentation is CBS, rather than insufficient denoising, I'd rather focus on finding a viable segmentation alternative.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1499,Energy Efficiency,efficient,efficient,1499,"c.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible for hypersegmentation is CBS, rather than insufficient denoising, I'd rather focus on finding a viable segmentation alternative.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:961,Performance,optimiz,optimize,961,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1167,Performance,perform,performing,1167,"e up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1066,Safety,avoid,avoid,1066,"tes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:387,Testability,test,test,387,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:580,Usability,simpl,simply,580,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1368,Usability,learn,learn,1368,"c.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible for hypersegmentation is CBS, rather than insufficient denoising, I'd rather focus on finding a viable segmentation alternative.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,Availability,recover,recover,696,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1263,Availability,recover,recovered,1263,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:274,Performance,perform,performance,274,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,Performance,perform,performs,1522,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,Safety,recover,recover,696,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1263,Safety,recover,recovered,1263,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:425,Testability,log,log,425,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1703,Usability,simpl,simply,1703,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2775,Performance,perform,perform,2775,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2809,Testability,test,tests,2809,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2802,Usability,simpl,simple,2802,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:91,Availability,failure,failure,91,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1167,Availability,down,down,1167,"xtensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:526,Performance,perform,performance,526,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1162,Performance,tune,tune,1162,"xtensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1964,Performance,perform,perform,1964,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1981,Testability,test,test,1981,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1593,Usability,simpl,simply,1593,"ddition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible chang",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:2774,Usability,simpl,simple,2774,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2866,Availability,recover,recovering,2866,"weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2312,Performance,perform,perform,2312,"ges.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:1082,Safety,detect,detected,1082,"ore refinements to the method and have settled on the following procedure:. Assume we have _N_ data points:; ![1](https://user-images.githubusercontent.com/11076296/29580954-bea24562-8745-11e7-9c8c-d68504ba31da.png); To find segments, we:; 1) Select _C<sub>max</sub>_, the maximum number of changepoints to discover. In practice, _C<sub>max</sub> = 100_ per chromosome should more than suffice.; 2) Select a kernel (linear for sensitivity to changes in the distribution mean, Gaussian with a specified variance _σ<sup>2</sup>_ for multimodal data, etc.) and a subsample of _p_ points to approximate it using SVD.; 3) Select window sizes _w<sub>j</sub>_ for which to compute local costs at each point. To be precise, we compute the cost of a changepoint at the point with index _i_, assuming adjacent segments containing the points with indices _[i - w<sub>j</sub> + 1, i]_ and _[i + 1, i + w<sub>j</sub>]_. Selecting a minimum window size and then doubling up to relevant length scales (noting that longer window lengths allow for more subtle changepoints to be detected) works well in practice. For example, here are what the cost functions look like for window sizes of 8, 16, 32, and 64:. ![2](https://user-images.githubusercontent.com/11076296/29582011-210d37b8-8749-11e7-9383-0c657232347e.png). ![3](https://user-images.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2866,Safety,recover,recovering,2866,"weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2711,Testability,log,log,2711,"ima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-436189",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2004,Usability,simpl,simplification,2004,"scales (noting that longer window lengths allow for more subtle changepoints to be detected) works well in practice. For example, here are what the cost functions look like for window sizes of 8, 16, 32, and 64:. ![2](https://user-images.githubusercontent.com/11076296/29582011-210d37b8-8749-11e7-9383-0c657232347e.png). ![3](https://user-images.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/1107",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:3824,Usability,simpl,simple,3824," segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the relevant p-value thresholds to zero still yields segments.). Although the above procedure has a number of parameters that need to be chosen, in practice they are all straightforward and relatively easy to understand. Being a combination of local and global methods, it allows for multiscale sensitivity to small events while still allowing for sensible control of the final number of segments via the BIC-like penalty on the global cost. All algorithms used are linear complexity and are straightforward to implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206:288,Modifiability,rewrite,rewrite,288,"@davidbenjamin I thought you had implemented something a little more sophisticated initially, but then reverted to the ReCapSeg caller for some reason?. Anything that is relatively simple to implement yet sufficiently more principled than the ReCapSeg caller would be reasonable for this rewrite. Thought you might've had something that fit the bill originally, but maybe I'm remembering wrong. If so, then we can try leaving it as is for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206:181,Usability,simpl,simple,181,"@davidbenjamin I thought you had implemented something a little more sophisticated initially, but then reverted to the ReCapSeg caller for some reason?. Anything that is relatively simple to implement yet sufficiently more principled than the ReCapSeg caller would be reasonable for this rewrite. Thought you might've had something that fit the bill originally, but maybe I'm remembering wrong. If so, then we can try leaving it as is for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1333,Availability,down,down,1333,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:214,Deployability,pipeline,pipeline,214,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:939,Energy Efficiency,reduce,reduced,939,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1289,Performance,optimiz,optimization,1289,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:105,Testability,test,tests,105,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:141,Testability,test,tests,141,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:93,Usability,simpl,simple,93,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:259,Usability,simpl,simply,259,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1895,Availability,redundant,redundant,1895,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3944,Availability,down,down,3944,"mals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4820,Availability,error,error,4820,"IT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global paramete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4838,Availability,error,error,4838,"IT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global paramete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5745,Availability,down,downsample,5745,"nnot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5960,Availability,down,downsample,5960,"tire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:7608,Availability,down,down,7608,"imation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:8,Deployability,pipeline,pipeline,8,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:424,Deployability,pipeline,pipeline,424,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:713,Deployability,toggle,toggled,713,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1159,Deployability,update,update,1159,"entUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2300,Deployability,pipeline,pipeline,2300," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10751,Deployability,update,update,10751,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10946,Deployability,update,update,10946,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6221,Energy Efficiency,reduce,reduced,6221,"action model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:616,Integrability,depend,depending,616,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:9138,Integrability,depend,dependency,9138,"nds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally took several hours for a single WGS sample...) We can now make plotting much faster with the ordering enforced by `TSVLocatableCollection` (see below).; - There are now two plotting tools, `PlotDenoisedCopyRatios` and `PlotModeledSegments`. This is in contrast to the old `PlotSegmentedCopyRatio` and `PlotACNVResults`.; - Because `ModelSegments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10316,Integrability,interface,interface,10316,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10484,Integrability,depend,depending,10484,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1544,Performance,perform,performed,1544,"flow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] O",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1806,Performance,optimiz,optimizations,1806,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,Performance,perform,performed,2269," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3504,Performance,perform,perform,3504,"ld code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old all",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3795,Performance,bottleneck,bottleneck,3795,"ed to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4560,Performance,perform,perform,4560,"Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5690,Performance,bottleneck,bottleneck,5690,"ore modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density est",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6616,Performance,perform,performing,6616,"eed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6745,Performance,perform,perform,6745,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:7677,Performance,perform,performance,7677,"imation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1895,Safety,redund,redundant,1895,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4036,Safety,detect,detection,4036,"e to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:372,Security,validat,validation,372,"The new pipeline is in a complete state. Nearly all tools and scripts were rewritten, many from scratch. I've tried to minimize interaction with old `tools/exome` code (notably, `ReadCountCollection` and `SegmentUtils`). There are still some improvements that can be made (especially in segment union and the subsequent modeling), but we should be ready to go for a first validation. Some notes:. WDL:; - I've moved the old pipeline to `somatic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1437,Security,expose,exposed,1437,"atic_old` folders.; - There is now just a matched-pair workflow and a panel workflow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalizati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2899,Testability,test,tests,2899,"orcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2975,Testability,test,test,2975,"is TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segme",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6000,Testability,test,tests,6000,"consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6047,Testability,test,test,6047,"consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6704,Testability,log,logit,6704,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3676,Usability,simpl,simple,3676,"ssibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instea",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5426,Usability,learn,learn,5426,"off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5732,Usability,simpl,simply,5732,"nnot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5853,Usability,simpl,simple,5853,"tire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6594,Usability,simpl,simply,6594,"eed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:8299,Usability,simpl,simple,8299," some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally took several hours for a single WGS sample...) We can now make plotting much faster with the ordering enforced by `TSVLocatableCollection` (see below).; - There are now two plotting tools, `PlotDenoisedCopyRatios` and `PlotModeledSegments`. This is in contrast to the old `PlotSegmentedCopyRatio` and `PlotACNVResults`.; - Because `ModelSegments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:10099,Usability,simpl,simply,10099,"gments` optionally takes denoised copy ratio and/or allelic counts, `PlotModeledSegments` outputs only the corresponding plots appropriately.; - I added a dependency on the R package `data.table` to slightly speed up the reading of input files.; - Setting `pch="".""` also sped up the generation of scatter plots.; - Plotting now takes a couple of minutes, most of which is I/O (#3554).; - AAF (rather than MAF) is now plotted for allele fraction (#2957). Other:; - I've introduced a `LocatableCollection` class to unify how allelic counts, copy ratios, and segments are stored and read/written from/to TSV (#2836). Intervals are always output in lexicographical order for now, to be consistent with the old coverage collection (#2951). Once @asmirnov239's `CollectReadCounts` is in, we can change everything over to ordering determined by the sequence dictionary.; - Column headers and log2 copy ratio output have been standardized throughout (#2886).; - [x] I've also introduced a `NamedSampleFile` abstract class to tag files that have `#SAMPLE_NAME=...` as the first comment line. For `CollectAllelicCounts`, this simply uses code borrowed from `GetSampleName`. We should unify the reading and storing of sample names at some point (#2910).; - [x] We will need to replace `SimpleReadCountCollection` (which currently serves as the interface between the old coverage collection files and the new code) with one of these subclasses when `CollectReadCounts` is in. We can also change `NamedSampleFile` depending on what he's implemented.; - [x] We should eventually write proper SAM headers with useful tags to all TSV and HDF5 files generated by our tools that represent annotated intervals that can be associated with a single sample. Documentation:; - [x] I need to update class javadoc and example invocations throughout. The initial PR will already be quite massive, so I'll leave this until later. Perhaps @sooheelee might want to be involved?; - [ ] I will update the white paper at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828
https://github.com/broadinstitute/gatk/issues/2923#issuecomment-459396390:50,Usability,guid,guide,50,I think this should be contained in the WDL style guide.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2923#issuecomment-459396390
https://github.com/broadinstitute/gatk/issues/2944#issuecomment-474483603:190,Deployability,upgrade,upgrade,190,"Sorry, I know this is old, but i'm currently dealing with this exact issue using `gatk-4.beta.5`. It sounds like this has been solved, but the solution isn't clear to me. . EDIT: Perhaps an upgrade to 4.1 will solve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944#issuecomment-474483603
https://github.com/broadinstitute/gatk/issues/2944#issuecomment-474483603:158,Usability,clear,clear,158,"Sorry, I know this is old, but i'm currently dealing with this exact issue using `gatk-4.beta.5`. It sounds like this has been solved, but the solution isn't clear to me. . EDIT: Perhaps an upgrade to 4.1 will solve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944#issuecomment-474483603
https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:129,Safety,predict,predicts,129,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993
https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:68,Usability,learn,learn,68,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993
https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:194,Usability,learn,learned,194,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:94,Availability,error,error,94,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:5572,Availability,down,down,5572,"g traversal; 01:16:17.470 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 01:16:17.493 INFO ProgressMeter - unmapped 0.0 4 10434.8; 01:16:17.493 INFO ProgressMeter - Traversal complete. Processed 4 total variants in 0.0 minutes.; 01:16:17.493 INFO FilterByOrientationBias - Tagging whether genotypes are in one of the artifact modes.; 01:16:17.496 INFO ProgressMeter - Starting traversal; 01:16:17.496 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.497 INFO ProgressMeter - unmapped 0.0 8 480000.0; 01:16:17.498 INFO ProgressMeter - Traversal complete. Processed 8 total records in 0.0 minutes.; 01:16:17.500 INFO OrientationBiasFilterer - NORMAL: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - TUMOR: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - Updating genotypes and creating final list of variants...; 01:16:17.500 INFO ProgressMeter - Starting traversal; 01:16:17.501 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.501 INFO ProgressMeter - unmapped 0.0 4 Infinity; 01:16:17.501 INFO ProgressMeter - Traversal complete. Processed 4 total records in 0.0 minutes.; 01:16:17.501 INFO FilterByOrientationBias - Writing variants to VCF...; 01:16:17.512 INFO FilterByOrientationBias - Writing a simple summary table...; 01:16:17.576 INFO FilterByOrientationBias - Shutting down engine; [June 6, 2017 1:16:17 AM EDT] org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=232259584; WMCF9-CB5:hellbender shlee$ ; ```. This is the case for the file that I augmented with what the sed command was meant to do. Otherwise, the command errors. I think the tool should be able to take CollectSequencingArtifactMetrics whether run using Picard or GATK. I say this since folks may have these metrics from old runs before the as-of-yet-available Picard tools in the GATK jar.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:5897,Availability,error,errors,5897,"g traversal; 01:16:17.470 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 01:16:17.493 INFO ProgressMeter - unmapped 0.0 4 10434.8; 01:16:17.493 INFO ProgressMeter - Traversal complete. Processed 4 total variants in 0.0 minutes.; 01:16:17.493 INFO FilterByOrientationBias - Tagging whether genotypes are in one of the artifact modes.; 01:16:17.496 INFO ProgressMeter - Starting traversal; 01:16:17.496 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.497 INFO ProgressMeter - unmapped 0.0 8 480000.0; 01:16:17.498 INFO ProgressMeter - Traversal complete. Processed 8 total records in 0.0 minutes.; 01:16:17.500 INFO OrientationBiasFilterer - NORMAL: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - TUMOR: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - Updating genotypes and creating final list of variants...; 01:16:17.500 INFO ProgressMeter - Starting traversal; 01:16:17.501 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.501 INFO ProgressMeter - unmapped 0.0 4 Infinity; 01:16:17.501 INFO ProgressMeter - Traversal complete. Processed 4 total records in 0.0 minutes.; 01:16:17.501 INFO FilterByOrientationBias - Writing variants to VCF...; 01:16:17.512 INFO FilterByOrientationBias - Writing a simple summary table...; 01:16:17.576 INFO FilterByOrientationBias - Shutting down engine; [June 6, 2017 1:16:17 AM EDT] org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=232259584; WMCF9-CB5:hellbender shlee$ ; ```. This is the case for the file that I augmented with what the sed command was meant to do. Otherwise, the command errors. I think the tool should be able to take CollectSequencingArtifactMetrics whether run using Picard or GATK. I say this since folks may have these metrics from old runs before the as-of-yet-available Picard tools in the GATK jar.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:6093,Availability,avail,available,6093,"g traversal; 01:16:17.470 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 01:16:17.493 INFO ProgressMeter - unmapped 0.0 4 10434.8; 01:16:17.493 INFO ProgressMeter - Traversal complete. Processed 4 total variants in 0.0 minutes.; 01:16:17.493 INFO FilterByOrientationBias - Tagging whether genotypes are in one of the artifact modes.; 01:16:17.496 INFO ProgressMeter - Starting traversal; 01:16:17.496 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.497 INFO ProgressMeter - unmapped 0.0 8 480000.0; 01:16:17.498 INFO ProgressMeter - Traversal complete. Processed 8 total records in 0.0 minutes.; 01:16:17.500 INFO OrientationBiasFilterer - NORMAL: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - TUMOR: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - Updating genotypes and creating final list of variants...; 01:16:17.500 INFO ProgressMeter - Starting traversal; 01:16:17.501 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.501 INFO ProgressMeter - unmapped 0.0 4 Infinity; 01:16:17.501 INFO ProgressMeter - Traversal complete. Processed 4 total records in 0.0 minutes.; 01:16:17.501 INFO FilterByOrientationBias - Writing variants to VCF...; 01:16:17.512 INFO FilterByOrientationBias - Writing a simple summary table...; 01:16:17.576 INFO FilterByOrientationBias - Shutting down engine; [June 6, 2017 1:16:17 AM EDT] org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=232259584; WMCF9-CB5:hellbender shlee$ ; ```. This is the case for the file that I augmented with what the sed command was meant to do. Otherwise, the command errors. I think the tool should be able to take CollectSequencingArtifactMetrics whether run using Picard or GATK. I say this since folks may have these metrics from old runs before the as-of-yet-available Picard tools in the GATK jar.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:485,Deployability,install,install,485,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:567,Deployability,install,install,567,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:1024,Deployability,install,install,1024," was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:421,Integrability,wrap,wrapper,421,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:5494,Usability,simpl,simple,5494,"g traversal; 01:16:17.470 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 01:16:17.493 INFO ProgressMeter - unmapped 0.0 4 10434.8; 01:16:17.493 INFO ProgressMeter - Traversal complete. Processed 4 total variants in 0.0 minutes.; 01:16:17.493 INFO FilterByOrientationBias - Tagging whether genotypes are in one of the artifact modes.; 01:16:17.496 INFO ProgressMeter - Starting traversal; 01:16:17.496 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.497 INFO ProgressMeter - unmapped 0.0 8 480000.0; 01:16:17.498 INFO ProgressMeter - Traversal complete. Processed 8 total records in 0.0 minutes.; 01:16:17.500 INFO OrientationBiasFilterer - NORMAL: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - TUMOR: Nothing to filter.; 01:16:17.500 INFO OrientationBiasFilterer - Updating genotypes and creating final list of variants...; 01:16:17.500 INFO ProgressMeter - Starting traversal; 01:16:17.501 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 01:16:17.501 INFO ProgressMeter - unmapped 0.0 4 Infinity; 01:16:17.501 INFO ProgressMeter - Traversal complete. Processed 4 total records in 0.0 minutes.; 01:16:17.501 INFO FilterByOrientationBias - Writing variants to VCF...; 01:16:17.512 INFO FilterByOrientationBias - Writing a simple summary table...; 01:16:17.576 INFO FilterByOrientationBias - Shutting down engine; [June 6, 2017 1:16:17 AM EDT] org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=232259584; WMCF9-CB5:hellbender shlee$ ; ```. This is the case for the file that I augmented with what the sed command was meant to do. Otherwise, the command errors. I think the tool should be able to take CollectSequencingArtifactMetrics whether run using Picard or GATK. I say this since folks may have these metrics from old runs before the as-of-yet-available Picard tools in the GATK jar.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891
https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3041?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@9ca461c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3041 +/- ##; ==========================================; Coverage ? 79.996% ; Complexity ? 16751 ; ==========================================; Files ? 1139 ; Lines ? 60989 ; Branches ? 9443 ; ==========================================; Hits ? 48789 ; Misses ? 8403 ; Partials ? 3797; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934
https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3041?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@9ca461c`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3041 +/- ##; ==========================================; Coverage ? 79.996% ; Complexity ? 16751 ; ==========================================; Files ? 1139 ; Lines ? 60989 ; Branches ? 9443 ; ==========================================; Hits ? 48789 ; Misses ? 8403 ; Partials ? 3797; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3041#issuecomment-306570934
https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@6f5bab9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3044 +/- ##; ==========================================; Coverage ? 80.026% ; Complexity ? 16934 ; ==========================================; Files ? 1142 ; Lines ? 61616 ; Branches ? 9594 ; ==========================================; Hits ? 49309 ; Misses ? 8476 ; Partials ? 3831; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `71.622% <100%> (ø)` | `34 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461
https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@6f5bab9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3044 +/- ##; ==========================================; Coverage ? 80.026% ; Complexity ? 16934 ; ==========================================; Files ? 1142 ; Lines ? 61616 ; Branches ? 9594 ; ==========================================; Hits ? 49309 ; Misses ? 8476 ; Partials ? 3831; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3044?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `71.622% <100%> (ø)` | `34 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3044#issuecomment-306604461
https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307794543:223,Availability,error,error-prone,223,"Have to disagree with you on that point, @magicDGS. Comma-separated values for lists seems like the most straightforward/simple/human-editable approach, whereas the other options seem more complex (and therefore more messy/error-prone). (Unless it turns out that we need nested lists, which I'm hoping is not the case!)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307794543
https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307794543:121,Usability,simpl,simple,121,"Have to disagree with you on that point, @magicDGS. Comma-separated values for lists seems like the most straightforward/simple/human-editable approach, whereas the other options seem more complex (and therefore more messy/error-prone). (Unless it turns out that we need nested lists, which I'm hoping is not the case!)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307794543
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:527,Deployability,configurat,configuration,527,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,Energy Efficiency,adapt,adapting,197,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,Modifiability,adapt,adapting,197,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:527,Modifiability,config,configuration,527,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:25,Usability,simpl,simply,25,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:83,Availability,failure,failures,83,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:16,Deployability,upgrade,upgrade,16,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:398,Deployability,upgrade,upgrade,398,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:37,Integrability,depend,dependency,37,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:134,Integrability,depend,dependencies,134,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:78,Testability,test,test,78,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:292,Testability,test,tests,292,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:185,Usability,clear,clear,185,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:914,Availability,avail,available,914,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:583,Deployability,release,release,583,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:816,Deployability,update,update,816,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:907,Modifiability,plugin,plugin,907,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:942,Modifiability,plugin,plugin,942,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:113,Testability,test,testing,113,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:17,Usability,simpl,simple,17,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:416,Usability,simpl,simple,416,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834
https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1303,Modifiability,plugin,plugin,1303,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988
https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1036,Security,access,access,1036,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988
https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:9,Usability,clear,clear,9,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988
https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319602267:52,Usability,guid,guides,52,"As I understand, there are two ways:; 1) Update all guides that include tools not ported to GATK4 so users could use GATK4 to get the results as they did earlier.; 2) Add all tools from GATK3.6 to GATK4. Otherwise non official forks will appear.. For now could you please add all tools to GATK4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319602267
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:1017,Availability,down,downstream,1017,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:435,Deployability,pipeline,pipeline,435,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:801,Testability,test,test,801,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:1349,Testability,test,tests,1349,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:903,Usability,usab,usable,903,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2221,Availability,avail,available,2221,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:443,Deployability,pipeline,pipeline,443,"@sooheelee - I think that to have a proper test suite similar to GATK's 3, we need also some test that exercise some code paths that requires some specific scientifically meaningful data (for example, known indels that are also included in some sample). I am not that familiar with the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2181,Deployability,integrat,integration,2181,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2181,Integrability,integrat,integration,2181,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1619,Performance,perform,performed,1619,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:43,Testability,test,test,43,"@sooheelee - I think that to have a proper test suite similar to GATK's 3, we need also some test that exercise some code paths that requires some specific scientifically meaningful data (for example, known indels that are also included in some sample). I am not that familiar with the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:93,Testability,test,test,93,"@sooheelee - I think that to have a proper test suite similar to GATK's 3, we need also some test that exercise some code paths that requires some specific scientifically meaningful data (for example, known indels that are also included in some sample). I am not that familiar with the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:286,Testability,test,test,286,"@sooheelee - I think that to have a proper test suite similar to GATK's 3, we need also some test that exercise some code paths that requires some specific scientifically meaningful data (for example, known indels that are also included in some sample). I am not that familiar with the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1286,Testability,test,test,1286,"h the test data on this or GATK3 repository, so I am not sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2193,Testability,test,tests,2193,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1952,Usability,simpl,simpler,1952,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:2174,Usability,simpl,simple,2174,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560:225,Testability,test,test,225,"I have to say, too bad we don't have a mechanism in place that allows for the full reference, e.g. NIO only the contigs or portions thereof that are needed for a particular analysis @droazen @cmnbroad. That would make making test data so much easier. I would imagine this is simple to implement, given the reference is indexed. Such a feature would be useful for cloud analyses. I have to jump through ridiculous hoops to make small test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560:433,Testability,test,test,433,"I have to say, too bad we don't have a mechanism in place that allows for the full reference, e.g. NIO only the contigs or portions thereof that are needed for a particular analysis @droazen @cmnbroad. That would make making test data so much easier. I would imagine this is simple to implement, given the reference is indexed. Such a feature would be useful for cloud analyses. I have to jump through ridiculous hoops to make small test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560:275,Usability,simpl,simple,275,"I have to say, too bad we don't have a mechanism in place that allows for the full reference, e.g. NIO only the contigs or portions thereof that are needed for a particular analysis @droazen @cmnbroad. That would make making test data so much easier. I would imagine this is simple to implement, given the reference is indexed. Such a feature would be useful for cloud analyses. I have to jump through ridiculous hoops to make small test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373131560
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:158,Deployability,update,updates,158,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:1112,Security,validat,validation,1112,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:66,Testability,test,test,66,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:107,Testability,test,test,107,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:178,Testability,test,test,178,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:294,Testability,test,tests,294,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:774,Testability,test,testing,774,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:520,Usability,usab,usable,520,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600
https://github.com/broadinstitute/gatk/pull/3124#issuecomment-309833042:23,Usability,feedback,feedback,23,I've incorporated your feedback @davidbenjamin. Many thanks for the review. Waiting for checks to pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124#issuecomment-309833042
https://github.com/broadinstitute/gatk/pull/3146#issuecomment-310676868:23,Usability,feedback,feedback,23,I've incorporated your feedback @samuelklee. Thanks for the review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-310676868
https://github.com/broadinstitute/gatk/issues/3151#issuecomment-356736140:15,Usability,learn,learned,15,"I did this and learned some things. However, it will be easier to evaluate the impact of GC correction with a standard evaluation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3151#issuecomment-356736140
https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:24,Availability,error,error,24,"> . @lbergelson Yes the error was from a previous version as I didn't have the new error recorded. I ran it again and here is the code and error. Just for clarity, I simplified the code paste here, removed long paths and stuff but some of those are shown in the error. Thanks!. `java -jar /home/apps/software/picard/2.27.5-Java-1.8.0_201/picard.jar CollectInsertSizeMetrics \; I=HG03125.final.cram \; O=insertSize_metrics.txt \; H=insertSize_hist.pdf \; R=GRCh38_full_analysis_set_plus_decoy_hla.fa \; M=0.5`. `[Thu Feb 02 13:05:04 CST 2023] picard.analysis.CollectInsertSizeMetrics HISTOGRAM_FILE=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_hist.pdf MINIMUM_PCT=0.5 INPUT=./crams/AFR/HG03125.final.cram OUTPUT=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_metrics.txt REFERENCE_SEQUENCE=/home/groups/h3abionet/RefGraph/data/genomes/human/GRCh38/GRCh38_full_analysis_set_plus_decoy_hla.fa DEVIATIONS=10.0 METRIC_ACCUMULATION_LEVEL=[ALL_READS] INCLUDE_DUPLICATES=false ASSUME_SORTED=true STOP_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Thu Feb 02 13:05:04 CST 2023] Executing as valizad2@compute-5-1 on Linux 4.18.0-348.23.1.el8_5.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Picard version: 2.9.0-1-gf5b9f50-SNAPSHOT; INFO	2023-02-02 13:05:15	SinglePassSamProgram	Processed 1,000,000 records. Elapsed time: 00:00:11s. Time for last 1,000,000: 6s. Last read position: chr1:3,185,445; INFO	2023-02-02 13:05:20	SinglePassSamProgram	Processed 2,000,000 records. Elapsed time: 00:00:16s. Time for last 1,000,000: 5s. Last read position: chr1:6,814,058; INFO	2023-02-02 13:05:25	SinglePassSamProgram	Processed 3,000,000 records. Elapsed time: 00:00:21s. Time for last 1,000,000: 4s. Last read position: chr1:10,463,599; INFO	2023-02-02 13:05:30	SinglePassSamProgram	Processed 4,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417
https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:83,Availability,error,error,83,"> . @lbergelson Yes the error was from a previous version as I didn't have the new error recorded. I ran it again and here is the code and error. Just for clarity, I simplified the code paste here, removed long paths and stuff but some of those are shown in the error. Thanks!. `java -jar /home/apps/software/picard/2.27.5-Java-1.8.0_201/picard.jar CollectInsertSizeMetrics \; I=HG03125.final.cram \; O=insertSize_metrics.txt \; H=insertSize_hist.pdf \; R=GRCh38_full_analysis_set_plus_decoy_hla.fa \; M=0.5`. `[Thu Feb 02 13:05:04 CST 2023] picard.analysis.CollectInsertSizeMetrics HISTOGRAM_FILE=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_hist.pdf MINIMUM_PCT=0.5 INPUT=./crams/AFR/HG03125.final.cram OUTPUT=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_metrics.txt REFERENCE_SEQUENCE=/home/groups/h3abionet/RefGraph/data/genomes/human/GRCh38/GRCh38_full_analysis_set_plus_decoy_hla.fa DEVIATIONS=10.0 METRIC_ACCUMULATION_LEVEL=[ALL_READS] INCLUDE_DUPLICATES=false ASSUME_SORTED=true STOP_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Thu Feb 02 13:05:04 CST 2023] Executing as valizad2@compute-5-1 on Linux 4.18.0-348.23.1.el8_5.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Picard version: 2.9.0-1-gf5b9f50-SNAPSHOT; INFO	2023-02-02 13:05:15	SinglePassSamProgram	Processed 1,000,000 records. Elapsed time: 00:00:11s. Time for last 1,000,000: 6s. Last read position: chr1:3,185,445; INFO	2023-02-02 13:05:20	SinglePassSamProgram	Processed 2,000,000 records. Elapsed time: 00:00:16s. Time for last 1,000,000: 5s. Last read position: chr1:6,814,058; INFO	2023-02-02 13:05:25	SinglePassSamProgram	Processed 3,000,000 records. Elapsed time: 00:00:21s. Time for last 1,000,000: 4s. Last read position: chr1:10,463,599; INFO	2023-02-02 13:05:30	SinglePassSamProgram	Processed 4,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417
https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:139,Availability,error,error,139,"> . @lbergelson Yes the error was from a previous version as I didn't have the new error recorded. I ran it again and here is the code and error. Just for clarity, I simplified the code paste here, removed long paths and stuff but some of those are shown in the error. Thanks!. `java -jar /home/apps/software/picard/2.27.5-Java-1.8.0_201/picard.jar CollectInsertSizeMetrics \; I=HG03125.final.cram \; O=insertSize_metrics.txt \; H=insertSize_hist.pdf \; R=GRCh38_full_analysis_set_plus_decoy_hla.fa \; M=0.5`. `[Thu Feb 02 13:05:04 CST 2023] picard.analysis.CollectInsertSizeMetrics HISTOGRAM_FILE=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_hist.pdf MINIMUM_PCT=0.5 INPUT=./crams/AFR/HG03125.final.cram OUTPUT=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_metrics.txt REFERENCE_SEQUENCE=/home/groups/h3abionet/RefGraph/data/genomes/human/GRCh38/GRCh38_full_analysis_set_plus_decoy_hla.fa DEVIATIONS=10.0 METRIC_ACCUMULATION_LEVEL=[ALL_READS] INCLUDE_DUPLICATES=false ASSUME_SORTED=true STOP_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Thu Feb 02 13:05:04 CST 2023] Executing as valizad2@compute-5-1 on Linux 4.18.0-348.23.1.el8_5.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Picard version: 2.9.0-1-gf5b9f50-SNAPSHOT; INFO	2023-02-02 13:05:15	SinglePassSamProgram	Processed 1,000,000 records. Elapsed time: 00:00:11s. Time for last 1,000,000: 6s. Last read position: chr1:3,185,445; INFO	2023-02-02 13:05:20	SinglePassSamProgram	Processed 2,000,000 records. Elapsed time: 00:00:16s. Time for last 1,000,000: 5s. Last read position: chr1:6,814,058; INFO	2023-02-02 13:05:25	SinglePassSamProgram	Processed 3,000,000 records. Elapsed time: 00:00:21s. Time for last 1,000,000: 4s. Last read position: chr1:10,463,599; INFO	2023-02-02 13:05:30	SinglePassSamProgram	Processed 4,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417
https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:262,Availability,error,error,262,"> . @lbergelson Yes the error was from a previous version as I didn't have the new error recorded. I ran it again and here is the code and error. Just for clarity, I simplified the code paste here, removed long paths and stuff but some of those are shown in the error. Thanks!. `java -jar /home/apps/software/picard/2.27.5-Java-1.8.0_201/picard.jar CollectInsertSizeMetrics \; I=HG03125.final.cram \; O=insertSize_metrics.txt \; H=insertSize_hist.pdf \; R=GRCh38_full_analysis_set_plus_decoy_hla.fa \; M=0.5`. `[Thu Feb 02 13:05:04 CST 2023] picard.analysis.CollectInsertSizeMetrics HISTOGRAM_FILE=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_hist.pdf MINIMUM_PCT=0.5 INPUT=./crams/AFR/HG03125.final.cram OUTPUT=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_metrics.txt REFERENCE_SEQUENCE=/home/groups/h3abionet/RefGraph/data/genomes/human/GRCh38/GRCh38_full_analysis_set_plus_decoy_hla.fa DEVIATIONS=10.0 METRIC_ACCUMULATION_LEVEL=[ALL_READS] INCLUDE_DUPLICATES=false ASSUME_SORTED=true STOP_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Thu Feb 02 13:05:04 CST 2023] Executing as valizad2@compute-5-1 on Linux 4.18.0-348.23.1.el8_5.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Picard version: 2.9.0-1-gf5b9f50-SNAPSHOT; INFO	2023-02-02 13:05:15	SinglePassSamProgram	Processed 1,000,000 records. Elapsed time: 00:00:11s. Time for last 1,000,000: 6s. Last read position: chr1:3,185,445; INFO	2023-02-02 13:05:20	SinglePassSamProgram	Processed 2,000,000 records. Elapsed time: 00:00:16s. Time for last 1,000,000: 5s. Last read position: chr1:6,814,058; INFO	2023-02-02 13:05:25	SinglePassSamProgram	Processed 3,000,000 records. Elapsed time: 00:00:21s. Time for last 1,000,000: 4s. Last read position: chr1:10,463,599; INFO	2023-02-02 13:05:30	SinglePassSamProgram	Processed 4,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417
https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:166,Usability,simpl,simplified,166,"> . @lbergelson Yes the error was from a previous version as I didn't have the new error recorded. I ran it again and here is the code and error. Just for clarity, I simplified the code paste here, removed long paths and stuff but some of those are shown in the error. Thanks!. `java -jar /home/apps/software/picard/2.27.5-Java-1.8.0_201/picard.jar CollectInsertSizeMetrics \; I=HG03125.final.cram \; O=insertSize_metrics.txt \; H=insertSize_hist.pdf \; R=GRCh38_full_analysis_set_plus_decoy_hla.fa \; M=0.5`. `[Thu Feb 02 13:05:04 CST 2023] picard.analysis.CollectInsertSizeMetrics HISTOGRAM_FILE=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_hist.pdf MINIMUM_PCT=0.5 INPUT=./crams/AFR/HG03125.final.cram OUTPUT=results/bowtie2/assembly/Read-Prep/Bowtie/Aligned/stats/HG03125_insertSize_metrics.txt REFERENCE_SEQUENCE=/home/groups/h3abionet/RefGraph/data/genomes/human/GRCh38/GRCh38_full_analysis_set_plus_decoy_hla.fa DEVIATIONS=10.0 METRIC_ACCUMULATION_LEVEL=[ALL_READS] INCLUDE_DUPLICATES=false ASSUME_SORTED=true STOP_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Thu Feb 02 13:05:04 CST 2023] Executing as valizad2@compute-5-1 on Linux 4.18.0-348.23.1.el8_5.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_201-b09; Picard version: 2.9.0-1-gf5b9f50-SNAPSHOT; INFO	2023-02-02 13:05:15	SinglePassSamProgram	Processed 1,000,000 records. Elapsed time: 00:00:11s. Time for last 1,000,000: 6s. Last read position: chr1:3,185,445; INFO	2023-02-02 13:05:20	SinglePassSamProgram	Processed 2,000,000 records. Elapsed time: 00:00:16s. Time for last 1,000,000: 5s. Last read position: chr1:6,814,058; INFO	2023-02-02 13:05:25	SinglePassSamProgram	Processed 3,000,000 records. Elapsed time: 00:00:21s. Time for last 1,000,000: 4s. Last read position: chr1:10,463,599; INFO	2023-02-02 13:05:30	SinglePassSamProgram	Processed 4,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417
https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311458151:29,Usability,feedback,feedback,29,@samuelklee Expanded on your feedback. Let me know if the changes are okay.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311458151
https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3158?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3158 +/- ##; =========================================; Coverage ? 9.901% ; Complexity ? 2034 ; =========================================; Files ? 1145 ; Lines ? 61641 ; Branches ? 9606 ; =========================================; Hits ? 6103 ; Misses ? 54604 ; Partials ? 934; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489
https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3158?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3158 +/- ##; =========================================; Coverage ? 9.901% ; Complexity ? 2034 ; =========================================; Files ? 1145 ; Lines ? 61641 ; Branches ? 9606 ; =========================================; Hits ? 6103 ; Misses ? 54604 ; Partials ? 934; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310720489
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3159 +/- ##; ==========================================; Coverage ? 62.624% ; Complexity ? 12641 ; ==========================================; Files ? 1145 ; Lines ? 61646 ; Branches ? 9606 ; ==========================================; Hits ? 38605 ; Misses ? 19096 ; Partials ? 3945; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `68.224% <100%> (ø)` | `22 <0> (?)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `69.231% <100%> (ø)` | `10 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@8ab015d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3159 +/- ##; ==========================================; Coverage ? 62.624% ; Complexity ? 12641 ; ==========================================; Files ? 1145 ; Lines ? 61646 ; Branches ? 9606 ; ==========================================; Hits ? 38605 ; Misses ? 19096 ; Partials ? 3945; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `68.224% <100%> (ø)` | `22 <0> (?)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3159?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `69.231% <100%> (ø)` | `10 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310752620
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:299,Availability,down,downloaded,299,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:129,Integrability,depend,dependencies,129,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:363,Integrability,depend,dependencies,363,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:88,Testability,log,log,88,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:112,Testability,test,test,112,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:278,Testability,test,test,278,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:327,Testability,test,test,327,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:336,Testability,test,tests,336,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:378,Testability,test,testRuntime,378,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:391,Testability,test,test,391,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:57,Usability,clear,clear,57,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367
https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311465823:19,Usability,simpl,simpler,19,Would it be better/simpler to just have both `./gradlew gatkDoc` and `./gradlew phpDoc`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311465823
https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311473535:183,Usability,intuit,intuitive,183,"@droazen re: your earlier question, I think it's preferable to use the same base command and add a qualifier -- we may add other output format shortcuts in future, and I find it more intuitive to specify the format as an extra arg rather than a different target altogether.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311473535
https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566:18,Testability,test,test,18,"I am working on a test for #1572. I am not sure what a test for #3069 would look like, or if it is really necessary. We simply changed the way GKL outputs warnings and information. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566
https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566:55,Testability,test,test,55,"I am working on a test for #1572. I am not sure what a test for #3069 would look like, or if it is really necessary. We simply changed the way GKL outputs warnings and information. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566
https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566:120,Usability,simpl,simply,120,"I am working on a test for #1572. I am not sure what a test for #3069 would look like, or if it is really necessary. We simply changed the way GKL outputs warnings and information. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312295566
https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358768726:21,Usability,clear,clear,21,"@yfarjoun Just to be clear, are you saying that `getMateAlignmentEnd() + 1` is ideal for forward strand reads but `read.getStart() + abs(read.getFragmentLength())` will have to do if the `MC` tag is missing, and that we can leave it as `getMateStart() + 1` for reverse strand reads?. @droazen A priori I would expect the additional cost of parsing each read's mate CIGAR to be negligible compared to other stuff we do but I also understand the virtue of being careful. Given, however, that this is invoked for every assembled read in HaplotypeCaller it should suffice just to measure the wall clock time of HaplotypeCaller. Would seeing no change there be enough?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358768726
https://github.com/broadinstitute/gatk/pull/3195#issuecomment-327440250:268,Usability,clear,clear,268,"Next commit includes:. * Change the name to PrimaryLineReadFilter; * Remove impl notes completely, because if they aren't tags, they will be populated to the user documentation. With the name change, I believe that it isn't necessary anymore: with the current text is clear that the concept of primary alignment is more stringent for this filter, the name change clarify that it is a different filter than the previous GATK versions, and the name of HTSJDK flag is also different. Back to you @cmnbroad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3195#issuecomment-327440250
https://github.com/broadinstitute/gatk/pull/3195#issuecomment-329124206:61,Usability,feedback,feedback,61,Thanks you for accepting it... and all the reviewers for the feedback!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3195#issuecomment-329124206
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:299,Availability,down,downsampling,299,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:470,Availability,down,downsampling,470,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:897,Availability,down,downsampling,897,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:1104,Availability,down,downsampling,1104,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:340,Deployability,patch,patch,340,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:346,Energy Efficiency,reduce,reduces,346,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:529,Testability,test,test,529,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:726,Testability,benchmark,benchmarking,726,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:739,Testability,test,test,739,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233:1078,Usability,simpl,simplified,1078,"@davidbenjamin As discussed in person, it's my hope that the `AssemblyRegionWalker` changes in https://github.com/broadinstitute/gatk/commit/1ef09b3ca265209e0777c77a8519da74480908ce (which have now been merged into master!) will address `Mutect2` memory usage, and make these somewhat confusing new downsampling arguments unnecessary. That patch reduces the number of reads stored in memory at once by the engine by roughly an order of magnitude without doing any extra downsampling at all. I suggest that we do an evaluation to test whether this really resolves the issues you encountered. `Mutect2` is already hooked up to the new, lower-memory traversal code in the latest gatk/master, so all you have to do is re-run your benchmarking test. I'd suggest that you:. 1. Run with default settings in the latest master, and see if that alone does the trick!. 2. If not, try turning up the existing downsampling a bit. Eg., run with `--maxReadsPerAlignmentStart 10` instead of the default of 50. 3. If that still doesn't resolve the problem, we can revisit this PR and consider a simplified version of the downsampling args here for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-325073233
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629:28,Availability,failure,failures,28,"The patch clears up the 503 failures due to `fetchSize()`, but we are STILL seeing 503's with other metadata operations such as `Files.exists()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:586); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:428); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:217); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:223); ```. I'm going to continue modifying the patch until we see all 503s go away, then post here once it's ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629:4,Deployability,patch,patch,4,"The patch clears up the 503 failures due to `fetchSize()`, but we are STILL seeing 503's with other metadata operations such as `Files.exists()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:586); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:428); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:217); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:223); ```. I'm going to continue modifying the patch until we see all 503s go away, then post here once it's ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629:1241,Deployability,patch,patch,1241,"The patch clears up the 503 failures due to `fetchSize()`, but we are STILL seeing 503's with other metadata operations such as `Files.exists()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:586); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:428); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:217); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:223); ```. I'm going to continue modifying the patch until we see all 503s go away, then post here once it's ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629:10,Usability,clear,clears,10,"The patch clears up the 503 failures due to `fetchSize()`, but we are STILL seeing 503's with other metadata operations such as `Files.exists()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:586); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:428); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:217); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:223); ```. I'm going to continue modifying the patch until we see all 503s go away, then post here once it's ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923:116,Availability,failure,failures,116,"@jean-philippe-martin Yeah, I was being a bit over-aggressive with the retries to maximize my chances of fixing the failures. We could make the retries conditional, and I did extract `CloudStorageRetryHandler.isRetryable()` and `CloudStorageRetryHandler.isReopenable()` methods, but are we 100% sure that in `CloudStorageFileSystemProvider` we wouldn't want to retry any of the errors that in `CloudStorageReadChannel` result in a reopen? That wasn't clear to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923:378,Availability,error,errors,378,"@jean-philippe-martin Yeah, I was being a bit over-aggressive with the retries to maximize my chances of fixing the failures. We could make the retries conditional, and I did extract `CloudStorageRetryHandler.isRetryable()` and `CloudStorageRetryHandler.isReopenable()` methods, but are we 100% sure that in `CloudStorageFileSystemProvider` we wouldn't want to retry any of the errors that in `CloudStorageReadChannel` result in a reopen? That wasn't clear to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923
https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923:451,Usability,clear,clear,451,"@jean-philippe-martin Yeah, I was being a bit over-aggressive with the retries to maximize my chances of fixing the failures. We could make the retries conditional, and I did extract `CloudStorageRetryHandler.isRetryable()` and `CloudStorageRetryHandler.isReopenable()` methods, but are we 100% sure that in `CloudStorageFileSystemProvider` we wouldn't want to retry any of the errors that in `CloudStorageReadChannel` result in a reopen? That wasn't clear to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315471923
https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:74,Modifiability,plugin,plugin,74,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230
https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:138,Usability,usab,usable,138,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:149,Availability,down,downstream,149,"A couple of months ago I made an attempt to factor SATagBuilder out to a; public place and change its API to be a little more friendly to different; downstream uses, but I found some unexpected behavior in how it was parsing; SA tags that made me give up for fear of breaking the tools that already; rely on it. Within the SV tools, I'm currently working on a branch in which; I've written some code to parse SA tags. Perhaps we can work out what we; both need from a shared API and implement that. On Fri, Jul 21, 2017 at 12:56 PM, Valentin Ruano Rubio <; notifications@github.com> wrote:. > I have to deal with this component recently and I found the design rather; > awkward.... In general between GATK and htsjdk we don't seem to have a; > proper support for managing and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at som",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:2109,Availability,error,error,2109,"naging and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3324>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZft11VTCtCHT_xr89kPL7hMFYQyhks5sQNghgaJpZM4Ofpkb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:1573,Deployability,update,updates,1573,"th this component recently and I found the design rather; > awkward.... In general between GATK and htsjdk we don't seem to have a; > proper support for managing and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:2115,Integrability,message,message,2115,"naging and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3324>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZft11VTCtCHT_xr89kPL7hMFYQyhks5sQNghgaJpZM4Ofpkb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:2350,Security,validat,validate,2350,"naging and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3324>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZft11VTCtCHT_xr89kPL7hMFYQyhks5sQNghgaJpZM4Ofpkb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323:1530,Usability,clear,clearing,1530," Fri, Jul 21, 2017 at 12:56 PM, Valentin Ruano Rubio <; notifications@github.com> wrote:. > I have to deal with this component recently and I found the design rather; > awkward.... In general between GATK and htsjdk we don't seem to have a; > proper support for managing and querying Supplementary alignment; > information from read alignment records:; >; > Some of the things that I think smell:; >; > 1.; >; > Querying: implemented in htsjdk consists in forging artificial; > SAMRecords that contain only the alignment info in the SA tag element... It; > seems to me that it makes more sense to create class to hold this; > information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder; > already has defined a private inner class with that in mind ""SARead"" so why; > not flesh it out and make it public.; > 2.; >; > Writing: currently SATagBuilder gets attached to a read, parsing its; > current SA attribute content into SARead instances. It provides the; > possibility adding additional SAM record one by one or clearing the list.; > ... then it actually updates the SA attribute on the original read when a; > method (setTag) is explicitly called.; >; > I don't see the need to attach the SATag Builder to a read... it could; > perfectly be free standing; the same builder could be re-apply to several; > reads for that matter and I don't see any gain in hiding the read SA tag; > setting process,... even if typically this builder output would go to the; > ""SA"" tag, perhaps at some point we would like to also write SA coordinate; > list somewhere else, some other tag name or perhaps an error message... why; > impose this single purpose limitation?; >; > I suggest to drop the notion of a builder for a more general custom; > ReadAlignmentInfo (or whatever name) list. Such list could be making; > reference to a dictionary to validate its elements, prevent duplicates,; > keep the primary SA in the first position... etc.; >; > —; > You are receiving this because you are subscr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324#issuecomment-317065323
https://github.com/broadinstitute/gatk/issues/3334#issuecomment-353612077:10,Usability,guid,guidance,10,Requested guidance (in a different ticket) on how this works when running locally.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3334#issuecomment-353612077
https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301:379,Testability,test,testing,379,"@kgururaj and @kdatta Here's some GVCF data for a trio of samples, each called to haploid (ploidy 1) and tetraploid (4) genotypes. I included the reference (just chromosome 20) and the intervals list. This is data from one of our workshop tutorials so many of the intervals in the list I used don't have any data (so lots of no-calls) but there should be enough usable calls for testing purposes. Let me know if this isn't sufficient to get you started. . Thanks for looking into this, it's very important for a non-trivial subset of our users. . [genomicsdb.zip](https://github.com/broadinstitute/gatk/files/1171416/genomicsdb.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301
https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301:362,Usability,usab,usable,362,"@kgururaj and @kdatta Here's some GVCF data for a trio of samples, each called to haploid (ploidy 1) and tetraploid (4) genotypes. I included the reference (just chromosome 20) and the intervals list. This is data from one of our workshop tutorials so many of the intervals in the list I used don't have any data (so lots of no-calls) but there should be enough usable calls for testing purposes. Let me know if this isn't sufficient to get you started. . Thanks for looking into this, it's very important for a non-trivial subset of our users. . [genomicsdb.zip](https://github.com/broadinstitute/gatk/files/1171416/genomicsdb.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3342#issuecomment-317577301
https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655:88,Availability,ping,ping,88,"If anyone wants to learn more about the horrors of HLA (and MHC more generally) naming, ping me elsewhere, probably best at https://github.com/nmdp-bioinformatics/genotype-list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655
https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655:19,Usability,learn,learn,19,"If anyone wants to learn more about the horrors of HLA (and MHC more generally) naming, ping me elsewhere, probably best at https://github.com/nmdp-bioinformatics/genotype-list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655
https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249:362,Integrability,depend,dependants,362,"Looking at the existing code in Hadoop_BAM it makes the assumption that coordinates are always of the form ```chr:start-stop``` never things like ```chr```, ```chr:pos```, ```char:star+```... I've just generalized a bit more so that it can handle ':' and '-' inside the ```chr``` in a PR. I guess is not ideal but In any case this addresses the current fire and dependants have a clear work around which is to provide their intervals in the expected ```chr:start-stop``` format.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249
https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249:380,Usability,clear,clear,380,"Looking at the existing code in Hadoop_BAM it makes the assumption that coordinates are always of the form ```chr:start-stop``` never things like ```chr```, ```chr:pos```, ```char:star+```... I've just generalized a bit more so that it can handle ':' and '-' inside the ```chr``` in a PR. I guess is not ideal but In any case this addresses the current fire and dependants have a clear work around which is to provide their intervals in the expected ```chr:start-stop``` format.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-331202249
https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849:173,Testability,test,tests,173,Because there is currently no way in travis to prevent the build stages from being triggered in every pull request it was decided to simply upload the nightly build without tests instead. An example of how to use build stages can be seen in this branch for future reference: https://github.com/broadinstitute/gatk/tree/je_travisBuildStages,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849
https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849:133,Usability,simpl,simply,133,Because there is currently no way in travis to prevent the build stages from being triggered in every pull request it was decided to simply upload the nightly build without tests instead. An example of how to use build stages can be seen in this branch for future reference: https://github.com/broadinstitute/gatk/tree/je_travisBuildStages,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3396#issuecomment-319721849
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320262613:56,Usability,guid,guidance,56,Maybe @lbergelson would be willing to review or provide guidance for a new engine team member?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320262613
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:85,Deployability,integrat,integration,85,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:85,Integrability,integrat,integration,85,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:201,Integrability,message,message,201,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:75,Testability,test,tests,75,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:97,Testability,test,tests,97,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124:42,Usability,simpl,simple,42,"@ldgauthier This looks good, its a pretty simple change and there are unit tests and integration tests that enforce the new behavior. I would squash the two commits and give them an informative commit message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409#issuecomment-320707124
https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019:40,Testability,test,tests,40,I've addressed all the feedback and all tests are passing so I'm going to squash and merge this now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019
https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019:23,Usability,feedback,feedback,23,I've addressed all the feedback and all tests are passing so I'm going to squash and merge this now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3452#issuecomment-325607019
https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:400,Integrability,depend,depending,400,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699
https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:812,Safety,risk,risk,812,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699
https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:869,Safety,risk,risky,869,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699
https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:360,Testability,log,log,360,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699
https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:691,Usability,simpl,simpler,691,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699
https://github.com/broadinstitute/gatk/pull/3464#issuecomment-327882765:147,Usability,simpl,simple,147,"This brings to us approximately 60 variants (without any filter applied). @cwhelan Please take time to review, another PR (supposedly dealing with simple ""translocation""s) is going to line up after this. Then the major graph-based one, but expected to take sometime to codeup. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464#issuecomment-327882765
https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:881,Performance,optimiz,optimizations,881,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806
https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:716,Testability,test,testing,716,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806
https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:64,Usability,simpl,simpler,64,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:62,Availability,error,errors,62,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:239,Availability,error,error,239,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:535,Availability,error,error,535,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:863,Availability,error,error,863,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:353,Modifiability,variab,variable,353,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:654,Modifiability,variab,variable,654,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:137,Testability,test,test,137,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:427,Testability,test,test,427,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:738,Testability,test,test,738,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:1047,Usability,learn,learning-combined-copy-number,1047,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259
https://github.com/broadinstitute/gatk/pull/3479#issuecomment-324144199:48,Usability,simpl,simple,48,@jonn-smith Could you take a look at this super simple PR when you get a chance?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3479#issuecomment-324144199
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756:926,Deployability,patch,patch,926,"User is reporting a nearly exact 2 minute pause at tool startup. Seems very suspicious, possibly some sort of gcs operation trying and timing out?. ```; 14:33:39.416 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 5; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:35:46.844 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:46.844 INFO BaseRecalibrator - Deflater: IntelDeflater; 14:35:46.844 INFO BaseRecalibrator - Inflater: IntelInflater; 14:35:46.844 INFO BaseRecalibrator - GCS max retries/reopens: 20; 14:35:46.844 INFO BaseRecalibrator - Using google-cloud-java patch 317951be3c2e898e3916a4b1abf5a9c220d84df8; 14:35:46.844 INFO BaseRecalibrator - Initializing engine; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756:42,Usability,pause,pause,42,"User is reporting a nearly exact 2 minute pause at tool startup. Seems very suspicious, possibly some sort of gcs operation trying and timing out?. ```; 14:33:39.416 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 5; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:35:46.844 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:46.844 INFO BaseRecalibrator - Deflater: IntelDeflater; 14:35:46.844 INFO BaseRecalibrator - Inflater: IntelInflater; 14:35:46.844 INFO BaseRecalibrator - GCS max retries/reopens: 20; 14:35:46.844 INFO BaseRecalibrator - Using google-cloud-java patch 317951be3c2e898e3916a4b1abf5a9c220d84df8; 14:35:46.844 INFO BaseRecalibrator - Initializing engine; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427371743:90,Usability,pause,pause,90,"Also renamed this ticket to be less scary and more precise, since we know it's a 2-minute pause in the NIO library. It clearly doesn't always happen, though, as I don't think I've ever seen it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427371743
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427371743:119,Usability,clear,clearly,119,"Also renamed this ticket to be less scary and more precise, since we know it's a 2-minute pause in the NIO library. It clearly doesn't always happen, though, as I don't think I've ever seen it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427371743
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427415516:31,Usability,clear,clear,31,"@jean-philippe-martin It's not clear to me how widespread this issue is, or what conditions trigger it -- @lbergelson care to comment?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427415516
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:175,Availability,error,error,175,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141
https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:385,Safety,avoid,avoid,385,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141
