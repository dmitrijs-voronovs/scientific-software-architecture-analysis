quality_attribute,keyword,matched_word,sentence,source,filename,author,repo,version,wiki,url
Security,validat,validation,"is_developer, is_service_account, hail_identity, hail_credentials_secret_name); VALUES (%s, %s, %s, %s, %s, %s, %s);; """"""; # keeping authorization_url and state for backwards compatibility; # pylint: disable=unused-argument; """"""; INSERT INTO `roles` (`name`); VALUES (%s);; """"""; """"""; SELECT id, username, login_id, state, is_developer, is_service_account, hail_identity; FROM users;; """"""; """"""; SELECT id, username, login_id, state, is_developer, is_service_account, hail_identity FROM users; WHERE username = %s;; """"""; # backwards compatibility with older versions of hailctl; """"""; SELECT sessions.session_id AS session_id, users.username AS username FROM copy_paste_tokens; INNER JOIN sessions ON sessions.session_id = copy_paste_tokens.session_id; INNER JOIN users ON users.id = sessions.user_id; WHERE copy_paste_tokens.id = %s; AND NOW() < TIMESTAMPADD(SECOND, copy_paste_tokens.max_age_secs, copy_paste_tokens.created); AND users.state = 'active';""""""; """"""; SELECT users.*; FROM users; WHERE (users.login_id = %s OR users.hail_identity_uid = %s) AND users.state = 'active'; """"""; # b64 encoding of 32-byte session ID is 44 bytes; """"""; SELECT users.*; FROM users; INNER JOIN sessions ON users.id = sessions.user_id; WHERE users.state = 'active' AND sessions.session_id = %s AND (ISNULL(sessions.max_age_secs) OR (NOW() < TIMESTAMPADD(SECOND, sessions.max_age_secs, sessions.created)));; """"""; # The below are used by gateway / Envoy reverse proxies for auth checks, but; # Envoy calls those auth endpoints with the same HTTP method as the original; # user's request. In the case where a user is trying to POST to a protected; # service, that will additionally trigger a CSRF check on the auth endpoint; # which cannot always be conducted if, for example, the backend service is; # Grafana which conducts its own CSRF mitigations separate from our own.; # These auth endpoints are not CSRF-vulnerable so we opt out of CSRF-token; # validation.; # See: https://github.com/envoyproxy/envoy/issues/5357",MatchSource.CODE_COMMENT,auth/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/auth.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,auth/auth/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,auth/auth/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py
Modifiability,config,config,"""""""; DELETE FROM sessions; WHERE session_id = %s;; """"""; # length of gsa account_id must be >= 6; # auth services in test namespaces cannot/should not be creating and deleting namespaces; # don't bother deleting the session since all sessions are; # deleted below; # auth services in test namespaces cannot/should not be creating and deleting namespaces; # don't bother deleting database-server-config since we're; # deleting the namespace; """"""; DELETE FROM sessions WHERE user_id = %s;; UPDATE users SET state = 'deleted' WHERE id = %s;; """"""; """"""; UPDATE users; SET hail_identity_uid = %s; WHERE hail_identity = %s; """"""",MatchSource.CODE_COMMENT,auth/auth/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py
Testability,test,test,"""""""; DELETE FROM sessions; WHERE session_id = %s;; """"""; # length of gsa account_id must be >= 6; # auth services in test namespaces cannot/should not be creating and deleting namespaces; # don't bother deleting the session since all sessions are; # deleted below; # auth services in test namespaces cannot/should not be creating and deleting namespaces; # don't bother deleting database-server-config since we're; # deleting the namespace; """"""; DELETE FROM sessions WHERE user_id = %s;; UPDATE users SET state = 'deleted' WHERE id = %s;; """"""; """"""; UPDATE users; SET hail_identity_uid = %s; WHERE hail_identity = %s; """"""",MatchSource.CODE_COMMENT,auth/auth/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,auth/auth/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,auth/auth/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py
Integrability,interface,interface-files,"# See below for a nice breakdown of the cpu cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/cpu-controller.html#interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1038; # OSError: [Errno 19] No such device; # See below for a nice breakdown of the memory cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/memory-controller.html#core-interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1156; # OSError: [Errno 19] No such device; # pylint: disable=consider-using-with",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
Usability,guid,guide,"# See below for a nice breakdown of the cpu cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/cpu-controller.html#interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1038; # OSError: [Errno 19] No such device; # See below for a nice breakdown of the memory cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/memory-controller.html#core-interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1156; # OSError: [Errno 19] No such device; # pylint: disable=consider-using-with",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
Energy Efficiency,power,power,"# Azure bills for specific disk sizes so we must round the storage_in_gib to the nearest power of two; # pylint: disable=unused-argument; # Azure bills for specific disk sizes so we must round the storage_in_gib to the nearest power of two; # storage is in units of MiB",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/resources.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/resources.py
Deployability,deploy,deployments,"# BORROWED; """"""; INSERT INTO regions (region) VALUES (%s); ON DUPLICATE KEY UPDATE region = region;; """"""; # https://docs.microsoft.com/en-us/rest/api/resources/deployments/list-by-resource-group#provisioningstate",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/driver/driver.py
Security,authenticat,authentication,"# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#az-acr-login-with---expose-token; # https://github.com/Azure/azure-storage-fuse; # https://docs.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux#mount; # blobfuse cleans up the temporary directory when unmounting; # https://github.com/Azure/acr/blob/main/docs/AAD-OAuth.md#calling-post-oauth2exchange-to-get-an-acr-refresh-token; # type: ignore; # token expires in 3 hours so we refresh after 1 hour",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/worker/worker_api.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py
Testability,log,login-with---expose-token,"# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#az-acr-login-with---expose-token; # https://github.com/Azure/azure-storage-fuse; # https://docs.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux#mount; # blobfuse cleans up the temporary directory when unmounting; # https://github.com/Azure/acr/blob/main/docs/AAD-OAuth.md#calling-post-oauth2exchange-to-get-an-acr-refresh-token; # type: ignore; # token expires in 3 hours so we refresh after 1 hour",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/worker/worker_api.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py
Availability,echo,echo,"""""""; #!/bin/bash; set -x. NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'); ZONE=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google'). if [ -f ""/started"" ]; then; echo ""instance $NAME has previously been started""; while true; do; gcloud -q compute instances delete $NAME --zone=$ZONE; sleep 1; done; exit; else; touch /started; fi. curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/run_script"" >./run.sh. nohup /bin/bash run.sh >run.log 2>&1 &; """"""; """"""; set -x. INSTANCE_ID=$(curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/instance_id""); NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'). journalctl -u docker.service > dockerd.log; """"""",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/create_instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py
Testability,log,log,"""""""; #!/bin/bash; set -x. NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'); ZONE=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google'). if [ -f ""/started"" ]; then; echo ""instance $NAME has previously been started""; while true; do; gcloud -q compute instances delete $NAME --zone=$ZONE; sleep 1; done; exit; else; touch /started; fi. curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/run_script"" >./run.sh. nohup /bin/bash run.sh >run.log 2>&1 &; """"""; """"""; set -x. INSTANCE_ID=$(curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/instance_id""); NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'). journalctl -u docker.service > dockerd.log; """"""",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/create_instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py
Deployability,deploy,deployments,"# BORROWED; """"""; INSERT INTO regions (region) VALUES (%s); ON DUPLICATE KEY UPDATE region = region;; """"""; # The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
Testability,log,logging,"# BORROWED; """"""; INSERT INTO regions (region) VALUES (%s); ON DUPLICATE KEY UPDATE region = region;; """"""; # The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
Modifiability,variab,variable,"# BORROWED; # BORROWED; # FIXME: data_disk_size_gb is assumed to be constant across all instances, but it is; # passed as a variable parameter to this function!!",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/zones.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/zones.py
Security,access,access,"# https://cloud.google.com/compute/docs/metadata/querying-metadata; # token is not included in the recursive version, presumably as that; # is not simple metadata but requires requesting an access token; # `gcloud` does not properly respect `charset`, which aiohttp automatically; # sets so we have to explicitly erase it; # See https://github.com/googleapis/google-auth-library-python/blob/b935298aaf4ea5867b5778bcbfc42408ba4ec02c/google/auth/compute_engine/_metadata.py#L170",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/worker/metadata_server.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py
Usability,simpl,simple,"# https://cloud.google.com/compute/docs/metadata/querying-metadata; # token is not included in the recursive version, presumably as that; # is not simple metadata but requires requesting an access token; # `gcloud` does not properly respect `charset`, which aiohttp automatically; # sets so we have to explicitly erase it; # See https://github.com/googleapis/google-auth-library-python/blob/b935298aaf4ea5867b5778bcbfc42408ba4ec02c/google/auth/compute_engine/_metadata.py#L170",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/worker/metadata_server.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py
Availability,redundant,redundant,"# this prevents having too many resources in the database with redundant information; """"""; SELECT COALESCE(MAX(resource_id), 0) AS last_resource_id; FROM resources; FOR UPDATE; """"""; """"""; INSERT INTO `resources` (resource, rate); VALUES (%s, %s); """"""; """"""; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource_id > %s AND deduped_resource_id IS NULL; """"""; """"""; INSERT INTO `latest_product_versions` (product, version, sku); VALUES (%s, %s, %s); ON DUPLICATE KEY UPDATE version = VALUES(version); """"""",MatchSource.CODE_COMMENT,batch/batch/driver/billing_manager.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py
Safety,redund,redundant,"# this prevents having too many resources in the database with redundant information; """"""; SELECT COALESCE(MAX(resource_id), 0) AS last_resource_id; FROM resources; FOR UPDATE; """"""; """"""; INSERT INTO `resources` (resource, rate); VALUES (%s, %s); """"""; """"""; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource_id > %s AND deduped_resource_id IS NULL; """"""; """"""; INSERT INTO `latest_product_versions` (product, version, sku); VALUES (%s, %s, %s); ON DUPLICATE KEY UPDATE version = VALUES(version); """"""",MatchSource.CODE_COMMENT,batch/batch/driver/billing_manager.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py
Energy Efficiency,charge,charges,"""""""; INSERT INTO instances (name, state, activation_token, token, cores_mcpu,; time_created, last_updated, version, location, inst_coll, machine_type, preemptible, instance_config); VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);; """"""; """"""; INSERT INTO instances_free_cores_mcpu (name, free_cores_mcpu); VALUES (%s, %s);; """"""; # pending, active, inactive, deleted; # there might be jobs to reschedule; """"""A possibly negative measure of the free cores in millicpu. See free_cores_mcpu_nonnegative for a more useful property.; """"""; """"""A nonnegative measure of the free cores in millicpu. free_cores_mcpu can be negative temporarily if the worker is oversubscribed.; """"""; """"""A nonnegative measure of the used cores in millicpu. The free_cores_mcpu can be negative temporarily if the worker is oversubscribed, so this; property uses free_cores_mcpu_nonnegative to calculate used cores. """"""; """"""The percent of cores currently in use.""""""; """"""; The charges incurred from the cloud for this instance, ignoring attached disks.; """"""; """"""; The revenue generated by in-use resources on this instance.; """"""; """"""; UPDATE instances; SET last_updated = %s,; failed_request_count = 0; WHERE name = %s;; """"""; """"""; UPDATE instances; SET failed_request_count = failed_request_count + 1 WHERE name = %s;; """"""",MatchSource.CODE_COMMENT,batch/batch/driver/instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance.py
Energy Efficiency,schedul,scheduling,".resource, COALESCE(`usage` * rate, 0)) AS cost_breakdown; FROM (; SELECT resource_id, CAST(COALESCE(SUM(`usage`), 0) AS SIGNED) AS `usage`; FROM aggregated_job_group_resources_v3; WHERE job_group_self_and_ancestors.batch_id = aggregated_job_group_resources_v3.batch_id AND; job_group_self_and_ancestors.ancestor_id = aggregated_job_group_resources_v3.job_group_id; GROUP BY resource_id; ) AS usage_t; LEFT JOIN resources ON usage_t.resource_id = resources.resource_id; ) AS cost_t ON TRUE; LEFT JOIN LATERAL (; SELECT 1 AS cancelled; FROM job_group_self_and_ancestors AS self_and_ancestors; INNER JOIN job_groups_cancelled; ON self_and_ancestors.batch_id = job_groups_cancelled.id AND; self_and_ancestors.ancestor_id = job_groups_cancelled.job_group_id; WHERE self_and_ancestors.batch_id = job_group_self_and_ancestors.batch_id AND; self_and_ancestors.job_group_id = job_group_self_and_ancestors.ancestor_id; ) AS t ON TRUE; WHERE job_group_self_and_ancestors.batch_id = %s AND; job_group_self_and_ancestors.job_group_id = %s AND; job_group_self_and_ancestors.ancestor_id != %s AND; NOT deleted AND; job_groups.callback IS NOT NULL AND; job_groups.`state` = 'complete';; """"""; # only jobs from CI may use batch's TLS identity; # This must be sorted in order to match the order of values in the actual SQL table!; """"""; INSERT INTO `attempt_resources` (batch_id, job_id, attempt_id, resource_id, deduped_resource_id, quantity); VALUES (%s, %s, %s, %s, %s, %s); ON DUPLICATE KEY UPDATE quantity = quantity;; """"""; # may also create scheduling opportunities, set above; # already complete, do nothing; """"""; CALL mark_job_started(%s, %s, %s, %s, %s);; """"""; """"""; CALL mark_job_creating(%s, %s, %s, %s, %s);; """"""; # job that was running is now ready to be cancelled; # backwards compatibility; # ServiceAccounts created prior to Kubernetes 1.24 have autogenerated secrets; # ServiceAccounts post v1.24 don't have autogenerated secrets and we make those ourselves; """"""; CALL schedule_job(%s, %s, %s, %s);; """"""",MatchSource.CODE_COMMENT,batch/batch/driver/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/job.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # type: ignore; # type: ignore",MatchSource.CODE_COMMENT,batch/batch/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # type: ignore; # type: ignore",MatchSource.CODE_COMMENT,batch/batch/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py
Energy Efficiency,schedul,scheduled,"# BORROWED; # BORROWED; """"""; SELECT instances.*, instances_free_cores_mcpu.free_cores_mcpu; FROM instances; INNER JOIN instances_free_cores_mcpu; ON instances.name = instances_free_cores_mcpu.name; WHERE removed = 0 AND inst_coll = %s;; """"""; # BORROWED; # BORROWED; # BORROWED; # type: ignore; # 20 queries/s; our GCE long-run quota; # parallelism will be bounded by thread pool; # estimate of number of jobs scheduled per scheduling loop approximately every second; # job_group_id must be part of the ordering when selecting records; # because the scheduler selects records by job group in order; """"""; SELECT user,; CAST(COALESCE(SUM(ready_cores_mcpu), 0) AS SIGNED) AS ready_cores_mcpu; FROM user_inst_coll_resources; WHERE inst_coll = %s; GROUP BY user;; """"""; # 20 queries/s; our GCE long-run quota; # BORROWED; # BORROWED; """"""; SELECT user,; CAST(COALESCE(SUM(n_ready_jobs), 0) AS SIGNED) AS n_ready_jobs,; CAST(COALESCE(SUM(ready_cores_mcpu), 0) AS SIGNED) AS ready_cores_mcpu,; CAST(COALESCE(SUM(n_running_jobs), 0) AS SIGNED) AS n_running_jobs,; CAST(COALESCE(SUM(running_cores_mcpu), 0) AS SIGNED) AS running_cores_mcpu; FROM user_inst_coll_resources; WHERE inst_coll = %s; GROUP BY user; HAVING n_ready_jobs + n_running_jobs > 0;; """"""; # type: ignore; # type: ignore; """"""; SELECT job_groups.batch_id, job_groups.job_group_id, t.cancelled IS NOT NULL AS cancelled, userdata, job_groups.user, format_version; FROM job_groups; LEFT JOIN batches ON job_groups.batch_id = batches.id; LEFT JOIN LATERAL (; SELECT 1 AS cancelled; FROM job_group_self_and_ancestors; INNER JOIN job_groups_cancelled; ON job_group_self_and_ancestors.batch_id = job_groups_cancelled.id AND; job_group_self_and_ancestors.ancestor_id = job_groups_cancelled.job_group_id; WHERE job_groups.batch_id = job_group_self_and_ancestors.batch_id AND; job_groups.job_group_id = job_group_self_and_ancestors.job_group_id; ) AS t ON TRUE; WHERE job_groups.user = %s AND job_groups.`state` = 'running'; ORDER BY job_groups.batch_id, jo",MatchSource.CODE_COMMENT,batch/batch/driver/instance_collection/pool.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py
Availability,error,error-reference,"groups; WHERE batch_id = %s; ORDER BY job_group_id DESC; LIMIT 1; FOR UPDATE;; """"""; # restrict to what's necessary; """"""; SELECT `state`, format_version, `committed`, start_job_id, start_job_group_id; FROM batch_updates; INNER JOIN batches ON batch_updates.batch_id = batches.id; WHERE batch_updates.batch_id = %s AND batch_updates.update_id = %s AND user = %s AND NOT deleted;; """"""; # Clients stopped using `mount_tokens` prior to the introduction of terra deployments; # jobs in non-initial updates of a batch always start out as pending; # because they may have currently running parents in previous updates; # and we dont take those into account here when calculating the number; # of pending parents; """"""; INSERT INTO jobs (batch_id, job_id, update_id, job_group_id, state, spec, always_run, cores_mcpu, n_pending_parents, inst_coll, n_regions, regions_bits_rep); VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);; """"""; # 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry; """"""; INSERT INTO `job_parents` (batch_id, job_id, parent_id); VALUES (%s, %s, %s);; """"""; # 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry; """"""; INSERT INTO `job_attributes` (batch_id, job_id, `key`, `value`); VALUES (%s, %s, %s, %s);; """"""; """"""; INSERT INTO jobs_telemetry (batch_id, job_id, time_ready); VALUES (%s, %s, %s);; """"""; # job_groups_inst_coll_staging tracks the num of resources recursively for all children job groups; """"""; INSERT INTO job_groups_inst_coll_staging (batch_id, update_id, job_group_id, inst_coll, token, n_jobs, n_ready_jobs, ready_cores_mcpu); SELECT %s, %s, ancestor_id, %s, %s, %s, %s, %s; FROM job_group_self_and_ancestors; WHERE batch_id = %s AND job_group_id = %s; ON DUPLICATE KEY UPDATE; n_jobs = n_jobs + VALUES(n_jobs),; n_ready_jobs = n_ready_jobs + VALUES(n_ready_jobs),; ready_cores_mcpu = ready_cores_mcpu + VALUES(ready_cores_mcpu);; """"""; # job_group_inst_coll_",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
Deployability,deploy,deployments,", `value`); VALUES (%s, %s, %s, %s);; """"""; """"""; SELECT `state`, format_version, `committed`, start_job_group_id; FROM batch_updates; INNER JOIN batches ON batch_updates.batch_id = batches.id; WHERE batch_updates.batch_id = %s AND batch_updates.update_id = %s AND `user` = %s AND NOT deleted; LOCK IN SHARE MODE;; """"""; """"""; SELECT job_group_id; FROM job_groups; WHERE batch_id = %s; ORDER BY job_group_id DESC; LIMIT 1; FOR UPDATE;; """"""; # restrict to what's necessary; """"""; SELECT `state`, format_version, `committed`, start_job_id, start_job_group_id; FROM batch_updates; INNER JOIN batches ON batch_updates.batch_id = batches.id; WHERE batch_updates.batch_id = %s AND batch_updates.update_id = %s AND user = %s AND NOT deleted;; """"""; # Clients stopped using `mount_tokens` prior to the introduction of terra deployments; # jobs in non-initial updates of a batch always start out as pending; # because they may have currently running parents in previous updates; # and we dont take those into account here when calculating the number; # of pending parents; """"""; INSERT INTO jobs (batch_id, job_id, update_id, job_group_id, state, spec, always_run, cores_mcpu, n_pending_parents, inst_coll, n_regions, regions_bits_rep); VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);; """"""; # 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry; """"""; INSERT INTO `job_parents` (batch_id, job_id, parent_id); VALUES (%s, %s, %s);; """"""; # 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry; """"""; INSERT INTO `job_attributes` (batch_id, job_id, `key`, `value`); VALUES (%s, %s, %s, %s);; """"""; """"""; INSERT INTO jobs_telemetry (batch_id, job_id, time_ready); VALUES (%s, %s, %s);; """"""; # job_groups_inst_coll_staging tracks the num of resources recursively for all children job groups; """"""; INSERT INTO job_groups_inst_coll_staging (batch_id, update_id, job_group_id, inst_coll, token, n_jobs, n_ready_job",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
Safety,avoid,avoid,"g_projects.`status` as `status`; FROM billing_projects; WHERE billing_projects.name_cs = %s AND billing_projects.`status` != 'deleted'; FOR UPDATE;; """"""; """"""; UPDATE billing_projects SET `limit` = %s WHERE name_cs = %s;; """"""; # type: ignore; # pylint: disable=lost-exception; """"""; SELECT billing_projects.name_cs as billing_project,; billing_projects.`status` as `status`,; `user`; FROM billing_projects; LEFT JOIN (; SELECT billing_project_users.* FROM billing_project_users; LEFT JOIN billing_projects ON billing_projects.name = billing_project_users.billing_project; WHERE billing_projects.name_cs = %s AND user_cs = %s; FOR UPDATE; ) AS t ON billing_projects.name = t.billing_project; WHERE billing_projects.name_cs = %s;; """"""; """"""; DELETE billing_project_users FROM billing_project_users; LEFT JOIN billing_projects ON billing_projects.name = billing_project_users.billing_project; WHERE billing_projects.name_cs = %s AND user_cs = %s;; """"""; # pylint: disable=lost-exception; # we want to be case-insensitive here to avoid duplicates with existing records; """"""; SELECT billing_projects.name as billing_project,; billing_projects.`status` as `status`,; user; FROM billing_projects; LEFT JOIN (; SELECT *; FROM billing_project_users; LEFT JOIN billing_projects ON billing_projects.name = billing_project_users.billing_project; WHERE billing_projects.name_cs = %s AND user = %s; FOR UPDATE; ) AS t; ON billing_projects.name = t.billing_project; WHERE billing_projects.name_cs = %s AND billing_projects.`status` != 'deleted' LOCK IN SHARE MODE;; """"""; """"""; INSERT INTO billing_project_users(billing_project, user, user_cs); VALUES (%s, %s, %s);; """"""; # type: ignore; # pylint: disable=lost-exception; # we want to avoid having billing projects with different cases but the same name; """"""; SELECT name_cs, `status`; FROM billing_projects; WHERE name = %s; FOR UPDATE;; """"""; """"""; INSERT INTO billing_projects(name, name_cs); VALUES (%s, %s);; """"""; # type: ignore; # pylint: disable=lost-exception; """"""; ",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
Testability,log,logs,"M jobs; INNER JOIN batches ON jobs.batch_id = batches.id; LEFT JOIN attempts ON jobs.batch_id = attempts.batch_id and jobs.job_id = attempts.job_id; WHERE jobs.batch_id = %s AND NOT deleted AND jobs.job_id = %s;; """"""; # elapsed time if attempt is still running; """"""; Get the resource_usage data for a job. The data is returned as a JSON object; transformed from a pandas DataFrame using the 'split' orientation. Returns; -------; Example response:; {; // eg: input, main, output; ""[job_stage]"": {; ""columns"":[; ""time_msecs"",; ""memory_in_bytes"",; ""cpu_usage"",; ""non_io_storage_in_bytes"",; ""io_storage_in_bytes"",; ""network_bandwidth_upload_in_bytes_per_second"",; ""network_bandwidth_download_in_bytes_per_second""; ],; ""index"":[0, 1, ...],; ""data"": [[<records>]],; }, ...; }; """"""; # pull this out separately as billing_project_users_only() does a permission; # check for us, but has a fixed signature; # empty response if not available yet; # type: ignore; # type: ignore; # type: ignore; # backwards compatibility; # Not all logs will be proper utf-8 but we attempt to show them as; # str or else Jinja will present them surrounded by b''; # This should really be the exact same thing as the REST endpoint; """"""; SELECT billing_projects.name as billing_project,; billing_projects.`status` as `status`; FROM billing_projects; WHERE billing_projects.name_cs = %s AND billing_projects.`status` != 'deleted'; FOR UPDATE;; """"""; """"""; UPDATE billing_projects SET `limit` = %s WHERE name_cs = %s;; """"""; # type: ignore; # pylint: disable=lost-exception; """"""; SELECT billing_projects.name_cs as billing_project,; billing_projects.`status` as `status`,; `user`; FROM billing_projects; LEFT JOIN (; SELECT billing_project_users.* FROM billing_project_users; LEFT JOIN billing_projects ON billing_projects.name = billing_project_users.billing_project; WHERE billing_projects.name_cs = %s AND user_cs = %s; FOR UPDATE; ) AS t ON billing_projects.name = t.billing_project; WHERE billing_projects.name_cs = %s;; """"""; """"""",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
Security,validat,validate,"# FIXME validate image; # https://github.com/docker/distribution/blob/master/reference/regexp.go#L68; # DEPRECATED:; # command -> process/command; # image -> process/image; # mount_docker_socket -> process/mount_docker_socket; # pvc_size -> resources/storage; # gcsfuse -> cloudfuse; # DEPRECATED",MatchSource.CODE_COMMENT,batch/batch/front_end/validate.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/validate.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,batch/batch/front_end/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,batch/batch/front_end/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py
Security,validat,validated,"""""""; ((job_groups.batch_id, job_groups.job_group_id) IN; (SELECT batch_id, job_group_id FROM job_group_attributes; WHERE `key` = %s AND `value` = %s)); """"""; """"""; ((job_groups.batch_id, job_groups.job_group_id) IN; (SELECT batch_id, job_group_id FROM job_group_attributes; WHERE `key` = %s)); """"""; """"""; (batches.`user` = %s); """"""; """"""; (billing_projects.name_cs = %s); """"""; # need complete because there might be no jobs; # batch has already been validated; """"""; ((jobs.batch_id, jobs.job_group_id) IN; (SELECT batch_id, job_group_id FROM job_group_self_and_ancestors; WHERE batch_id = %s AND ancestor_id = %s)); """"""; """"""; ((jobs.batch_id, jobs.job_id) IN; (SELECT batch_id, job_id FROM job_attributes; WHERE `key` = %s AND `value` = %s)); """"""; """"""; ((jobs.batch_id, jobs.job_id) IN; (SELECT batch_id, job_id FROM job_attributes; WHERE `key` = %s)); """"""",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v1.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v1.py
Security,validat,validated,"operator> ::= ""="" | ""=="" | ""!=""; # <partial-match-operator> ::= ""!~"" | ""=~""; # <match-operator> ::= <exact-match-operator> | <partial-match-operator>; # <comparison-operator> ::= "">="" | ""<="" | "">"" | ""<"" | <exact-match-operator>; # <batch-id-query> ::= ""batch_id"" <comparison-operator> <int>; # <state-query> ::= ""state"" <exact-match-operator> <str>; # <start-time-query> ::= ""start_time"" <comparison-operator> <datetime_str>; # <end-time-query> ::= ""end_time"" <comparison-operator> <datetime_str>; # <duration-query> ::= ""duration"" <comparison-operator> <float>; # <cost-query> ::= ""cost"" <comparison-operator> <float>; # <billing-project-query> ::= ""billing_project"" <exact-match-operator> <str>; # <user-query> ::= ""user"" <exact-match-operator> <str>; # <quoted-exact-match-query> ::= \"" <str> \""; # <unquoted-partial-match-query> ::= <str>; # logic to make time interval queries fast; # this is to make time interval queries fast by using the bounds on both indices; # batch has already been validated; # <jobs-query-list> ::= """" | <jobs-query> ""\n"" <jobs-query-list>; # <jobs-query> ::= <instance-query> | <instance-collection-query> | <job-id-query> | <state-query> |; # <start-time-query> | <end-time-query> | <duration-query> | <cost-query> |; # <quoted-exact-match-query> | <unquoted-partial-match-query>; # <exact-match-operator> ::= ""="" | ""=="" | ""!=""; # <partial-match-operator> ::= ""!~"" | ""=~""; # <match-operator> ::= <exact-match-operator> | <partial-match-operator>; # <comparison-operator> ::= "">="" | ""<="" | "">"" | ""<"" | <exact-match-operator>; # <instance-query> ::= ""instance"" <match-operator> <str>; # <instance-collection-query> ::= ""instance_collection"" <match-operator> <str>; # <job-id-query> ::= ""job_id"" <comparison-operator> <int>; # <state-query> ::= ""state"" <exact-match-operator> <str>; # <start-time-query> ::= ""start_time"" <comparison-operator> <datetime_str>; # <end-time-query> ::= ""end_time"" <comparison-operator> <datetime_str>; # <duration-query> ::= ""duration"" <comp",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
Testability,log,logic,"ration-query> | <cost-query> | <quoted-exact-match-query> | <unquoted-partial-match-query> |; # <billing-project-query> | <user-query>; # <exact-match-operator> ::= ""="" | ""=="" | ""!=""; # <partial-match-operator> ::= ""!~"" | ""=~""; # <match-operator> ::= <exact-match-operator> | <partial-match-operator>; # <comparison-operator> ::= "">="" | ""<="" | "">"" | ""<"" | <exact-match-operator>; # <batch-id-query> ::= ""batch_id"" <comparison-operator> <int>; # <state-query> ::= ""state"" <exact-match-operator> <str>; # <start-time-query> ::= ""start_time"" <comparison-operator> <datetime_str>; # <end-time-query> ::= ""end_time"" <comparison-operator> <datetime_str>; # <duration-query> ::= ""duration"" <comparison-operator> <float>; # <cost-query> ::= ""cost"" <comparison-operator> <float>; # <billing-project-query> ::= ""billing_project"" <exact-match-operator> <str>; # <user-query> ::= ""user"" <exact-match-operator> <str>; # <quoted-exact-match-query> ::= \"" <str> \""; # <unquoted-partial-match-query> ::= <str>; # logic to make time interval queries fast; # this is to make time interval queries fast by using the bounds on both indices; # batch has already been validated; # <jobs-query-list> ::= """" | <jobs-query> ""\n"" <jobs-query-list>; # <jobs-query> ::= <instance-query> | <instance-collection-query> | <job-id-query> | <state-query> |; # <start-time-query> | <end-time-query> | <duration-query> | <cost-query> |; # <quoted-exact-match-query> | <unquoted-partial-match-query>; # <exact-match-operator> ::= ""="" | ""=="" | ""!=""; # <partial-match-operator> ::= ""!~"" | ""=~""; # <match-operator> ::= <exact-match-operator> | <partial-match-operator>; # <comparison-operator> ::= "">="" | ""<="" | "">"" | ""<"" | <exact-match-operator>; # <instance-query> ::= ""instance"" <match-operator> <str>; # <instance-collection-query> ::= ""instance_collection"" <match-operator> <str>; # <job-id-query> ::= ""job_id"" <comparison-operator> <int>; # <state-query> ::= ""state"" <exact-match-operator> <str>; # <start-time-query> ::= ""start_time",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
Availability,error,error," OpenBucket: Bad credentials for bucket ""BUCKET"". Check the; # bucket name and your credentials.\n'); # regarding no-member: https://github.com/PyCQA/pylint/issues/4223; # pylint: disable=no-member; # bind mounts are given the dummy type 'none'; # Mount events should not be propagated from the job container to the host; # https://github.com/opencontainers/runtime-spec/blob/master/config.md; # uid/gid *inside the container*; # https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md; # 'blockIO': {'weight': min(weight, 1000)}, FIXME blkio.weight not supported; # Only supports empty volumes; # Recommended filesystems:; # https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#default-filesystems; # User-defined env variables should take precedence; # {; # name: str,; # state: str, (pending, running, succeeded, error, failed); # timing: dict(str, float),; # error: str, (optional); # short_error: str, (optional); # container_status: {; # state: str,; # started_at: int, (date); # finished_at: int, (date); # out_of_memory: bool,; # exit_code: int; # }; # }; # The reason for not giving each job 5 Gi (for example) is the; # maximum number of simultaneous jobs on a worker is 64 which; # basically fills the disk not allowing for caches etc. Most jobs; # would need an external disk in that case.; # {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # job_group_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int}; # region: str # type: ignore; # }; # this would be a job_spec, but we haven't read it yet; # will this work?; # this will be the user credentials; # create containers; # disk name must be 63 characters or less; # https://cloud.google.com/compute/docs/reference/rest/v1/di",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Deployability,patch,patched,"# type: ignore; # type: ignore; # We patched aiodocker's utility function `compose_auth_header` because it does not base64 encode strings; # in urlsafe mode which is required for Azure's credentials.; # https://github.com/aio-libs/aiodocker/blob/17e08844461664244ea78ecd08d1672b1779acc1/aiodocker/utils.py#L297; # type: ignore; # ACTIVATION_TOKEN; # x:// where x is one or more characters; # ACTIVATION_TOKEN; # Jobs are allowed at minimum a quarter core; # Jobs on the private network should have access to the metadata server; # and our vdc. The public network should not so we use google's public; # resolver.; # Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.; # Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.; # We want the ""hailgenetics/python-dill"" translate to (based on the prefix):; # * gcr.io/hail-vdc/hailgenetics/python-dill; # * us-central1-docker.pkg.dev/hail-vdc/hail/hailgenetics/python-dill; # Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.; # inspect non-deterministically fails sometimes; # FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # Opening GCS connection...\n', b'daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithConn:; # fs.NewServer: create file system: SetUpBucket: OpenBucket: Bad credentials for bucket ""BUCKET"". Che",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Energy Efficiency,allocate,allocate," # pylint: disable=consider-using-with; # close is idempotent; # We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.; # mark_complete moves ownership of `mjs_fut` into another task; # but if it either is not run or is cancelled then we need; # to cancel MJS; # {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed, cancelled); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int},; # jvm: str; # }; # We allocate 60% of memory per core to off heap memory; # pylint: disable=lost-exception; # NOTE we assume that the JVM logs hail emits will be valid utf-8; # retrieve exceptions; # tell process to cancel; # Do not include the JVM log in the exception as this is sent to the user and; # the JVM log might inadvetantly contain sensitive information.; # already running; # check worker hasn't started shutting down; # pylint: disable=unused-argument; # Don't retry. If it doesn't go through, the driver; # monitoring loops will recover. If the driver is; # gone (e.g. testing a PR), this would go into an; # infinite loop and the instance won't be deleted.; # pylint: disable=unused-argument; # pylint: disable=no-member; # unlist job after 3m or half the run duration; # exponentially back off, up to (expected) max of 2m; # If the job is already complete, just send MJC. No need for MJS",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Modifiability,config,config,"dill; # Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.; # inspect non-deterministically fails sometimes; # FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # Opening GCS connection...\n', b'daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithConn:; # fs.NewServer: create file system: SetUpBucket: OpenBucket: Bad credentials for bucket ""BUCKET"". Check the; # bucket name and your credentials.\n'); # regarding no-member: https://github.com/PyCQA/pylint/issues/4223; # pylint: disable=no-member; # bind mounts are given the dummy type 'none'; # Mount events should not be propagated from the job container to the host; # https://github.com/opencontainers/runtime-spec/blob/master/config.md; # uid/gid *inside the container*; # https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md; # 'blockIO': {'weight': min(weight, 1000)}, FIXME blkio.weight not supported; # Only supports empty volumes; # Recommended filesystems:; # https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#default-filesystems; # User-defined env variables should take precedence; # {; # name: str,; # state: str, (pending, running, succeeded, error, failed); # timing: dict(str, float),; # error: str, (optional); # short_error: str, (optional); # container_status: {; # state: str,; # started_at: int, (date); # finished_at: int, (date); # out_of_memory: bool,; # exit_code: int; # }; # }; # The reason for not giving each job 5 Gi (for example) is the; # maximum number of simultaneous jobs on a worker is 64 which; # basically fills the disk not allowing for caches etc. Most jobs; # would need an external disk in that case.; # {; # version: int,; #",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Performance,perform,performance," # Jobs are allowed at minimum a quarter core; # Jobs on the private network should have access to the metadata server; # and our vdc. The public network should not so we use google's public; # resolver.; # Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.; # Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.; # We want the ""hailgenetics/python-dill"" translate to (based on the prefix):; # * gcr.io/hail-vdc/hailgenetics/python-dill; # * us-central1-docker.pkg.dev/hail-vdc/hail/hailgenetics/python-dill; # Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.; # inspect non-deterministically fails sometimes; # FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # Opening GCS connection...\n', b'daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithConn:; # fs.NewServer: create file system: SetUpBucket: OpenBucket: Bad credentials for bucket ""BUCKET"". Check the; # bucket name and your credentials.\n'); # regarding no-member: https://github.com/PyCQA/pylint/issues/4223; # pylint: disable=no-member; # bind mounts are given the dummy type 'none'; # Mount events should not be propagated from the job container to the host; # https://github.com/opencontainers/runtime-spec/blob/master/config.md; # uid/gid *inside the container*; # https://github.com/opencontainer",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Safety,avoid,avoid,"worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # job_group_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int}; # region: str # type: ignore; # }; # this would be a job_spec, but we haven't read it yet; # will this work?; # this will be the user credentials; # create containers; # disk name must be 63 characters or less; # https://cloud.google.com/compute/docs/reference/rest/v1/disks#resource:-disk; # under the information for the name field; # Quota will not be applied to `/io` if the job has an attached disk mounted there; # mark_complete moves ownership of `mjs_fut` into another task; # but if it either is not run or is cancelled then we need; # to cancel MJS; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`; # This path must already be bind mounted into the JVM; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # Make sure this path isn't in self.scratch to avoid accidental bucket deletions!; # pylint: disable=consider-using-with; # close is idempotent; # We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwar",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Security,access,access,"# type: ignore; # type: ignore; # We patched aiodocker's utility function `compose_auth_header` because it does not base64 encode strings; # in urlsafe mode which is required for Azure's credentials.; # https://github.com/aio-libs/aiodocker/blob/17e08844461664244ea78ecd08d1672b1779acc1/aiodocker/utils.py#L297; # type: ignore; # ACTIVATION_TOKEN; # x:// where x is one or more characters; # ACTIVATION_TOKEN; # Jobs are allowed at minimum a quarter core; # Jobs on the private network should have access to the metadata server; # and our vdc. The public network should not so we use google's public; # resolver.; # Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.; # Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.; # We want the ""hailgenetics/python-dill"" translate to (based on the prefix):; # * gcr.io/hail-vdc/hailgenetics/python-dill; # * us-central1-docker.pkg.dev/hail-vdc/hail/hailgenetics/python-dill; # Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.; # inspect non-deterministically fails sometimes; # FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # Opening GCS connection...\n', b'daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithConn:; # fs.NewServer: create file system: SetUpBucket: OpenBucket: Bad credentials for bucket ""BUCKET"". Che",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Testability,test,test,"ublic; # resolver.; # Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.; # Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.; # We want the ""hailgenetics/python-dill"" translate to (based on the prefix):; # * gcr.io/hail-vdc/hailgenetics/python-dill; # * us-central1-docker.pkg.dev/hail-vdc/hail/hailgenetics/python-dill; # Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.; # inspect non-deterministically fails sometimes; # FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # Opening GCS connection...\n', b'daemonize.Run: readFromProcess: sub-process: mountWithArgs: mountWithConn:; # fs.NewServer: create file system: SetUpBucket: OpenBucket: Bad credentials for bucket ""BUCKET"". Check the; # bucket name and your credentials.\n'); # regarding no-member: https://github.com/PyCQA/pylint/issues/4223; # pylint: disable=no-member; # bind mounts are given the dummy type 'none'; # Mount events should not be propagated from the job container to the host; # https://github.com/opencontainers/runtime-spec/blob/master/config.md; # uid/gid *inside the container*; # https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md; # 'blockIO': {'weight': min(weight, 1000)}, FIXME blkio.weight not supported; # Only supports empty volumes; # Recommended filesystems:; # http",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
Energy Efficiency,schedul,scheduling,"# Need to set hailctl remote_tmpdir; # the number of quarter-core jobs that can be running on n machines (assuming each machines has 16 cores); # the amount of time for which each job sleeps; # the number of jobs is the max scheduling rate times the scheduling duration",MatchSource.CODE_COMMENT,batch/load-test/test-max-scheduling-rate.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/load-test/test-max-scheduling-rate.py
Security,firewall,firewall-fees,"'''; INSERT INTO latest_product_versions (product, version); VALUES ('gcp-support-logs-specs-and-firewall-fees', '1');; '''; # 0.005 USD per core-hour; '''; INSERT INTO resources (resource, rate); VALUES ('gcp-support-logs-specs-and-firewall-fees/1', 0.000000000001388888888888889);; '''; '''; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource = 'gcp-support-logs-specs-and-firewall-fees/1';; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
Testability,log,logs-specs-and-firewall-fees,"'''; INSERT INTO latest_product_versions (product, version); VALUES ('gcp-support-logs-specs-and-firewall-fees', '1');; '''; # 0.005 USD per core-hour; '''; INSERT INTO resources (resource, rate); VALUES ('gcp-support-logs-specs-and-firewall-fees/1', 0.000000000001388888888888889);; '''; '''; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource = 'gcp-support-logs-specs-and-firewall-fees/1';; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
Testability,log,log,"# 22 is 16 bytes of entropy; # math.log((2**8)**16, 62) = 21.497443706501368; '''; INSERT INTO globals (; instance_id, internal_token,; worker_cores, worker_type, worker_disk_size_gb, max_instances, pool_size); VALUES (%s, %s, %s, %s, %s, %s, %s);; '''",MatchSource.CODE_COMMENT,batch/sql/insert_globals.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/insert_globals.py
Security,audit,audit,"ld.cost) AS cost_diff; FROM (; SELECT batch_id, job_id, COALESCE(SUM(`usage` * rate), 0) AS cost; FROM aggregated_job_resources; LEFT JOIN batches ON batches.id = aggregated_job_resources.batch_id; LEFT JOIN resources ON aggregated_job_resources.resource = resources.resource; WHERE format_version >= 3; GROUP BY batch_id, job_id; ) AS old; LEFT JOIN (; SELECT batch_id, job_id, COALESCE(SUM(`usage` * rate), 0) AS cost; FROM aggregated_job_resources_v2; LEFT JOIN resources ON aggregated_job_resources_v2.resource_id = resources.resource_id; GROUP BY batch_id, job_id; ) AS new ON old.batch_id = new.batch_id AND old.job_id = new.job_id; WHERE ABS(new.cost - old.cost) >= 0.00001; LIMIT 100;; '''; # we had a bug in billing for job private instances that failed to activate; # this was fixed in #10069; '''; SELECT * FROM attempts; WHERE batch_id = %s AND job_id = %s AND reason = ""activation_timeout""; LIMIT 10;; '''; '''; SELECT old.batch_id, old.cost, new.cost, ABS(new.cost - old.cost) AS cost_diff; FROM (; SELECT batch_id, COALESCE(SUM(`usage` * rate), 0) AS cost; FROM aggregated_batch_resources; LEFT JOIN batches ON batches.id = aggregated_batch_resources.batch_id; LEFT JOIN resources ON aggregated_batch_resources.resource = resources.resource; WHERE format_version >= 3; GROUP BY batch_id; ) AS old; LEFT JOIN (; SELECT batch_id, COALESCE(SUM(`usage` * rate), 0) AS cost; FROM aggregated_batch_resources_v2; LEFT JOIN resources ON aggregated_batch_resources_v2.resource_id = resources.resource_id; GROUP BY batch_id; ) AS new ON old.batch_id = new.batch_id; WHERE ABS(new.cost - old.cost) >= 0.00001; LIMIT 100;; '''; # we had a bug in billing for job private instances that failed to activate; # this was fixed in #10069; '''; SELECT * FROM attempts; WHERE batch_id = %s AND reason = ""activation_timeout""; LIMIT 10;; '''; # cannot audit billing project records because they are partially filled in from batches with format version < 3; # 4 core database, parallelism = 10 maxes out CPU",MatchSource.CODE_COMMENT,batch/sql/populate_agg_billing_by_date.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/populate_agg_billing_by_date.py
Testability,test,test,"# test idempotent; # test idempotent; # test idempotent; # test idempotent; # test idempotent; # test idempotent; # Mitigation for https://github.com/hail-is/hail-production-issues/issues/3; # list batches results for user1; # list batches results for user2; # make sure deleted batches don't show up; # list batches results for user2; # create billing project; # create one batch with the correct billing project; # create batch; # edit billing limit; # get billing project; # add user to project; # remove user from project; # close billing project; # delete billing project; # list batches for a billing project; # list batches for a user; # list batches for a user that submitted the batch",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
Availability,echo,echo,"# https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989; # Transitively is not valid for terra; # pylint: disable=pointless-statement; # pylint: disable=pointless-statement; # list_batches returns all batches for all prev run tests so we set a limit; # list_batches returns all batches for all prev run tests so we set a limit; """"""; job_id >=1; instance == foo; foo = bar; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-02-24T17:18:25Z; """"""; # GCP projects can't be strictly numeric; # FIXME after batch1 goes away, check running status; # verify doesn't exist; # cancelled job has no log; # redirect to auth/login; """"""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""; # get a batch untainted by the FailureInjectingClientSession; # unexpected field fleep; # billing project None/missing; # n_jobs None/missing; # n_jobs wrong type; # token None/missing; # empty gcsfuse bucket name; # empty gcsfuse mount_path name; # attribute key/value None; """"""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""; """"""+-------+; | idx |; +-------+; | int32 |; +-------+; | 0 |; | 1 |; | 2 |; | 3 |; | 4 |; | 5 |; | 6 |; | 7 |; | 8 |; | 9 |; +-------+; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 localhost 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 127.0.0.1 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 $(hostname -i) 5000; """"""; # Transitively is not valid for terra; # G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail; # do not commit update",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
Deployability,deploy,deploy-config,"# https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989; # Transitively is not valid for terra; # pylint: disable=pointless-statement; # pylint: disable=pointless-statement; # list_batches returns all batches for all prev run tests so we set a limit; # list_batches returns all batches for all prev run tests so we set a limit; """"""; job_id >=1; instance == foo; foo = bar; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-02-24T17:18:25Z; """"""; # GCP projects can't be strictly numeric; # FIXME after batch1 goes away, check running status; # verify doesn't exist; # cancelled job has no log; # redirect to auth/login; """"""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""; # get a batch untainted by the FailureInjectingClientSession; # unexpected field fleep; # billing project None/missing; # n_jobs None/missing; # n_jobs wrong type; # token None/missing; # empty gcsfuse bucket name; # empty gcsfuse mount_path name; # attribute key/value None; """"""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""; """"""+-------+; | idx |; +-------+; | int32 |; +-------+; | 0 |; | 1 |; | 2 |; | 3 |; | 4 |; | 5 |; | 6 |; | 7 |; | 8 |; | 9 |; +-------+; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 localhost 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 127.0.0.1 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 $(hostname -i) 5000; """"""; # Transitively is not valid for terra; # G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail; # do not commit update",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
Modifiability,config,config,"# https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989; # Transitively is not valid for terra; # pylint: disable=pointless-statement; # pylint: disable=pointless-statement; # list_batches returns all batches for all prev run tests so we set a limit; # list_batches returns all batches for all prev run tests so we set a limit; """"""; job_id >=1; instance == foo; foo = bar; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-02-24T17:18:25Z; """"""; # GCP projects can't be strictly numeric; # FIXME after batch1 goes away, check running status; # verify doesn't exist; # cancelled job has no log; # redirect to auth/login; """"""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""; # get a batch untainted by the FailureInjectingClientSession; # unexpected field fleep; # billing project None/missing; # n_jobs None/missing; # n_jobs wrong type; # token None/missing; # empty gcsfuse bucket name; # empty gcsfuse mount_path name; # attribute key/value None; """"""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""; """"""+-------+; | idx |; +-------+; | int32 |; +-------+; | 0 |; | 1 |; | 2 |; | 3 |; | 4 |; | 5 |; | 6 |; | 7 |; | 8 |; | 9 |; +-------+; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 localhost 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 127.0.0.1 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 $(hostname -i) 5000; """"""; # Transitively is not valid for terra; # G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail; # do not commit update",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
Testability,test,tests,"# https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989; # Transitively is not valid for terra; # pylint: disable=pointless-statement; # pylint: disable=pointless-statement; # list_batches returns all batches for all prev run tests so we set a limit; # list_batches returns all batches for all prev run tests so we set a limit; """"""; job_id >=1; instance == foo; foo = bar; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-02-24T17:18:25Z; """"""; # GCP projects can't be strictly numeric; # FIXME after batch1 goes away, check running status; # verify doesn't exist; # cancelled job has no log; # redirect to auth/login; """"""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""; # get a batch untainted by the FailureInjectingClientSession; # unexpected field fleep; # billing project None/missing; # n_jobs None/missing; # n_jobs wrong type; # token None/missing; # empty gcsfuse bucket name; # empty gcsfuse mount_path name; # attribute key/value None; """"""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""; """"""+-------+; | idx |; +-------+; | int32 |; +-------+; | 0 |; | 1 |; | 2 |; | 3 |; | 4 |; | 5 |; | 6 |; | 7 |; | 8 |; | 9 |; +-------+; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 localhost 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 127.0.0.1 5000; """"""; """"""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 $(hostname -i) 5000; """"""; # Transitively is not valid for terra; # G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail; # do not commit update",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
Safety,timeout,timeout,"# seconds; # Python should execute these lookups before the one second timeout on keys 3 and 10, but this; # code is unavoidably a data race. This test can fail under extraordinary conditions which cause; # a stall in the Python interpreter.; # seconds; # this task will sleep then boom",MatchSource.CODE_COMMENT,batch/test/test_time_limited_max_size_cache.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py
Testability,test,test,"# seconds; # Python should execute these lookups before the one second timeout on keys 3 and 10, but this; # code is unavoidably a data race. This test can fail under extraordinary conditions which cause; # a stall in the Python interpreter.; # seconds; # this task will sleep then boom",MatchSource.CODE_COMMENT,batch/test/test_time_limited_max_size_cache.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py
Performance,cache,cache,"# Reboot the cache on each use. The kube client isn't; # refreshing tokens correctly.; # https://github.com/kubernetes-client/python/issues/741; # Note, that is in the kubenetes-client repo, the; # kubernetes_asyncio. I'm assuming it has the same; # issue.",MatchSource.CODE_COMMENT,ci/bootstrap.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap.py
Modifiability,config,config,"# noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position; # noqa: E402 pylint: disable=wrong-import-position,ungrouped-imports; """"""; INSERT INTO users (state, username, login_id, is_developer, is_service_account, hail_identity, hail_credentials_secret_name, namespace_name); VALUES (%s, %s, %s, %s, %s, %s, %s, %s);; """"""; # username, login_id, is_developer, is_service_account; # kube.config.load_incluster_config()",MatchSource.CODE_COMMENT,ci/bootstrap_create_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap_create_accounts.py
Security,checksum,checksum,"# version to migrate to; # the 0th migration migrates from 1 to 2; # migrate; # verify checksum; # create if not exists",MatchSource.CODE_COMMENT,ci/create_database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/create_database.py
Deployability,deploy,deploy,"""""""Path to repository on the ci (locally).""""""; """"""Bash script to checkout out the code in the current directory.""""""; # transitively close requested_step_names over dependencies; # build.yaml allows for multiple namespaces, but; # in actuality we only ever use 1 and make many assumptions; # around there being a 1:1 correspondence between builds and namespaces; # pylint: disable=unused-argument; # CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME label; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME config_file; # pylint: disable=unused-argument; """"""\; set -ex; date; """"""; # FIXME what if the cluster isn't big enough?; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME configuration; # FIXME validate; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); # pylint: disable=unused-argument; # pylint: disable=unused-argument",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
Integrability,depend,dependencies,"""""""Path to repository on the ci (locally).""""""; """"""Bash script to checkout out the code in the current directory.""""""; # transitively close requested_step_names over dependencies; # build.yaml allows for multiple namespaces, but; # in actuality we only ever use 1 and make many assumptions; # around there being a 1:1 correspondence between builds and namespaces; # pylint: disable=unused-argument; # CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME label; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME config_file; # pylint: disable=unused-argument; """"""\; set -ex; date; """"""; # FIXME what if the cluster isn't big enough?; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME configuration; # FIXME validate; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); # pylint: disable=unused-argument; # pylint: disable=unused-argument",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
Modifiability,config,configuration,"""""""Path to repository on the ci (locally).""""""; """"""Bash script to checkout out the code in the current directory.""""""; # transitively close requested_step_names over dependencies; # build.yaml allows for multiple namespaces, but; # in actuality we only ever use 1 and make many assumptions; # around there being a 1:1 correspondence between builds and namespaces; # pylint: disable=unused-argument; # CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME label; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME config_file; # pylint: disable=unused-argument; """"""\; set -ex; date; """"""; # FIXME what if the cluster isn't big enough?; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME configuration; # FIXME validate; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); # pylint: disable=unused-argument; # pylint: disable=unused-argument",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
Performance,cache,cache,"""""""Path to repository on the ci (locally).""""""; """"""Bash script to checkout out the code in the current directory.""""""; # transitively close requested_step_names over dependencies; # build.yaml allows for multiple namespaces, but; # in actuality we only ever use 1 and make many assumptions; # around there being a 1:1 correspondence between builds and namespaces; # pylint: disable=unused-argument; # CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME label; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME config_file; # pylint: disable=unused-argument; """"""\; set -ex; date; """"""; # FIXME what if the cluster isn't big enough?; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME configuration; # FIXME validate; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); # pylint: disable=unused-argument; # pylint: disable=unused-argument",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
Security,validat,validate,"""""""Path to repository on the ci (locally).""""""; """"""Bash script to checkout out the code in the current directory.""""""; # transitively close requested_step_names over dependencies; # build.yaml allows for multiple namespaces, but; # in actuality we only ever use 1 and make many assumptions; # around there being a 1:1 correspondence between builds and namespaces; # pylint: disable=unused-argument; # CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME label; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME config_file; # pylint: disable=unused-argument; """"""\; set -ex; date; """"""; # FIXME what if the cluster isn't big enough?; """"""; date; """"""; # FIXME configuration; # pylint: disable=unused-argument; # FIXME configuration; # FIXME validate; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); # pylint: disable=unused-argument; # pylint: disable=unused-argument",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
Deployability,deploy,deploy,"# type: ignore; # type: ignore; # FIXME generate links to the merge log; # FIXME recent deploy history; # FIXME generate links to the merge log; # the order of this list is the order in which the states will be displayed on the page; # FIXME; # type: ignore; # pylint: disable=broad-except; """"""; UPDATE globals SET frozen_merge_deploy = 1;; """"""; """"""; UPDATE globals SET frozen_merge_deploy = 0;; """"""; """"""; SELECT active_namespaces.*, JSON_OBJECTAGG(service, rate_limit_rps) as services; FROM active_namespaces; LEFT JOIN deployed_services; ON active_namespaces.namespace = deployed_services.namespace; GROUP BY active_namespaces.namespace""""""; """"""; SELECT 1 FROM deployed_services; WHERE namespace = %s AND service = %s; """"""; """"""UPDATE deployed_services SET rate_limit_rps = %s WHERE namespace = %s AND service = %s""""""; # pylint: disable=broad-except; """"""; SELECT frozen_merge_deploy FROM globals;; """"""",MatchSource.CODE_COMMENT,ci/ci/ci.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py
Testability,log,log,"# type: ignore; # type: ignore; # FIXME generate links to the merge log; # FIXME recent deploy history; # FIXME generate links to the merge log; # the order of this list is the order in which the states will be displayed on the page; # FIXME; # type: ignore; # pylint: disable=broad-except; """"""; UPDATE globals SET frozen_merge_deploy = 1;; """"""; """"""; UPDATE globals SET frozen_merge_deploy = 0;; """"""; """"""; SELECT active_namespaces.*, JSON_OBJECTAGG(service, rate_limit_rps) as services; FROM active_namespaces; LEFT JOIN deployed_services; ON active_namespaces.namespace = deployed_services.namespace; GROUP BY active_namespaces.namespace""""""; """"""; SELECT 1 FROM deployed_services; WHERE namespace = %s AND service = %s; """"""; """"""UPDATE deployed_services SET rate_limit_rps = %s WHERE namespace = %s AND service = %s""""""; # pylint: disable=broad-except; """"""; SELECT frozen_merge_deploy FROM globals;; """"""",MatchSource.CODE_COMMENT,ci/ci/ci.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py
Modifiability,config,config,"# The config is set per load balancer pod, so we must account for; # multiple replicas of the load balancer",MatchSource.CODE_COMMENT,ci/ci/envoy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py
Performance,load,load,"# The config is set per load balancer pod, so we must account for; # multiple replicas of the load balancer",MatchSource.CODE_COMMENT,ci/ci/envoy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py
Availability,failure,failure,"# type: ignore; """"""; SELECT sha; FROM alerted_failed_shas; WHERE sha = %s; """"""; """"""; INSERT INTO alerted_failed_shas (sha) VALUES (%s); """"""; # record the context for a merge failure; # pending, changes_requested, approve; # 'error', 'success', 'failure', None; # don't need to set github_changed because we are refreshing github; # passed > unknown > failed; # oldest first; # FIXME improve; # This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":; # Should be impossible, per https://docs.github.com/en/graphql/reference/enums#pullrequestreviewdecision; # clear current batch; # pylint: disable=broad-except; # FIXME save merge failure output for UI; # find the latest non-cancelled batch for source; # can't merge target if we don't know what it is; # success, failure, pending; # update everything; # merge candidate if up-to-date build passing, or; # pending but haven't failed; # cancel orphan builds; # not deploying; # pylint: disable=broad-except",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
Deployability,configurat,configuration,"# type: ignore; """"""; SELECT sha; FROM alerted_failed_shas; WHERE sha = %s; """"""; """"""; INSERT INTO alerted_failed_shas (sha) VALUES (%s); """"""; # record the context for a merge failure; # pending, changes_requested, approve; # 'error', 'success', 'failure', None; # don't need to set github_changed because we are refreshing github; # passed > unknown > failed; # oldest first; # FIXME improve; # This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":; # Should be impossible, per https://docs.github.com/en/graphql/reference/enums#pullrequestreviewdecision; # clear current batch; # pylint: disable=broad-except; # FIXME save merge failure output for UI; # find the latest non-cancelled batch for source; # can't merge target if we don't know what it is; # success, failure, pending; # update everything; # merge candidate if up-to-date build passing, or; # pending but haven't failed; # cancel orphan builds; # not deploying; # pylint: disable=broad-except",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
Modifiability,config,configuration,"# type: ignore; """"""; SELECT sha; FROM alerted_failed_shas; WHERE sha = %s; """"""; """"""; INSERT INTO alerted_failed_shas (sha) VALUES (%s); """"""; # record the context for a merge failure; # pending, changes_requested, approve; # 'error', 'success', 'failure', None; # don't need to set github_changed because we are refreshing github; # passed > unknown > failed; # oldest first; # FIXME improve; # This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":; # Should be impossible, per https://docs.github.com/en/graphql/reference/enums#pullrequestreviewdecision; # clear current batch; # pylint: disable=broad-except; # FIXME save merge failure output for UI; # find the latest non-cancelled batch for source; # can't merge target if we don't know what it is; # success, failure, pending; # update everything; # merge candidate if up-to-date build passing, or; # pending but haven't failed; # cancel orphan builds; # not deploying; # pylint: disable=broad-except",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
Usability,clear,clear,"# type: ignore; """"""; SELECT sha; FROM alerted_failed_shas; WHERE sha = %s; """"""; """"""; INSERT INTO alerted_failed_shas (sha) VALUES (%s); """"""; # record the context for a merge failure; # pending, changes_requested, approve; # 'error', 'success', 'failure', None; # don't need to set github_changed because we are refreshing github; # passed > unknown > failed; # oldest first; # FIXME improve; # This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":; # Should be impossible, per https://docs.github.com/en/graphql/reference/enums#pullrequestreviewdecision; # clear current batch; # pylint: disable=broad-except; # FIXME save merge failure output for UI; # find the latest non-cancelled batch for source; # can't merge target if we don't know what it is; # success, failure, pending; # update everything; # merge candidate if up-to-date build passing, or; # pending but haven't failed; # cancel orphan builds; # not deploying; # pylint: disable=broad-except",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,ci/ci/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,ci/ci/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py
Testability,test,test,"# This fix is only necessary for dev/test namespaces; '''; DELETE FROM `active_namespaces` WHERE `namespace` = 'default';; INSERT INTO `active_namespaces` (`namespace`) VALUES (%s);; INSERT INTO `deployed_services` (`namespace`, `service`) VALUES; (%s, 'auth'), (%s, 'batch'), (%s, 'batch-driver'), (%s, 'ci');; '''",MatchSource.CODE_COMMENT,ci/sql/fix-default-namespace.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/sql/fix-default-namespace.py
Testability,test,test,"""""""Import a GTF file. The GTF file format is identical to the GFF version 2 file format,; and so this function can be used to import GFF version 2 files as; well. See https://www.ensembl.org/info/website/upload/gff.html for more; details on the GTF/GFF2 file format. The :class:`.Table` returned by this function will include the following; row fields:. .. code-block:: text. 'seqname': str; 'source': str; 'feature': str; 'start': int32; 'end': int32; 'score': float64; 'strand': str; 'frame': int32. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. .. note::. The ""end"" field in the table will be incremented by 1 in; comparison to the value found in the GTF file, as the end; coordinate in a GTF file is inclusive while the end; coordinate in Hail is exclusive. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf', key='gene_id'); >>> ht.describe(). .. code-block:: text. ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'seqname': str; 'source': str; 'feature': str; 'start': int32; 'end': int32; 'score': float64; 'strand': str; 'frame': int32; 'havana_gene': str; 'exon_id': str; 'havana_transcript': str; 'transcript_name': str; 'gene_type': str; 'tag': str; 'transcript_status': str; 'exon_number': str; 'level': str; 'transcript_id': str; 'transcript_type': str; 'gene_id': str; 'gene_name': str; 'gene_status': str; ----------------------------------------; Key: ['gene_id']; ----------------------------------------. Parameters; ----------. path : :obj:`str`; File to import.; key : :obj:`str` or :obj:`list` of :obj:`str`; Key field(s). Can be tag name(s) found in the attribute field; of the GTF file. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,datasets/load/load.GTEx.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx.py
Testability,test,test,"rame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. . If the ``reference_genome`` parameter is specified, the start and end; points of the ``interval`` field will be of type :class:`.tlocus`.; Otherwise, the start and end points of the ``interval`` field will be of; type :class:`.tstruct` with fields ``seqname`` (type :class:`str`) and; ``position`` (type :class:`.tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf', ; reference_genome='GRCh37',; skip_invalid_contigs=True); >>> ht.describe(). .. code-block:: text. ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :obj:`str`; File to import.; reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqna",MatchSource.CODE_COMMENT,datasets/load/load.GTEx_v7.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx_v7.py
Modifiability,rewrite,rewrite,"# Make links point back to the local dev server and not use; # the dev namespace path rewrite shenanigans.",MatchSource.CODE_COMMENT,devbin/dev_proxy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/devbin/dev_proxy.py
Integrability,rout,routes,"# Only web routes should redirect by default; # pylint: disable=broad-except; # Favor browser cookie to Bearer token auth; # request.url is a yarl.URL",MatchSource.CODE_COMMENT,gear/gear/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/auth.py
Security,authenticat,authentication,"# CSRF prevention is only relevant to requests that use browser; # cookies for authentication.",MatchSource.CODE_COMMENT,gear/gear/csrf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/csrf.py
Availability,rollback,rollback,"# 1040 - Too many connections; # 1213 - Deadlock found when trying to get lock; try restarting transaction; # 2003 - Can't connect to MySQL server on ...; # 2013 - Lost connection to MySQL server during query ([Errno 104] Connection reset by peer); # 1205 - Lock wait timeout exceeded; try restarting transaction; """"""If a database exception is retryable, return its `logging` log level.""""""; # pylint: disable=unnecessary-dunder-call; # type: ignore; # type: ignore; # type: ignore; # type: ignore; # connection args; # Discard stale connections, see https://stackoverflow.com/questions/69373128/db-connection-issue-with-encode-databases-library.; # pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
Deployability,rollback,rollback,"# 1040 - Too many connections; # 1213 - Deadlock found when trying to get lock; try restarting transaction; # 2003 - Can't connect to MySQL server on ...; # 2013 - Lost connection to MySQL server during query ([Errno 104] Connection reset by peer); # 1205 - Lock wait timeout exceeded; try restarting transaction; """"""If a database exception is retryable, return its `logging` log level.""""""; # pylint: disable=unnecessary-dunder-call; # type: ignore; # type: ignore; # type: ignore; # type: ignore; # connection args; # Discard stale connections, see https://stackoverflow.com/questions/69373128/db-connection-issue-with-encode-databases-library.; # pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
Safety,timeout,timeout,"# 1040 - Too many connections; # 1213 - Deadlock found when trying to get lock; try restarting transaction; # 2003 - Can't connect to MySQL server on ...; # 2013 - Lost connection to MySQL server during query ([Errno 104] Connection reset by peer); # 1205 - Lock wait timeout exceeded; try restarting transaction; """"""If a database exception is retryable, return its `logging` log level.""""""; # pylint: disable=unnecessary-dunder-call; # type: ignore; # type: ignore; # type: ignore; # type: ignore; # connection args; # Discard stale connections, see https://stackoverflow.com/questions/69373128/db-connection-issue-with-encode-databases-library.; # pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
Testability,log,logging,"# 1040 - Too many connections; # 1213 - Deadlock found when trying to get lock; try restarting transaction; # 2003 - Can't connect to MySQL server on ...; # 2013 - Lost connection to MySQL server during query ([Errno 104] Connection reset by peer); # 1205 - Lock wait timeout exceeded; try restarting transaction; """"""If a database exception is retryable, return its `logging` log level.""""""; # pylint: disable=unnecessary-dunder-call; # type: ignore; # type: ignore; # type: ignore; # type: ignore; # connection args; # Discard stale connections, see https://stackoverflow.com/questions/69373128/db-connection-issue-with-encode-databases-library.; # pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
Integrability,rout,route,"# type: ignore; # type: ignore; # Use the path template given to @route.<METHOD>, not the fully resolved one; # type: ignore",MatchSource.CODE_COMMENT,gear/gear/metrics.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/metrics.py
Availability,down,down,"# item stash; # used internally; # Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log; # `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.; # on the final iteration, perform the required teardown for the test; # prevent other plugins running that might invoke the benchmark again; # prevent other plugins running that might invoke the benchmark again; # make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
Modifiability,plugin,plugins,"# item stash; # used internally; # Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log; # `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.; # on the final iteration, perform the required teardown for the test; # prevent other plugins running that might invoke the benchmark again; # prevent other plugins running that might invoke the benchmark again; # make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
Performance,perform,perform,"# item stash; # used internally; # Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log; # `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.; # on the final iteration, perform the required teardown for the test; # prevent other plugins running that might invoke the benchmark again; # prevent other plugins running that might invoke the benchmark again; # make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
Testability,benchmark,benchmark,"# item stash; # used internally; # Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log; # `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.; # on the final iteration, perform the required teardown for the test; # prevent other plugins running that might invoke the benchmark again; # prevent other plugins running that might invoke the benchmark again; # make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
Safety,timeout,timeout-on-a-function-call,"# make JVM do something to ensure that it is fresh; # https://stackoverflow.com/questions/492519/timeout-on-a-function-call/494273#494273; # we're fucked.; #",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/utils.py
Performance,race condition,race condition,"# turn it on; # turn it off; # turn it back on (Google Dataproc with Spark 3 has issues immediately restarting context after stopping, some kind of race condition in yarn 3); # hl.init()",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-start-stop.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-start-stop.py
Testability,assert,assert,"# vep_result_agrees = actual._same(expected); # if vep_result_agrees:; # print('TEST PASSED'); # else:; # print('TEST FAILED'); # assert vep_result_agrees",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-vep-check-GRCh37.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh37.py
Testability,assert,assert,"# vep_result_agrees = actual._same(expected); # if vep_result_agrees:; # print('TEST PASSED'); # else:; # print('TEST FAILED'); # assert vep_result_agrees",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-vep-check-GRCh38.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh38.py
Availability,checkpoint,checkpoint,"is gets run once per process -- must avoid race conditions; # ds = hl.import_vcf('data/sample.vcf.bgz'); # ds = ds.sample_rows(0.035); # ds = ds.annotate_rows(use_as_marker=hl.rand_bool(0.5),; # panel_maf=0.1,; # anno1=5,; # anno2=0,; # consequence=""LOF"",; # gene=""A"",; # score=5.0); # ds = ds.annotate_rows(a_index=1); # ds = hl.sample_qc(hl.variant_qc(ds)); # ds = ds.annotate_cols(is_case=True,; # pheno=hl.struct(is_case=hl.rand_bool(0.5),; # is_female=hl.rand_bool(0.5),; # age=hl.rand_norm(65, 10),; # height=hl.rand_norm(70, 10),; # blood_pressure=hl.rand_norm(120, 20),; # cohort_name=""cohort1""),; # cov=hl.struct(PC1=hl.rand_norm(0, 1)),; # cov1=hl.rand_norm(0, 1),; # cov2=hl.rand_norm(0, 1),; # cohort=""SIGMA""); # ds = ds.annotate_globals(global_field_1=5,; # global_field_2=10,; # pli={'SCN1A': 0.999, 'SONIC': 0.014},; # populations=['AFR', 'EAS', 'EUR', 'SAS', 'AMR', 'HIS']); # ds = ds.annotate_rows(gene=['TTN']); # ds = ds.annotate_cols(cohorts=['1kg'], pop='EAS'); # ds.checkpoint('data/example.mt', overwrite=True); # small_mt = hl.balding_nichols_model(3, 4, 4); # small_mt.checkpoint('data/small.mt', overwrite=True); # Table; # table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); # table1 = table1.annotate_globals(global_field_1=5, global_field_2=10); # table1.write('data/kt_example1.ht'); # table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); # table2.write('data/kt_example2.ht'); # table4 = hl.import_table('data/kt_example4.tsv', impute=True,; # types={'B': hl.tstruct(B0=hl.tbool, B1=hl.tstr),; # 'D': hl.tstruct(cat=hl.tint32, dog=hl.tint32),; # 'E': hl.tstruct(A=hl.tint32, B=hl.tint32)}); # table4.write('data/kt_example4.ht'); # people_table = hl.import_table('data/explode_example.tsv', delimiter='\\s+',; # types={'Age': hl.tint32, 'Children': hl.tarray(hl.tstr)},; # key='Name'); # people_table.write('data/explode_example.ht'); # TDT; # tdt_dataset = hl.import_vcf('data/tdt_tiny.vcf'); # tdt_dataset.write('data/tdt_tiny.",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
Performance,race condition,race conditions,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions; # ds = hl.import_vcf('data/sample.vcf.bgz'); # ds = ds.sample_rows(0.035); # ds = ds.annotate_rows(use_as_marker=hl.rand_bool(0.5),; # panel_maf=0.1,; # anno1=5,; # anno2=0,; # consequence=""LOF"",; # gene=""A"",; # score=5.0); # ds = ds.annotate_rows(a_index=1); # ds = hl.sample_qc(hl.variant_qc(ds)); # ds = ds.annotate_cols(is_case=True,; # pheno=hl.struct(is_case=hl.rand_bool(0.5),; # is_female=hl.rand_bool(0.5),; # age=hl.rand_norm(65, 10),; # height=hl.rand_norm(70, 10),; # blood_pressure=hl.rand_norm(120, 20),; # cohort_name=""cohort1""),; # cov=hl.struct(PC1=hl.rand_norm(0, 1)),; # cov1=hl.rand_norm(0, 1),; # cov2=hl.rand_norm(0, 1),; # cohort=""SIGMA""); # ds = ds.annotate_globals(global_field_1=5,; # global_field_2=10,; # pli={'SCN1A': 0.999, 'SONIC': 0.014},; # populations=['AFR', 'EAS', 'EUR', 'SAS', 'AMR', 'HIS']); # ds = ds.annotate_rows(gene=['TTN']); # ds = ds.annotate_cols(cohorts=['1kg'], pop='EAS'); # ds.checkpoint('data/example.mt', overwrite=True); # small_mt = hl.balding_nichols_model(3, 4, 4); # small_mt.checkpoint('data/small.mt', overwrite=True); # Table; # table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); # table1 = table1.annotate_globals(global_field_1=5, global_field_2=10); # table1.write('data/kt_example1.ht'); # table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); # table2.write('data/kt_example2.ht'); # table4 = hl.import_table('data/kt_example4.tsv', impute=True,; # types={'B': hl.tstruct(B0=hl.tbool, B1=hl.tstr),; # 'D': hl.tstruct(cat=hl.tint32, dog=hl.tint32),; # 'E': hl.tstruct(A=hl.tint32, B=hl.tint32)}); # table4.write('data/kt_example4.ht'); # people_table = hl.import_table('data/explode_example.tsv', delimiter='\\s+',; # types={'Age': hl.tint32, 'Children': hl.tarray(hl.tstr)},; # key='Name'); # people_table.write('data/explode_example.ht'); # TDT; # tdt_dataset = hl.import_vcf('data/",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
Safety,avoid,avoid,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions; # ds = hl.import_vcf('data/sample.vcf.bgz'); # ds = ds.sample_rows(0.035); # ds = ds.annotate_rows(use_as_marker=hl.rand_bool(0.5),; # panel_maf=0.1,; # anno1=5,; # anno2=0,; # consequence=""LOF"",; # gene=""A"",; # score=5.0); # ds = ds.annotate_rows(a_index=1); # ds = hl.sample_qc(hl.variant_qc(ds)); # ds = ds.annotate_cols(is_case=True,; # pheno=hl.struct(is_case=hl.rand_bool(0.5),; # is_female=hl.rand_bool(0.5),; # age=hl.rand_norm(65, 10),; # height=hl.rand_norm(70, 10),; # blood_pressure=hl.rand_norm(120, 20),; # cohort_name=""cohort1""),; # cov=hl.struct(PC1=hl.rand_norm(0, 1)),; # cov1=hl.rand_norm(0, 1),; # cov2=hl.rand_norm(0, 1),; # cohort=""SIGMA""); # ds = ds.annotate_globals(global_field_1=5,; # global_field_2=10,; # pli={'SCN1A': 0.999, 'SONIC': 0.014},; # populations=['AFR', 'EAS', 'EUR', 'SAS', 'AMR', 'HIS']); # ds = ds.annotate_rows(gene=['TTN']); # ds = ds.annotate_cols(cohorts=['1kg'], pop='EAS'); # ds.checkpoint('data/example.mt', overwrite=True); # small_mt = hl.balding_nichols_model(3, 4, 4); # small_mt.checkpoint('data/small.mt', overwrite=True); # Table; # table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); # table1 = table1.annotate_globals(global_field_1=5, global_field_2=10); # table1.write('data/kt_example1.ht'); # table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); # table2.write('data/kt_example2.ht'); # table4 = hl.import_table('data/kt_example4.tsv', impute=True,; # types={'B': hl.tstruct(B0=hl.tbool, B1=hl.tstr),; # 'D': hl.tstruct(cat=hl.tint32, dog=hl.tint32),; # 'E': hl.tstruct(A=hl.tint32, B=hl.tint32)}); # table4.write('data/kt_example4.ht'); # people_table = hl.import_table('data/explode_example.tsv', delimiter='\\s+',; # types={'Age': hl.tint32, 'Children': hl.tarray(hl.tstr)},; # key='Name'); # people_table.write('data/explode_example.ht'); # TDT; # tdt_dataset = hl.import_vcf('data/",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
Testability,test,test,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions; # ds = hl.import_vcf('data/sample.vcf.bgz'); # ds = ds.sample_rows(0.035); # ds = ds.annotate_rows(use_as_marker=hl.rand_bool(0.5),; # panel_maf=0.1,; # anno1=5,; # anno2=0,; # consequence=""LOF"",; # gene=""A"",; # score=5.0); # ds = ds.annotate_rows(a_index=1); # ds = hl.sample_qc(hl.variant_qc(ds)); # ds = ds.annotate_cols(is_case=True,; # pheno=hl.struct(is_case=hl.rand_bool(0.5),; # is_female=hl.rand_bool(0.5),; # age=hl.rand_norm(65, 10),; # height=hl.rand_norm(70, 10),; # blood_pressure=hl.rand_norm(120, 20),; # cohort_name=""cohort1""),; # cov=hl.struct(PC1=hl.rand_norm(0, 1)),; # cov1=hl.rand_norm(0, 1),; # cov2=hl.rand_norm(0, 1),; # cohort=""SIGMA""); # ds = ds.annotate_globals(global_field_1=5,; # global_field_2=10,; # pli={'SCN1A': 0.999, 'SONIC': 0.014},; # populations=['AFR', 'EAS', 'EUR', 'SAS', 'AMR', 'HIS']); # ds = ds.annotate_rows(gene=['TTN']); # ds = ds.annotate_cols(cohorts=['1kg'], pop='EAS'); # ds.checkpoint('data/example.mt', overwrite=True); # small_mt = hl.balding_nichols_model(3, 4, 4); # small_mt.checkpoint('data/small.mt', overwrite=True); # Table; # table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); # table1 = table1.annotate_globals(global_field_1=5, global_field_2=10); # table1.write('data/kt_example1.ht'); # table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); # table2.write('data/kt_example2.ht'); # table4 = hl.import_table('data/kt_example4.tsv', impute=True,; # types={'B': hl.tstruct(B0=hl.tbool, B1=hl.tstr),; # 'D': hl.tstruct(cat=hl.tint32, dog=hl.tint32),; # 'E': hl.tstruct(A=hl.tint32, B=hl.tint32)}); # table4.write('data/kt_example4.ht'); # people_table = hl.import_table('data/explode_example.tsv', delimiter='\\s+',; # types={'Age': hl.tint32, 'Children': hl.tarray(hl.tstr)},; # key='Name'); # people_table.write('data/explode_example.ht'); # TDT; # tdt_dataset = hl.import_vcf('data/",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
Availability,avail,available,"sh` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Deployability,configurat,configuration,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Integrability,message,messages,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Modifiability,config,configure,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Performance,load,loaded,"is recursively deleted when the context manager exits. If the filesystem has a notion of directories, then we ensure the directory exists. Warning; -------. The directory name is generated randomly and is extraordinarly unlikely to already exist, but; this function does not satisfy the strict requirements of Python's :class:`.TemporaryDirectory`. Examples; --------. >>> with TemporaryDirectory() as dir: # doctest: +SKIP; ... open(f'{dir}/hello', 'w').write('hello hail'); ... print(open(f'{dir}/hello').read()); hello hail. Returns; -------; :class:`._TemporaryDirectoryManager`. """"""; """"""With no argument, returns the default reference genome (``'GRCh37'`` by default).; With an argument, sets the default reference genome to the argument. Returns; -------; :class:`.ReferenceGenome`; """"""; """"""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """"""; """"""Restore global randomness to initial state for test reproducibility.""""""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Security,access,accessing,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Testability,log,log,"nd `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
Availability,down,downstream,"lass:`.Expression`; Column fields to group by.; named_exprs : keyword args of :class:`.Expression`; Column-indexed expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; """"""Aggregate rows by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a set as a new row field:. >>> dataset_resul",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Deployability,pipeline,pipeline,"expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix. Can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; """"""Group columns. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the call rate; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate(call_rate = hl.agg.fraction(hl.is_defined(dataset.GT)))). Notes; -----; All complex expressions must be passed as named expressions. Parameters; ----------; exprs : args of :class:`str` or :class:`.Expression`; Column fields to group by.; named_exprs : keyword args of :class:`.Expression`; Column-indexed expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Energy Efficiency,reduce,reduces,"lass:`.Expression`; Column fields to group by.; named_exprs : keyword args of :class:`.Expression`; Column-indexed expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; """"""Aggregate rows by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a set as a new row field:. >>> dataset_resul",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Integrability,depend,dependent,"---------; named_exprs : keyword args of :class:`.Expression`; Annotation expressions. Returns; -------; :class:`.MatrixTable`; """"""; """"""Aggregate over rows to a local value. Examples; --------; Aggregate over rows:. >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by row. This method should be thought of as a more convenient alternative to; the following:. >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; """"""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by column. This method should be thought of as a more convenient alternative to; the following:. >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; ----; This method supports (and expects!) aggregation over columns. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value depende",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Modifiability,variab,variable-length,"erwriting; the first. The arguments to the method should either be :class:`.Expression`; objects, or should be implicitly interpretable as expressions. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; Matrix table with new row-and-column-indexed field(s).; """"""; """"""Select existing global fields or create new fields by name, dropping the rest. Examples; --------; Select one existing field and compute a new one:. >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; -----; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified global fields.; """"""; """"""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail. Row keys are preserved. To drop or change a row key field, use; :meth:`MatrixTable.key_rows_by`. Note; ----. See :meth:`.Table.select` for more information about using ``sel",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Performance,optimiz,optimizer,"ll rate; as an entry field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate(call_rate = hl.agg.fraction(hl.is_defined(dataset.GT)))). Notes; -----; All complex expressions must be passed as named expressions. Parameters; ----------; exprs : args of :class:`str` or :class:`.Expression`; Column fields to group by.; named_exprs : keyword args of :class:`.Expression`; Column-indexed expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varar",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Safety,avoid,avoid,"with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; # checkpoint rather than write to use fast codec; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Paramet",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Testability,log,logging,"func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; """"""Write to disk. Examples; --------. >>> dataset.write('output/dataset.mt'). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_matrix_table`. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination.; """"""; """"""Print the first few rows of the matrix table to the console. .. include:: _templates/experimental.rst. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> mt.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n_rows : :obj:`int`; Maximum number of rows to show.; n_cols : :obj:`int`; Maximum number of columns to show.; width : :obj:`int`; Horizontal width at which to break fields.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; handler : Callable[[str], Any]; Handler function for data string.; """"""; # borders; # 4 for the column index; """"""Returns a table with a single row with the globals of the matrix table. Examples; --------; Extract the globals table:. >>> globals_table = dataset.globals_table(). Returns; -------; :class:`.Table`; Table with the globals from the matrix, with a single row.; """"""; """"""Returns a table with all row fields in the matrix. Examples; --------; Extract the row table:. >>> rows_table = dataset.rows(). Returns; -------; :class:`.Table",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Usability,guid,guide,"Parameters; ----------; handler : Callable[[str], None]; Handler function for returned string.; widget : bool; Create an interactive IPython widget.; """"""; """"""Choose a new set of columns from a list of old column indices. Examples; --------. Randomly shuffle column order:. >>> import random; >>> indices = list(range(dataset.count_cols())); >>> random.shuffle(indices); >>> dataset_reordered = dataset.choose_cols(indices). Take the first ten columns:. >>> dataset_result = dataset.choose_cols(list(range(10))). Parameters; ----------; indices : :obj:`list` of :obj:`int`; List of old column indices. Returns; -------; :class:`.MatrixTable`; """"""; """"""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept ",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
Availability,down,downstream," to worry about memory leak; # key is in __dir for methods; # key is in __dict__ for private class fields; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregat",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Deployability,pipeline,pipeline,"""""""Sort by `col` ascending.""""""; """"""Sort by `col` descending.""""""; # this can only grow as big as the object dir, so no need to worry about memory leak; # key is in __dir for methods; # key is in __dict__ for private class fields; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; --",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Energy Efficiency,reduce,reduces," to worry about memory leak; # key is in __dir for methods; # key is in __dict__ for private class fields; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregat",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Integrability,depend,dependent,"on language restrictions. >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; ----; This method does not support aggregation in key expressions. Arguments; ---------; exprs : varargs of type str or :class:`.Expression`; Field names or field reference expressions.; named_exprs : keyword args of type :class:`.Expression`; Field names and expressions to compute them. Returns; -------; :class:`.GroupedTable`; Grouped table; use :meth:`.GroupedTable.aggregate` to complete the aggregation.; """"""; """"""Aggregate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; """"""Write to disk. Examples; --------. >>> table1.write('output/table1.ht', overwrite=True). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_table`. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to tem",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Modifiability,variab,variable-length,"---; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:. >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; -----; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified global fields. """"""; """"""Similar to :meth:`.Table.annotate_globals`, but drops referenced fields. Notes; -----; Consider a table with global fields `population`, `area`, and `year`:. >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(population=1000000, area=500, year=2020). Compute a new field, `density` from `population` and `area` and also drop the latter two; fields:. >>> ht = ht.transmute_globals(density=ht.population / ht.area); >>> ht.globals.show(); +-------------+----------------+; | <expr>.year | <expr>.density |; +-------------+----------------+; | int32 | float64 |; +-------------+----------------+; | 2020 | 2.00e+03 |; +-------------+----------------+. Introduce a new global field `",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Performance,optimiz,optimizer,"""""""Sort by `col` ascending.""""""; """"""Sort by `col` descending.""""""; # this can only grow as big as the object dir, so no need to worry about memory leak; # key is in __dir for methods; # key is in __dict__ for private class fields; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """"""; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; """"""Aggregate by group, used after :meth:`.Table.group_by`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; --",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Safety,avoid,avoid,": table[m.global_index_into_table]; # match on indices to determine join type; # foreign-key join; # contains original key and join key; # FIXME: Maybe zip join here?; # key is already correct; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; """"""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X`",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Security,expose,expose,"prs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; # FIXME: this should be OK: table[m.global_index_into_table]; # match on indices to determine join type; # foreign-key join; # contains ",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Testability,log,login,"5, 'b': 10}, {'a': 0, 'b': 200}],; ... schema=hl.tstruct(a=hl.tint, b=hl.tint),; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+------------",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Usability,guid,guide,"value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.Table`; Aggregated table.; """"""; """"""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
Safety,detect,detect,"# F403 'from .expr import *' used; unable to detect undefined names; # F401 '.expr.*' imported but unused; # E402 module level import not at top of file; # ruff: noqa: E402; # noqa: F403; # noqa: F403; # noqa: F403; # don't overwrite builtins in `from hail import *`; # set by hail.version(); # set by hail.revision()",MatchSource.CODE_COMMENT,hail/python/hail/__init__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/__init__.py
Availability,down,down,"# This defaults to 65536 and fails if a header is longer than _MAXLINE; # The timing json that we output can exceed 65536 bytes so we raise the limit; # The original `get_return_value` is not patched, it's idempotent.; # only patch the one used in py4j.java_gateway (call Java API); # py4j catches NoSuchElementExceptions to stop array iteration; """"""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""; # The thread should be a daemon so that it shuts down when the parent thread is killed; # By default, py4j's version of this function does extra; # work to support python 2. This eliminates that.; # This has to go after creating the SparkSession. Unclear why.; # Maybe it does its own patch?",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
Deployability,patch,patched,"# This defaults to 65536 and fails if a header is longer than _MAXLINE; # The timing json that we output can exceed 65536 bytes so we raise the limit; # The original `get_return_value` is not patched, it's idempotent.; # only patch the one used in py4j.java_gateway (call Java API); # py4j catches NoSuchElementExceptions to stop array iteration; """"""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""; # The thread should be a daemon so that it shuts down when the parent thread is killed; # By default, py4j's version of this function does extra; # work to support python 2. This eliminates that.; # This has to go after creating the SparkSession. Unclear why.; # Maybe it does its own patch?",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
Integrability,message,messages,"# This defaults to 65536 and fails if a header is longer than _MAXLINE; # The timing json that we output can exceed 65536 bytes so we raise the limit; # The original `get_return_value` is not patched, it's idempotent.; # only patch the one used in py4j.java_gateway (call Java API); # py4j catches NoSuchElementExceptions to stop array iteration; """"""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""; # The thread should be a daemon so that it shuts down when the parent thread is killed; # By default, py4j's version of this function does extra; # work to support python 2. This eliminates that.; # This has to go after creating the SparkSession. Unclear why.; # Maybe it does its own patch?",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
Usability,simpl,simple,"# This defaults to 65536 and fails if a header is longer than _MAXLINE; # The timing json that we output can exceed 65536 bytes so we raise the limit; # The original `get_return_value` is not patched, it's idempotent.; # only patch the one used in py4j.java_gateway (call Java API); # py4j catches NoSuchElementExceptions to stop array iteration; """"""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""; # The thread should be a daemon so that it shuts down when the parent thread is killed; # By default, py4j's version of this function does extra; # work to support python 2. This eliminates that.; # This has to go after creating the SparkSession. Unclear why.; # Maybe it does its own patch?",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
Integrability,protocol,protocol,"# is.hail.backend.service.Main protocol; # is.hail.backend.service.ServiceBackendSocketAPI2 protocol; # it is not possible for the batch to be finished in less than 600ms; # Sequence and liftover information is stored on the ReferenceGenome; # and there is no persistent backend to keep in sync.; # Sequence and liftover information are passed on RPC; # pylint: disable=unused-argument; # FIXME Not only should this be in the cloud, it should be in the *right* cloud; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # FIXME: should use context manager to clean up persisted resources",MatchSource.CODE_COMMENT,hail/python/hail/backend/service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/service_backend.py
Performance,load,load,"# Setting extraClassPath in HDInsight overrides the classpath entirely so you can't; # load the Scala standard library. Interestingly, setting extraClassPath is not; # necessary in HDInsight.",MatchSource.CODE_COMMENT,hail/python/hail/backend/spark_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/spark_backend.py
Availability,avail,available,"nc: etc. cross-reference text.; #; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; #; # add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; #; # show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; # A list of ignored prefixes for module index sorting.; # modindex_common_prefix = []; # If true, keep warnings as ""system message"" paragraphs in the built documents.; # keep_warnings = False; # If true, `todo` and `todoList` produce output, else they produce nothing.; # -- Options for HTML output ----------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; #; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #; # Add any paths that contain custom themes here, relative to this directory.; # The name for this set of Sphinx documents.; # ""<project> v<release> documentation"" by default.; #; # A shorter title for the navigation bar. Default is the same as html_title.; #; # html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; #; # html_logo = None; # The name of an image file (relative to this directory) to use as a favicon of; # the docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32; # pixels large.; #; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # Add any extra paths that contain custom files (such as robots.txt or; # .htaccess) here, relative to this directory. These files are c",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #; # sys.path.insert(0, os.path.abspath('.')); # import sphinx_rtd_theme; # -- General configuration ------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # https://github.com/spatialaudio/nbsphinx/issues/24#issuecomment-187172022 and https://github.com/ContinuumIO/anaconda-issues/issues/1430; # F821 undefined name 'tags'; # noqa: F821; # these flags have the same effect: they run the notebook and save the output in the generated; # rST file.; # autoclass_content = ""both""; # https://github.com/hail-is/hail/pull/9403#issuecomment-703776111; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; #; # source_suffix = ['.rst', '.md']; # The encoding of source files.; #; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The full version, including alph",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
Integrability,message,message,"t is used:; #; # today = ''; #; # Else, today_fmt is used as the format for a strftime call.; #; # today_fmt = '%B %d, %Y'; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # This patterns also effect to html_static_path and html_extra_path; # The reST default role (used for this markup: `text`) to use for all; # documents.; #; # default_role = None; # If true, '()' will be appended to :func: etc. cross-reference text.; #; # add_function_parentheses = True; # If true, the current module name will be prepended to all description; # unit titles (such as .. function::).; #; # add_module_names = True; # If true, sectionauthor and moduleauthor directives will be shown in the; # output. They are ignored by default.; #; # show_authors = False; # The name of the Pygments (syntax highlighting) style to use.; # A list of ignored prefixes for module index sorting.; # modindex_common_prefix = []; # If true, keep warnings as ""system message"" paragraphs in the built documents.; # keep_warnings = False; # If true, `todo` and `todoList` produce output, else they produce nothing.; # -- Options for HTML output ----------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; #; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #; # Add any paths that contain custom themes here, relative to this directory.; # The name for this set of Sphinx documents.; # ""<project> v<release> documentation"" by default.; #; # A shorter title for the navigation bar. Default is the same as html_title.; #; # html_short_title = None; # The name of an image file (relative to this directory) to place at the top; # of the sidebar.; #; # html_logo = None; # The name of an image file (relative to this directory) to use as a favicon of;",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
Modifiability,config,configuration,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #; # sys.path.insert(0, os.path.abspath('.')); # import sphinx_rtd_theme; # -- General configuration ------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # https://github.com/spatialaudio/nbsphinx/issues/24#issuecomment-187172022 and https://github.com/ContinuumIO/anaconda-issues/issues/1430; # F821 undefined name 'tags'; # noqa: F821; # these flags have the same effect: they run the notebook and save the output in the generated; # rST file.; # autoclass_content = ""both""; # https://github.com/hail-is/hail/pull/9403#issuecomment-703776111; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; #; # source_suffix = ['.rst', '.md']; # The encoding of source files.; #; # source_encoding = 'utf-8-sig'; # The master toctree document.; # General information about the project.; # The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The full version, including alph",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
Safety,avoid,avoid,"ize ('10pt', '11pt' or '12pt').; #; # 'pointsize': '10pt',; # Additional stuff for the LaTeX preamble.; #; # 'preamble': '',; # Latex figure (float) alignment; #; # 'figure_align': 'htbp',; # Grouping the document tree into LaTeX files. List of tuples; # (source start file, target name, title,; # author, documentclass [howto, manual, or own class]).; # The name of an image file (relative to this directory) to place at the top of; # the title page.; #; # latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; #; # latex_use_parts = False; # If true, show page references after internal links.; #; # latex_show_pagerefs = False; # If true, show URL addresses after external links.; #; # latex_show_urls = False; # Documents to append as an appendix to all manuals.; #; # latex_appendices = []; # It false, will not define \strong, \code, 	itleref, \crossref ... but only; # \sphinxstrong, ..., \sphinxtitleref, ... To help avoid clash with user added; # packages.; #; # latex_keep_old_macro_names = True; # If false, no module index is generated.; #; # latex_domain_indices = True; # -- Options for manual page output ---------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).; # If true, show URL addresses after external links.; #; # man_show_urls = False; # -- Options for Texinfo output -------------------------------------------; # Grouping the document tree into Texinfo files. List of tuples; # (source start file, target name, title, author,; # dir menu entry, description, category); # Documents to append as an appendix to all manuals.; #; # texinfo_appendices = []; # If false, no module index is generated.; #; # texinfo_domain_indices = True; # How to display URL addresses: 'footnote', 'no', or 'inline'.; #; # texinfo_show_urls = 'footnote'; # If true, do not generate a @detailmenu in the ""Top"" node's menu.; #; # texinfo_no_det",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
Availability,avail,available,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
Performance,load,load,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
Availability,avail,available,""""""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """"""; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; """"""Find the prefix of the given index",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
Deployability,configurat,configuration,"---------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versio",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
Modifiability,config,configuration,"---------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versio",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
Security,access,access,""""""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """"""; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; """"""Find the prefix of the given index",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
Availability,avail,available,"invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """"""; """"""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platf",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
Performance,load,load,"Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """"""; """"""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Param",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
Testability,test,test,"; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. If the ``reference_genome`` parameter is specified, the start and end; points of the ``interval`` field will be of type :class:`.tlocus`.; Otherwise, the start and end points of the ``interval`` field will be of; type :class:`.tstruct` with fields ``seqname`` (type :class:`str`) and; ``position`` (type :obj:`.tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip l",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
Deployability,continuous,continuous,"""""""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; -------; :func:`.ld_score` will fail if ``entry_expr`` results in any missing; values. The special float value ``nan`` is not considered a; missing value. **Further reading**. For more in-depth discussion of LD scores, see:. - `LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4495769/>`__; ",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscore.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscore.py
Availability,error,error,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; """"""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """"""; # multi-trait annotation-informed; # multi-trait correlated infinitesimal; # two trait correlated spike & slab; # single trait infinitesimal/spike & slab; # seed random state for replicability; # seed random state for replicability; # check positive semidefinite; # expected number of rg values, given number of traits; # set upper triangle of correlation matrix to be rg; # covariance matrix decomposition; # check positive semidefinite; # if >1 traits; # normalize genetic component of phenotype to have variance of exactly h2; # TODO: Add MAF filter to remove invariant SNPs?; # when axis='rows' we're searching for annotations, axis='cols' searching for covariates; # take all row (or col) fields in mt matching keys in coef_dict; # if intersect is empty: return error; # return subset of ref_coef_dict; # str_expr search in list of row (or col) fields; # fields in ref_coef_dict; # if >0 fields returned by search are not in ref_coef_dict; # if none of the fields returned by search are in ref_coef_dict; # ""+ 1"" because of zero indexing; # use inverse CDF",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscsim.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py
Testability,test,testing,"#!/usr/bin/env python3; # -*- coding: utf-8 -*-; """"""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """"""; # multi-trait annotation-informed; # multi-trait correlated infinitesimal; # two trait correlated spike & slab; # single trait infinitesimal/spike & slab; # seed random state for replicability; # seed random state for replicability; # check positive semidefinite; # expected number of rg values, given number of traits; # set upper triangle of correlation matrix to be rg; # covariance matrix decomposition; # check positive semidefinite; # if >1 traits; # normalize genetic component of phenotype to have variance of exactly h2; # TODO: Add MAF filter to remove invariant SNPs?; # when axis='rows' we're searching for annotations, axis='cols' searching for covariates; # take all row (or col) fields in mt matching keys in coef_dict; # if intersect is empty: return error; # return subset of ref_coef_dict; # str_expr search in list of row (or col) fields; # fields in ref_coef_dict; # if >0 fields returned by search are not in ref_coef_dict; # if none of the fields returned by search are in ref_coef_dict; # ""+ 1"" because of zero indexing; # use inverse CDF",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscsim.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py
Performance,load,loadings,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
Integrability,wrap,wrapper," Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y. In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn't participate in allele transmission). Note; ----; :func:`~.phase_trio_matrix_by_transmission` provides a convenience wrapper for phasing a trio matrix. Parameters; ----------; locus : :class:`.LocusExpression`; Expression for the locus in the trio matrix; alleles : :class:`.ArrayExpression`; Expression for the alleles in the trio matrix; proband_call : :class:`.CallExpression`; Expression for the proband call in the trio matrix; father_call : :class:`.CallExpression`; Expression for the father call in the trio matrix; mother_call : :class:`.CallExpression`; Expression for the mother call in the trio matrix. Returns; -------; :class:`.ArrayExpression`; Array containing: [phased proband call, phased father call, phased mother call]""""""; """"""; Get the set of all different one-hot-encoded allele-vectors in a genotype call.; It is returned as an ordered array where the first vector corresponds to the first allele,; and the second vector (only present if het) the second allele. :param CallExpression call: genotype; :param ArrayExpression alleles: Alleles at the site; :return: Array of one-hot-enc",MatchSource.CODE_COMMENT,hail/python/hail/experimental/phase_by_transmission.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/phase_by_transmission.py
Modifiability,variab,variable-length,"""""""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields.""""""; """"""Spread a key-value pair of fields across multiple fields. :func:`.spread` mimics the functionality of the `spread()` function in R's; `tidyr` package. This is a way to turn ""long"" format data into ""wide""; format data. Given a ``field``, :func:`.spread` will create a new table by grouping; ``ht`` by its row key and, optionally, any additional fields passed to the; ``key`` argument. After collapsing ``ht`` by these keys, :func:`.spread` creates a new row field; for each unique value of ``field``, where the row field values are given by the; corresponding ``value`` in the original ``ht``. Parameters; ----------; ht : :class:`.Table`; A Hail table.; field : :class:`str`; The name of the factor field in `ht`.; value : :class:`str`; The name of the value field in `ht`.; key : optional, obj:`str` or list of :class:`str`; The name of any fields to group by, in addition to the; row key fields of ``ht``. Returns; -------; :class:`.Table`; Table with original ``key`` and ``value`` fields spread across multiple columns.""""""; """"""Separate a field into multiple fields by splitting on a delimiter; character or position. :func:`.separate` mimics the functionality of the `separate()` function in R's; ``tidyr`` package. This function will create a new table where ``field`` has been split into; multiple new fields, whose names are given by ``into``. If ``delim`` is a ``str`` (including r",MatchSource.CODE_COMMENT,hail/python/hail/experimental/tidyr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/tidyr.py
Availability,down,downstream,"""""""Convert sparse matrix table to a dense VCF-like representation by expanding reference blocks. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to densify. The first row key field must; be named ``locus`` and have type ``locus``. Must have an; ``END`` entry field of type ``int32``. Returns; -------; :class:`.MatrixTable`; The densified MatrixTable. The ``END`` entry field is dropped. While computationally expensive, this; operation is necessary for many downstream analyses, and should be thought of as; roughly costing as much as reading a matrix table created by importing a dense; project VCF.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/densify.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/densify.py
Availability,down,downcoded," This function drops the `LA` (local alleles) field, as it re-computes; entry fields based on the new, split globals alleles. Variants are split thus:. - A row with only one (reference) or two (reference and alternate) alleles; is unchanged, as local and global alleles are the same. - A row with multiple alternate alleles will be split, with one row for; each alternate allele, and each row will contain two alleles: ref and alt.; The reference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with ei",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
Deployability,update,updated,"ference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that st",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
Testability,log,logic,"""""""Splits multiallelic variants on a sparse matrix table. Analogous to :func:`.split_multi_hts` (splits entry fields) for sparse; representations. Takes a dataset formatted like the output of :func:`.run_combiner`. The; splitting will add `was_split` and `a_index` fields, as :func:`.vds.split_multi`; does. This function drops the `LA` (local alleles) field, as it re-computes; entry fields based on the new, split globals alleles. Variants are split thus:. - A row with only one (reference) or two (reference and alternate) alleles; is unchanged, as local and global alleles are the same. - A row with multiple alternate alleles will be split, with one row for; each alternate allele, and each row will contain two alleles: ref and alt.; The reference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row ",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
Deployability,patch,patch,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/blockmatrix_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/blockmatrix_type.py
Availability,error,error," return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Add a test for missingness. If the `base` expression is missing,; returns `then`. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBuilder.default` method calls must be the; same type. Parameters; ----------; missing_false: :obj:`.bool`; Treat missing predicates as ``False``. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`; """"""; """"""Add a branch. If `condition` is ``True``, then returns `then`. Warning; -------; Missingne",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
Integrability,message,message," return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Add a test for missingness. If the `base` expression is missing,; returns `then`. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; """"""Class for chaining multiple if-else statements. Examples; --------. >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.CaseBuilder.when` or; :meth:`~hail.expr.builders.CaseBuilder.default` method calls must be the; same type. Parameters; ----------; missing_false: :obj:`.bool`; Treat missing predicates as ``False``. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`; """"""; """"""Add a branch. If `condition` is ``True``, then returns `then`. Warning; -------; Missingne",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
Testability,test,test,"""""""Class for generating conditional trees based on value of an expression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.SwitchBuilder.when` or; :meth:`~hail.expr.builders.SwitchBuilder.default` method calls must be the; same type. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`. Parameters; ----------; expr : :class:`.Expression`; Value to match against.; """"""; # build cond chain bottom-up; # needs to be on the outside, because upstream missingness would propagate; """"""Add a value test. If the `base` expression is equal to `value`, then; returns `then`. Warning; -------; Missingness always compares to missing. Both ``NA == NA`` and; ``NA != NA`` return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Add a test for missingness. If the `base` expression is missing,; returns `then`. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""; """"""Finish the switch statement by adding a default case. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch statement by returning missing. Notes; -----; If no value from a :meth:`~.SwitchBuilder.when` call is matched, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; """"""Finish the switch state",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
Availability,error,error,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; # no compactions ergo no error; """"""Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.missing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; """"""Deprecated in favor of :func:`.missing`. Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Energy Efficiency,power,power,"ssion` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368248444, ci_95_upper=9.677929632035475). Notes; -----; This method is identical to the version implemented in; `R <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/fisher.test.html>`_ with default; parameters (two-sided, alph",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Modifiability,variab,variable,"sing(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.missing('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; """"""Deprecated in favor of :func:`.missing`. Creates an expression representing a missing value of a specified type. Examples; --------. >>> hl.eval(hl.null(hl.tarray(hl.tstr))); None. >>> hl.eval(hl.null('array<str>')); None. Notes; -----; This method is useful for constructing an expression that includes missing; values, since :obj:`None` cannot be interpreted as an expression. Parameters; ----------; t : :class:`str` or :class:`.HailType`; Type of the missing expression. Returns; -------; :class:`.Expression`; A missing expression of type `t`.; """"""; """"""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; """"""Deprecated in favor of",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Performance,perform,performs,"); 3.0. Parameters; ----------; x : :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference g",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Safety,safe,safeguards,"ation of `7` is `111`, so:. >>> hl.eval(hl.bit_count(7)); 3. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; ----------; :class:`.Int32Expression`; """"""; """"""Binary search `array` for the insertion point of `elem`. Parameters; ----------; array : :class:`.Expression` of type :class:`.tarray`; elem : :class:`.Expression`. Returns; -------; :class:`.Int32Expression`. Notes; -----; This function assumes that `array` is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last. The returned index is the lower bound on the insertion point of `elem` into; the ordered array, or the index of the first element in `array` not smaller; than `elem`. This is a value between 0 and the length of `array`, inclusive; (if all elements in `array` are smaller than `elem`, the returned value is; the length of `array` or the index of the first missing value, if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""; """"""Randomly permute an array. Example; -------. >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters; ----------; a : :class:`.ArrayExpression`; Array to permute.; seed : :obj:`int`, optional; Random seed. Returns; -------; :class:`.ArrayExpression`; """"""; """"""Query records from a table corresponding to a given point or range of keys. Notes; -----; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in :meth:`.Table.annotate`. Warning; -------; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters; ----------; path : :class:`str`; Table path.; point_or_interval; Point or interval to query. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Testability,test,tests," hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""; # Special handling of numpy. Have to extract from numpy scalars, do nothing on numpy arrays; """"""Deprecated in favor of :func:`.if_else`. Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `predicate` is missing, returns; missing. Note; ----; The type of `consequent` and `alternate` must be the same. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test.; consequent : :class:`.Expression`; Branch to return if the condition is ``True``.; alternate : :class:`.Expression`; Branch to return if the condition is ``False``.; missing_false : :obj:`.bool`; If ``True``, treat missing `condition` as ``False``. See Also; --------; :func:`.case`, :func:`.switch`, :func:`.if_else`. Returns; -------; :class:`.Expression`; One of `consequent`, `alternate`, or missing, based on `cond",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
Deployability,patch,patch,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/matrix_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/matrix_type.py
Deployability,patch,patch,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/table_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/table_type.py
Deployability,patch,patch,"; Interval point type. See Also; --------; :class:`.IntervalExpression`, :class:`.Interval`, :func:`.interval`,; :func:`.parse_locus_interval`; """"""; """"""Interval point type. Returns; -------; :class:`.HailType`; Interval point type.; """"""; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int32Expression`, :func:`.int`, :func:`.int32`; """"""; """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`. See Also; --------; :class:`.Int64Expression`, :func:`.int64`; """"""; """"""Alias for :py:data:`.tint32`.""""""; """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`. See Also; --------; :class:`.Float32Expression`, :func:`.float64`; """"""; """"""Hail type for 64-bit floating point numbers. In Python, these are represented as :obj:`float`. See Also; --------; :class:`.Float64Expression`, :func:`.float`, :func:`.float64`; """"""; """"""Alias for :py:data:`.tfloat64`.""""""; """"""Hail type for text strings. In Python, these are represented as strings. See Also; --------; :class:`.StringExpression`, :func:`.str`; """"""; """"""Hail type for Boolean (``True`` or ``False``) values. In Python, these are represented as :obj:`bool`. See Also; --------; :class:`.BooleanExpression`, :func:`.bool`; """"""; """"""Hail type for a diploid genotype. In Python, these are represented by :class:`.Call`. See Also; --------; :class:`.CallExpression`, :class:`.Call`, :func:`.call`, :func:`.parse_call`,; :func:`.unphased_diploid_gt_index_call`; """"""; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; # monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
Integrability,wrap,wrapper,"represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """"""; """"""Set element type. Returns; -------; :class:`.HailType`; Element type.; """"""; """"""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """"""; """"""Dict key type. Returns; -------; :class:`.HailType`; Key type.; """"""; """"""Dict value type. Returns; -------; :class:`.HailType`; Value type.; """"""; # NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.; """"""Hail type for structured groups of heterogeneous fields. In Python, these are represented as :class:`.Struct`. Hail's :class:`.tstruct` type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique. Structs are very common in Hail. Each component of a :class:`.Table` and :class:`.MatrixTable`; is a struct:. - :meth:`.Table.row`; - :meth:`.Table.globals`; - :meth:`.MatrixTable.row`; - :meth:`.MatrixTable.col`; - :meth:`.MatrixTable.entry`; - :meth:`.MatrixTable.globals`. Structs appear below the top-level component types as well. Consider the following join:. >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to ``table1`` called ``table2_fields``. In the new table,; ``table2_fields`` will be a struct containing all the non-key fields from",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
Modifiability,parameteriz,parameterized,"n if; the function returns ``True``.; """"""; """"""Hail type for signed 32-bit integers. Their values can range from :math:`-2^{31}` to :math:`2^{31} - 1`; (approximately 2.15 billion). In Python, these are represented as :obj:`int`.; """"""; """"""Hail type for signed 64-bit integers. Their values can range from :math:`-2^{63}` to :math:`2^{63} - 1`. In Python, these are represented as :obj:`int`.; """"""; """"""Hail type for 32-bit floating point numbers. In Python, these are represented as :obj:`float`.; """"""; """"""Hail type for 64-bit floating point numbers. In Python, these are represented as :obj:`float`.; """"""; """"""Hail type for text strings. In Python, these are represented as strings.; """"""; """"""Hail type for Boolean (``True`` or ``False``) values. In Python, these are represented as :obj:`bool`.; """"""; """"""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """"""; """"""NDArray element type. Returns; -------; :class:`.HailType`; Element type.; """"""; """"""NDArray number of dimensions. Returns; -------; :obj:`int`; Number of dimensions.; """"""; """"""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """"""; """"""Array element type. Returns; -------; :class:`.HailType`; Element type.; """"""; """"""Hail type for collections of distinct elements. In Python, these are r",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
Testability,assert,assert,"truct containing all the non-key fields from ``table2``. Parameters; ----------; field_types : keyword args of :class:`.HailType`; Fields. See Also; --------; :class:`.StructExpression`, :class:`.Struct`; """"""; """"""Struct field types. Returns; -------; :obj:`tuple` of :class:`.HailType`; """"""; """"""Struct field names. Returns; -------; :obj:`tuple` of :class:`str`; Tuple of struct field names.; """"""; """"""Tagged union type. Values of type union represent one of several; heterogenous, named cases. Parameters; ----------; cases : keyword args of :class:`.HailType`; The union cases. """"""; """"""Return union case names. Returns; -------; :obj:`tuple` of :class:`str`; Tuple of union case names; """"""; """"""Hail type for tuples. In Python, these are represented as :obj:`tuple`. Parameters; ----------; types: varargs of :class:`.HailType`; Element types. See Also; --------; :class:`.TupleExpression`; """"""; """"""Tuple element types. Returns; -------; :obj:`tuple` of :class:`.HailType`; """"""; # TODO another assert; """"""Hail type for a diploid genotype. In Python, these are represented by :class:`.Call`.; """"""; """"""Hail type for a genomic coordinate with a contig and a position. In Python, these are represented by :class:`.Locus`. Parameters; ----------; reference_genome: :class:`.ReferenceGenome` or :class:`str`; Reference genome to use. See Also; --------; :class:`.LocusExpression`, :func:`.locus`, :func:`.parse_locus`,; :class:`.Locus`; """"""; # must match TLocus.schemaFromRG; """"""Reference genome. Returns; -------; :class:`.ReferenceGenome`; Reference genome.; """"""; """"""Hail type for intervals of ordered values. In Python, these are represented by :class:`.Interval`. Parameters; ----------; point_type: :class:`.HailType`; Interval point type. See Also; --------; :class:`.IntervalExpression`, :class:`.Interval`, :func:`.interval`,; :func:`.parse_locus_interval`; """"""; """"""Interval point type. Returns; -------; :class:`.HailType`; Interval point type.; """"""; """"""Hail type for signed 32-bit integers. Their ",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
Availability,down,downstream,"aining two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; """"""Compute an array of approximate quantiles. Examples; --------; Estimate the median of the `HT` field. >>> table1.aggregate(hl.agg.approx_quantiles(table1.HT, 0.5)) # doctest: +SKIP_OUTPUT_CHECK; 64. Estimate the quartiles of the `HT` field. >>> table1.aggregate(hl.agg.approx_quantiles(table1.HT, [0, 0.25, 0.5, 0.75, 1])) # doctest: +SKIP_OUTPUT_CHECK; [50, 60, 64, 71, 86]. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; qs : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`; Number or arra",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Integrability,depend,dependent,"d standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; # residual standard error squared; """"""Computes the; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pears",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Modifiability,variab,variable,"_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""; # residual standard error squared; """"""Computes the; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between `x` and `y`. Examples; --------; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) # doctest: +SKIP_OUTPUT_CHECK; 0.16592876044845484. Notes; -----; Only records where both `x` and `y` are non-missing will be included in the; calculation. In the case that there are no non-missing pairs, the result will be missing. See Also; --------; :func:`linreg`. Parameter",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Performance,perform,performs,"f type :py:data:`.tint64` or :py:data:`.tfloat64`; Product of records of `expr`.; """"""; """"""Compute the fraction of records where `predicate` is ``True``. Examples; --------; Compute the fraction of rows where `SEX` is ""F"" and `HT` > 65:. >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Security,access,accessing,"# Tricky, all of initial_value, seq_op, comb_op need to be same type. Need to see if any change the others.; # Now, that might have changed comb_op type? Could be more general than other 2.; """"""Compute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; """"""Compute an array of approximate quantiles. Examples; --------; Estimate the median",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Testability,test,test,"the number of records where a predicate is ``True``. Examples; --------. Count the number of individuals with `HT` greater than 68:. >>> table1.aggregate(hl.agg.count_where(table1.HT > 68)); 2. Parameters; ----------; condition : :class:`.BooleanExpression`; Criteria for inclusion. Returns; -------; :class:`.Expression` of type :py:data:`.tint64`; Total number of records where `condition` is ``True``.; """"""; """"""Returns ``True`` if `condition` is ``True`` for any record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(any_over_70 = hl.agg.any(table1.HT > 70)); ... .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; -----; If there are no records to aggregate, the result is ``False``. Missing records are not considered. If every record is missing,; the result is also ``False``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; """"""Returns ``True`` if `condition` is ``True`` for every record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; -----; If there are no records to aggregate, the result is ``True``. Missing records are not considered. If every record is missing,; the result is also ``True``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""; """"""Count the occurrences of each unique record and return a dictionary. Examples; --------; Count the number of individuals for each unique `SEX` value:. >>> table1.aggregate(hl.agg.counter(table1.SEX)); {'F': 2, 'M': 2}; <BLANKLINE>. Compute the total height ",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Usability,intuit,intuition,"ompute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y  values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""; """"""Compute an array of approximate quantiles. Examples; --------; Estimate the median of the `HT` field. >>> table1.aggregate(hl.agg.approx_quantiles(table1.HT, 0.5)) # doctest: +SKIP_OUTPUT_CHECK; 64. Estimate the quartiles of the `HT` field. >>> table1.aggregate(hl.agg.approx_q",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
Availability,error,error,"# these are not container types and cannot contain expressions if we got here; # Here I use `to_expr` to call `lit` the keys and values separately.; # I anticipate a common mode is statically-known keys and Expression; # values.; # source mismatch; # only one distinct class; # assert there are at least 2 numeric types; # all types are the same; # cannot coerce all types to the same type; """"""Base class for Hail expressions.""""""; # disable NumPy coercions, so Hail coercions take priority; """"""Print information about type, index, and dependencies.""""""; # Float64 or Float32; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; |",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
Integrability,depend,dependencies,"# these are not container types and cannot contain expressions if we got here; # Here I use `to_expr` to call `lit` the keys and values separately.; # I anticipate a common mode is statically-known keys and Expression; # values.; # source mismatch; # only one distinct class; # assert there are at least 2 numeric types; # all types are the same; # cannot coerce all types to the same type; """"""Base class for Hail expressions.""""""; # disable NumPy coercions, so Hail coercions take priority; """"""Print information about type, index, and dependencies.""""""; # Float64 or Float32; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; |",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
Testability,assert,assert,"# these are not container types and cannot contain expressions if we got here; # Here I use `to_expr` to call `lit` the keys and values separately.; # I anticipate a common mode is statically-known keys and Expression; # values.; # source mismatch; # only one distinct class; # assert there are at least 2 numeric types; # all types are the same; # cannot coerce all types to the same type; """"""Base class for Hail expressions.""""""; # disable NumPy coercions, so Hail coercions take priority; """"""Print information about type, index, and dependencies.""""""; # Float64 or Float32; """"""The data type of the expression. Returns; -------; :class:`.HailType`. """"""; """"""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""; """"""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; |",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
Usability,learn,learning,"# check for stray indices by subtracting expected axes from observed; # one or more out-of-scope fields; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; # check for stray indices; # one or more out-of-scope fields; """"""Evaluate a Hail expression, returning the result and the times taken for; each stage in the evaluation process. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (Any, dict); Result of evaluating `expression` and a dictionary of the timings; """"""; """"""Evaluate a Hail expression, returning the result. This method is extremely useful for learning about Hail expressions and; understanding how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.Table` or :class:`.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; Any; """"""; """"""Evaluate a Hail expression, returning the result and the type of the result. This method is extremely useful for learning about Hail expressions and understanding; how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.hail.Table` or :class:`.hail.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval_typed(hl.if_else(x % 2 == 0, 'Even', 'Odd')); ('Even', dtype('str')). Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (any, :class:`.HailType`); Result of evaluating `expression`, and its type. """"""; """"""Returns a set of references in `exprs` with indices `indices`. Parameters; ----------; expr : Expression; indices :",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/expression_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/expression_utils.py
Availability,error,error,"(name='Bob', age=12)]),; ... hl.set([hl.struct(name='Charlie', age=34)])]). See Also; --------; :class:`.SetExpression`, class:`.CollectionExpression`, :class:`.SetStructExpression`; """"""; """"""Get a field from each struct in this set. Examples; --------. >>> x = hl.set({hl.struct(a='foo', b=3), hl.struct(a='bar', b=4)}); >>> hl.eval(x.a) == {'foo', 'bar'}; True. >>> a = hl.set({hl.struct(b={hl.struct(inner=1),; ... hl.struct(inner=2)}),; ... hl.struct(b={hl.struct(inner=3)})}); >>> hl.eval(hl.flatten(a.b).inner) == {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """"""; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """"""; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""; """"""Returns the value associated with key `k` or a default value if that key is not present. Examples; --------. >>> hl.eval(d.get('Alice')); 43. >>> hl.eval(d.get('Anne')); None. >>> hl.eval(d.get('Anne', 0)); 0. Parameters; ----------; item : :class:`.Expression`; Key.; default : :class:`.Expression`; Default value. Must be same type as dictionary values. Returns; -------; :cl",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Deployability,pipeline,pipeline,"harlie'. Slicing is also supported:. >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters; ----------; item : slice or :class:`.Expression` of type :py:data:`.tint32`; Index or slice. Returns; -------; :class:`.Expression`; Element or array slice.; """"""; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(names.filter(lambda x: x.startswith('D')).first()); None; """"""; # FIXME: this should generate short-circuiting IR when that is possible; """"""Returns the last element of the array,",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Energy Efficiency,efficient,efficient,"harlie'. Slicing is also supported:. >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters; ----------; item : slice or :class:`.Expression` of type :py:data:`.tint32`; Index or slice. Returns; -------; :class:`.Expression`; Element or array slice.; """"""; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(names.filter(lambda x: x.startswith('D')).first()); None; """"""; # FIXME: this should generate short-circuiting IR when that is possible; """"""Returns the last element of the array,",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Integrability,message,message,"mples; --------. >>> hl.eval(d.items()) # doctest: +SKIP_OUTPUT_CHECK; [('Alice', 430), ('Bob', 330), ('Charles', 440)]. Returns; -------; :class:`.ArrayExpression`; All key/value pairs in the dictionary.; """"""; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; """"""Check each field for equality. Parameters; ----------; other : :class:`.Expression`; An expression of the same type.; """"""; """"""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression in `named_exprs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Fields to add. Returns; -------; :class:`.StructExpression`; Struct with new or updated fields.; """"""; """"""Select existing fields and compute new",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Modifiability,extend,extend,"""""Returns the first index of `x`, or missing. Parameters; ----------; x : :class:`.Expression` or :obj:`typing.Callable`; Value to find, or function from element to Boolean expression. Returns; -------; :class:`.Int32Expression`. Examples; --------; >>> hl.eval(names.index('Bob')); 1. >>> hl.eval(names.index('Beth')); None. >>> hl.eval(names.index(lambda x: x.endswith('e'))); 0. >>> hl.eval(names.index(lambda x: x.endswith('h'))); None; """"""; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.ArrayExpression`.; """"""; """"""Partition an array into fixed size subarrays. Examples; --------; >>> a = hl.array([0, 1, 2, 3, 4]). >>> hl.eval(a.grouped(2)); [[0, 1], [2, 3], [4]]. Parameters; ----------; group_size : :class:`.Int32Expression`; The number of elements per group. Returns; -------; :class:",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Performance,load,load,"enome`. Examples; --------. >>> hl.eval(locus.in_autosome()); True. Returns; -------; :class:`.BooleanExpression`; """"""; """"""Returns ``True`` if the locus is on an autosome or; a pseudoautosomal region of chromosome X or Y. Examples; --------. >>> hl.eval(locus.in_autosome_or_par()); True. Returns; -------; :class:`.BooleanExpression`; """"""; """"""Returns ``True`` if the locus is on mitochondrial DNA. Examples; --------. >>> hl.eval(locus.in_mito()); False. Returns; -------; :class:`.BooleanExpression`; """"""; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """"""; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:d",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Safety,safe,safer,". Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; Size of the dictionary.; """"""; """"""Returns an array with all values in the dictionary. Examples; --------. >>> hl.eval(d.values()) # doctest: +SKIP_OUTPUT_CHECK; [33, 44, 43]. Returns; -------; :class:`.ArrayExpression`; All values in the dictionary.; """"""; """"""Returns an array of tuples containing key/value pairs in the dictionary. Examples; --------. >>> hl.eval(d.items()) # doctest: +SKIP_OUTPUT_CHECK; [('Alice', 430), ('Bob', 330), ('Charles', 440)]. Returns; -------; :class:`.ArrayExpression`; All key/value pairs in the dictionary.; """"""; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; """"""Check each field for equality. Parameters; ----------; other : :class:`.Expression`; An expression of the same type.; """"""; """"""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression ",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Security,access,accessing,"Expression` of type :py:data:`.tint32`; The number of elements in the collection.; """"""; """"""Returns the size of a collection. Examples; --------. >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; The number of elements in the collection.; """"""; """"""Expression of type :class:`.tarray`. >>> names = hl.literal(['Alice', 'Bob', 'Charlie']). See Also; --------; :class:`.CollectionExpression`; """"""; """"""Index into or slice the array. Examples; --------. Index with a single integer:. >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:. >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters; ----------; item : slice or :class:`.Expression` of type :py:data:`.tint32`; Index or slice. Returns; -------; :class:`.Expression`; Element or array slice.; """"""; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Exa",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Testability,test,test,"---; :class:`.CollectionExpression`; """"""; """"""Index into or slice the array. Examples; --------. Index with a single integer:. >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:. >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters; ----------; item : slice or :class:`.Expression` of type :py:data:`.tint32`; Index or slice. Returns; -------; :class:`.Expression`; Element or array slice.; """"""; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
Usability,simpl,simply,"# this method for combatibility with hadoop_open in 0.2; """"""Get information about a path other than its file/directory status. In the cloud, determining if a given path is a file, a directory, or both is expensive. This; method simply returns file metadata if there is a file at this path. If there is no file at; this path, this operation will fail. The presence or absence of a directory at this path; does not affect the behaviors of this method. """"""",MatchSource.CODE_COMMENT,hail/python/hail/fs/hadoop_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/fs/hadoop_fs.py
Testability,test,test,"""""""ID of father in trio, may be missing. :rtype: str or None; """"""; """"""ID of mother in trio, may be missing. :rtype: str or None; """"""; """"""Family ID. :rtype: str or None; """"""; """"""Returns ``True`` if the proband is a reported male,; ``False`` if reported female, and ``None`` if no sex is defined. :rtype: bool or None; """"""; """"""Returns ``True`` if the proband is a reported female,; ``False`` if reported male, and ``None`` if no sex is defined. :rtype: bool or None; """"""; """"""Returns True if the trio has a defined mother and father. The considered fields are :meth:`mat_id` and :meth:`pat_id`.; Recall that ``s`` may never be missing. The :meth:`fam_id`; and :meth:`is_female` fields may be missing in a complete trio. :rtype: bool; """"""; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """"""; """"""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """"""; # 1 is male, 2 is female, 0 is unknown.; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """"""; """"""List of trio objects that have a defined father and mother. :rtype: list of :class:`.Trio`; """"""; """"""Filter the pedigree to a given list of sample IDs. **Notes**. For any trio, the following steps will be applied:. - If the proband is not in the list of samples provided, the trio is removed.; - If the father is not in the list of samples provided, `pat_id` is set to ``None``.; - If the mother is not in the list of samples provided, `mat_id` is set to ``None``. Parameters; ----------; samples: :obj:`list` [:obj:`str`]; Sample IDs to keep. Returns; -------; :class:`.Pedigree`; """"""; """"""Write a .fam file to the given path. **Example",MatchSource.CODE_COMMENT,hail/python/hail/genetics/pedigree.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/pedigree.py
Availability,down,download,"ce genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; """"""Remove the reference sequence.""""""; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
Performance,load,loaded,"at. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; """"""Remove the reference sequence.""""""; """"""Create reference genome from a ",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
Security,access,access,"""""""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
Testability,test,test,"at. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; """"""Remove the reference sequence.""""""; """"""Create reference genome from a ",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
Availability,down,down,"ill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; # Result variables:; # State variables:; # (fx, fy) is",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
Integrability,interface,interface,"ureAttribute`; The geom to be applied.; """"""; """"""Create a line plot. Supported aesthetics: ``x``, ``y``, ``color``, ``tooltip``. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; """"""Create a scatter plot where each point is text from the ``text`` aesthetic. Supported aesthetics: ``x``, ``y``, ``label``, ``color``, ``tooltip``. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; """"""Create a bar chart that counts occurrences of the various values of the ``x`` aesthetic. Supported aesthetics: ``x``, ``color``, ``fill``, ``weight``. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; """"""Create a bar chart that uses bar heights specified in y aesthetic. Supported aesthetics: ``x``, ``y``, ``color``, ``fill``. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; # This assumes it doesn't really make sense to use another stat for geom_histogram; """"""Creates a histogram. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; min_val: `int` or `float`; Minimum value to include in histogram; max_val: `int` or `float`; Maximum value to include in histogram; bins: `int`; Number of bins to plot. 30 by default.; fill:; A single fill color for all bars of histogram, overrides ``fill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; # Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is a",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
Modifiability,variab,variables," and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; # Result variables:; # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low.; # Line must bend down at x[li]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; # Line must bend up at x[ui]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.; """"""Creates a smoothed density plot. This method uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
Deployability,update,updated,"""""""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """"""; # We only know how to come up with a few default scales.; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """"""; # Is there anything to precompute?; # Create scaling functions based on all the data:; # Need to know what I've added to legend already so we don't do it more than once.; # Important to update axes after labels, axes names take precedence.; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; """"""Render and show the plot, either in a browser or notebook.""""""; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
Integrability,interface,interface,"""""""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """"""; # We only know how to come up with a few default scales.; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """"""; # Is there anything to precompute?; # Create scaling functions based on all the data:; # Need to know what I've added to legend already so we don't do it more than once.; # Important to update axes after labels, axes names take precedence.; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; """"""Render and show the plot, either in a browser or notebook.""""""; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
Deployability,continuous,continuous,"# What else do discrete and continuous scales have in common?; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default discrete x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`str`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the tick",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
Testability,log,log,"# What else do discrete and continuous scales have in common?; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; """"""The default discrete x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`str`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the tick",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
Modifiability,variab,variables,"""""""String to be added after IR name in serialized representation. Returns; -------; str; """"""; """"""Compare non-child-BaseIR attributes of the BaseIR. Parameters; ----------; other; BaseIR of the same class. Returns; -------; bool; """"""; """"""Compute variables bound in child 'i'. Returns; -------; dict; mapping from bound variables to 'default_value', if provided,; otherwise to their types; """"""; # Used as a variable, bound by any node which defines the meaning of; # aggregations (e.g. MatrixMapRows, AggFilter, etc.), and ""referenced"" by; # any node which performs aggregations (e.g. AggFilter, ApplyAggOp, etc.).; """"""Elaborate rng semantics in stream typed IR. Recursive transformation of stream typed IRs. Ensures that all; contained seeded randomness gets a unique rng state on every stream; iteration. Optionally inserts a uid in the returned stream element type.; The uid may be an int64, or arbitrary tuple of int64s. The only; requirement is that all stream elements contain distinct uid values.; """"""; """"""Elaborate rng semantics. Recursively transform IR to ensure that all contained seeded randomness; gets a unique rng state on every table row. Optionally inserts a uid; field in the returned table. The uid may be an int64, or arbitrary; tuple of int64s. The only requirement is that all table rows contain; distinct uid values.; """"""; """"""Elaborate rng semantics. Recursively transform IR to ensure that all contained seeded randomness; gets a unique rng state on every evaluation. Optionally inserts a uid; row field and/or column field in the returned matrix table. The uids may; be an int64, or arbitrary tuple of int64s. The only requirement is that; all rows contain distinct uid values, and likewise for columns.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
Performance,perform,performs,"""""""String to be added after IR name in serialized representation. Returns; -------; str; """"""; """"""Compare non-child-BaseIR attributes of the BaseIR. Parameters; ----------; other; BaseIR of the same class. Returns; -------; bool; """"""; """"""Compute variables bound in child 'i'. Returns; -------; dict; mapping from bound variables to 'default_value', if provided,; otherwise to their types; """"""; # Used as a variable, bound by any node which defines the meaning of; # aggregations (e.g. MatrixMapRows, AggFilter, etc.), and ""referenced"" by; # any node which performs aggregations (e.g. AggFilter, ApplyAggOp, etc.).; """"""Elaborate rng semantics in stream typed IR. Recursive transformation of stream typed IRs. Ensures that all; contained seeded randomness gets a unique rng state on every stream; iteration. Optionally inserts a uid in the returned stream element type.; The uid may be an int64, or arbitrary tuple of int64s. The only; requirement is that all stream elements contain distinct uid values.; """"""; """"""Elaborate rng semantics. Recursively transform IR to ensure that all contained seeded randomness; gets a unique rng state on every table row. Optionally inserts a uid; field in the returned table. The uid may be an int64, or arbitrary; tuple of int64s. The only requirement is that all table rows contain; distinct uid values.; """"""; """"""Elaborate rng semantics. Recursively transform IR to ensure that all contained seeded randomness; gets a unique rng state on every evaluation. Optionally inserts a uid; row field and/or column field in the returned matrix table. The uids may; be an int64, or arbitrary tuple of int64s. The only requirement is that; all rows contain distinct uid values, and likewise for columns.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
Modifiability,variab,variable,"# FIXME: If body uses randomness, create a new uid induction variable; # There are occations when handle_randomness is called twice on a; # `StreamMap`: once with `create_uids=False` and the second time; # with `True`. In these cases, we only need to propagate the uid.; # Overloaded to add space after 'agg_op' even if there are no children.; # FIXME: This is pretty fucked, Joins should probably be tracked on Expression?",MatchSource.CODE_COMMENT,hail/python/hail/ir/ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/ir.py
Integrability,depend,dependent,"# FIXME: might cause issues being dependent on col order",MatchSource.CODE_COMMENT,hail/python/hail/ir/matrix_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/matrix_ir.py
Modifiability,variab,variables,"marizing of all lets to be inserted. For each 'node'; # which will have lets inserted immediately above it, maps 'id(node)' to a; # 'BindingSite' recording the depth of 'node' and a Dict 'lifted_lets',; # where for each descendant 'x' which will be bound above 'node',; # 'lifted_lets' maps 'id(x)' to the unique id 'x' will be bound to.; # mark node as visited at potential let insertion site; # if any lets being inserted here, add node to registry of; # binding sites; # 'lets' is either assigned before one of the 'br/has; # we've seen 'child' before, should not traverse (or we will; # find too many lifts); # second time we've seen 'child', lift to a let; # avoid lifting refs; # first time visiting 'child'; # Immutable:; # The node corresponding to this stack frame.; # If 'node' is to be bound in a let, the let may not rise higher; # than this (its depth must be >= 'min_binding_depth').; # The binding context of 'node'. Maps variables bound above to the; # depth at which they were bound (more precisely, if; # 'context[var] == depth', then 'stack[depth-1].node' binds 'var' in; # the subtree rooted at 'stack[depth].node').; # If 'node' is to be bound in a let above this (at a smaller depth); # then it must be in an AggLet, with 'scan_scope' determining; # whether it is bound in the agg scope or scan scope.; # Mutable:; # Sets of visited descendants. For each descendant 'x' of 'node',; # 'id(x)' is added to 'visited'. This allows us to recognize when; # we see a node for a second time.; # A single descendant may need to be bound here in three separate; # scopes, so we must track each scope separately.; # For each descendant 'x' of 'node', if 'x' is to be bound in a Let; # (resp. an AggLet) immediately above 'node', then 'lifted_lets'; # (resp. (agg/scan)_lifted_lets) contains 'id(x)', along with the; # unique id to bind 'x' to.; # The child currently being visited (satisfies the invariant; # 'stack[i].node.children[stack[i].child_idx] is stack[i+1].node').; # Starts at -",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
Safety,avoid,avoid,"# At top of main loop, we are considering the node 'node' and its; # 'child_idx'th child, or if 'child_idx' = 'len(node.children)', we are; # about to do post-processing on 'node' before moving back up to its parent.; #; # 'stack' is a stack of 'StackFrame's, one for each node on the path from; # 'root' to 'node. See 'StackFrame' for descriptions of the maintained state.; #; # Returns a Dict summarizing of all lets to be inserted. For each 'node'; # which will have lets inserted immediately above it, maps 'id(node)' to a; # 'BindingSite' recording the depth of 'node' and a Dict 'lifted_lets',; # where for each descendant 'x' which will be bound above 'node',; # 'lifted_lets' maps 'id(x)' to the unique id 'x' will be bound to.; # mark node as visited at potential let insertion site; # if any lets being inserted here, add node to registry of; # binding sites; # 'lets' is either assigned before one of the 'br/has; # we've seen 'child' before, should not traverse (or we will; # find too many lifts); # second time we've seen 'child', lift to a let; # avoid lifting refs; # first time visiting 'child'; # Immutable:; # The node corresponding to this stack frame.; # If 'node' is to be bound in a let, the let may not rise higher; # than this (its depth must be >= 'min_binding_depth').; # The binding context of 'node'. Maps variables bound above to the; # depth at which they were bound (more precisely, if; # 'context[var] == depth', then 'stack[depth-1].node' binds 'var' in; # the subtree rooted at 'stack[depth].node').; # If 'node' is to be bound in a let above this (at a smaller depth); # then it must be in an AggLet, with 'scan_scope' determining; # whether it is bound in the agg scope or scan scope.; # Mutable:; # Sets of visited descendants. For each descendant 'x' of 'node',; # 'id(x)' is added to 'visited'. This allows us to recognize when; # we see a node for a second time.; # A single descendant may need to be bound here in three separate; # scopes, so we must track ea",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
Availability,resilien,resilience,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Deployability,pipeline,pipelined,"pied to ``output``.; """"""; """"""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Energy Efficiency,power,power,"""""""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the sam",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Integrability,depend,dependency,". One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the same; effect can be achieved for ``*`` by using ``@``. Warning; -------. For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read(",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Performance,perform,performance,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Safety,avoid,avoiding,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Testability,log,logarithm," ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """"""; """"""Reads a block matrix. Parameters; ----------; path: :class:`str`; Path to input file. Returns; -------; :class:`.BlockMatrix`; """"""; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__,; reads a binary file of float64 values in row-major order, such as that; produced by `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__; or :meth:`BlockMatrix.tofile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`Block",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Usability,clear,clear,"ile.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.bu",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
Availability,error,error," pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:class:`.tstruct`) - Column fields on the mother.; - **id** (:py:data:`.tstr`) - Column key for the proband.; - **is_female** (:py:data:`.tbool`) - Proband is female.; ``True`` for female, ``False`` for male, missing if unknown.; - **fam_id** (:py:data:`.tstr`) - Family ID. The new entry fields are:. - **proband_entry** (:class:`.tstruct`) - Proband entry fields.; - **father_entry** (:class:`.tstruct`) - Father entry fields.; - **mother_entry** (:class:`.tstruct`) - Mother entry fields. Parameters; ----------; pedigree : :class:`.Pedigree`. Returns; -------; :class:`.MatrixTable`; """"""; # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; # dummy; # kid, dad, mom, copy, t, u; # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; # this mode is used when families larger than a single trio are observed, in which; # an allele might be de novo in a parent and transmitted to a child in the dataset.; # The original model does not handle this case correctly, and so this experimental; # mode can be used to treat each trio as if it were the only one in the dataset.; # subtract 1 from __alt_alleles to correct for the observed genotype",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
Modifiability,config,config," pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:class:`.tstruct`) - Column fields on the mother.; - **id** (:py:data:`.tstr`) - Column key for the proband.; - **is_female** (:py:data:`.tbool`) - Proband is female.; ``True`` for female, ``False`` for male, missing if unknown.; - **fam_id** (:py:data:`.tstr`) - Family ID. The new entry fields are:. - **proband_entry** (:class:`.tstruct`) - Proband entry fields.; - **father_entry** (:class:`.tstruct`) - Father entry fields.; - **mother_entry** (:class:`.tstruct`) - Mother entry fields. Parameters; ----------; pedigree : :class:`.Pedigree`. Returns; -------; :class:`.MatrixTable`; """"""; # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; # dummy; # kid, dad, mom, copy, t, u; # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; # this mode is used when families larger than a single trio are observed, in which; # an allele might be de novo in a parent and transmitted to a child in the dataset.; # The original model does not handle this case correctly, and so this experimental; # mode can be used to treat each trio as if it were the only one in the dataset.; # subtract 1 from __alt_alleles to correct for the observed genotype",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
Safety,avoid,avoids," pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:class:`.tstruct`) - Column fields on the mother.; - **id** (:py:data:`.tstr`) - Column key for the proband.; - **is_female** (:py:data:`.tbool`) - Proband is female.; ``True`` for female, ``False`` for male, missing if unknown.; - **fam_id** (:py:data:`.tstr`) - Family ID. The new entry fields are:. - **proband_entry** (:class:`.tstruct`) - Proband entry fields.; - **father_entry** (:class:`.tstruct`) - Father entry fields.; - **mother_entry** (:class:`.tstruct`) - Mother entry fields. Parameters; ----------; pedigree : :class:`.Pedigree`. Returns; -------; :class:`.MatrixTable`; """"""; # in implicated, idx 0 is dad, idx 1 is mom, idx 2 is child; # dummy; # kid, dad, mom, copy, t, u; # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; # this mode is used when families larger than a single trio are observed, in which; # an allele might be de novo in a parent and transmitted to a child in the dataset.; # The original model does not handle this case correctly, and so this experimental; # mode can be used to treat each trio as if it were the only one in the dataset.; # subtract 1 from __alt_alleles to correct for the observed genotype",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
Availability,down,downstream,"om their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the `metadata` parameter to supply a dictionary; with the relevant information. See; :func:`get_vcf_metadata` for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured. The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset. The output VCF header will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, re",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Deployability,pipeline,pipeline,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Integrability,interface,interface,"a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""; """"""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a ",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Modifiability,parameteriz,parameterized," VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""; """"""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Performance,load,loaded,"arget** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """"""; """"""invalid interval format. Acceptable formats:; 'chr:start-end'; 'chr start end' (tab-separated); 'chr start end strand target' (tab-separated, strand is '+' or '-')""""""; """"""Import a UCSC BED file as a :class:`.Table`. Examples; --------. The file formats are. .. code-block:: text. $ cat data/file1.bed; track name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field `cnv_region` indicating inclusion in; at least one interval",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Safety,avoid,avoid,"ence_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. """"""; """"""Read in a :class:`.Table` written with :meth:`.Table.write`. Parameters; ----------; path : :class:`str`; File to read. Returns; -------; :class:`.Table`; """"""; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS; """"""Import a csv file as a :class:`.Table`. Examples; --------. Let's import fields from a CSV file with missing data:. .. code-block:: text. $ cat data/samples2.csv; Batch,PT-ID; 1kg,PT-0001; 1kg,PT-0002; study1,PT-0003; study3,PT-0003; .,PT-0004; 1kg,PT-0005; .,PT-0006; 1kg,PT-0007. In this case, we should:. - Pass the non-default missing value ``.``. >>> table = hl.import_csv('data/samples2.csv', missing='.'); >>> table.show(); +----------+-----------+; | Batch | PT-ID |; +----------+-----------+; | str | str |; +----------+-----------+; | ""1kg"" | ""PT-0001"" |; | ""1kg"" | ""PT-0002"" |; | ""study1"" | ""PT-0003"" |; | ""study3"" | ""PT-0003"" |; | NA | ""PT-0004"" |; | ""1kg"" | ""PT-0005"" |; | NA",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Testability,test,test,"indexed) and the column keys 0, 1, ... N.; force_bgz : :obj:`bool`; If ``True``, load **.gz** files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep : :class:`str`; This parameter is a deprecated name for `delimiter`, please use that; instead.; delimiter : :class:`str`; A single character string which separates values in the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns; -------; :class:`.MatrixTable`; MatrixTable constructed from imported data.; """"""; """"""import_matrix_table expects entry types to be one of:; 'int32', 'int64', 'float32', 'float64', 'str': found '{}'""""""; # for checking every header matches; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_refer",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Usability,guid,guide,"g_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. """"""; """"""Read in a :class:`.Table` written with :meth:`.Table.write`. Parameters; ----------; path : :class:`str`; File to read. Returns; -------; :class:`.Table`; """"""; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS; """"""Import a csv file as a :class:`.Table`. Examples; --------. Let's import fields from a CSV file with missing data:. .. code-block:: text. $ cat data/samples2.csv; Batch,PT-ID; 1kg,PT-0001; 1kg,PT-0002; study1,PT-0003; study3,PT-0003; .,PT-0004; 1kg,PT-0005; .,PT-0006; 1kg,PT-0007. In this case, we should:. - Pass the non-default missing value ``.``. >>> table = hl.import_csv('data/samples2.csv', missing='.'); >>> table.show(); +----------+-----------+; | Batch | PT-ID |; +----------+-----------+; | str | str |; +----------+-----------+; | ""1kg"" | ""PT-0001"" |; | ""1kg"" | ""PT-0002"" |; | ""study1"" | ""PT-0003"" |; | ""study3"" | ""PT-0003"" |; | NA | ""PT-0004"" |; | ""1kg"" | ""PT-0005"" |; | NA | ""PT-0006"" |; | ""1kg"" | ""PT-0007"" |; +----------+-----------+; <BLANKLINE>. Notes; -----. The `i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
Modifiability,parameteriz,parameterized,"``False``, return vertices removed.; tie_breaker : function; Function used to order nodes with equal degree.; keyed : :obj:`bool`; If ``True``, key the resulting table by the `node` field, this requires; a sort. Returns; -------; :class:`.Table`; Table with the set of independent vertices. The table schema is one row; field `node` which has the same type as input expressions `i` and `j`.; """"""; """"""Rename duplicate column keys. .. include:: ../_templates/req_tstring.rst. Examples; --------. >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; -----. This method produces a new column field from the string column key by; appending a unique suffix ``_N`` as necessary. For example, if the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """"""; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameter",MatchSource.CODE_COMMENT,hail/python/hail/methods/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py
Performance,load,loaded," the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """"""; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """"""; """"""Segment the interval keys of `ht` at a given set of points. Parameters; ----------; ht : :class:`.Table`; Table with interval keys.; points : :class:`.Table` or :class:`.ArrayExpression`; Points at which to segment the intervals, a table or an array. Returns; -------; :class:`.Tabl",MatchSource.CODE_COMMENT,hail/python/hail/methods/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py
Availability,error,error,"-------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; -----. This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; `name` parameter. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `call_rate` (``float64``) -- Fraction of calls not missing or filtered.; Equivalent to `n_called` divided by :meth:`.count_rows`.; - `n_called` (``int64``) -- Number of non-missing calls.; - `n_not_called` (``int64``) -- Number of missing calls.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_hom_ref` (``int64``) -- Number of homozygous reference calls.; - `n_het` (``int64``) -- Number of heterozygous calls.; - `n_hom_var` (``int64``) -- Number of homozygous alternate calls.; - `n_non_ref` (``int64``) -- Sum of `n_het` and `n_hom_var`.; - `n_snp` (``int64``) -- Number of SNP alternate alleles.; - `n_insertion` (``int64``) -- Number of insertion alternate alleles.; - `n_deletion` (``int64``) -- Number of deletion alternate alleles.; - `n_singleton` (``int64``) -- Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; - ",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Deployability,configurat,configuration,"statistics. This; value is the number of genotypes which were called (homozygous reference,; heterozygous, or homozygous variant) in both datasets, but where the call; did not match between the two. The column `concordance` matches the structure of the global summmary,; which is detailed above. Once again, the first index into this array is the; state on the left, and the second index is the state on the right. For; example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the; left that were called homozygous variant on the right. Parameters; ----------; left : :class:`.MatrixTable`; First dataset to compare.; right : :class:`.MatrixTable`; Second dataset to compare. Returns; -------; (list of list of int, :class:`.Table`, :class:`.Table`); The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. """"""; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Integrability,depend,depending,"""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOU",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Modifiability,config,configuring,"s a convenience,; because this is often one of the most useful concordance statistics. This; value is the number of genotypes which were called (homozygous reference,; heterozygous, or homozygous variant) in both datasets, but where the call; did not match between the two. The column `concordance` matches the structure of the global summmary,; which is detailed above. Once again, the first index into this array is the; state on the left, and the second index is the state on the right. For; example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the; left that were called homozygous variant on the right. Parameters; ----------; left : :class:`.MatrixTable`; First dataset to compare.; right : :class:`.MatrixTable`; Second dataset to compare. Returns; -------; (list of list of int, :class:`.Table`, :class:`.Table`); The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. """"""; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run V",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Performance,perform,performs,"sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Testability,test,test," - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rs",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
Availability,error,errors,"`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Deployability,update,updates,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Energy Efficiency,reduce,reduced," one for each dependent variable.; # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # FIXME: the order of the columns is irrelevant to regression; # FIXME: we should test a whole block of variants at a time not one-by-one; # (N, K); # (N,); # (K,); # (N,); # (K,); # (K, K); """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It wil",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Integrability,depend,depending,"p based on given idx. Returns a single struct.; # For every field we care about, map across all y's, getting the row_idxth one from each.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # Helpers for logreg:; # (K,); # (N, K); """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # X is samples by covs.; # y is length num samples, for one cov.; # Reshape so we do a rowwise multiply; # num covs used to fit null model.; # (K,); # (N, K); # (N,); """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; # 1 variant; # _warn_if_no_intercept('logistic_regression_rows', covariates); # Handle filtering columns with missing values:; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # covmat rows are samples, columns are the different covariates; # yvecs is a list of sample-length vectors, one for each dependent variable.; # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # FIXME: the order of the columns is irrelevant to regression; # FIXME: we should test a whole block of variants at a time not one-by-one; # (N, K); # (N,); # (K,); # (N,); # (K,); # (K, K); """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; # Instead of finding the best-fit beta, we go directly to the best-pred",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Modifiability,variab,variable,"p based on given idx. Returns a single struct.; # For every field we care about, map across all y's, getting the row_idxth one from each.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # Helpers for logreg:; # (K,); # (N, K); """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # X is samples by covs.; # y is length num samples, for one cov.; # Reshape so we do a rowwise multiply; # num covs used to fit null model.; # (K,); # (N, K); # (N,); """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; # 1 variant; # _warn_if_no_intercept('logistic_regression_rows', covariates); # Handle filtering columns with missing values:; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # covmat rows are samples, columns are the different covariates; # yvecs is a list of sample-length vectors, one for each dependent variable.; # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # FIXME: the order of the columns is irrelevant to regression; # FIXME: we should test a whole block of variants at a time not one-by-one; # (N, K); # (N,); # (K,); # (N,); # (K,); # (K, K); """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; # Instead of finding the best-fit beta, we go directly to the best-pred",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Performance,optimiz,optimized,"ty of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; # Q=ht.y_residual @ (ht.G * ht.weight) @ ht.G.T @ ht.y_residual.T; # See linear SKAT code comment for an extensive description of the mathematics here.; # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weights.; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; # FIXME: remove this logic when annotation is better optimized; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; """"""Split multiallelic variants. Warning; -------; In order to support a wide variety of data types, this function splits only; the variants on a :class:`.MatrixTable`, but **not the genotypes**. Use; :func:`.split_multi_hts` if possible, or split the genotypes yourself using; one of the entry modification methods: :meth:`.MatrixTable.annotate_entries`,; :meth:`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if thi",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Safety,predict,predicted," one for each dependent variable.; # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # FIXME: the order of the columns is irrelevant to regression; # FIXME: we should test a whole block of variants at a time not one-by-one; # (N, K); # (N,); # (K,); # (N,); # (K,); # (K, K); """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; # Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It wil",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Testability,log,logreg,"# allow silent pass through of key fields; # allow silent pass through of key fields; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # Wrapping in a list since the code is written for the more general chained case.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # Given a hail array, get the mean of the nonmissing entries and; # return new array where the missing entries are the mean.; # cov_arrays is per sample, then per cov.; # sample_ys is an array of samples, with each element being an array of the y_values; # Processes one block group based on given idx. Returns a single struct.; # For every field we care about, map across all y's, getting the row_idxth one from each.; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # Helpers for logreg:; # (K,); # (N, K); """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # X is samples by covs.; # y is length num samples, for one cov.; # Reshape so we do a rowwise multiply; # num covs used to fit null model.; # (K,); # (N, K); # (N,); """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; # n_covariates or n_covariates + 1, depending on improved null fit vs full fit; # 1 variant; # _warn_if_no_intercept('logistic_regression_rows', covariates); # Handle filtering columns with missing values:; # FIXME: selecting an existing entry field should be emitted as a SelectFields; # covmat rows are samples, columns are the different covariates; # yvecs is a list of sample-length vectors, one for each dependent variable.; # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; # FIXME: selecting an existing entry field should be emitted as a SelectFie",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
Performance,perform,perform,"""""""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Forces the estimations for ``Z0``, ``Z1``, ``Z2``, and ``PI_HAT`` to take; on",MatchSource.CODE_COMMENT,hail/python/hail/methods/relatedness/identity_by_descent.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/relatedness/identity_by_descent.py
Modifiability,variab,variables," hl.nd.ones((5, 7)). It is possible to specify a type other than :py:data:`.tfloat64` with the `dtype` argument. >>> hl.nd.ones((5, 7), dtype=hl.tfloat32). Parameters; ----------; shape : `tuple` or :class:`.TupleExpression`; Desired shape.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. See Also; --------; :func:`.full`. Returns; -------; :class:`.NDArrayNumericExpression`; ndarray of the specified size full of ones.; """"""; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpression`, (N, N); Triangular coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables.; lower : `bool`:; If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N",MatchSource.CODE_COMMENT,hail/python/hail/nd/nd.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/nd/nd.py
Availability,down,down,"""""""Configure the Bokeh output state to generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; # invisible, there to support tooltips; # fixed x; # fixed y; # index of lower slope; # index of upper slope; # line must bend down at j; # line must bend up at j; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; leg",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
Deployability,continuous,continuous,"loat), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; # Use python prettier float -> str function; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; # Cast non-string types to string; # Assign color mappers to columns; # Create initial glyphs; # Add legend / color bar; """"""Create an interactive scatter plot. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentatio",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
Modifiability,variab,variable," -------; :class:`bokeh.plotting.figure`; """"""; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; """"""Set most of the font sizes in a bokeh figure. Parameters; ----------; p : :class:`bokeh.plotting.figure`; Input figure.; font_size : str; String of font size in points (e.g. '12pt'). Returns; -------; :class:`bokeh.plotting.figure`; """"""; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be consid",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
Testability,log,log,"""""""Configure the Bokeh output state to generate output in notebook; cells when :func:`bokeh.io.show` is called. Calls; :func:`bokeh.io.output_notebook`. """"""; """"""Immediately display a Bokeh object or application. Calls; :func:`bokeh.io.show`. Parameters; ----------; obj; A Bokeh object to display.; interact; A handle returned by a plotting method with `interactive=True`.; """"""; """"""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; # invisible, there to support tooltips; # fixed x; # fixed y; # index of lower slope; # index of upper slope; # line must bend down at j; # line must bend up at j; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; leg",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
Availability,error,errors,"# reject str because of errors due to sequenceof(strlike) permitting str; """"""Type checker that performs argument transformations. The `fs` argument should be a varargs of 2-tuples that each contain a; TypeChecker and a lambda function, e.g.:. ((only(int), lambda x: x * 2),; sequenceof(int), lambda x: x[0])); """"""; # ensure that the typecheck signature is appropriate and matches the function signature; # consume the rest of the positional arguments; # kwargs now holds all variable kwargs",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
Modifiability,variab,variable,"# reject str because of errors due to sequenceof(strlike) permitting str; """"""Type checker that performs argument transformations. The `fs` argument should be a varargs of 2-tuples that each contain a; TypeChecker and a lambda function, e.g.:. ((only(int), lambda x: x * 2),; sequenceof(int), lambda x: x[0])); """"""; # ensure that the typecheck signature is appropriate and matches the function signature; # consume the rest of the positional arguments; # kwargs now holds all variable kwargs",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
Performance,perform,performs,"# reject str because of errors due to sequenceof(strlike) permitting str; """"""Type checker that performs argument transformations. The `fs` argument should be a varargs of 2-tuples that each contain a; TypeChecker and a lambda function, e.g.:. ((only(int), lambda x: x * 2),; sequenceof(int), lambda x: x[0])); """"""; # ensure that the typecheck signature is appropriate and matches the function signature; # consume the rest of the positional arguments; # kwargs now holds all variable kwargs",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
Availability,error,error,"://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
Security,access,access,"---; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. Try using :func:`.hadoop_open` first, it's simpler, but not great; for large data! For example:. >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
Testability,log,log,"------; :obj:`dict`; """"""; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
Usability,simpl,simpler,"e provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. Try using :func:`.hadoop_open` first, it's simpler, but not great; for large data! For example:. >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
Availability,error,error,""""""":class:`.HailUserError` is an error thrown by Hail when the user makes an error.""""""; """""":class:`.FatalError` is an error thrown by Hail method failures""""""",MatchSource.CODE_COMMENT,hail/python/hail/utils/java.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py
Availability,redundant,redundant,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Performance,optimiz,optimized,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Safety,redund,redundant,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Security,access,access,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Testability,test,testing,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Usability,learn,learning,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; # Table will never get here; # don't handle 'private' attribute access; # check collisions with fields on other axes; # check duplicate fields; # don't clog the IR with redundant field names; # Make sure joins happen in order",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
Deployability,update,updated,"s or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """"""; # Set this way to avoid an infinite recursion in `__getattr__`.; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.; named_exprs : keyword args; New field. Returns; -------; :class:`.Struct`; Struct containing specified existing fields and computed fields. Examples; --------; Define a Struct 's'. >>> s = hl.Struct(foo=5, apple=10). Keep just one original field. >>> s.select('foo'); Struct(fo",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
Safety,avoid,avoid,"""""""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """"""; # Set this way to avoid an infinite recursion in `__getattr__`.; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; -------",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
Security,access,accessing,"""""""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """"""; # Set this way to avoid an infinite recursion in `__getattr__`.; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; -------",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
Availability,down,download,"""""""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""; # utility functions for importing movies",MatchSource.CODE_COMMENT,hail/python/hail/utils/tutorial.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py
Availability,checkpoint,checkpoint,"4*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (*float64*): Mean depth on calling intervals on chromosome.; - ``y_ploidy`` (*float64*): Estimated ploidy on Y chromosome. Equal to ``2 * y_mean_db / autosomal_mean_dp``. Parameters; ----------; vds : vds: :class:`.VariantDataset`; Dataset.; calling_intervals : :class:`.Table` or :class:`.ArrayExpression`; Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig : str; Autosomal contig for depth comparison.; use_variant_dataset : bool; Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns; -------; :class:`.Table`; """"""; # segment on PAR interval boundaries; # remove intervals overlapping PAR; # checkpoint for efficient multiple downstream usages; """"""Filter variants in a :class:`.VariantDataset`, without removing reference; data. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; variants_table : :class:`.Table`; Variants to filter on.; keep: :obj:`bool`; Whether to keep (default), or filter out the variants from `variants_table`. Returns; -------; :class:`.VariantDataset`.; """"""; """"""Filter chromosomes of a :class:`.VariantDataset` in several possible modes. Notes; -----; There are three modes for :func:`filter_chromosomes`, based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. - ``keep``: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a :class:`.VariantDataset` with only those; chromosomes.; - ``remove``: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
Deployability,patch,patch,"or DP if present). Returns; -------; :class:`.MatrixTable`; Interval-by-sample matrix; """"""; """"""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; """"""Merge adjacent reference blocks according to user equivalence criteria. Examples; --------; Coarsen GQ granularity into bins of 10 and merges blocks with the same GQ in order to; compress reference data. >>> rd = vds.reference_data # doctest: +SKIP; >>> vds.reference_data = rd.annotate_entries(GQ = rd.GQ - rd.GQ % 10) # doctest: +SKIP; >>> vds2 = hl.vds.merge_reference_blocks(vds,; ... equivalence_function=lambda block1, block2: block1.GQ == block2.GQ),; ... merge_functions={'MIN_DP': 'min'}) # doctest: +SKIP. Note",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
Energy Efficiency,efficient,efficient,"4*): Mean depth on calling intervals on normalization contig.; - ``x_mean_dp`` (*float64*): Mean depth on calling intervals on X chromosome.; - ``x_ploidy`` (*float64*): Estimated ploidy on X chromosome. Equal to ``2 * x_mean_dp / autosomal_mean_dp``.; - ``y_mean_dp`` (*float64*): Mean depth on calling intervals on chromosome.; - ``y_ploidy`` (*float64*): Estimated ploidy on Y chromosome. Equal to ``2 * y_mean_db / autosomal_mean_dp``. Parameters; ----------; vds : vds: :class:`.VariantDataset`; Dataset.; calling_intervals : :class:`.Table` or :class:`.ArrayExpression`; Calling intervals with consistent read coverage (for exomes, trim the capture intervals).; normalization_contig : str; Autosomal contig for depth comparison.; use_variant_dataset : bool; Whether to use depth of variant data within calling intervals instead of reference data. Default will use reference data. Returns; -------; :class:`.Table`; """"""; # segment on PAR interval boundaries; # remove intervals overlapping PAR; # checkpoint for efficient multiple downstream usages; """"""Filter variants in a :class:`.VariantDataset`, without removing reference; data. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; variants_table : :class:`.Table`; Variants to filter on.; keep: :obj:`bool`; Whether to keep (default), or filter out the variants from `variants_table`. Returns; -------; :class:`.VariantDataset`.; """"""; """"""Filter chromosomes of a :class:`.VariantDataset` in several possible modes. Notes; -----; There are three modes for :func:`filter_chromosomes`, based on which argument is passed; to the function. Exactly one of the below arguments must be passed by keyword. - ``keep``: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the function returns a :class:`.VariantDataset` with only those; chromosomes.; - ``remove``: This argument expects a single chromosome identifier or a list of chromosome; identifiers, and the",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
Modifiability,rewrite,rewrite,"antDataset`; Dataset in VariantDataset representation.; filter_changed_loci : :obj:`bool`; If any REF/ALT pair changes locus under :func:`.min_rep`, filter that; variant instead of throwing an error. Returns; -------; :class:`.VariantDataset`; """"""; """"""Returns a matrix table of reference blocks segmented according to intervals. Loci outside the given intervals are discarded. Reference blocks that start before; but span an interval will appear at the interval start locus. Note; ----; Assumes disjoint intervals which do not span contigs. Requires start-inclusive intervals. Parameters; ----------; ref : :class:`.MatrixTable`; MatrixTable of reference blocks.; intervals : :class:`.Table`; Table of intervals at which to segment reference blocks. Returns; -------; :class:`.MatrixTable`; """"""; # at this point, 'dense' is a table with dense rows of reference blocks, keyed by locus; # remove rows that are not contained in an interval, and rows that are the start of an; # interval (interval starts come from the 'dense' table); # union dense interval starts with filtered table; # rewrite reference blocks to end at the first of (interval end, reference block end); """"""Compute statistics about base coverage by interval. Returns a :class:`.MatrixTable` with interval row keys and sample column keys. Contains the following row fields:; - ``interval`` (*interval*): Genomic interval of interest.; - ``interval_size`` (*int32*): Size of interval, in bases. Computes the following entry fields:. - ``bases_over_gq_threshold`` (*tuple of int64*): Number of bases in the interval; over each GQ threshold.; - ``fraction_over_gq_threshold`` (*tuple of float64*): Fraction of interval (in bases); above each GQ threshold. Computed by dividing each member of *bases_over_gq_threshold*; by *interval_size*.; - ``bases_over_dp_threshold`` (*tuple of int64*): Number of bases in the interval; over each DP threshold.; - ``fraction_over_dp_threshold`` (*tuple of float64*): Fraction of interval (in bases); abo",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
Availability,down,downstream,"""""""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; """"""Class for representing cohort-level genomic data. This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; :class:`.MatrixTable` objects. Parameters; ----------; reference_data : :class:`.MatrixTable`; MatrixTable containing only reference block data.; variant_data : :class:`.MatrixTable`; MatrixTable containing only variant data.; """"""; #: Name of global field that indicates max reference block length.; """"""Create a VariantDataset from a sparse MatrixTable containing variant and reference data.""""""; # remove LGT/GT and LA fields, which are trivial for reference blocks and do not need to be represented; """"""Write to `path`.""""""; """"""Write to `path` and then read from `path`.""""""; """"""The number of samples present.""""""; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; """"""Eagerly checks necessary representational properties of the VDS.""""""; # check cols; # check locus distinctness; # check END",MatchSource.CODE_COMMENT,hail/python/hail/vds/variant_dataset.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py
Deployability,patch,patch,"""""""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; """"""Class for representing cohort-level genomic data. This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; :class:`.MatrixTable` objects. Parameters; ----------; reference_data : :class:`.MatrixTable`; MatrixTable containing only reference block data.; variant_data : :class:`.MatrixTable`; MatrixTable containing only variant data.; """"""; #: Name of global field that indicates max reference block length.; """"""Create a VariantDataset from a sparse MatrixTable containing variant and reference data.""""""; # remove LGT/GT and LA fields, which are trivial for reference blocks and do not need to be represented; """"""Write to `path`.""""""; """"""Write to `path` and then read from `path`.""""""; """"""The number of samples present.""""""; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; """"""Eagerly checks necessary representational properties of the VDS.""""""; # check cols; # check locus distinctness; # check END",MatchSource.CODE_COMMENT,hail/python/hail/vds/variant_dataset.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py
Availability,checkpoint,checkpointing,"vcf_info`` entry; field. By default, all ``INFO`` fields except ``END`` and ``DP`` are kept. Returns; -------; :obj:`.VariantDataset`; A single sample variant dataset. Notes; -----; This function will parse the following allele specific annotations from; pipe delimited strings into proper values. ::. AS_QUALapprox; AS_RAW_MQ; AS_RAW_MQRankSum; AS_RAW_ReadPosRankSum; AS_SB_TABLE; AS_VarDP. """"""; # if some mts have max ref len but not all, drop it; # bad field, possibly 'NaN', just set it null; # global index of alternate (non-ref) alleles; """"""Merges gvcfs and/or sparse matrix tables. Parameters; ----------; mts : :obj:`List[Union[Table, MatrixTable]]`; The matrix tables (or localized versions) to combine. Returns; -------; :class:`.MatrixTable`. Notes; -----; All of the input tables/matrix tables must have the same partitioning. This; module provides no method of repartitioning data.; """"""; """"""takes a table, keyed by ['locus', ...] and produces a list of intervals suitable; for repartitioning a combiner matrix table. Parameters; ----------; mt : :class:`.MatrixTable`; Sparse MT intermediate.; desired_average_partition_size : :obj:`int`; Average target number of rows for each partition.; tmp_path : :obj:`str`; Temporary path for scan checkpointing. Returns; -------; (:obj:`List[Interval]`, :obj:`.Type`); """"""; # split by a weight function that takes into account the number of; # dense entries per row. However, give each row some base weight; # to prevent densify computations from becoming unbalanced (these; # scale roughly linearly with N_ROW * N_COL); """"""create a list of locus intervals suitable for importing and merging gvcfs. Parameters; ----------; reference_genome: :class:`str` or :class:`.ReferenceGenome`,; Reference genome to use. NOTE: only GRCh37 and GRCh38 references; are supported.; interval_size: :obj:`int` The ceiling and rough target of interval size.; Intervals will never be larger than this, but may be smaller. Returns; -------; :obj:`List[Interval]`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
Security,hash,hashable,"# we drop PL/PGT by default, but if `entry_to_keep` has them, we need to; # convert them to local versions for consistency.; # hashable stable value; # hashable stable value; # hashable stable value; # hashable stable value; """"""Transforms a GVCF into a single sample VariantDataSet. The input to this should be some result of :func:`.import_vcf`; ``array_elements_required=False``. There is an assumption that this function will be called on a matrix table; with one column (or a localized table version of the same). Parameters; ----------; mt : :class:`.MatrixTable`; The GVCF being transformed.; reference_entry_fields_to_keep : :class:`list` of :class:`str`; Genotype fields to keep in the reference table. If empty, the first; 10,000 reference block rows of ``mt`` will be sampled and all fields; found to be defined other than ``GT``, ``AD``, and ``PL`` will be entry; fields in the resulting reference matrix in the dataset.; info_to_keep : :class:`list` of :class:`str`; Any ``INFO`` fields in the GVCF that are to be kept and put in the ``gvcf_info`` entry; field. By default, all ``INFO`` fields except ``END`` and ``DP`` are kept. Returns; -------; :obj:`.VariantDataset`; A single sample variant dataset. Notes; -----; This function will parse the following allele specific annotations from; pipe delimited strings into proper values. ::. AS_QUALapprox; AS_RAW_MQ; AS_RAW_MQRankSum; AS_RAW_ReadPosRankSum; AS_SB_TABLE; AS_VarDP. """"""; # if some mts have max ref len but not all, drop it; # bad field, possibly 'NaN', just set it null; # global index of alternate (non-ref) alleles; """"""Merges gvcfs and/or sparse matrix tables. Parameters; ----------; mts : :obj:`List[Union[Table, MatrixTable]]`; The matrix tables (or localized versions) to combine. Returns; -------; :class:`.MatrixTable`. Notes; -----; All of the input tables/matrix tables must have the same partitioning. This; module provides no method of repartitioning data.; """"""; """"""takes a table, keyed by ['locus', ...] and produ",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
Availability,failure,failure-tolerant,"""""""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """"""; """"""A container for the types of a VDS""""""; """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }""""""; # pylint: disable=too-many-instance-attributes; """"""A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. Examples; --------. A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences ::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner cr",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
Integrability,depend,depends,"10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :cl",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
Performance,load,load,"combiner; # The genome interval size results in 2568 partitions for GRCh38. The exome; # interval size assumes that they are around 2% the size of a genome and; # result in 65 partitions for GRCh38.; """"""The number of GVCFs to combine into a Variant Dataset at once.""""""; """"""Have all GVCFs and input Variant Datasets been combined?""""""; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; """"""Combine the specified GVCFs and Variant Datasets.""""""; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; """"""A serializable representation of this combiner.""""""; # put this here for humans; """"""Run one layer of combinations. :meth:`.run` effectively runs :meth:`.step` until all GVCFs and Variant Datasets have been; combined. """"""; # we use the reference data since it generally has more rows than the variant data; # this ensures that we don't somehow stick a vds at the end of; # the same bin, ending up with a weird ordering issue; # compute max length at the end; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); # We do the first save_path check now after validating the arguments; # we need to compute the type that the combiner will have, this will allow us to read matrix; # tables quickly, especially in an asynchronous environment like query on batch where typing; # a read uses a blocking round trip.; # sync up gvcf_reference_entry_fields_to_keep and they reference entry types from the VDS; # sync up call_fields and call fields present in the VDS; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
Security,hash,hash,"combiner; # The genome interval size results in 2568 partitions for GRCh38. The exome; # interval size assumes that they are around 2% the size of a genome and; # result in 65 partitions for GRCh38.; """"""The number of GVCFs to combine into a Variant Dataset at once.""""""; """"""Have all GVCFs and input Variant Datasets been combined?""""""; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; """"""Combine the specified GVCFs and Variant Datasets.""""""; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; """"""A serializable representation of this combiner.""""""; # put this here for humans; """"""Run one layer of combinations. :meth:`.run` effectively runs :meth:`.step` until all GVCFs and Variant Datasets have been; combined. """"""; # we use the reference data since it generally has more rows than the variant data; # this ensures that we don't somehow stick a vds at the end of; # the same bin, ending up with a weird ordering issue; # compute max length at the end; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); # We do the first save_path check now after validating the arguments; # we need to compute the type that the combiner will have, this will allow us to read matrix; # tables quickly, especially in an asynchronous environment like query on batch where typing; # a read uses a blocking round trip.; # sync up gvcf_reference_entry_fields_to_keep and they reference entry types from the VDS; # sync up call_fields and call fields present in the VDS; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
Usability,resume,resume,"combiner; # The genome interval size results in 2568 partitions for GRCh38. The exome; # interval size assumes that they are around 2% the size of a genome and; # result in 65 partitions for GRCh38.; """"""The number of GVCFs to combine into a Variant Dataset at once.""""""; """"""Have all GVCFs and input Variant Datasets been combined?""""""; """"""Save a :class:`.VariantDatasetCombiner` to its `save_path`.""""""; # these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail; """"""Combine the specified GVCFs and Variant Datasets.""""""; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; """"""A serializable representation of this combiner.""""""; # put this here for humans; """"""Run one layer of combinations. :meth:`.run` effectively runs :meth:`.step` until all GVCFs and Variant Datasets have been; combined. """"""; # we use the reference data since it generally has more rows than the variant data; # this ensures that we don't somehow stick a vds at the end of; # the same bin, ending up with a weird ordering issue; # compute max length at the end; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); # We do the first save_path check now after validating the arguments; # we need to compute the type that the combiner will have, this will allow us to read matrix; # tables quickly, especially in an asynchronous environment like query on batch where typing; # a read uses a blocking round trip.; # sync up gvcf_reference_entry_fields_to_keep and they reference entry types from the VDS; # sync up call_fields and call fields present in the VDS; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
Security,certificate,certificates,"# FIXME: mTLS; # _server_ssl_context.verify_mode = ssl.CERT_REQURIED; # clients have no hostnames; # setting cafile in `create_default_context` ignores the system default; # certificates. We must explicitly request them again with; # load_default_certs.",MatchSource.CODE_COMMENT,hail/python/hailtop/tls.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/tls.py
Availability,failure,failures,"ng name: LastModified is creation time.; # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; #",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Energy Efficiency,efficient,efficient,"# type: ignore; # https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html#API_HeadObject_ResponseSyntax; # Misleading name: LastModified is creation time.; # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html#API_GetObject_ResponseSyntax; # Misleading name: LastModified is creation time.; # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the d",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Integrability,interface,interface,"le=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).; # pylint: disable=raise-missing-from; # unreachable",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Performance,load,loaded,"# type: ignore; # https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html#API_HeadObject_ResponseSyntax; # Misleading name: LastModified is creation time.; # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html#API_GetObject_ResponseSyntax; # Misleading name: LastModified is creation time.; # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the d",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Safety,safe,safe," # S3 Python library strips dashes from header names; # S3 objects are immutable, so creation == modified; # pylint: disable=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (un",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Security,hash,hash,"le=unused-argument; # pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.; # pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).; # pylint: disable=raise-missing-from; # unreachable",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
Availability,error,errors,"# pylint: disable=unused-argument; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # cannot set the default to 0 because this will fail on an empty file; # offset means to start at the first byte; # type: ignore; # type: ignore; # ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # type: ignore; # Look for a terminating SAS token.; # We will accept it as a token string if it begins with at least 1 key-value pair of the form 'k=v'.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # pylint: disable=unused-argument; # if object name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # type: ignore; # type: ignore; # type: ignore; # pylint: disable=raise-missing-from",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
Deployability,configurat,configuration,"# pylint: disable=unused-argument; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # cannot set the default to 0 because this will fail on an empty file; # offset means to start at the first byte; # type: ignore; # type: ignore; # ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # type: ignore; # Look for a terminating SAS token.; # We will accept it as a token string if it begins with at least 1 key-value pair of the form 'k=v'.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # pylint: disable=unused-argument; # if object name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # type: ignore; # type: ignore; # type: ignore; # pylint: disable=raise-missing-from",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
Modifiability,config,configuration,"# pylint: disable=unused-argument; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # cannot set the default to 0 because this will fail on an empty file; # offset means to start at the first byte; # type: ignore; # type: ignore; # ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # type: ignore; # Look for a terminating SAS token.; # We will accept it as a token string if it begins with at least 1 key-value pair of the form 'k=v'.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # pylint: disable=unused-argument; # if object name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # type: ignore; # type: ignore; # type: ignore; # pylint: disable=raise-missing-from",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
Security,access,access,"# pylint: disable=unused-argument; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # azure allows both BlockBlob and the string id here, despite; # only having BlockBlob annotations; # type: ignore; # cannot set the default to 0 because this will fail on an empty file; # offset means to start at the first byte; # type: ignore; # type: ignore; # ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # type: ignore; # Look for a terminating SAS token.; # We will accept it as a token string if it begins with at least 1 key-value pair of the form 'k=v'.; # https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration; # type: ignore; # pylint: disable=unused-argument; # if object name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # type: ignore; # type: ignore; # type: ignore; # pylint: disable=raise-missing-from",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
Deployability,deploy,deployments,"# https://docs.microsoft.com/en-us/rest/api/resources/deployments/list-by-resource-group",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/client/arm_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/client/arm_client.py
Integrability,protocol,protocol,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful; # protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful; # 5m; # https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#applications",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
Security,access,access-token,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful; # protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful; # 5m; # https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#applications",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
Testability,log,log-http,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful; # protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful; # 5m; # https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#applications",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
Deployability,configurat,configuration,"# https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md#cloud-storage-requester-pays-feature-configuration; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py
Modifiability,config,configuration,"# https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md#cloud-storage-requester-pays-feature-configuration; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py
Testability,log,logging,"# in case a response is empty but there are more pages; # an empty page has no entries; # docs:; # https://cloud.google.com/logging/docs/reference/v2/rest; # https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/list",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py
Availability,avail,available,"# pylint: disable=unused-import # pylint: disable=unused-import; # If the last request was unsuccessful, we are out of sync; # with the server and we don't know what byte to send; # next. Perform a status check to find out. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#status-check; # note: this retries; # status check can advance the offset, so there might not be a; # full chunk available to write; # Upload a single chunk. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#chunked-upload; # https://docs.aiohttp.org/en/stable/streams.html#aiohttp.StreamReader.read; # Read up to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src``",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Modifiability,rewrite,rewrite,"to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Performance,perform,performing-resumable-uploads,"# pylint: disable=unused-import # pylint: disable=unused-import; # If the last request was unsuccessful, we are out of sync; # with the server and we don't know what byte to send; # next. Perform a status check to find out. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#status-check; # note: this retries; # status check can advance the offset, so there might not be a; # full chunk available to write; # Upload a single chunk. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#chunked-upload; # https://docs.aiohttp.org/en/stable/streams.html#aiohttp.StreamReader.read; # Read up to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src``",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Safety,timeout,timeout,"# pylint: disable=unused-import # pylint: disable=unused-import; # If the last request was unsuccessful, we are out of sync; # with the server and we don't know what byte to send; # next. Perform a status check to find out. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#status-check; # note: this retries; # status check can advance the offset, so there might not be a; # full chunk available to write; # Upload a single chunk. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#chunked-upload; # https://docs.aiohttp.org/en/stable/streams.html#aiohttp.StreamReader.read; # Read up to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src``",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Security,access,access,"to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Usability,progress bar,progress bar,"to n bytes. If n is not provided, or set to -1, read until EOF; # and return all read bytes.; # Around May 2022, GCS started timing out a lot with our default 5s timeout; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/json_api/v1/buckets`_ for the list of bucket; properties in the response.; """"""; # docs:; # https://cloud.google.com/storage/docs/json_api/v1; # Insert an object. See:; # https://cloud.google.com/storage/docs/json_api/v1/objects/insert; # Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads; # compute dest_dirname so gs://{bucket}/{dest_dirname}file; # refers to a file in dest_dirname with no double slashes; # pylint: disable=unused-argument; # each chunk gets q, and the first r get one more; """"""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""; # pylint: disable=raise-missing-from; # if name is empty, get_object_metadata behaves like list objects; # the urls are the same modulo the object name; # unreachable; """"""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
Security,authenticat,authentication,"""""""Return HTTP authentication headers and the time of expiration in seconds since the epoch (Unix time). None indicates a non-expiring credentials.""""""; """"""Return an access token and the time of expiration in seconds since the epoch (Unix time). None indicates a non-expiring credentials.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/common/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/common/credentials.py
Availability,error,error,"# only advance if file or directory removal was successful, not on error; """"""Examples:. python3 -m hailtop.aiotools.delete dir1/ file1 dir2/file1 dir2/file3 dir3. python3 -m hailtop.aiotools.delete gs://bucket1/dir1 gs://bucket1/file1 gs://bucket2/abc/123; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/delete.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/delete.py
Performance,concurren,concurrency,"# The concurrency limit is the number of worker corooutines, not the queue size. Queue size must; # be large because a single driver process is trying to feed max_simultaneous workers.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/diff.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/diff.py
Security,validat,validation,"""""""; Validates a URI's scheme if the ``validate_scheme`` flag was provided, and its cloud location's default storage; policy if the URI points to a cloud with an ``AsyncFS`` implementation that supports checking that policy. Raises; ------; :class:`ValueError`; If one of the validation steps fails.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/validators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/validators.py
Performance,concurren,concurrently,"""""""This class implements copy from a single source. In general, a; transfer will have multiple sources, and a SourceCopier will be; created for each source.; """"""; # We know dest is a dir, but we're copying to; # dest/basename(src), and we don't know its type.; # pylint: disable=unused-argument; # skip files with empty names; # In cloud FSes, status and size never make a network request. In local FS, they; # can make system calls on symlinks. This line will be fairly expensive if; # copying a tree with a lot of symlinks.; # gather with return_exceptions=True to make copy; # deterministic with respect to exceptions; """"""; This class implements copy for a list of transfers.; """"""; # This is essentially a limit on amount of memory in temporary; # buffers during copying. We allow ~10 full-sized copies to; # run concurrently.; """"""Return the (real or assumed) type of `dest`. If the transfer assumes the type of `dest`, return that rather; than the real type. A return value of `None` mean `dest` does; not exist.; """"""; # raise potential exception",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/copier.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/copier.py
Availability,error,error,"""""""The basename of the object. Examples; --------. The basename of all of these objects is ""file"":. - s3://bucket/folder/file; - gs://bucket/folder/file; - https://account.blob.core.windows.net/container/folder/file; - https://account.blob.core.windows.net/container/folder/file?sv=2023-01-01&sr=bv&sig=abc123&sp=rcw; - /folder/file; """"""; """"""The URL of the object without any query parameters. Examples; --------. - s3://bucket/folder/file; - gs://bucket/folder/file; - https://account.blob.core.windows.net/container/folder/file; - /folder/file. Note that the following URL. https://account.blob.core.windows.net/container/folder/file?sv=2023-01-01&sr=bv&sig=abc123&sp=rcw. becomes. https://account.blob.core.windows.net/container/folder/file. """"""; """"""The time the object was created in seconds since the epcoh, UTC. Some filesystems do not support creation time. In that case, an error is raised. """"""; """"""The time the object was last modified in seconds since the epoch, UTC. The meaning of modification time is cloud-defined. In some clouds, it is the creation; time. In some clouds, it is the more recent of the creation time or the time of the most; recent metadata modification. """"""; """"""The basename of the object. Examples; --------. The basename of all of these objects is ""file"":. - s3://bucket/folder/file; - gs://bucket/folder/file; - https://account.blob.core.windows.net/container/folder/file; - https://account.blob.core.windows.net/container/folder/file?sv=2023-01-01&sr=bv&sig=abc123&sp=rcw; - /folder/file; """"""; """"""The URL of the object without any query parameters. Examples; --------. - s3://bucket/folder/file; - gs://bucket/folder/file; - https://account.blob.core.windows.net/container/folder/file; - /folder/file. Note that the following URL. https://account.blob.core.windows.net/container/folder/file?sv=2023-01-01&sr=bv&sig=abc123&sp=rcw. becomes. https://account.blob.core.windows.net/container/folder/file. """"""; """"""The URL of the object with any query parameters. Examples;",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/fs.py
Availability,error,error,"# Read at most up to n bytes. If n == -1, then; # return all bytes up to the end of the file; # Read exactly n bytes. If there is an EOF before; # n bytes have been read, raise an UnexpectedEOFError.; # https://docs.python.org/3/library/io.html#io.RawIOBase.read; # Read up to size bytes from the object and return them. As a; # convenience, if size is unspecified or -1, all bytes until EOF are returned.; # self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.; # If readinto only partially fills b without hitting the end; # of stream, then the upload_obj returns an EntityTooSmall; # error in some cases.; # drain the q so the writer doesn't deadlock; # Unfortunately, we can't use a memory view here, we; # need to slice/copy buf, since the S3 upload_part API; # call requires byte or bytearray:; # Invalid type for parameter Body, value: <memory at 0x7f512801b6d0>, type: <class 'memoryview'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
Energy Efficiency,drain,drain,"# Read at most up to n bytes. If n == -1, then; # return all bytes up to the end of the file; # Read exactly n bytes. If there is an EOF before; # n bytes have been read, raise an UnexpectedEOFError.; # https://docs.python.org/3/library/io.html#io.RawIOBase.read; # Read up to size bytes from the object and return them. As a; # convenience, if size is unspecified or -1, all bytes until EOF are returned.; # self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.; # If readinto only partially fills b without hitting the end; # of stream, then the upload_obj returns an EntityTooSmall; # error in some cases.; # drain the q so the writer doesn't deadlock; # Unfortunately, we can't use a memory view here, we; # need to slice/copy buf, since the S3 upload_part API; # call requires byte or bytearray:; # Invalid type for parameter Body, value: <memory at 0x7f512801b6d0>, type: <class 'memoryview'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
Safety,safe,safe,"# Read at most up to n bytes. If n == -1, then; # return all bytes up to the end of the file; # Read exactly n bytes. If there is an EOF before; # n bytes have been read, raise an UnexpectedEOFError.; # https://docs.python.org/3/library/io.html#io.RawIOBase.read; # Read up to size bytes from the object and return them. As a; # convenience, if size is unspecified or -1, all bytes until EOF are returned.; # self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.; # If readinto only partially fills b without hitting the end; # of stream, then the upload_obj returns an EntityTooSmall; # error in some cases.; # drain the q so the writer doesn't deadlock; # Unfortunately, we can't use a memory view here, we; # need to slice/copy buf, since the S3 upload_part API; # call requires byte or bytearray:; # Invalid type for parameter Body, value: <memory at 0x7f512801b6d0>, type: <class 'memoryview'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
Security,access,accessed,"# Read at most up to n bytes. If n == -1, then; # return all bytes up to the end of the file; # Read exactly n bytes. If there is an EOF before; # n bytes have been read, raise an UnexpectedEOFError.; # https://docs.python.org/3/library/io.html#io.RawIOBase.read; # Read up to size bytes from the object and return them. As a; # convenience, if size is unspecified or -1, all bytes until EOF are returned.; # self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.; # If readinto only partially fills b without hitting the end; # of stream, then the upload_obj returns an EntityTooSmall; # error in some cases.; # drain the q so the writer doesn't deadlock; # Unfortunately, we can't use a memory view here, we; # need to slice/copy buf, since the S3 upload_part API; # call requires byte or bytearray:; # Invalid type for parameter Body, value: <memory at 0x7f512801b6d0>, type: <class 'memoryview'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
Modifiability,config,configured,"# Absence of specific oauth credentials means Hail should use latent credentials; # We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.; # Logout any legacy auth tokens that might still exist; # Logout newer OAuth2-based credentials",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
Security,access,access,"# Absence of specific oauth credentials means Hail should use latent credentials; # We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.; # Logout any legacy auth tokens that might still exist; # Logout newer OAuth2-based credentials",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
Deployability,install,installed,"# In Azure, a Tenant ID. In Google, a domain name.; """"""; The unique identifier of the organization (e.g. Azure Tenant, Google Organization) in; which this Hail Batch instance lives.; """"""; """"""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; """"""Revokes the OAuth2 credentials on the user's machine.""""""; """"""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""; # type: ignore; # type: ignore; # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email,; # but we should eventually use the sub as that is guaranteed to be unique to the user.; # confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580; # AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get; # This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
Performance,perform,perform,"# In Azure, a Tenant ID. In Google, a domain name.; """"""; The unique identifier of the organization (e.g. Azure Tenant, Google Organization) in; which this Hail Batch instance lives.; """"""; """"""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; """"""Revokes the OAuth2 credentials on the user's machine.""""""; """"""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""; # type: ignore; # type: ignore; # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email,; # but we should eventually use the sub as that is guaranteed to be unique to the user.; # confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580; # AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get; # This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
Security,secur,secure,"# In Azure, a Tenant ID. In Google, a domain name.; """"""; The unique identifier of the organization (e.g. Azure Tenant, Google Organization) in; which this Hail Batch instance lives.; """"""; """"""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; """"""Revokes the OAuth2 credentials on the user's machine.""""""; """"""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""; # type: ignore; # type: ignore; # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email,; # but we should eventually use the sub as that is guaranteed to be unique to the user.; # confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580; # AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get; # This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
Testability,log,login,"# In Azure, a Tenant ID. In Google, a domain name.; """"""; The unique identifier of the organization (e.g. Azure Tenant, Google Organization) in; which this Hail Batch instance lives.; """"""; """"""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; """"""Revokes the OAuth2 credentials on the user's machine.""""""; """"""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""; # type: ignore; # type: ignore; # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email,; # but we should eventually use the sub as that is guaranteed to be unique to the user.; # confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580; # AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get; # This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
Usability,learn,learn,"# In Azure, a Tenant ID. In Google, a domain name.; """"""; The unique identifier of the organization (e.g. Azure Tenant, Google Organization) in; which this Hail Batch instance lives.; """"""; """"""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; """"""Revokes the OAuth2 credentials on the user's machine.""""""; """"""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""; # type: ignore; # type: ignore; # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email,; # but we should eventually use the sub as that is guaranteed to be unique to the user.; # confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580; # AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get; # This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
Availability,echo,echo,"lint: disable=R0915; """"""; Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; """"""; # pylint: disable-msg=W0640; """"""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-accoun",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Deployability,configurat,configuration," Instead, use :meth:`.batch.Batch.run`. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; """"""; # pylint: disable-msg=W0640; """"""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_pr",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Energy Efficiency,monitor,monitor,"# pylint: disable=unused-import; """"""; The type of value returned by :py:meth:`.Backend._run`. The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; """"""; """"""; Abstract class for backends.; """"""; """"""; See :meth:`._async_run`. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`.; """"""; """"""; Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`.; """"""; """"""; Close a Hail Batch Backend. Notes; -----; This method should be called after executing your batches at the; end of your script.; """"""; """"""; Backend that executes batches on a local computer.; Examples; --------. >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters; ----------; tmp_dir:; Temporary directory to use.; gsa_key_file:; Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable `HAIL_BATCH_GSA_KEY_FILE`.; extra_docker_run_flags:; Additional flags to pass to `docker run`. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`.; """"""; # pylint: disable=R0915; """"""; Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; """"""; # pylint: disable-msg=W0640; """"""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> i",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Modifiability,variab,variable," type of value returned by :py:meth:`.Backend._run`. The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; """"""; """"""; Abstract class for backends.; """"""; """"""; See :meth:`._async_run`. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`.; """"""; """"""; Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`.; """"""; """"""; Close a Hail Batch Backend. Notes; -----; This method should be called after executing your batches at the; end of your script.; """"""; """"""; Backend that executes batches on a local computer.; Examples; --------. >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters; ----------; tmp_dir:; Temporary directory to use.; gsa_key_file:; Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable `HAIL_BATCH_GSA_KEY_FILE`.; extra_docker_run_flags:; Additional flags to pass to `docker run`. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`.; """"""; # pylint: disable=R0915; """"""; Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; """"""; # pylint: disable-msg=W0640; """"""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_b",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Security,authoriz,authorization,"nfig set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""; """"""A special value that indicates a job may run in any region.""""""; """"""; Get the supported cloud regions. Examples; --------; >>> regions = ServiceBackend.supported_regions(). Returns; -------; A list of the supported cloud regions; """"""; # pylint: disable-msg=too-many-statements; """"""Execute a batch. Warning; -------; This method should not be called directl",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Usability,progress bar,progress bar,"tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""; """"""A special value that indicates a job may run in any region.""""""; """"""; Get the supported cloud regions. Examples; --------; >>> regions = ServiceBackend.supported_regions(). Returns; -------; A list of the supported cloud regions; """"""; # pylint: disable-msg=too-many-statements; """"""Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`; and pass :class:`.ServiceBackend` specific arguments as key-word arguments. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; wait:; If `True`, wait for the batch to finish executing before returning.; open:; If `True`, open the UI page for the batch.; disable_progress_bar:; If `True`, disable the progress bar.; callback:; If not `None`, a URL that will receive at most one POST request; after the entire batch completes.; token:; If not `None`, a string used for idempotency of batch submission.; """"""; # type: ignore; # type: ignore; # it is not possible for the batch to be finished in less than 600ms",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
Availability,echo,echo,"""""""Object representing the distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; def",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Deployability,configurat,configurations,"ch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Energy Efficiency,charge,charges,"m` and the Python; packages specified by ``python_requirements`` will be installed. The default name; of the image is `batch-python` with a random string for the tag unless ``python_build_image_name``; is specified. If the :class:`.ServiceBackend` is the backend, the locally built; image will be pushed to the repository specified by ``image_repository``. Parameters; ----------; name:; Name of the job.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; """"""; # pylint: disable=no-member; # Avoid os.fspath(), which causes some pathlikes to return a path to a downloaded copy instead.; # Take care not to include an Azure SAS token query string in the local name.; # pylint: disable=no-member; # pylint: disable=W0123; # pylint: disable=no-member; # pylint: disable=no-member; """"""; Create a new input resource file object representing a single file. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read the file `hello.txt`:. >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters; ----------; path: :obj:`str`; File path to read.; """"""; """"""Create a new resource group representing a mapping of identifier to; input resource files. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read a binary PLINK file:. >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() # doctest: +SKIP. Read a FASTA file and it's index (file extensions matter!):. >>> fasta ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Modifiability,variab,variable,"e distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Safety,timeout,timeout,"he; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Security,access,accessing,"iles to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Testability,assert,assert,"-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""; # pylint: disable=import-outside-toplevel; """"""; Select all jobs in the batch whose name matches `pattern`. Examples; --------. Select jobs in batch matching `qc`:. >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters; ----------; pattern:; Regex pattern matching job names.; """"""; # Do not try to overload this based on dry_run. LocalBackend.run also returns None.; """"""; Execute a batch. Examples; --------. Create a simple batch with one job and execute it:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters; ----------; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; backend_kwargs:; See :meth:`.Backend._run` for backend-specific arguments.; """"""; # type: ignore; # Do not try to overload this based on dry_run. LocalBackend.run also returns None.; # pylint: disable=assignment-from-no-return; # best effort only because this is deprecated",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Usability,simpl,simply,"lo.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""; # pylint: disable=import-outside-toplevel; """"""; Select all jobs in the batch whose name matches `pattern`. Examples; --------. Select jobs in batch matching `qc`:. >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters; ----------; pattern:; Regex pattern matching job names.; """"""; # Do not try to overload this based on dry_run. LocalBackend.run also returns None.; """"""; Execute a batch. Examples; --------. Create a simple batch with one job and execute it:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters; ----------; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delet",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
Availability,avail,available,"""""""An executor which executes Python functions in the cloud. :class:`.concurrent.futures.ProcessPoolExecutor` and; :class:`.concurrent.futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python pack",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Deployability,install,installed," parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Energy Efficiency,allocate,allocate,"end the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""; """"""DEPRECATED. Do not use.""""""; """"""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Modifiability,variab,variables,"+SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""; """"""DEPRECATED. Do not use.""""""; """"""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest:",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Performance,concurren,concurrent,"""""""An executor which executes Python functions in the cloud. :class:`.concurrent.futures.ProcessPoolExecutor` and; :class:`.concurrent.futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python pack",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Safety,safe,safely,"futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Pyth",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Security,access,access,"e.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""; """"""Aysncio compatible version of :meth:`.map`.""""""; """"""Call `fn` on a cloud machine with all remaining arguments and keyword arguments. The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the `default_image` argument to :class:`.BatchPoolExecutor`. This function does not return the function's output, it returns a; :class:`.BatchPoolFuture` whose :meth:`.BatchPoolFuture.result` method; can be used to access the value. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(lambda x: x, 4); ... future.result(); 4. Call a function with two arguments and one keyword argument, on the; cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(lambda x, y, z: x + y + z,; ... ""poly"", ""ethyl"", z=""ene""); ... future.result(); ""polyethylene"". Generate a product of two random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(random_product, 1); ... future.result(); [23.325755364428026]. Parameters; ----------; fn:; The function to execute.; args:; Arguments for the funciton.; kwargs:; Keyword arguments for the function.; """"""; """"""Aysncio compatible version of :meth:`BatchPoolExecutor.submit",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
Performance,race condition,race conditions,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
Safety,avoid,avoid,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
Testability,test,test,"# FIXME: remove once test output matches docs; # This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
Deployability,install,installed,"""""""; Build a new Python image with dill and the specified pip packages installed. Notes; -----. This function is used to build Python images for :class:`.PythonJob`. Examples; --------. >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) # doctest: +SKIP. Parameters; ----------; fullname:; Full name of where to build the image including any repository prefix and tags; if desired (default tag is `latest`).; requirements:; List of pip packages to install.; python_version:; String in the format of `major_version.minor_version` (ex: `3.9`). Defaults to; current version of Python that is running.; _tmp_dir:; Location to place local temporary files used while building the image.; show_docker_output:; Print the output from Docker when building / pushing the image. Returns; -------; Full name where built image is located.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docker.py
Availability,echo,echo,"# pylint: disable=cyclic-import; """"""; Object representing a single job to execute. Notes; -----; This class should never be created directly by the user. Use :meth:`.Batch.new_job`,; :meth:`.Batch.new_bash_job`, or :meth:`.Batch.new_python_job` instead.; """"""; # pylint: disable=consider-using-f-string; # resources used in the command; # resources declared in the appropriate place; """"""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""; """"""; Set the job's storage size. Examples; --------. Set the job's disk requirements to 10 Gi:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; -----. The storage expression must be of the form {number}{suffix}; where valid optional suffixes are *K*, *Ki*, *M*, *Mi*,; *G*, *Gi*, *T*, *Ti*, *P*, and *Pi*. Omitting a suffix means; the value is in bytes. For the :class:`.ServiceBackend`, jobs requesting one or more cores receive; 5 GiB of storage for the root file system `/`. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at `/io`. Batch automatically writes all :class:`.ResourceFile` to; `/io`. The default storage size is 0 Gi. The minimum storage size is 0 Gi a",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Deployability,install,installed,"s; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""; """"""; Object representing a single Python job to execute. Examples; --------. Create a new Python job that multiplies two numbers and then adds 5 to the result:. .. code-block:: python. # Create a batch object with a default Python image. b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def multiply(x, y):; return x * y. def add(x, y):; return x + y. j = b.new_python_job(); result = j.call(multiply, 2, 3); result = j.call(add, result, 5). # Write out the str representation of result to a file. b.write_output(result.as_str(), 'hello.txt'). b.run(). Notes; -----; This class should never be created directly by the user. Use :meth:`.Batch.new_python_job`; instead.; """"""; """"""; Set the job's docker image. Notes; -----. `image` must already exist and have the same version of Python as what is; being used on the computer submitting the Batch. It also must have the; `dill` Python package installed. You can use the function :func:`.docker.build_python_image`; to build a new image containing `dill` and additional Python packages. Examples; --------. Set the job's docker image to `hailgenetics/python-dill:3.9-slim`:. >>> b = Batch(); >>> j = b.new_python_job(); >>> (j.image('hailgenetics/python-dill:3.9-slim'); ... .call(print, 'hello')); >>> b.run() # doctest: +SKIP. Parameters; ----------; image:; Docker image to use. Returns; -------; Same job object with docker image set.; """"""; """"""Execute a Python function. Examples; --------. .. code-block:: python. import json. def add(x, y):; return x + y. def multiply(x, y):; return x * y. def format_as_csv(x, y, add_result, mult_result):; return f'{x},{y},{add_result},{mult_result}'. def csv_to_json(path):; data = []; with open(path) as f:; for line in f:; line = line.rstrip(); fields = line.split(','); d = {'x': int(fields[0]),; 'y': int(fields[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Energy Efficiency,power,power," -----. The memory expression must be of the form {number}{suffix}; where valid optional suffixes are *K*, *Ki*, *M*, *Mi*,; *G*, *Gi*, *T*, *Ti*, *P*, and *Pi*. Omitting a suffix means; the value is in bytes. For the :class:`.ServiceBackend`, the values 'lowmem', 'standard',; and 'highmem' are also valid arguments. 'lowmem' corresponds to; approximately 1 Gi/core, 'standard' corresponds to approximately; 4 Gi/core, and 'highmem' corresponds to approximately 7 Gi/core.; The default value is 'standard'. Parameters; ----------; memory:; Units are in bytes if `memory` is an :obj:`int`. If `None`,; use the default value for the :class:`.ServiceBackend` ('standard'). Returns; -------; Same job object with memory requirements set.; """"""; """"""; Set the job's CPU requirements. Notes; -----. The string expression must be of the form {number}{suffix}; where the optional suffix is *m* representing millicpu.; Omitting a suffix means the value is in cpu. For the :class:`.ServiceBackend`, `cores` must be a power of; two between 0.25 and 16. Examples; --------. Set the job's CPU requirement to 250 millicpu:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters; ----------; cores:; Units are in cpu if `cores` is numeric. If `None`,; use the default value for the :class:`.ServiceBackend`; (1 cpu). Returns; -------; Same job object with CPU requirements set.; """"""; """"""; Set the job to always run, even if dependencies fail. Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""; """"""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Integrability,depend,dependencies,"# pylint: disable=cyclic-import; """"""; Object representing a single job to execute. Notes; -----; This class should never be created directly by the user. Use :meth:`.Batch.new_job`,; :meth:`.Batch.new_bash_job`, or :meth:`.Batch.new_python_job` instead.; """"""; # pylint: disable=consider-using-f-string; # resources used in the command; # resources declared in the appropriate place; """"""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""; """"""; Set the job's storage size. Examples; --------. Set the job's disk requirements to 10 Gi:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; -----. The storage expression must be of the form {number}{suffix}; where valid optional suffixes are *K*, *Ki*, *M*, *Mi*,; *G*, *Gi*, *T*, *Ti*, *P*, and *Pi*. Omitting a suffix means; the value is in bytes. For the :class:`.ServiceBackend`, jobs requesting one or more cores receive; 5 GiB of storage for the root file system `/`. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at `/io`. Batch automatically writes all :class:`.ResourceFile` to; `/io`. The default storage size is 0 Gi. The minimum storage size is 0 Gi a",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Performance,latency,latency,"Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""; """"""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters; ----------; is_spot:; If False, this job will be run on non-spot instances. Returns; -------; Same job object.; """"""; """"""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""; """"""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Safety,avoid,avoid,"Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""; """"""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters; ----------; is_spot:; If False, this job will be run on non-spot instances. Returns; -------; Same job object.; """"""; """"""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""; """"""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Testability,test,test," value for the :class:`.ServiceBackend` ('standard'). Returns; -------; Same job object with memory requirements set.; """"""; """"""; Set the job's CPU requirements. Notes; -----. The string expression must be of the form {number}{suffix}; where the optional suffix is *m* representing millicpu.; Omitting a suffix means the value is in cpu. For the :class:`.ServiceBackend`, `cores` must be a power of; two between 0.25 and 16. Examples; --------. Set the job's CPU requirement to 250 millicpu:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters; ----------; cores:; Units are in cpu if `cores` is numeric. If `None`,; use the default value for the :class:`.ServiceBackend`; (1 cpu). Returns; -------; Same job object with CPU requirements set.; """"""; """"""; Set the job to always run, even if dependencies fail. Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""; """"""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters; ----------; is_spot:; If False, this job will be run on non-spot instances. Returns; -------; Same job object.; """"""; """"""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceB",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
Availability,echo,echo,"""""""; Abstract class for resources.; """"""; """"""; Class representing a single file resource. There exist two subclasses:; :class:`.InputResourceFile` and :class:`.JobResourceFile`.; """"""; # pylint: disable=consider-using-f-string; # pylint: disable=W0613; # pylint: disable=no-member; # pylint: disable=no-member; """"""; Class representing a resource from an input file. Examples; --------; `input` is an :class:`.InputResourceFile` of the batch `b`; and is used in job `j`:. >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(); """"""; """"""; Class representing an intermediate file from a job. Examples; --------; `j.ofile` is a :class:`.JobResourceFile` on the job`j`:. >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.Batch.write_output` if the output needs; to be saved.; """"""; """"""; Specify the file extension to use. Examples; --------. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; -----; The default file name for a :class:`.JobResourceFile` is the name; of the identifier. Parameters; ----------; extension: :obj:`str`; File extension to use. Returns; -------; :class:`.JobResourceFile`; Same resource file with the extension specified; """"""; """"""; Class representing a mapping of identifiers to a resource file. Examples; --------. Initialize a batch and create a new job:. >>> b = Batch(); >>> j = b.new_job(). Read a set of input files as a resource group:. >>> bfile = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'). Create a resource group from a job intermediate:. >>> j.declare_resource_group(ofile={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam'}); >>> ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/resource.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/resource.py
Availability,avail,available,"ue; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; #; # source_suffix = ['.rst', '.md']; # The master toctree document.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #; # This is also used if you do content translation via gettext catalogs.; # Usually you set ""language"" from the command line for these cases.; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # This pattern also affects html_static_path and html_extra_path.; # The name of the Pygments (syntax highlighting) style to use.; # -- Options for HTML output -------------------------------------------------; # The theme to use for HTML and HTML Help pages. See the documentation for; # a list of builtin themes.; #; # Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #; # html_theme_options = {}; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # html_static_path = ['_static']; # Custom sidebar templates, must be a dictionary that maps document names; # to template names.; #; # The default sidebars (for documents that don't match any pattern) are; # defined by theme itself. Builtin themes are using these templates by; # default: ``['localtoc.html', 'relations.html', 'sourcelink.html',; # 'searchbox.html']``.; #; # html_sidebars = {}; # https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html; # -- Extension configuration -------------------------------------------------; # fallback to __qualname__ parsing; # handle special descriptor objects",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
Deployability,configurat,configuration,"# -*- coding: utf-8 -*-; #; # Configuration file for the Sphinx documentation builder.; #; # This file does only contain a selection of the most common options. For a; # full list see the documentation:; # http://www.sphinx-doc.org/en/master/config; # -- Path setup --------------------------------------------------------------; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #; # import os; # import sys; # sys.path.insert(0, os.path.abspath('.')); # -- Project information -----------------------------------------------------; # The short X.Y version; # The full version, including alpha/beta/rc tags; # -- General configuration ---------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # napoleon_include_private_with_doc = True; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; #; # source_suffix = ['.rst', '.md']; # The master toctree document.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #; # This is also used if you do content translation via gettext catalogs.; # Usually you set ""language"" from the command line for these cases.; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # This pattern also affects html_static_path and html_extra_path.; # The name of the Pygments (syntax highlighting) style to use.; # -- Options for HTML output -------------------------------------------------; # The theme to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
Modifiability,config,config,"# -*- coding: utf-8 -*-; #; # Configuration file for the Sphinx documentation builder.; #; # This file does only contain a selection of the most common options. For a; # full list see the documentation:; # http://www.sphinx-doc.org/en/master/config; # -- Path setup --------------------------------------------------------------; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #; # import os; # import sys; # sys.path.insert(0, os.path.abspath('.')); # -- Project information -----------------------------------------------------; # The short X.Y version; # The full version, including alpha/beta/rc tags; # -- General configuration ---------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #; # Add any Sphinx extension module names here, as strings. They can be; # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom; # ones.; # napoleon_include_private_with_doc = True; # Add any paths that contain templates here, relative to this directory.; # The suffix(es) of source filenames.; # You can specify multiple suffix as a list of string:; #; # source_suffix = ['.rst', '.md']; # The master toctree document.; # The language for content autogenerated by Sphinx. Refer to documentation; # for a list of supported languages.; #; # This is also used if you do content translation via gettext catalogs.; # Usually you set ""language"" from the command line for these cases.; # List of patterns, relative to source directory, that match files and; # directories to ignore when looking for source files.; # This pattern also affects html_static_path and html_extra_path.; # The name of the Pygments (syntax highlighting) style to use.; # -- Options for HTML output -------------------------------------------------; # The theme to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
Testability,test,test,"""""""; QC data and get association test statistics; """"""; """"""; Clump association results with PLINK; """"""; """"""; Merge clumped results files together; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py
Testability,test,testing,"# read in data; # split training and testing data for the current window; # run random forest; # apply the trained random forest on testing data; # store obs and pred values for this window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py
Testability,test,testing,"# read in data; # split training and testing data for the current window; # run random forest; # apply the trained random forest on testing data; # store obs and pred values for this window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py
Testability,test,testing,"# read in data; # split training and testing data for the current window; # run random forest; # apply the trained random forest on testing data; # store obs and pred values for this window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py
Availability,error,error,"# don't return status error; # type: ignore; # {; # batch_id: int; # job_id: int; # user: str; # billing_project: str; # name: optional(str); # state: str (Ready, Running, Success, Error, Failure, Cancelled); # exit_code: optional(int); # duration: optional(int) (msecs); # msec_mcpu: int; # cost: float; # }; # https://stackoverflow.com/a/76515675/6823256; """"""; Get resource usage for a job, one key for each section of the task.; This doesn't return a dictionary of dataframes, but could be converted with:. dataframes = {; key: pd.DataFrame(data=values['data'], columns=values['columns']); for key, values in job.resource_usage.items(); }. Returns:; dict[str, dict]: values are convertible to dataframe; """"""; # note, not a dataframe, but easy to get with:; # updates _last_known_status; # max 44.5s; # updates _last_known_status; # max 44.5s; # if/when we add nested job groups, then a job group must always be submitted after its parents",MatchSource.CODE_COMMENT,hail/python/hailtop/batch_client/aioclient.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py
Deployability,update,updates,"# don't return status error; # type: ignore; # {; # batch_id: int; # job_id: int; # user: str; # billing_project: str; # name: optional(str); # state: str (Ready, Running, Success, Error, Failure, Cancelled); # exit_code: optional(int); # duration: optional(int) (msecs); # msec_mcpu: int; # cost: float; # }; # https://stackoverflow.com/a/76515675/6823256; """"""; Get resource usage for a job, one key for each section of the task.; This doesn't return a dictionary of dataframes, but could be converted with:. dataframes = {; key: pd.DataFrame(data=values['data'], columns=values['columns']); for key, values in job.resource_usage.items(); }. Returns:; dict[str, dict]: values are convertible to dataframe; """"""; # note, not a dataframe, but easy to get with:; # updates _last_known_status; # max 44.5s; # updates _last_known_status; # max 44.5s; # if/when we add nested job groups, then a job group must always be submitted after its parents",MatchSource.CODE_COMMENT,hail/python/hailtop/batch_client/aioclient.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py
Integrability,depend,depend,"# FIXME: should depend on ssl context; # pylint: disable=import-outside-toplevel; # no encryption on the internal gateway; # local mode does not have access to self-signed certs; # Terra app networking doesn't use self-signed certs; # Terra app services are in the same pod and just use http",MatchSource.CODE_COMMENT,hail/python/hailtop/config/deploy_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py
Security,encrypt,encryption,"# FIXME: should depend on ssl context; # pylint: disable=import-outside-toplevel; # no encryption on the internal gateway; # local mode does not have access to self-signed certs; # Terra app networking doesn't use self-signed certs; # Terra app services are in the same pod and just use http",MatchSource.CODE_COMMENT,hail/python/hailtop/config/deploy_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py
Modifiability,config,config,"# in older versions, the config file was accidentally named; # config.yaml, if the new config does not exist, and the old; # one does, silently rename it",MatchSource.CODE_COMMENT,hail/python/hailtop/config/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py
Availability,error,error,"le Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you ca",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
Performance,load,load,"Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:o",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
Security,access,access,"""""""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws a",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
Availability,error,error,"# type: ignore # https://github.com/python/typeshed/blob/a40d79a4e63c4e750a8d3a8012305da942251eb4/stdlib/http/client.pyi#L81; # type: ignore # https://github.com/python/typeshed/blob/a40d79a4e63c4e750a8d3a8012305da942251eb4/stdlib/http/client.pyi#L81; # type: ignore # typeshed is wrong, this *is* an IOBase; # Hadoop semantics: creation time is used if the object has no notion of last modification time.; # Unless we are using a glob pattern, a path referring to no files should error",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/router_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/router_fs.py
Security,authenticat,authenticated,"""""""Print version information and exit.""""""; # pylint: disable=import-outside-toplevel; """"""Issue authenticated curl requests to Hail infrastructure.""""""; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/__main__.py
Testability,log,login,"""""""Obtain Hail credentials.""""""; # pylint: disable=import-outside-toplevel; """"""Obtain Hail credentials with a copy paste token.""""""; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; """"""Revoke Hail credentials.""""""; # pylint: disable=import-outside-toplevel; """"""List Hail credentials.""""""; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; """"""Get Hail user information.""""""; # pylint: disable=import-outside-toplevel; # deprecated - backwards compatibility; # deprecated - backwards compatibility; """"""; Create a new Hail user with username USERNAME and login ID LOGIN_ID.; """"""; # pylint: disable=import-outside-toplevel; """"""; Delete the Hail user with username USERNAME.; """"""; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/auth/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/cli.py
Testability,log,logged,"# Confirm that the logged in user is registered with the hail service",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/auth/login.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/login.py
Testability,log,log,"""""""List batches.""""""; """"""Get information on the batch with id BATCH_ID.""""""; # pylint: disable=import-outside-toplevel; """"""Cancel the batch with id BATCH_ID.""""""; # pylint: disable=import-outside-toplevel; """"""Delete the batch with id BATCH_ID.""""""; # pylint: disable=import-outside-toplevel; """"""Get the log for the job with id JOB_ID in the batch with id BATCH_ID.""""""; # pylint: disable=import-outside-toplevel; """"""Wait for the batch with id BATCH_ID to complete, then print status.""""""; # pylint: disable=import-outside-toplevel; """"""Get the status and specification for the job with id JOB_ID in the batch with id BATCH_ID.""""""; # pylint: disable=import-outside-toplevel; # https://stackoverflow.com/q/71986632/6823256; """"""Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS. If you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl batch submit --image-name docker.io/image my_script.py -- some-argument --animal dog; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/batch/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/batch/cli.py
Deployability,configurat,configuration,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""; """"""Set a Hail configuration parameter.""""""; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; """"""Unset a Hail configuration parameter (restore to default behavior).""""""; # pylint: disable=import-outside-toplevel; """"""Get the value of a Hail configuration parameter.""""""; # pylint: disable=import-outside-toplevel; """"""Print the location of the config file.""""""; # pylint: disable=import-outside-toplevel; """"""Lists every config variable in the section.""""""; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
Modifiability,config,configuration,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""; """"""Set a Hail configuration parameter.""""""; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; """"""Unset a Hail configuration parameter (restore to default behavior).""""""; # pylint: disable=import-outside-toplevel; """"""Get the value of a Hail configuration parameter.""""""; # pylint: disable=import-outside-toplevel; """"""Print the location of the config file.""""""; # pylint: disable=import-outside-toplevel; """"""Lists every config variable in the section.""""""; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
Availability,down,down,"# If gcloud's output format changes in the future and the version can't be parsed,; # then continue and attempt to run gcloud.; # arguments with default parameters; # initialization action flags; # requester pays; """"""; Start a Dataproc cluster configured for Hail.; """"""; """"""; Shut down a Dataproc cluster.; """"""; # print underlying gcloud command; """"""; List active Dataproc clusters.; """"""; """"""; Connect to a running Dataproc cluster with name NAME and start; the web service SERVICE.; """"""; """"""Submit the Python script at path SCRIPT to a running Dataproc cluster with name NAME. You may pass arguments to the script being submitted by listing them after the script; however,; if you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl dataproc submit name --image-name docker.io/image my_script.py -- some-argument --animal dog. """"""; """"""; Diagnose problems in a Dataproc cluster with name NAME.; """"""; """"""; Modify an active dataproc cluster with name NAME.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py
Modifiability,config,configured,"# If gcloud's output format changes in the future and the version can't be parsed,; # then continue and attempt to run gcloud.; # arguments with default parameters; # initialization action flags; # requester pays; """"""; Start a Dataproc cluster configured for Hail.; """"""; """"""; Shut down a Dataproc cluster.; """"""; # print underlying gcloud command; """"""; List active Dataproc clusters.; """"""; """"""; Connect to a running Dataproc cluster with name NAME and start; the web service SERVICE.; """"""; """"""Submit the Python script at path SCRIPT to a running Dataproc cluster with name NAME. You may pass arguments to the script being submitted by listing them after the script; however,; if you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl dataproc submit name --image-name docker.io/image my_script.py -- some-argument --animal dog. """"""; """"""; Diagnose problems in a Dataproc cluster with name NAME.; """"""; """"""; Modify an active dataproc cluster with name NAME.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py
Deployability,configurat,configuration,"# pylint: disable=import-outside-toplevel; # https://stackoverflow.com/questions/40674914/google-chrome-path-in-windows-10; # Dataproc port mapping; # open SSH tunnel to master node; # open Chrome with SOCKS proxy configuration; # pylint: disable=consider-using-with",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py
Modifiability,config,configuration,"# pylint: disable=import-outside-toplevel; # https://stackoverflow.com/questions/40674914/google-chrome-path-in-windows-10; # Dataproc port mapping; # open SSH tunnel to master node; # open Chrome with SOCKS proxy configuration; # pylint: disable=consider-using-with",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py
Deployability,configurat,configuration,"""""""Run a gcloud command.""""""; """"""Get a gcloud configuration value.""""""; """"""Get gcloud version as a tuple.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/gcloud.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py
Modifiability,config,configuration,"""""""Run a gcloud command.""""""; """"""Get a gcloud configuration value.""""""; """"""Get gcloud version as a tuple.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/gcloud.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py
Energy Efficiency,charge,charges,"# leadre (master) machine type to memory map, used for setting; # spark.driver.memory property; # pylint: disable=unused-argument; # default to highmem machines if using VEP; # default initialization script to start up cluster with; # requester pays support; # Need to pick requester pays project.; # gcloud version 277 and onwards requires you to specify a region. Let's just require it for all hailctl users for consistency.; # add VEP init script; # VEP is too expensive if you have to pay egress charges.; # add custom init scripts; # if Python packages requested, add metadata variable; # 1. GCE only provides 51 GiB for an n1-highmem-8 (advertised as 52 GiB); # 2. System daemons use ~10 GiB based on syslog ""earlyoom"" log statements during VM startup; # A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container; # rewrite metadata and properties to escape them; # command to start cluster; # print underlying gcloud command; # spin up cluster",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
Modifiability,variab,variable,"# leadre (master) machine type to memory map, used for setting; # spark.driver.memory property; # pylint: disable=unused-argument; # default to highmem machines if using VEP; # default initialization script to start up cluster with; # requester pays support; # Need to pick requester pays project.; # gcloud version 277 and onwards requires you to specify a region. Let's just require it for all hailctl users for consistency.; # add VEP init script; # VEP is too expensive if you have to pay egress charges.; # add custom init scripts; # if Python packages requested, add metadata variable; # 1. GCE only provides 51 GiB for an n1-highmem-8 (advertised as 52 GiB); # 2. System daemons use ~10 GiB based on syslog ""earlyoom"" log statements during VM startup; # A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container; # rewrite metadata and properties to escape them; # command to start cluster; # print underlying gcloud command; # spin up cluster",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
Testability,log,log,"# leadre (master) machine type to memory map, used for setting; # spark.driver.memory property; # pylint: disable=unused-argument; # default to highmem machines if using VEP; # default initialization script to start up cluster with; # requester pays support; # Need to pick requester pays project.; # gcloud version 277 and onwards requires you to specify a region. Let's just require it for all hailctl users for consistency.; # add VEP init script; # VEP is too expensive if you have to pay egress charges.; # add custom init scripts; # if Python packages requested, add metadata variable; # 1. GCE only provides 51 GiB for an n1-highmem-8 (advertised as 52 GiB); # 2. System daemons use ~10 GiB based on syslog ""earlyoom"" log statements during VM startup; # A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container; # rewrite metadata and properties to escape them; # command to start cluster; # print underlying gcloud command; # spin up cluster",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
Availability,error,error,"# only print output on error",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/utils.py
Availability,echo,echo,"#!/opt/conda/default/bin/python3; # get role of machine (master or worker); # additional packages to install; # add user-requested packages; # VEP ENV; # the below are necessary to make 'submit' work; # Update python3 kernel spec with the environment variables and the hail; # spark monitor; # write python3 kernel spec file to default Jupyter kernel directory; # some old notebooks use the ""Hail"" kernel, so create that too; # create Jupyter configuration file; # setup jupyter-spark extension; """"""ipython profile create && echo ""c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')"" >> $(ipython profile locate default)/ipython_kernel_config.py""""""; # create systemd service file for Jupyter notebook server process; # add Jupyter service to autorun and start it",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
Deployability,install,install,"#!/opt/conda/default/bin/python3; # get role of machine (master or worker); # additional packages to install; # add user-requested packages; # VEP ENV; # the below are necessary to make 'submit' work; # Update python3 kernel spec with the environment variables and the hail; # spark monitor; # write python3 kernel spec file to default Jupyter kernel directory; # some old notebooks use the ""Hail"" kernel, so create that too; # create Jupyter configuration file; # setup jupyter-spark extension; """"""ipython profile create && echo ""c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')"" >> $(ipython profile locate default)/ipython_kernel_config.py""""""; # create systemd service file for Jupyter notebook server process; # add Jupyter service to autorun and start it",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
Energy Efficiency,monitor,monitor,"#!/opt/conda/default/bin/python3; # get role of machine (master or worker); # additional packages to install; # add user-requested packages; # VEP ENV; # the below are necessary to make 'submit' work; # Update python3 kernel spec with the environment variables and the hail; # spark monitor; # write python3 kernel spec file to default Jupyter kernel directory; # some old notebooks use the ""Hail"" kernel, so create that too; # create Jupyter configuration file; # setup jupyter-spark extension; """"""ipython profile create && echo ""c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')"" >> $(ipython profile locate default)/ipython_kernel_config.py""""""; # create systemd service file for Jupyter notebook server process; # add Jupyter service to autorun and start it",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
Modifiability,variab,variables,"#!/opt/conda/default/bin/python3; # get role of machine (master or worker); # additional packages to install; # add user-requested packages; # VEP ENV; # the below are necessary to make 'submit' work; # Update python3 kernel spec with the environment variables and the hail; # spark monitor; # write python3 kernel spec file to default Jupyter kernel directory; # some old notebooks use the ""Hail"" kernel, so create that too; # create Jupyter configuration file; # setup jupyter-spark extension; """"""ipython profile create && echo ""c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')"" >> $(ipython profile locate default)/ipython_kernel_config.py""""""; # create systemd service file for Jupyter notebook server process; # add Jupyter service to autorun and start it",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
Modifiability,config,config,"""""""Set dev config property PROPERTY to value VALUE.""""""; """"""List the settings in the dev config.""""""; # pylint: disable=import-outside-toplevel",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dev/config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dev/config.py
Modifiability,config,configured,"""""""; Start an HDInsight cluster configured for Hail.; """"""; # pylint: disable=import-outside-toplevel; """"""; Stop an HDInsight cluster configured for Hail.; """"""; """"""; Submit a job to an HDInsight cluster configured for Hail. If you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog; """"""; """"""; List HDInsight clusters configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
Security,password,password,"""""""; Start an HDInsight cluster configured for Hail.; """"""; # pylint: disable=import-outside-toplevel; """"""; Stop an HDInsight cluster configured for Hail.; """"""; """"""; Submit a job to an HDInsight cluster configured for Hail. If you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog; """"""; """"""; List HDInsight clusters configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
Availability,error,error,"# pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # I figured this out after looking at; # https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-manage-ambari-rest-api#restart-a-service-component; # and doing some trial and error",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/start.py
Integrability,protocol,protocol,"# pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # NB: Only the local protocol is permitted, the file protocol is banned #security",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/submit.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py
Security,secur,security,"# pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # pylint: disable=import-outside-toplevel; # NB: Only the local protocol is permitted, the file protocol is banned #security",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/submit.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py
Availability,down,down,"""""""; Renders the given number as a human-readable filesize using binary multiple-byte units (e.g. ""MiB"", not ""MB"").; Rounds down to the nearest integer value and multiple-byte unit (e.g. ``filesize((1024**3) - 1)`` returns; ``""1023MiB""``, not ``""1GiB""`` or ``""1023.9999990463257MiB""``).; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/filesize.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/filesize.py
Availability,error,error,"# pylint: disable=import-error; # type: ignore; """"""function retry() {; ""$@"" ||; (sleep 2 && ""$@"") ||; (sleep 5 && ""$@"");; }""""""; # pylint: disable=invalid-name; # pylint: disable=invalid-name; # pylint: disable=using-constant-test; # The appearance of the keyword `yield` forces Python to make this function into a generator; # 22 characters is math.log(62 ** 22, 2) == ~130 bits of randomness. OWASP; # recommends at least 128 bits:; # https://owasp.org/www-community/vulnerabilities/Insufficient_Session-ID_Length; # replace with itertools.batched in Python 3.12; # pylint: disable=try-except-raise; # pylint: disable=broad-except; """"""`OnlineBoundedGather2` provides the capability to run background; tasks with bounded parallelism. It is a context manager, and; waits for all background tasks to complete on exit. `OnlineBoundedGather2` supports cancellation of background tasks.; When a background task raises `asyncio.CancelledError`, the task; is considered complete and the pool and other background tasks; continue runnning. If a background task fails (raises an exception besides; `asyncio.CancelledError`), all running background tasks are; cancelled and the the pool is shut down. Subsequent calls to; `OnlineBoundedGather2.call()` raise `PoolShutdownError`. Because the pool runs tasks in the background, multiple exceptions; can occur simultaneously. The first exception raised, whether by; a background task or into the context manager exit, is raised by; the context manager exit, and any further exceptions are logged; and otherwise discarded.; """"""; # done if there are no pending tasks (the tasks are all; # complete), or if we've shutdown and the cancelled tasks are; # complete; # not pending tasks, so done; """"""Shut down the pool. Cancel all pending tasks and wait for them to complete.; Subsequent calls to call will raise `PoolShutdownError`.; """"""; # shut down the pending tasks; """"""Invoke a function as a background task. Return the task, which can be used to wait on (using; `On",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Energy Efficiency,reduce,reduce,"f Google rate-limiting as of 2023-10-15; # socket.EAI_AGAIN: [Errno -3] Temporary failure in name resolution; # socket.EAI_NONAME: [Errno 8] nodename nor servname provided, or not known; # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); # DockerError(500, 'unknown: Tag v1.11.2 was deleted or has expired. To pull, revive via time machine'); # pylint: disable=import-outside-toplevel,cyclic-import; # 503 service unavailable; # 429 ""Temporarily throttled, too many requests""; # do not set larger than 30 to avoid BigInt arithmetic; # Based on AWS' recommendations:; # - https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/; # - https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java; # pylint: disable=try-except-raise; # pylint: disable=try-except-raise; # 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.; """"""Return the basename of the path of the URL `url`.""""""; """"""Join the (relative or absolute) path `path` to the URL `url`.""""""; """"""Return scheme of `url`, or the empty string if there is no scheme.""""""; """"""Strip the query parameters from `url` and parse them into a dictionary.; Assumes that all query parameters are used only once, so have only one; value.; """"""; # https://github.com/distribution/distribution/blob/v2.7.1/reference/reference.go; """"""Determine if the current Python session is interactive. This should return True in IPython, a Python interpreter, and a Jupyter Notebook. """"""; # https://stackoverflow.com/questions/2356399/tell-if-python-is-in-interactive-mode",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Integrability,depend,dependency,"ception `exc`. """"""; """"""Run the partial functions `pfs` as tasks with parallelism bounded; by `sema`, which should be `asyncio.Semaphore` whose initial value; is the level of parallelism. The return value is the list of partial function results. The first exception raised by a partial function is raised by; bounded_gather2_raise_exceptions. If cancel_on_error is False (the default), the remaining partial; functions continue to run with bounded parallelism. If; cancel_on_error is True, the unfinished tasks are all cancelled. """"""; # type: ignore; # nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""; # these should match (where an equivalent exists) nettyRetryableErrorNumbers in; # is/hail/services/package.scala; # An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.; # pylint: disable=import-outside-toplevel,cyclic-import; # observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; ",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Performance,throttle,throttled,"sema`, which should be `asyncio.Semaphore` whose initial value; is the desired level of parallelism. The return value is the list of partial function results as pairs:; the pair `(value, None)` if the partial function returned value or; `(None, exc)` if the partial function raised the exception `exc`. """"""; """"""Run the partial functions `pfs` as tasks with parallelism bounded; by `sema`, which should be `asyncio.Semaphore` whose initial value; is the level of parallelism. The return value is the list of partial function results. The first exception raised by a partial function is raised by; bounded_gather2_raise_exceptions. If cancel_on_error is False (the default), the remaining partial; functions continue to run with bounded parallelism. If; cancel_on_error is True, the unfinished tasks are all cancelled. """"""; # type: ignore; # nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""; # these should match (where an equivalent exists) nettyRetryableErrorNumbers in; # is/hail/services/package.scala; # An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.; # pylint: disable=import-outside-toplevel,cyclic-import; # observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSoc",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Safety,timeout,timeout,"sema`, which should be `asyncio.Semaphore` whose initial value; is the desired level of parallelism. The return value is the list of partial function results as pairs:; the pair `(value, None)` if the partial function returned value or; `(None, exc)` if the partial function raised the exception `exc`. """"""; """"""Run the partial functions `pfs` as tasks with parallelism bounded; by `sema`, which should be `asyncio.Semaphore` whose initial value; is the level of parallelism. The return value is the list of partial function results. The first exception raised by a partial function is raised by; bounded_gather2_raise_exceptions. If cancel_on_error is False (the default), the remaining partial; functions continue to run with bounded parallelism. If; cancel_on_error is True, the unfinished tasks are all cancelled. """"""; # type: ignore; # nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""; # these should match (where an equivalent exists) nettyRetryableErrorNumbers in; # is/hail/services/package.scala; # An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.; # pylint: disable=import-outside-toplevel,cyclic-import; # observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSoc",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Testability,test,test,"# pylint: disable=import-error; # type: ignore; """"""function retry() {; ""$@"" ||; (sleep 2 && ""$@"") ||; (sleep 5 && ""$@"");; }""""""; # pylint: disable=invalid-name; # pylint: disable=invalid-name; # pylint: disable=using-constant-test; # The appearance of the keyword `yield` forces Python to make this function into a generator; # 22 characters is math.log(62 ** 22, 2) == ~130 bits of randomness. OWASP; # recommends at least 128 bits:; # https://owasp.org/www-community/vulnerabilities/Insufficient_Session-ID_Length; # replace with itertools.batched in Python 3.12; # pylint: disable=try-except-raise; # pylint: disable=broad-except; """"""`OnlineBoundedGather2` provides the capability to run background; tasks with bounded parallelism. It is a context manager, and; waits for all background tasks to complete on exit. `OnlineBoundedGather2` supports cancellation of background tasks.; When a background task raises `asyncio.CancelledError`, the task; is considered complete and the pool and other background tasks; continue runnning. If a background task fails (raises an exception besides; `asyncio.CancelledError`), all running background tasks are; cancelled and the the pool is shut down. Subsequent calls to; `OnlineBoundedGather2.call()` raise `PoolShutdownError`. Because the pool runs tasks in the background, multiple exceptions; can occur simultaneously. The first exception raised, whether by; a background task or into the context manager exit, is raised by; the context manager exit, and any further exceptions are logged; and otherwise discarded.; """"""; # done if there are no pending tasks (the tasks are all; # complete), or if we've shutdown and the cancelled tasks are; # complete; # not pending tasks, so done; """"""Shut down the pool. Cancel all pending tasks and wait for them to complete.; Subsequent calls to call will raise `PoolShutdownError`.; """"""; # shut down the pending tasks; """"""Invoke a function as a background task. Return the task, which can be used to wait on (using; `On",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
Availability,avail,available-in-fixtures,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
Testability,test,test-result-information-available-in-fixtures,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
Usability,simpl,simple,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
Performance,cache,cache,"""""""Asserts creation of execution cache folder""""""",MatchSource.CODE_COMMENT,hail/python/test/hail/test_call_caching.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_call_caching.py
Availability,error,error,"# Should be no error; # ensure functions are cleaned up without error; # Should be no error; # Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py
Availability,error,errors,"# TODO: Scala Pretty errors out with a StackOverflowError here; # ht._force_count(); # Check values that are pointers to other memory",MatchSource.CODE_COMMENT,hail/python/test/hail/test_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_ir.py
Availability,error,error,"# Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_no_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_no_context.py
Deployability,pipeline,pipelines,"# DSP's variants team depends on these functions which are in experimental. Normally we do not; # guarantee backwards compatibility but given the practical importance of these to production; # pipelines at Broad, we ensure they continue to exist.",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py
Integrability,depend,depends,"# DSP's variants team depends on these functions which are in experimental. Normally we do not; # guarantee backwards compatibility but given the practical importance of these to production; # pipelines at Broad, we ensure they continue to exist.",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py
Deployability,integrat,integration,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing; # idempotent; # idempotent; # duplicate col keys; # duplicate row keys",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
Integrability,integrat,integration,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing; # idempotent; # idempotent; # duplicate col keys; # duplicate row keys",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
Testability,test,test,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing; # idempotent; # idempotent; # duplicate col keys; # duplicate row keys",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
Security,hash,hashable,"# Want to make sure all implict conversions work.; # 10; # Testing types work out; # filter; # explode; # group_by; # works with _ctx; # FIXME: add boolean when function registry is removed; # Tested against R code; # y = c(0.22848042, 0.09159706, -0.43881935, -0.99106171, 2.12823289); # x = c(0.2575928, -0.3445442, 1.6590146, -1.1688806, 0.5587043); # df = data.frame(y, x); # fit <- lm(y ~ x, data=df); # sumfit = summary(fit); # coef = sumfit$coefficients; # mse = sumfit$sigma; # r2 = sumfit$r.squared; # r2adj = sumfit$adj.r.squared; # f = sumfit$fstatistic; # p = pf(f[1],f[2],f[3],lower.tail=F); # swapping the intercept and t.x; # weighted OLS; # the `s` key is stored before the `m` in java.util.HashMap; # NA != NA; # NA != NA; # stable sort; # noqa: PLR0124; # noqa: PLR0124; # noqa: PLR0124; # largest possible homozygote call; """"""+---------+; | col_idx |; +---------+; | int32 |; +---------+; | 0 |; | 1 |; | 2 |; +---------+; """"""; # Tests table.aggregate initOp; # Tests MatrixMapRows initOp; # Tests MatrixMapCols initOp; # must test that call_stats isn't null, because equality doesn't test for that; # https://cran.r-project.org/web/packages/samplesizeCMH/vignettes/samplesizeCMH-introduction.html; # https://www.biostathandbook.com/cmh.html; # Test that it's evalable, since python sets aren't hashable.; # Test that it's evalable, since python dicts aren't hashable.; # need to test laziness, so we will overwrite a file; # assumes cdf was computed from a (possibly shuffled) range table; # test NA; # test If; # test StreamIota; # test ToArray; # test ToStream; # test StreamZip; # test StreamFilter; # test StreamFilter; # test StreamFold; # test StreamScan; # test StreamAgg; # test AggExplode; # test TableCount; # test TableGetGlobals; # test TableCollect; # test TableAggregate; # test MatrixCount; # test MatrixAggregate",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
Testability,test,test,"# Want to make sure all implict conversions work.; # 10; # Testing types work out; # filter; # explode; # group_by; # works with _ctx; # FIXME: add boolean when function registry is removed; # Tested against R code; # y = c(0.22848042, 0.09159706, -0.43881935, -0.99106171, 2.12823289); # x = c(0.2575928, -0.3445442, 1.6590146, -1.1688806, 0.5587043); # df = data.frame(y, x); # fit <- lm(y ~ x, data=df); # sumfit = summary(fit); # coef = sumfit$coefficients; # mse = sumfit$sigma; # r2 = sumfit$r.squared; # r2adj = sumfit$adj.r.squared; # f = sumfit$fstatistic; # p = pf(f[1],f[2],f[3],lower.tail=F); # swapping the intercept and t.x; # weighted OLS; # the `s` key is stored before the `m` in java.util.HashMap; # NA != NA; # NA != NA; # stable sort; # noqa: PLR0124; # noqa: PLR0124; # noqa: PLR0124; # largest possible homozygote call; """"""+---------+; | col_idx |; +---------+; | int32 |; +---------+; | 0 |; | 1 |; | 2 |; +---------+; """"""; # Tests table.aggregate initOp; # Tests MatrixMapRows initOp; # Tests MatrixMapCols initOp; # must test that call_stats isn't null, because equality doesn't test for that; # https://cran.r-project.org/web/packages/samplesizeCMH/vignettes/samplesizeCMH-introduction.html; # https://www.biostathandbook.com/cmh.html; # Test that it's evalable, since python sets aren't hashable.; # Test that it's evalable, since python dicts aren't hashable.; # need to test laziness, so we will overwrite a file; # assumes cdf was computed from a (possibly shuffled) range table; # test NA; # test If; # test StreamIota; # test ToArray; # test ToStream; # test StreamZip; # test StreamFilter; # test StreamFilter; # test StreamFold; # test StreamScan; # test StreamAgg; # test AggExplode; # test TableCount; # test TableGetGlobals; # test TableCollect; # test TableAggregate; # test MatrixCount; # test MatrixAggregate",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
Security,hash,hashable,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.; # NB: See note in test_collect_dict_value_list.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
Testability,assert,assert,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.; # NB: See note in test_collect_dict_value_list.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
Testability,assert,assert,"# partial indexing; # ellipses inclusion; # np.newaxis inclusion; # ellipses inclusion; # out of bounds on start; # out of bounds on stop; # partial indexing; # ellipses inclusion; # ellipses inclusion; # Testing correct interpretation of numpy strides; # Testing from hail arrays; # Testing from nested hail arrays; # Testing missing data; # NDArrays don't correctly support elements that contain pointers at the moment.; # s = hl.nd.array([""hail"", ""is"", ""great""]); # s_lens = s.map(lambda e: hl.len(e)); # assert np.array_equal(hl.eval(s_lens), np.array([4, 2, 5])); # with lists/numerics; # Broadcasting; # Subtraction; # Broadcasting; # Multiplication; # Broadcasting; # Floor div; # Broadcasting; # Division; # Broadcasting; # Missingness tests; # Can't ask for the rank of something that has a 0 in its shape.; # Can't ask for the rank of something that has a 0 in its shape.; # Can't ask for the rank of something that has a 0 in its shape.; # check shapes; # Singular values match; # U is orthonormal; # V is orthonormal; # Multiplying together gets back to original; # check shapes; # eigvals match; # V is orthonormal; # V is eigenvectors; # issue #14559",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_ndarrays.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_ndarrays.py
Testability,test,test,"# NB: this test uses a file with more rows than partitions because Hadoop's Seekable input; # streams do not permit seeking past the end of the input (ref:; # https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/Seekable.html#seek-long-).; #; # Hail assumes that seeking past the end of the input does not raise an EOFException (see, for; # example `skip` in java.io.FileInputStream:; # https://docs.oracle.com/javase/7/docs/api/java/io/FileInputStream.html)",MatchSource.CODE_COMMENT,hail/python/test/hail/fs/test_worker_driver_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/fs/test_worker_driver_fs.py
Performance,load,loading,"# this test doesn't behave properly if these reference genomes are already defined in scope.; # loading different reference genome with same name should fail; # (different `test_rg_o` definition)",MatchSource.CODE_COMMENT,hail/python/test/hail/genetics/test_reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py
Testability,test,test,"# this test doesn't behave properly if these reference genomes are already defined in scope.; # loading different reference genome with same name should fail; # (different `test_rg_o` definition)",MatchSource.CODE_COMMENT,hail/python/test/hail/genetics/test_reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py
Availability,error,error,"# non-field expressions currently take a separate code path; # BlockMatrixMap requires very simple IRs on the SparkBackend. If I use; # `from_ndarray` here, it generates an `NDArrayRef` expression that it can't handle.; # Will be fixed by improving FoldConstants handling of ndarrays or fully lowering BlockMatrix.; # addition; # subtraction; # multiplication; # division; # other ops; # @fails_service_backend(); # @fails_local_backend(); # @pytest.mark.parametrize(; # 'nrows,ncols,block_size,split_size',; # [; # (nrows, ncols, block_size, split_size); # for (nrows, ncols) in [(50, 60), (60, 25)]; # for block_size in [7, 10]; # for split_size in [2, 9]; # ],; # ); # def test_tree_matmul_splits(block_size, split_size, nrows, ncols):; # # Variety of block sizes and splits; # ndarray = np.arange(nrows * ncols).reshape((nrows, ncols)); # bm = BlockMatrix.from_numpy(ndarray, block_size); # _assert_eq(bm.tree_matmul(bm.T, splits=split_size), ndarray @ ndarray.T); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # .assertTrue(bm.is_sparse); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # assert not bm.is_sparse; # assert bm.sparsify_triangle().is_sparse; # should run without error; # _svd; # left _svd_gramian; # right _svd_gramian; # left _svd_gramian when dimensions agree; # rank-deficient X sets negative eigenvalues to 0.0; # rank 1; # Summing vertically along a column vector to get a single value; # Summing horizontally along a row vector to create a single value; # Summing vertically along a row vector to make sure nothing changes; # Summing horizontally along a column vector to make sure nothing changes",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
Testability,assert,assertTrue,"# non-field expressions currently take a separate code path; # BlockMatrixMap requires very simple IRs on the SparkBackend. If I use; # `from_ndarray` here, it generates an `NDArrayRef` expression that it can't handle.; # Will be fixed by improving FoldConstants handling of ndarrays or fully lowering BlockMatrix.; # addition; # subtraction; # multiplication; # division; # other ops; # @fails_service_backend(); # @fails_local_backend(); # @pytest.mark.parametrize(; # 'nrows,ncols,block_size,split_size',; # [; # (nrows, ncols, block_size, split_size); # for (nrows, ncols) in [(50, 60), (60, 25)]; # for block_size in [7, 10]; # for split_size in [2, 9]; # ],; # ); # def test_tree_matmul_splits(block_size, split_size, nrows, ncols):; # # Variety of block sizes and splits; # ndarray = np.arange(nrows * ncols).reshape((nrows, ncols)); # bm = BlockMatrix.from_numpy(ndarray, block_size); # _assert_eq(bm.tree_matmul(bm.T, splits=split_size), ndarray @ ndarray.T); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # .assertTrue(bm.is_sparse); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # assert not bm.is_sparse; # assert bm.sparsify_triangle().is_sparse; # should run without error; # _svd; # left _svd_gramian; # right _svd_gramian; # left _svd_gramian when dimensions agree; # rank-deficient X sets negative eigenvalues to 0.0; # rank 1; # Summing vertically along a column vector to get a single value; # Summing horizontally along a row vector to create a single value; # Summing vertically along a row vector to make sure nothing changes; # Summing horizontally along a column vector to make sure nothing changes",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
Usability,simpl,simple,"# non-field expressions currently take a separate code path; # BlockMatrixMap requires very simple IRs on the SparkBackend. If I use; # `from_ndarray` here, it generates an `NDArrayRef` expression that it can't handle.; # Will be fixed by improving FoldConstants handling of ndarrays or fully lowering BlockMatrix.; # addition; # subtraction; # multiplication; # division; # other ops; # @fails_service_backend(); # @fails_local_backend(); # @pytest.mark.parametrize(; # 'nrows,ncols,block_size,split_size',; # [; # (nrows, ncols, block_size, split_size); # for (nrows, ncols) in [(50, 60), (60, 25)]; # for block_size in [7, 10]; # for split_size in [2, 9]; # ],; # ); # def test_tree_matmul_splits(block_size, split_size, nrows, ncols):; # # Variety of block sizes and splits; # ndarray = np.arange(nrows * ncols).reshape((nrows, ncols)); # bm = BlockMatrix.from_numpy(ndarray, block_size); # _assert_eq(bm.tree_matmul(bm.T, splits=split_size), ndarray @ ndarray.T); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # .assertTrue(bm.is_sparse); # FIXME doesn't work in service, if test_is_sparse works, uncomment below; # assert not bm.is_sparse; # assert bm.sparsify_triangle().is_sparse; # should run without error; # _svd; # left _svd_gramian; # right _svd_gramian; # left _svd_gramian when dimensions agree; # rank-deficient X sets negative eigenvalues to 0.0; # rank 1; # Summing vertically along a column vector to get a single value; # Summing horizontally along a row vector to create a single value; # Summing vertically along a row vector to make sure nothing changes; # Summing horizontally along a column vector to make sure nothing changes",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
Testability,test,tests,"# pytest sometimes uses background threads, named ""Dummy-1"", to collect tests. Asyncio dislikes; # automatically creating event loops in these threads, so we just explicitly create one.",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_file_formats.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_file_formats.py
Testability,test,tests,"# cannot aggregate cols when grouped by rows; # duplicate column field; # duplicate row field; # duplicate globals field; # duplicate column field; # duplicate row field; # duplicate globals field; # expression has to have global indices; # expression has to have global indices; # aggregation scope is rows only - entry field; # aggregation scope is rows only - column field; # cannot aggregate rows when grouped by cols; # duplicate row field; # duplicate column field; # duplicate globals field; # duplicate row field; # duplicate column field; # duplicate globals field; # expression has to have global indices; # expression has to have global indices; # aggregation scope is cols only - entry field; # aggregation scope is cols only - row field; # duplicate field; # duplicate field; # tests fixed indices; # tests fixed indices",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_grouped_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_grouped_matrix_table.py
Availability,error,error,"# should run without error; # test different row schemas; # triggered a RV bug; # MatrixAnnotateRowsTable uses left distinct join; # union_cols uses inner distinct join; # join on mt row key; # join on not mt row key; # join on interval of first field of mt row key; # join on mt row key; # join on not mt row key; # join on interval of first field of mt row key; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
Testability,test,test,"# should run without error; # test different row schemas; # triggered a RV bug; # MatrixAnnotateRowsTable uses left distinct join; # union_cols uses inner distinct join; # join on mt row key; # join on not mt row key; # join on interval of first field of mt row key; # join on mt row key; # join on not mt row key; # join on interval of first field of mt row key; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
Integrability,depend,depends,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""; """"""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""; # test annotations; # Make keys all null; # de_novo_finder doesn't know about y PAR",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
Testability,test,test,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""; """"""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""; # test annotations; # Make keys all null; # de_novo_finder doesn't know about y PAR",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
Availability,echo,echo,"<int32>>'.; """"""; """"""VCF does not support the type(s) for the following FORMAT field(s):; \t'boolean': 'bool'.; \t'arr_arr_i32': 'array<array<int32>>'.; """"""; # we've selected exactly the middle partition 1 position on either end; # make a combiner table; # permute columns so not in alphabetical order!; # this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8; # using totally random values leads rounding differences where; # identical GEN values get rounded differently, leading to; # differences in the GT call between import_{gen, bgen}; # 20% missing; # Note: the skip_invalid_loci.bgen has 16-bit probabilities, and Hail; # will crash if the genotypes are decoded; # Duplicated variant; # forcing seek to be called; # forcing each variant to be its own partition for testing duplicates wor",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Energy Efficiency,adapt,adapted,ream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); ,MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Integrability,rout,routine,"""""""Test that floating-point info fields are 64-bit regardless of the entry float type""""""; # issue 3277; # are py4 JavaMaps, not dicts, so can't use assertDictEqual; """"""VCF does not support the type(s) for the following INFO field(s):; \t'arr_bool': 'array<bool>'.; \t'arr_arr_i32': 'array<array<int32>>'.; """"""; """"""VCF does not support the type(s) for the following FORMAT field(s):; \t'boolean': 'bool'.; \t'arr_arr_i32': 'array<array<int32>>'.; """"""; # we've selected exactly the middle partition 1 position on either end; # make a combiner table; # permute columns so not in alphabetical order!; # this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8; # using totally random values leads rounding differences where; # identical GEN values get rounded differently, leading to; # differences in",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Modifiability,adapt,adapted,ream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); ,MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Performance,concurren,concurrent,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Testability,assert,assertDictEqual,"""""""Test that floating-point info fields are 64-bit regardless of the entry float type""""""; # issue 3277; # are py4 JavaMaps, not dicts, so can't use assertDictEqual; """"""VCF does not support the type(s) for the following INFO field(s):; \t'arr_bool': 'array<bool>'.; \t'arr_arr_i32': 'array<array<int32>>'.; """"""; """"""VCF does not support the type(s) for the following FORMAT field(s):; \t'boolean': 'bool'.; \t'arr_arr_i32': 'array<array<int32>>'.; """"""; # we've selected exactly the middle partition 1 position on either end; # make a combiner table; # permute columns so not in alphabetical order!; # this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8; # using totally random values leads rounding differences where; # identical GEN values get rounded differently, leading to; # differences in",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
Safety,safe,safe,"# prefer to remove nodes with higher index; # has multiallelics; # approximate, 1 place is safe; # approximate, 1 place is safe; # approximate, 1 place is safe",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py
Testability,test,test,"=c(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0)); # weights <- c(1,1,1,1); # null_model <- SKAT_Null_Model(cov$pheno ~ cov$cov1, out_type=""D"", Adjustment=FALSE); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); # cat(result$p.value, result$Q); # library('SKAT'); # dat <- matrix(c(2,1,0,1,; # 1,0,2,0,; # 1,1,0,0,; # 1,1,0,1,; # 0,1,2,1,; # 1,2,1,1,; # 1,0,1,1,; # 2,2,2,1,; # 1,1,2,1,; # 1,1,1,1,; # 2,0,1,1,; # 1,1,1,2,; # 0,1,0,2,; # 0,0,1,2,; # 1,0,1,0),; # 15,; # 4,; # byrow=TRUE); # cov <- data.frame(pheno=c(0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0),; # cov1=c(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0)); # # !!NOTA BENE!! In R, the ""weights"" parameter is actually the square root of the weights; # weights <- c(1,sqrt(2),1,1); # null_model <- SKAT_Null_Model(cov$pheno ~ cov$cov1, out_type=""D"", Adjustment=FALSE); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); # cat(result$p.value, result$Q); # dat <- as.matrix(read.csv('hail/src/test/resources/skat_genotype_matrix.csv', header=FALSE)); # cov = read.csv('hail/src/test/resources/skat_phenotypes.csv', header=FALSE); # cov$V1 = cov$V1 > 2; # weights <- rep(1, 100); # null_model <- SKAT_Null_Model(cov$V1 ~ 1, out_type=""D""); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); #; # cat(result$p.value, result$Q); #; #; # SKAT expects rows to be samples, so we transpose from the original input; # library('SKAT'); # dat <- matrix(c(0,1,0,1,; # 1,0,1,0,; # 0,0,2,0,; # 0,0,0,2,; # 0,0,2,1),; # 5,; # 4,; # byrow=TRUE); # cov <- data.frame(pheno=c(3., 4., 6., 4., 1.), cov1=c(1.,3.,0.,6.,1.), cov2=c(2.,4.,9.,1.,1.)); # weights <- c(1,1,1,1); # null_model <- SKAT_Null_Model(cov$pheno ~ cov$cov1+ cov$cov2, out_type=""D"", Adjustment=FALSE); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); # cat(result$p.value, result$Q); # library('SKAT'); # dat <- matrix(c(0,1,0,1,; # 1,0,1,0,; # 0,0,2,0,; # 0,0,0,2,; # 0,0,2,1),; # 5,; # 4,; # byrow=TRUE); # cov <- da",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
Testability,test,test,"# Outside of Spark backend, ""linear_regression_rows"" just defers to the underscore nd version.; # single group; # chained; # check types; # should run successfully with key fields; # complex expression; # test that chained linear regression can replicate separate calls with different missingness; # test differential missingness against each other; # comparing to R:; # y = c(1, 1, 2, 2, 2, 2); # x = c(0, 1, 0, 0, 0, 1); # c1 = c(0, 2, 1, -2, -2, 4); # c2 = c(-1, 3, 5, 0, -4, 3); # df = data.frame(y, x, c1, c2); # fit <- lm(y ~ x + c1 + c2, data=df); # summary(fit)[""coefficients""]; """"""Test that linear regressions on data converted from dosage to genotype returns the same results""""""; # null model is a global; # Outside the spark backend, ""logistic_regression_rows"" automatically defers to the _ version.; # Check that preweighted 1 and preweighted 2 match up with fields 1 and 2 of multiple; # comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]; # separable; # separable; # TODO test handling of missingness; # separable; # separable; # comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]; # separable; # comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, ",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
Availability,avail,available,"; """"""+-------+--------------+--------------------------------+------------+; | idx | x1 | x2 | x3 |; +-------+--------------+--------------------------------+------------+; | int32 | array<int32> | array<struct{y: array<int32>}> | set<int32> |; +-------+--------------+--------------------------------+------------+; | 0 | [1] | [([1])] | {1} |; +-------+--------------+--------------------------------+------------+. +------------------+-------------------------------+---------+------------+; | x4 | x5 | x6 | x7 |; +------------------+-------------------------------+---------+------------+; | dict<int32, str> | dict<struct{foo: int32}, str> | tuple() | tuple(str) |; +------------------+-------------------------------+---------+------------+; | {1:""foo""} | {(5):""bar""} | () | (""3"") |; +------------------+-------------------------------+---------+------------+. +-------------------+----------+---------------------+-------------------+; | x8 | x9 | x10 | x11 |; +-------------------+----------+---------------------+-------------------+; | tuple(str, int32) | float64 | dict<str, int32> | tuple(bool, bool) |; +-------------------+----------+---------------------+-------------------+; | (""3"",3) | 4.20e+00 | {""bar"":5,""hello"":3} | (True,False) |; +-------------------+----------+---------------------+-------------------+; """"""; # Testing after a filter; # Test that names other than ""grouped_fields"" work; # Just testing import without segault; # tests left join right distinct requiredness; # one key => one partition; # add a map node; # NOTE: we cannot use Hail during test parameter initialization; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # with sufficient available cores should take <=60s; # NB: range table is [0, ..., n - 1]; # https://github.com/hail-is/hail/issues/14506",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
Performance,optimiz,optimization,"# test map side combine and shuffle aggregation; # test sorted aggregation; # select no fields; # Pandas treats nan as missing. We don't.; # ht = [0, 1, 2], [3, 4, 5], ..., [21, 22]; # prevent count optimization; # write ensures that table is written with both key fields; """"""+-------+--------------+--------------------------------+------------+; | idx | x1 | x2 | x3 |; +-------+--------------+--------------------------------+------------+; | int32 | array<int32> | array<struct{y: array<int32>}> | set<int32> |; +-------+--------------+--------------------------------+------------+; | 0 | [1] | [([1])] | {1} |; +-------+--------------+--------------------------------+------------+. +------------------+-------------------------------+---------+------------+; | x4 | x5 | x6 | x7 |; +------------------+-------------------------------+---------+------------+; | dict<int32, str> | dict<struct{foo: int32}, str> | tuple() | tuple(str) |; +------------------+-------------------------------+---------+------------+; | {1:""foo""} | {(5):""bar""} | () | (""3"") |; +------------------+-------------------------------+---------+------------+. +-------------------+----------+---------------------+-------------------+; | x8 | x9 | x10 | x11 |; +-------------------+----------+---------------------+-------------------+; | tuple(str, int32) | float64 | dict<str, int32> | tuple(bool, bool) |; +-------------------+----------+---------------------+-------------------+; | (""3"",3) | 4.20e+00 | {""bar"":5,""hello"":3} | (True,False) |; +-------------------+----------+---------------------+-------------------+; """"""; # Testing after a filter; # Test that names other than ""grouped_fields"" work; # Just testing import without segault; # tests left join right distinct requiredness; # one key => one partition; # add a map node; # NOTE: we cannot use Hail during test parameter initialization; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with ",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
Testability,test,test,"# test map side combine and shuffle aggregation; # test sorted aggregation; # select no fields; # Pandas treats nan as missing. We don't.; # ht = [0, 1, 2], [3, 4, 5], ..., [21, 22]; # prevent count optimization; # write ensures that table is written with both key fields; """"""+-------+--------------+--------------------------------+------------+; | idx | x1 | x2 | x3 |; +-------+--------------+--------------------------------+------------+; | int32 | array<int32> | array<struct{y: array<int32>}> | set<int32> |; +-------+--------------+--------------------------------+------------+; | 0 | [1] | [([1])] | {1} |; +-------+--------------+--------------------------------+------------+. +------------------+-------------------------------+---------+------------+; | x4 | x5 | x6 | x7 |; +------------------+-------------------------------+---------+------------+; | dict<int32, str> | dict<struct{foo: int32}, str> | tuple() | tuple(str) |; +------------------+-------------------------------+---------+------------+; | {1:""foo""} | {(5):""bar""} | () | (""3"") |; +------------------+-------------------------------+---------+------------+. +-------------------+----------+---------------------+-------------------+; | x8 | x9 | x10 | x11 |; +-------------------+----------+---------------------+-------------------+; | tuple(str, int32) | float64 | dict<str, int32> | tuple(bool, bool) |; +-------------------+----------+---------------------+-------------------+; | (""3"",3) | 4.20e+00 | {""bar"":5,""hello"":3} | (True,False) |; +-------------------+----------+---------------------+-------------------+; """"""; # Testing after a filter; # Test that names other than ""grouped_fields"" work; # Just testing import without segault; # tests left join right distinct requiredness; # one key => one partition; # add a map node; # NOTE: we cannot use Hail during test parameter initialization; # test with no consumer randomness; # test with no consumer randomness; # test with no consumer randomness; # test with ",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
Availability,error,error,"# ensure that f1 and f2 both run with correct arguments; # check nullable; # check integral; # check numeric; # check strlike; # error because it should be typecheck_method",MatchSource.CODE_COMMENT,hail/python/test/hail/typecheck/test_typecheck.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/typecheck/test_typecheck.py
Testability,test,test,"# The gzip format permits metadata which makes the compressed file's size unpredictable. In; # practice, Hadoop creates a 175 byte file and gzip.GzipFile creates a 202 byte file. The 27; # extra bytes appear to include at least the filename (20 bytes) and a modification timestamp.; # subdir1subdir4_empty: in cloud fses, empty dirs do not exist and thus are not dirs; # subdir4_empty: in cloud fses, empty dirs do not exist and thus are not dirs; # subdir1subdir2: will exist in cloud, but not local, so do not test for it; # subdir1subdir2: will exist in cloud, but not local, so do not test for it",MatchSource.CODE_COMMENT,hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py
Testability,assert,assertions,"# see https://github.com/hail-is/hail/issues/13367 for why these assertions are here; # see https://github.com/hail-is/hail/issues/14564 for why this assertion is here",MatchSource.CODE_COMMENT,hail/python/test/hail/vds/test_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_combiner.py
Testability,assert,assert,"# the below fails because phasing uses the sum of j and k for its second allele.; # we cannot represent this allele index in 28 bits; # c2 = hl.call(1, 1, phased=True); # assert hl.eval(hl.vds.lgt_to_gt(c2, [0, 17495])) == hl.Call([17495, 17495], phased=True)",MatchSource.CODE_COMMENT,hail/python/test/hail/vds/test_vds_functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_vds_functions.py
Performance,perform,perform,"# type: ignore; # This is a white-box test. compose has a maximum of 32 inputs,; # so if we're composing more than 32 parts, the; # GoogleStorageAsyncFS does a multi-level hierarhical merge.; # > 32 so we perform at least 2 levels of merging; # do in parallel",MatchSource.CODE_COMMENT,hail/python/test/hailtop/test_aiogoogle.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py
Testability,test,test,"# type: ignore; # This is a white-box test. compose has a maximum of 32 inputs,; # so if we're composing more than 32 parts, the; # GoogleStorageAsyncFS does a multi-level hierarhical merge.; # > 32 so we perform at least 2 levels of merging; # do in parallel",MatchSource.CODE_COMMENT,hail/python/test/hailtop/test_aiogoogle.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py
Testability,test,tests,"# create a unique URL for each split of the tests",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/conftest.py
Availability,error,error,"# type: ignore; # too slow; # assert b.run().status()['state'] == 'success'; # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; # buckets do not exist (bucket names can't contain the string ""google"" per; # https://cloud.google.com/storage/docs/buckets); # bucket exists, but account does not have permissions on it; # bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data); # bucket exists and account has permissions, but is set to use cold storage by default; # no configuration, nonexistent buckets error; # no configuration, public access bucket doesn't error unless the object doesn't exist; # no configuration, no perms bucket errors; # no configuration, cold bucket errors; # hailctl config, allowlisted nonexistent buckets don't error; # environment variable config, only allowlisted nonexistent buckets don't error; # arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
Deployability,configurat,configuration,"# type: ignore; # too slow; # assert b.run().status()['state'] == 'success'; # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; # buckets do not exist (bucket names can't contain the string ""google"" per; # https://cloud.google.com/storage/docs/buckets); # bucket exists, but account does not have permissions on it; # bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data); # bucket exists and account has permissions, but is set to use cold storage by default; # no configuration, nonexistent buckets error; # no configuration, public access bucket doesn't error unless the object doesn't exist; # no configuration, no perms bucket errors; # no configuration, cold bucket errors; # hailctl config, allowlisted nonexistent buckets don't error; # environment variable config, only allowlisted nonexistent buckets don't error; # arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
Modifiability,config,configuration,"# type: ignore; # too slow; # assert b.run().status()['state'] == 'success'; # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; # buckets do not exist (bucket names can't contain the string ""google"" per; # https://cloud.google.com/storage/docs/buckets); # bucket exists, but account does not have permissions on it; # bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data); # bucket exists and account has permissions, but is set to use cold storage by default; # no configuration, nonexistent buckets error; # no configuration, public access bucket doesn't error unless the object doesn't exist; # no configuration, no perms bucket errors; # no configuration, cold bucket errors; # hailctl config, allowlisted nonexistent buckets don't error; # environment variable config, only allowlisted nonexistent buckets don't error; # arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
Security,access,access,"# type: ignore; # too slow; # assert b.run().status()['state'] == 'success'; # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; # buckets do not exist (bucket names can't contain the string ""google"" per; # https://cloud.google.com/storage/docs/buckets); # bucket exists, but account does not have permissions on it; # bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data); # bucket exists and account has permissions, but is set to use cold storage by default; # no configuration, nonexistent buckets error; # no configuration, public access bucket doesn't error unless the object doesn't exist; # no configuration, no perms bucket errors; # no configuration, cold bucket errors; # hailctl config, allowlisted nonexistent buckets don't error; # environment variable config, only allowlisted nonexistent buckets don't error; # arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
Testability,assert,assert,"# type: ignore; # too slow; # assert b.run().status()['state'] == 'success'; # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; # buckets do not exist (bucket names can't contain the string ""google"" per; # https://cloud.google.com/storage/docs/buckets); # bucket exists, but account does not have permissions on it; # bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data); # bucket exists and account has permissions, but is set to use cold storage by default; # no configuration, nonexistent buckets error; # no configuration, public access bucket doesn't error unless the object doesn't exist; # no configuration, no perms bucket errors; # no configuration, cold bucket errors; # hailctl config, allowlisted nonexistent buckets don't error; # environment variable config, only allowlisted nonexistent buckets don't error; # arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
Deployability,configurat,configuration,"""""""Fixture for gcloud configuration values.""""""; """"""Automatically replace gcloud functions with mocks.""""""; """"""Fixture for deploy.yaml values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
Modifiability,config,configuration,"""""""Fixture for gcloud configuration values.""""""; """"""Automatically replace gcloud functions with mocks.""""""; """"""Fixture for deploy.yaml values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
Testability,mock,mocks,"""""""Fixture for gcloud configuration values.""""""; """"""Automatically replace gcloud functions with mocks.""""""; """"""Fixture for deploy.yaml values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
Testability,mock,mock,"""""""Automatically mock subprocess module.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/test_connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/test_connect.py
Deployability,configurat,configuration,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""; # make sure dest_base exists",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
Integrability,depend,depends,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""; # make sure dest_base exists",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
Modifiability,config,configuration,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""; # make sure dest_base exists",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
Testability,test,test,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""; # make sure dest_base exists",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
Availability,error,error,"# This fixture is for test_copy_behavior. It runs a series of copy; # test ""specifications"" by calling run_test_spec. The set of; # specifications is enumerated by; # generate_copy_test_specs.py::copy_test_configurations which are then; # run against the local file system. This tests that (1) that copy; # runs without expected error for each enumerated spec, and that the; # semantics of each filesystem agree.; # make sure dest_base exists; # object stores can succeed or throw; # suppress exception; # mainly needs to be larger than the transfer block size (8K); # SourceCopier._copy_file creates destination directories as needed; # We ignore empty directories when copying",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_copy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py
Testability,test,test,"# This fixture is for test_copy_behavior. It runs a series of copy; # test ""specifications"" by calling run_test_spec. The set of; # specifications is enumerated by; # generate_copy_test_specs.py::copy_test_configurations which are then; # run against the local file system. This tests that (1) that copy; # runs without expected error for each enumerated spec, and that the; # semantics of each filesystem agree.; # make sure dest_base exists; # object stores can succeed or throw; # suppress exception; # mainly needs to be larger than the transfer block size (8K); # SourceCopier._copy_file creates destination directories as needed; # We ignore empty directories when copying",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_copy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py
Testability,test,test,"# end is inclusive; # doesn't exist yet; # mkdir with trailing slash; # can't test this until after creating foo; # mkdir without trailing slash; # can't test this until after creating foo; # subdir1subdir4_empty: in cloud fses, empty dirs do not exist and thus are not dirs; # subdir4_empty: in cloud fses, empty dirs do not exist and thus are not dirs; # statfile raises FileNotFound on directories; # create the following directory structure in base:; # foobar; # foo/a; # foo/b/c; # without trailing slash; # test FileListEntry.status raises on directory; # S3 has a minimum part size (except for the last part) of 5MiB; # do it in a fixed order; # do in parallel; # ensure the bucket is non-empty",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py
Availability,down,down,"#!/usr/bin/env python3; # https://github.com/pytest-dev/pytest/discussions/2039; # dont reformat; # If the benchmarks fail, we always want this file to exist otherwise; # later combine jobs will fail when localising.; # pytest keeps 3 test sessions worth of temp files by default.; # some benchmarks generate very large output files which can quickly; # fill the tmpfs and so we'll make pytest always delete tmp files; # immediately when tmp_path fixtures are torn-down.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
Testability,benchmark,benchmarks,"#!/usr/bin/env python3; # https://github.com/pytest-dev/pytest/discussions/2039; # dont reformat; # If the benchmarks fail, we always want this file to exist otherwise; # later combine jobs will fail when localising.; # pytest keeps 3 test sessions worth of temp files by default.; # some benchmarks generate very large output files which can quickly; # fill the tmpfs and so we'll make pytest always delete tmp files; # immediately when tmp_path fixtures are torn-down.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,monitoring/monitoring/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,monitoring/monitoring/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py
Availability,avail,available,"# gear, hailtop, and web_common are not available in the create_certs image; # this whole extfile nonsense is because OpenSSL has known, unfixed bugs; # in the x509 command. These really ought to be in the CSR.; # https://www.openssl.org/docs/man1.1.0/man1/x509.html#BUGS; # https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate; # pylint: disable=unused-argument; # FIXME: mTLS, only trust certain principals; # FIXME: mTLS; # http.write('ssl_verify_client on;\n')",MatchSource.CODE_COMMENT,tls/create_certs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py
Security,secur,security,"# gear, hailtop, and web_common are not available in the create_certs image; # this whole extfile nonsense is because OpenSSL has known, unfixed bugs; # in the x509 command. These really ought to be in the CSR.; # https://www.openssl.org/docs/man1.1.0/man1/x509.html#BUGS; # https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate; # pylint: disable=unused-argument; # FIXME: mTLS, only trust certain principals; # FIXME: mTLS; # http.write('ssl_verify_client on;\n')",MatchSource.CODE_COMMENT,tls/create_certs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py
Availability,down,download,"# type: ignore; # Chrome fails to download the tutorials.tar.gz file without the Content-Type header.; # so internal.hail.is/<namespace>/www can work without a /; # 2592000s = 30d",MatchSource.CODE_COMMENT,website/website/website.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/website.py
Modifiability,config,configure,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,website/website/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py
Testability,log,logging,"# configure logging before importing anything else; # noqa: E402 pylint: disable=wrong-import-position",MatchSource.CODE_COMMENT,website/website/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py
