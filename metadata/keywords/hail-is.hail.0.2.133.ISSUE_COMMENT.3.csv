id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:503,Deployability,pipeline,pipeline,503,"@catoverdrive Here's the output with docker commands:. ```bash; #!/bin/bash. # change cd to tmp directory; cd /tmp//pipeline.S9YTZap5/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1154,Deployability,pipeline,pipeline,1154," __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1179,Deployability,pipeline,pipeline,1179,"gold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevq",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1207,Deployability,pipeline,pipeline,1207,"Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLx",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1437,Deployability,pipeline,pipeline,1437," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1462,Deployability,pipeline,pipeline,1462," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1490,Deployability,pipeline,pipeline,1490," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1720,Deployability,pipeline,pipeline,1720," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1745,Deployability,pipeline,pipeline,1745," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1773,Deployability,pipeline,pipeline,1773," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:2001,Deployability,pipeline,pipeline,2001," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:2026,Deployability,pipeline,pipeline,2026," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:2054,Deployability,pipeline,pipeline,2054," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938
https://github.com/hail-is/hail/issues/4941#issuecomment-446173200:83,Testability,log,logreg,83,"I can see a fix using _annotate_all and then a select directly before entering the logreg, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4941#issuecomment-446173200
https://github.com/hail-is/hail/pull/4944#issuecomment-446447752:4,Testability,test,testing,4,not testing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4944#issuecomment-446447752
https://github.com/hail-is/hail/pull/4944#issuecomment-446448431:21,Testability,test,testing,21,@tpoterba was it not testing before? Looks like build 37 is running now. It's gonna test everything because a file was modified in the root of the repository.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4944#issuecomment-446448431
https://github.com/hail-is/hail/pull/4944#issuecomment-446448431:84,Testability,test,test,84,@tpoterba was it not testing before? Looks like build 37 is running now. It's gonna test everything because a file was modified in the root of the repository.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4944#issuecomment-446448431
https://github.com/hail-is/hail/pull/4947#issuecomment-446738202:32,Integrability,protocol,protocol,32,"Hey Alex, this is great. The PR protocol is for you to assign the PR to a random developer by using the random user generated by scorecard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947#issuecomment-446738202
https://github.com/hail-is/hail/pull/4947#issuecomment-449080254:34,Integrability,protocol,protocol,34,"> Hey Alex, this is great. The PR protocol is for you to assign the PR to a random developer by using the random user generated by scorecard. Thanks Cotton. I also need to make sure desktop notifications are enabled, across GitHub and Zulip. Apologies for not noticing this earlier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4947#issuecomment-449080254
https://github.com/hail-is/hail/issues/4948#issuecomment-446587917:103,Availability,error,error,103,"I have a thing today until 12:30, someone else should look at it. Quick fix: revert changes. 404 is an error anyway",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4948#issuecomment-446587917
https://github.com/hail-is/hail/pull/4965#issuecomment-447034807:137,Availability,failure,failures,137,"@tpoterba I also added a `hail-ci-build.sh` so we'll build the images in the PRs, ensuring we don't get deploy problems from image build failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807
https://github.com/hail-is/hail/pull/4965#issuecomment-447034807:104,Deployability,deploy,deploy,104,"@tpoterba I also added a `hail-ci-build.sh` so we'll build the images in the PRs, ensuring we don't get deploy problems from image build failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807
https://github.com/hail-is/hail/pull/4971#issuecomment-448317746:127,Availability,ping,pings,127,"@tpoterba @jbloom22 I've reviewed this PR but haven't approved it because I was waiting for the other one to go in. If someone pings me when it does, I'm happy to approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4971#issuecomment-448317746
https://github.com/hail-is/hail/pull/4971#issuecomment-448789826:55,Integrability,depend,dependent,55,see the comment - Arcturus approved but was waiting on dependent PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4971#issuecomment-448789826
https://github.com/hail-is/hail/pull/4974#issuecomment-447373995:40,Deployability,deploy,deploy,40,"> I tested this with a pile of hacks to deploy this into an anonymous namespace in vdc. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of vdc/ and gateway/ to be more modular. Oh, I read that as you weren't ready to merge this. It sounds like you aren't ready to PR the testing stuff?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995
https://github.com/hail-is/hail/pull/4974#issuecomment-447373995:4,Testability,test,tested,4,"> I tested this with a pile of hacks to deploy this into an anonymous namespace in vdc. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of vdc/ and gateway/ to be more modular. Oh, I read that as you weren't ready to merge this. It sounds like you aren't ready to PR the testing stuff?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995
https://github.com/hail-is/hail/pull/4974#issuecomment-447373995:384,Testability,test,testing,384,"> I tested this with a pile of hacks to deploy this into an anonymous namespace in vdc. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of vdc/ and gateway/ to be more modular. Oh, I read that as you weren't ready to merge this. It sounds like you aren't ready to PR the testing stuff?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995
https://github.com/hail-is/hail/issues/4984#issuecomment-448465860:22,Testability,log,log,22,Batch hung again. [ci.log](https://github.com/hail-is/hail/files/2693248/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693249/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448465860
https://github.com/hail-is/hail/issues/4984#issuecomment-448465860:76,Testability,log,log,76,Batch hung again. [ci.log](https://github.com/hail-is/hail/files/2693248/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693249/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448465860
https://github.com/hail-is/hail/issues/4984#issuecomment-448465860:89,Testability,log,log,89,Batch hung again. [ci.log](https://github.com/hail-is/hail/files/2693248/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693249/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448465860
https://github.com/hail-is/hail/issues/4984#issuecomment-448465860:146,Testability,log,log,146,Batch hung again. [ci.log](https://github.com/hail-is/hail/files/2693248/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693249/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448465860
https://github.com/hail-is/hail/issues/4984#issuecomment-448468337:16,Testability,log,log,16,and again?. [ci.log](https://github.com/hail-is/hail/files/2693282/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693283/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448468337
https://github.com/hail-is/hail/issues/4984#issuecomment-448468337:70,Testability,log,log,70,and again?. [ci.log](https://github.com/hail-is/hail/files/2693282/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693283/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448468337
https://github.com/hail-is/hail/issues/4984#issuecomment-448468337:83,Testability,log,log,83,and again?. [ci.log](https://github.com/hail-is/hail/files/2693282/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693283/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448468337
https://github.com/hail-is/hail/issues/4984#issuecomment-448468337:140,Testability,log,log,140,and again?. [ci.log](https://github.com/hail-is/hail/files/2693282/ci.log); [batch.log](https://github.com/hail-is/hail/files/2693283/batch.log),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-448468337
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:31,Availability,error,errors,31,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:154,Availability,error,errorNum,154,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1449,Availability,error,error,1449,"ionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2097,Availability,error,errors,2097,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2123,Availability,ERROR,ERROR,2123,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2470,Availability,ERROR,ERROR,2470,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2792,Availability,Error,Error,2792,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2917,Availability,Error,Error,2917,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2030,Energy Efficiency,Reduce,Reduce,2030,"._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to cat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1170,Integrability,depend,depending,1170,"b.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1054,Safety,timeout,timeout,1054,"except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:297,Testability,log,log,297,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2109,Testability,log,log,2109,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1094,Usability,simpl,simplest,1094,"b.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1569,Usability,simpl,simply,1569,"ionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
https://github.com/hail-is/hail/issues/4984#issuecomment-450915405:239,Availability,failure,failure,239,"When the kube event loop fails, it should be restarted by `run_forever`. Understanding why we lose connection to k8s seems fruitful and important, but does not explain why batch becomes unstable / non-communicative after a kube event loop failure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450915405
https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:78,Availability,error,errors,78,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690
https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:564,Energy Efficiency,green,green-thread,564,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690
https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:407,Safety,timeout,timeout,407,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690
https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:179,Testability,Test,Tests,179,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690
https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:652,Testability,test,test,652,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690
https://github.com/hail-is/hail/pull/4987#issuecomment-447929316:49,Availability,down,down,49,"I'm sure this is fine, but I would like to track down all places where .init() (or its side effects) are called, since otherwise I would be approving something I didn't fully understand; we could add these to the note. Currently having some spark version mismatch that I'm working through.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-447929316
https://github.com/hail-is/hail/pull/4987#issuecomment-448319952:13,Usability,clear,clear,13,"This is more clear to me, thanks Tim! Would it be worth explaining why it's called as `hail.init()` vs say `hail.context.init()` or `HailContext()`? Knowing the chain of custody makes this indirection feel less magical to me. ```python; // Instantiates the HailContext class, unless called with dempotent == True; // Calls def __init__ ; // Imported in hail/__init__.py for use as hail.init() ; ```. Also, wondering if it makes sense to uses/benefits of treating as singleton, though that may be for a different pr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-448319952
https://github.com/hail-is/hail/pull/4987#issuecomment-448322113:187,Security,expose,exposed,187,`HailContext()` was an 0.1 phenomenon. At some point (when we've fully moved things over to using IRs probably) we can totally remove it from Python. It's definitely not meant to be user-exposed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-448322113
https://github.com/hail-is/hail/pull/4994#issuecomment-448388700:115,Modifiability,config,configured,115,I *think* I've addressed all concerns. It would help if @danking could test all functions; I only have a partially configured environment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700
https://github.com/hail-is/hail/pull/4994#issuecomment-448388700:71,Testability,test,test,71,I *think* I've addressed all concerns. It would help if @danking could test all functions; I only have a partially configured environment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700
https://github.com/hail-is/hail/pull/4994#issuecomment-449272584:47,Usability,feedback,feedback,47,"Dan, should be resolved. Thanks for all of the feedback!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-449272584
https://github.com/hail-is/hail/pull/4995#issuecomment-448331406:38,Integrability,interface,interfaces,38,what do you think about keeping these interfaces private while we digest the change and how we want things to look in Python? That way we don't commit to anything right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4995#issuecomment-448331406
https://github.com/hail-is/hail/pull/4995#issuecomment-448336466:63,Security,expose,exposed,63,"Done. {Matrix}Table._schema is now private. ttable/tmatrix are exposed, but not documented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4995#issuecomment-448336466
https://github.com/hail-is/hail/pull/5004#issuecomment-448649659:6,Energy Efficiency,reduce,reduced,6,"which reduced the diff to 0, and GitHub automatically closed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5004#issuecomment-448649659
https://github.com/hail-is/hail/pull/5004#issuecomment-448650369:77,Security,hash,hash,77,"> wait, you force-pushed current master as this branch. Yep, I put the wrong hash in a rebase --onto and when it was the same as master, Github closed it and marked it as merged. Surprising! Fixed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5004#issuecomment-448650369
https://github.com/hail-is/hail/pull/5007#issuecomment-448473891:17,Deployability,deploy,deploy,17,this is breaking deploy so I'm gonna use admin override to merge it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5007#issuecomment-448473891
https://github.com/hail-is/hail/pull/5015#issuecomment-448764451:50,Performance,optimiz,optimization,50,"force count isn't the same thing as count without optimization -- force count will optimize, but with a top-level TableLiteral node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448764451
https://github.com/hail-is/hail/pull/5015#issuecomment-448764451:83,Performance,optimiz,optimize,83,"force count isn't the same thing as count without optimization -- force count will optimize, but with a top-level TableLiteral node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448764451
https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:124,Modifiability,rewrite,rewrite,124,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387
https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:26,Performance,optimiz,optimization,26,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387
https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:48,Performance,optimiz,optimizations,48,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387
https://github.com/hail-is/hail/pull/5015#issuecomment-448768620:157,Performance,optimiz,optimize,157,"> collectJSON. See #4971 . We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448768620
https://github.com/hail-is/hail/pull/5015#issuecomment-448839950:132,Performance,optimiz,optimize,132,"> We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do. I'm a bit dubious about this argument. I feel like we don't often sacrifice the right semantics for performance. What optimizations specifically are you concerned about here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448839950
https://github.com/hail-is/hail/pull/5015#issuecomment-448839950:266,Performance,perform,performance,266,"> We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do. I'm a bit dubious about this argument. I feel like we don't often sacrifice the right semantics for performance. What optimizations specifically are you concerned about here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448839950
https://github.com/hail-is/hail/pull/5015#issuecomment-448839950:284,Performance,optimiz,optimizations,284,"> We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do. I'm a bit dubious about this argument. I feel like we don't often sacrifice the right semantics for performance. What optimizations specifically are you concerned about here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448839950
https://github.com/hail-is/hail/pull/5015#issuecomment-448843503:187,Performance,optimiz,optimization,187,"For an aggregate, or for collect specifically? You can for aggregators that are commutative. Most are. Aggregate/collect just happens not to be. I personally think this is a mistake/over-optimization, and we should (1) focus on making shuffles fast, and (2) when you're collecting, you can just do the key_by sort locally at the end anyway, so it is super cheap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448843503
https://github.com/hail-is/hail/pull/5015#issuecomment-448843839:68,Modifiability,rewrite,rewrite,68,"I want to get rid of TableCount, too. I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448843839
https://github.com/hail-is/hail/pull/5015#issuecomment-448843839:86,Modifiability,rewrite,rewrites,86,"I want to get rid of TableCount, too. I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448843839
https://github.com/hail-is/hail/pull/5015#issuecomment-448844086:32,Modifiability,rewrite,rewrite,32,"> I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined. Ah, nice. Now that we can embed relational IRs inside value IRs inside relational IRs, this will be great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448844086
https://github.com/hail-is/hail/pull/5015#issuecomment-448844086:50,Modifiability,rewrite,rewrites,50,"> I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined. Ah, nice. Now that we can embed relational IRs inside value IRs inside relational IRs, this will be great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448844086
https://github.com/hail-is/hail/pull/5015#issuecomment-448848528:303,Usability,simpl,simplifies,303,"> Commutativity is a nice property for all aggregators to have. In particular, why add restrictive semantics when nobody is asking for them?. Commutativity isn't a property you can add. It either has it or is doesn't. And it doesn't. Same for take. ; ""adding"" it gets you collectAsSet. My proposal only simplifies. We have two semantics for collect, and I want only one. What do I need to do besides delete some key_by simplifier rules (if they are there)? Aggregators should be sequential anyway. > currently agg.collect is non-deterministic. It is? How's that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448848528
https://github.com/hail-is/hail/pull/5015#issuecomment-448848528:419,Usability,simpl,simplifier,419,"> Commutativity is a nice property for all aggregators to have. In particular, why add restrictive semantics when nobody is asking for them?. Commutativity isn't a property you can add. It either has it or is doesn't. And it doesn't. Same for take. ; ""adding"" it gets you collectAsSet. My proposal only simplifies. We have two semantics for collect, and I want only one. What do I need to do besides delete some key_by simplifier rules (if they are there)? Aggregators should be sequential anyway. > currently agg.collect is non-deterministic. It is? How's that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448848528
https://github.com/hail-is/hail/pull/5015#issuecomment-448851223:3,Integrability,depend,depends,3,It depends on task completion order. ☹️ . And I think you've convinced me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448851223
https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:178,Deployability,integrat,integration,178,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617
https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:178,Integrability,integrat,integration,178,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617
https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:95,Testability,test,testing,95,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617
https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:190,Testability,test,tests,190,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617
https://github.com/hail-is/hail/pull/5021#issuecomment-451248463:17,Integrability,rout,routines,17,"I also broke out routines to create examples of values (tables, matrix tables) with all types, and added a test case that runs expand_types on the table of values of all types.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5021#issuecomment-451248463
https://github.com/hail-is/hail/pull/5021#issuecomment-451248463:107,Testability,test,test,107,"I also broke out routines to create examples of values (tables, matrix tables) with all types, and added a test case that runs expand_types on the table of values of all types.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5021#issuecomment-451248463
https://github.com/hail-is/hail/pull/5022#issuecomment-449278737:194,Deployability,configurat,configuration,194,"> Hmm. Maybe the env-setup script should also set the default region/zone to us-central1-a?. Yep, that would remove the need for --zone or --region. We should probably document that part of the configuration, since its likely what prevented the get-credentials command from working for me as provided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737
https://github.com/hail-is/hail/pull/5022#issuecomment-449278737:194,Modifiability,config,configuration,194,"> Hmm. Maybe the env-setup script should also set the default region/zone to us-central1-a?. Yep, that would remove the need for --zone or --region. We should probably document that part of the configuration, since its likely what prevented the get-credentials command from working for me as provided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737
https://github.com/hail-is/hail/pull/5022#issuecomment-449439317:612,Availability,error,error,612,"> Noteworthy as well. Although I can't reproduce it consistently, sometimes conda may not like `-f environment.yml`, but will work perfectly well with `conda env create`. While I prefer `-f environment.yml` because it is more obvious, dropping that argument may be less fragile.; > ; > [conda/conda#3847](https://github.com/conda/conda/issues/3847); > ; > Edit: This was actually a typo in the documentation. Fixed in [3135dc1](https://github.com/hail-is/hail/commit/3135dc124c37ec6987d96f5de0b7dbb634487b7d). I find `conda`'s terminal UI supremely frustrating. That `conda env create -f does-not-exist` doesn't error with ""file not found"" blows my mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449439317
https://github.com/hail-is/hail/pull/5022#issuecomment-449440250:95,Deployability,release,release,95,🙏 https://github.com/conda/conda/pull/7385 🙏 ; They finally fixed it for the forthcoming 4.6.x release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449440250
https://github.com/hail-is/hail/pull/5027#issuecomment-451238849:10,Availability,failure,failures,10,"The final failures were doctests that called show on tables with duplicated keys, where the order of the duplicate rows, which we don't guarantee, were different. I disabled those tests. Also addressed comments, should be good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849
https://github.com/hail-is/hail/pull/5027#issuecomment-451238849:180,Testability,test,tests,180,"The final failures were doctests that called show on tables with duplicated keys, where the order of the duplicate rows, which we don't guarantee, were different. I disabled those tests. Also addressed comments, should be good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849
https://github.com/hail-is/hail/issues/5030#issuecomment-462493288:44,Availability,error,errors,44,"I fixed some of these with hacks that defer errors beyond the MatrixReader/TableReader constructors, but some remain (native reader, VCF, etc)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5030#issuecomment-462493288
https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:53,Deployability,pipeline,pipeline,53,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618
https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:122,Deployability,integrat,integration,122,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618
https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:122,Integrability,integrat,integration,122,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618
https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:134,Testability,test,tests,134,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618
https://github.com/hail-is/hail/pull/5033#issuecomment-449469191:14,Testability,test,test,14,Can we have a test that catches it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449469191
https://github.com/hail-is/hail/pull/5033#issuecomment-449469370:15,Testability,test,test,15,That's hard to test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449469370
https://github.com/hail-is/hail/pull/5033#issuecomment-449473700:74,Testability,test,test,74,"Exactly, this always works in local mode. We definitely need more ways to test behavior in the non-local environment, but I have no concrete ideas right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449473700
https://github.com/hail-is/hail/pull/5033#issuecomment-449479436:24,Testability,test,tests,24,You can write non-local tests by [submitting a python file](https://github.com/hail-is/hail/blob/master/hail/hail-ci-build.sh#L244-L245) to the cluster that is started by the CI.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449479436
https://github.com/hail-is/hail/pull/5033#issuecomment-449480073:39,Testability,test,tests,39,"We should really be running all of our tests on a cluster. We can run Spark in cluster mode on a single machine, that's probably what we should do",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449480073
https://github.com/hail-is/hail/pull/5033#issuecomment-449480646:124,Testability,test,tests,124,That still leaves open problems caused by true network communication between physically-distant cores. We could package our tests as a test JAR and submit that to the Spark cluster the CI starts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449480646
https://github.com/hail-is/hail/pull/5033#issuecomment-449480646:135,Testability,test,test,135,That still leaves open problems caused by true network communication between physically-distant cores. We could package our tests as a test JAR and submit that to the Spark cluster the CI starts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449480646
https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:55,Availability,avail,available,55,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579
https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:448,Deployability,configurat,configuration,448,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579
https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:448,Modifiability,config,configuration,448,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579
https://github.com/hail-is/hail/pull/5033#issuecomment-449491735:198,Testability,test,test,198,@tpoterba The only things that come to mind are shuffle issues and shared filesystem bugs. Regardless we already start a cluster and it's not hard to use that existing functionality run a non-local test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449491735
https://github.com/hail-is/hail/pull/5033#issuecomment-449492847:10,Testability,test,test,10,"putting a test in hail-ci-build seems like the wrong thing. We should just run all our tests against a cluster-mode spark, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449492847
https://github.com/hail-is/hail/pull/5033#issuecomment-449492847:87,Testability,test,tests,87,"putting a test in hail-ci-build seems like the wrong thing. We should just run all our tests against a cluster-mode spark, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449492847
https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:84,Availability,error,error,84,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275
https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:118,Modifiability,config,configure,118,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275
https://github.com/hail-is/hail/pull/5033#issuecomment-449506106:34,Testability,test,tests,34,"I also do not know how to run our tests in cluster-mode, but I know how to add a python file to this repo and submit it to the cluster in hail-ci-build.sh ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449506106
https://github.com/hail-is/hail/issues/5040#issuecomment-570313646:6,Testability,test,test,6,"I can test whether I'm finding natives for NDArray expressions, but that doesn't mean I know what natives netlib is using.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5040#issuecomment-570313646
https://github.com/hail-is/hail/issues/5040#issuecomment-570319184:97,Availability,error,error,97,Yeah I think this doesn't much matter once we've eliminated uses of netlib. Hail will get a link error instead of falling back on hopelessly slow non-native libraries.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5040#issuecomment-570319184
https://github.com/hail-is/hail/pull/5041#issuecomment-449743945:109,Availability,down,down,109,"Agh, this isn't good enough. It feels like the right thing might be to lift up the lets, optimize, then push down. This has the added benefit of making it trivial to collapse multiple identical let bindings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945
https://github.com/hail-is/hail/pull/5041#issuecomment-449743945:89,Performance,optimiz,optimize,89,"Agh, this isn't good enough. It feels like the right thing might be to lift up the lets, optimize, then push down. This has the added benefit of making it trivial to collapse multiple identical let bindings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945
https://github.com/hail-is/hail/pull/5041#issuecomment-449744021:94,Performance,optimiz,optimize,94,"For reference, here is the IR generated by `read_matrix_table().count_cols()` which I want to optimize to `I64(blah)`:. ```; (TableCount; (TableKeyBy (s) False; (TableParallelize None; (Let __cols_and_globals; (TableGetGlobals; (TableZipUnchecked; (TableMapGlobals; (TableMapGlobals; (TableRead ""/Users/tpoterba/data/profile.mt/rows"" True Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh37),alleles:Array[String]}}); (Literal Struct{} <literal value>)); (InsertFields; (Ref global); (__cols; (GetField rows; (TableCollect; (TableRead ""/Users/tpoterba/data/profile.mt/cols"" False Table{global:Struct{},key:[],row:Struct{s:String}})))))); (TableRead ""/Users/tpoterba/data/profile.mt/entries"" True Table{global:Struct{},key:[],row:Struct{`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{}]}}))); (MakeStruct; (rows; (ArrayMap elt; (ArraySort True; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref __cols_and_globals))); (I32 1)); (Let __cols_element; (ArrayRef; (GetField __cols; (Ref __cols_and_globals)); (Ref i)); (MakeStruct; (_1; (SelectFields (s); (Ref __cols_element))); (_2; (Ref __cols_element))))); (True)); (GetField _2; (Ref elt)))); (global; (SelectFields (); (Ref __cols_and_globals)))))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449744021
https://github.com/hail-is/hail/pull/5041#issuecomment-449744063:28,Availability,down,downstream,28,"(note this is coming from a downstream commit where I've written a lowerer for MatrixRead, that's what the `TableZipUnchecked` is for)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449744063
https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:200,Availability,down,down,200,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:164,Performance,optimiz,optimize,164,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:1704,Performance,optimiz,optimization,1704,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:600,Usability,simpl,simplifies,600,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:727,Usability,simpl,simplified,727,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:240,Availability,down,down,240,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790
https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:409,Modifiability,rewrite,rewrite,409,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790
https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:59,Performance,optimiz,optimization,59,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790
https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:647,Performance,optimiz,optimizer,647,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790
https://github.com/hail-is/hail/pull/5043#issuecomment-449876531:1568,Performance,optimiz,optimize,1568,"This isn't exactly meant to be merged as-is, but more to start a discussion. This set of changes (along with the PR before) turn the following IR:. ```; (TableCount; (MatrixColsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False True ""{\""name\"":\""MatrixNativeRead; ```. Into the lowered. ```; (TableCount; (TableKeyBy (s) False; (TableParallelize None; (Let __cols_and_globals; (TableGetGlobals; (TableZipUnchecked; (TableMapGlobals; (TableMapGlobals; (TableRead ""/Users/tpoterba/data/profile.mt/rows"" True Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh37),alleles:Array[String]}}); (Literal Struct{} <literal value>)); (InsertFields; (Ref global); (__cols; (GetField rows; (TableCollect; (TableRead ""/Users/tpoterba/data/profile.mt/cols"" False Table{global:Struct{},key:[],row:Struct{s:String}})))))); (TableRead ""/Users/tpoterba/data/profile.mt/entries"" True Table{global:Struct{},key:[],row:Struct{`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{}]}}))); (MakeStruct; (rows; (ArrayMap elt; (ArraySort True; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref __cols_and_globals))); (I32 1)); (Let __cols_element; (ArrayRef; (GetField __cols; (Ref __cols_and_globals)); (Ref i)); (MakeStruct; (_1; (SelectFields (s); (Ref __cols_element))); (_2; (Ref __cols_element))))); (True)); (GetField _2; (Ref elt)))); (global; (SelectFields (); (Ref __cols_and_globals)))))))); ```. And finally optimize this IR to:. ```; (I64 2535); ```. 😃",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5043#issuecomment-449876531
https://github.com/hail-is/hail/pull/5044#issuecomment-450278433:15,Testability,test,tested,15,"Haven't stress tested it yet. But just did a quick check and yes, this is exactly what I've wanted. So so so so good. As I mentioned to @tpoterba, the ability to write this out as an object (granted, not necessarily trival) would officially close the book on the @konradjk 0.2 wishlist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5044#issuecomment-450278433
https://github.com/hail-is/hail/pull/5044#issuecomment-452252677:64,Usability,simpl,simple,64,"also, a note on the comment - Konrad, Cotton, and I discussed a simple function ""write_expr"" in the experimental module that uses a dummy table and writes in globals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5044#issuecomment-452252677
https://github.com/hail-is/hail/pull/5063#issuecomment-451556932:41,Security,expose,exposes,41,I'll PR a fix to the artifacts page that exposes the docs directly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5063#issuecomment-451556932
https://github.com/hail-is/hail/pull/5065#issuecomment-451266277:255,Performance,load,loading,255,"Hmm, flake is failing on this; ```; + flake8 batch; batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; ```; But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451266277
https://github.com/hail-is/hail/pull/5065#issuecomment-451274130:273,Performance,load,loading,273,"> Hmm, flake is failing on this; > ; > ```; > + flake8 batch; > batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; > batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; > ```; > But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check. Strange. .serve is unmodified from master; run_once is only used in a test, could that be an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130
https://github.com/hail-is/hail/pull/5065#issuecomment-451274130:389,Testability,test,test,389,"> Hmm, flake is failing on this; > ; > ```; > + flake8 batch; > batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; > batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; > ```; > But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check. Strange. .serve is unmodified from master; run_once is only used in a test, could that be an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130
https://github.com/hail-is/hail/pull/5075#issuecomment-451720307:29,Testability,test,tests,29,"Thanks for the context! Unit tests pass, nice. A few warnings generated: `assertEquals` is deprecated in favor of `assertEqual` (I'm using pytest-4.0.2/python 3.7.1 may be new deprecation). Need to spend more time following the move of key from IR to table. Do the existing join unit tests cover the new join implementation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-451720307
https://github.com/hail-is/hail/pull/5075#issuecomment-451720307:74,Testability,assert,assertEquals,74,"Thanks for the context! Unit tests pass, nice. A few warnings generated: `assertEquals` is deprecated in favor of `assertEqual` (I'm using pytest-4.0.2/python 3.7.1 may be new deprecation). Need to spend more time following the move of key from IR to table. Do the existing join unit tests cover the new join implementation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-451720307
https://github.com/hail-is/hail/pull/5075#issuecomment-451720307:115,Testability,assert,assertEqual,115,"Thanks for the context! Unit tests pass, nice. A few warnings generated: `assertEquals` is deprecated in favor of `assertEqual` (I'm using pytest-4.0.2/python 3.7.1 may be new deprecation). Need to spend more time following the move of key from IR to table. Do the existing join unit tests cover the new join implementation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-451720307
https://github.com/hail-is/hail/pull/5075#issuecomment-451720307:284,Testability,test,tests,284,"Thanks for the context! Unit tests pass, nice. A few warnings generated: `assertEquals` is deprecated in favor of `assertEqual` (I'm using pytest-4.0.2/python 3.7.1 may be new deprecation). Need to spend more time following the move of key from IR to table. Do the existing join unit tests cover the new join implementation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-451720307
https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:285,Availability,error,error,285,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974
https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:423,Availability,error,error,423,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974
https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:280,Testability,test,test,280,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974
https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:295,Testability,test,test,295,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974
https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:432,Testability,test,test,432,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974
https://github.com/hail-is/hail/pull/5075#issuecomment-453376506:22,Performance,perform,performance,22,"I haven't checked for performance regressions. Tim, do you have a standard way of doing this? Given that you're on vacation tomorrow, @danking, could you help me understand your performance testing procedures?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453376506
https://github.com/hail-is/hail/pull/5075#issuecomment-453376506:178,Performance,perform,performance,178,"I haven't checked for performance regressions. Tim, do you have a standard way of doing this? Given that you're on vacation tomorrow, @danking, could you help me understand your performance testing procedures?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453376506
https://github.com/hail-is/hail/pull/5075#issuecomment-453376506:190,Testability,test,testing,190,"I haven't checked for performance regressions. Tim, do you have a standard way of doing this? Given that you're on vacation tomorrow, @danking, could you help me understand your performance testing procedures?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453376506
https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:76,Energy Efficiency,monitor,monitoring,76,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009
https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:40,Performance,perform,performance,40,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009
https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:157,Performance,perform,performance,157,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009
https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:138,Safety,risk,risks,138,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009
https://github.com/hail-is/hail/pull/5075#issuecomment-454044971:262,Performance,perform,performance,262,We have all of 1KG as vcfs in the `gs://hail-1kg/raw` bucket:; ```; (hail) 1 dking@wmb16-359 # gsutil du -sh gs://hail-1kg/raw/ALL.chr\*raw.HC.vcf.bgz; 260.94 GiB gs://hail-1kg/raw/ALL.chr*raw.HC.vcf.bgz; ```; Some subset of those chromosomes should satisfy any performance testing needs we have.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454044971
https://github.com/hail-is/hail/pull/5075#issuecomment-454044971:274,Testability,test,testing,274,We have all of 1KG as vcfs in the `gs://hail-1kg/raw` bucket:; ```; (hail) 1 dking@wmb16-359 # gsutil du -sh gs://hail-1kg/raw/ALL.chr\*raw.HC.vcf.bgz; 260.94 GiB gs://hail-1kg/raw/ALL.chr*raw.HC.vcf.bgz; ```; Some subset of those chromosomes should satisfy any performance testing needs we have.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454044971
https://github.com/hail-is/hail/pull/5075#issuecomment-454046705:10,Testability,benchmark,benchmarking,10,"we have a benchmarking suite in Python - it's very early, though, and not run as part of the CI. I'll add something there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454046705
https://github.com/hail-is/hail/pull/5075#issuecomment-454049236:46,Performance,perform,performance,46,"Thank you both. If you're comfortable without performance testing at this stage (since this is part of a broader move, and optimization seems like it is out of scope, as it is mostly now a function of the Table implementation), this PR is approved, once the commented-out old code block is removed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454049236
https://github.com/hail-is/hail/pull/5075#issuecomment-454049236:123,Performance,optimiz,optimization,123,"Thank you both. If you're comfortable without performance testing at this stage (since this is part of a broader move, and optimization seems like it is out of scope, as it is mostly now a function of the Table implementation), this PR is approved, once the commented-out old code block is removed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454049236
https://github.com/hail-is/hail/pull/5075#issuecomment-454049236:58,Testability,test,testing,58,"Thank you both. If you're comfortable without performance testing at this stage (since this is part of a broader move, and optimization seems like it is out of scope, as it is mostly now a function of the Table implementation), this PR is approved, once the commented-out old code block is removed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454049236
https://github.com/hail-is/hail/pull/5077#issuecomment-452945059:21,Availability,error,error,21,"looks like a compile error in TestUtils.scala. The rest looks good, will approve when tests pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059
https://github.com/hail-is/hail/pull/5077#issuecomment-452945059:30,Testability,Test,TestUtils,30,"looks like a compile error in TestUtils.scala. The rest looks good, will approve when tests pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059
https://github.com/hail-is/hail/pull/5077#issuecomment-452945059:86,Testability,test,tests,86,"looks like a compile error in TestUtils.scala. The rest looks good, will approve when tests pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059
https://github.com/hail-is/hail/pull/5077#issuecomment-453215900:23,Availability,error,error,23,"> looks like a compile error in TestUtils.scala.; > ; > The rest looks good, will approve when tests pass. I fixed `TestUtils.scala`; the issue was a missing parameter in a call to the (changed) `MatrixVCFReader`. Should I rebase and squash all of my commits into a single commit, or are you OK with merging my branch with the discrete feature commits (i.e., non-""Merge remote-tracking branch"" commits)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900
https://github.com/hail-is/hail/pull/5077#issuecomment-453215900:32,Testability,Test,TestUtils,32,"> looks like a compile error in TestUtils.scala.; > ; > The rest looks good, will approve when tests pass. I fixed `TestUtils.scala`; the issue was a missing parameter in a call to the (changed) `MatrixVCFReader`. Should I rebase and squash all of my commits into a single commit, or are you OK with merging my branch with the discrete feature commits (i.e., non-""Merge remote-tracking branch"" commits)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900
https://github.com/hail-is/hail/pull/5077#issuecomment-453215900:95,Testability,test,tests,95,"> looks like a compile error in TestUtils.scala.; > ; > The rest looks good, will approve when tests pass. I fixed `TestUtils.scala`; the issue was a missing parameter in a call to the (changed) `MatrixVCFReader`. Should I rebase and squash all of my commits into a single commit, or are you OK with merging my branch with the discrete feature commits (i.e., non-""Merge remote-tracking branch"" commits)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900
https://github.com/hail-is/hail/pull/5077#issuecomment-453215900:116,Testability,Test,TestUtils,116,"> looks like a compile error in TestUtils.scala.; > ; > The rest looks good, will approve when tests pass. I fixed `TestUtils.scala`; the issue was a missing parameter in a call to the (changed) `MatrixVCFReader`. Should I rebase and squash all of my commits into a single commit, or are you OK with merging my branch with the discrete feature commits (i.e., non-""Merge remote-tracking branch"" commits)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900
https://github.com/hail-is/hail/pull/5078#issuecomment-452419381:0,Deployability,update,update,0,update: took 160s on profile225 (2.0GB as .vcf.gz). The size input to LD Prune (after filtering and split-multi) is 700MB (as mt). 1KG is 16MB as an mt. There's clearly a lot of overhead for small datasets.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381
https://github.com/hail-is/hail/pull/5078#issuecomment-452419381:161,Usability,clear,clearly,161,update: took 160s on profile225 (2.0GB as .vcf.gz). The size input to LD Prune (after filtering and split-multi) is 700MB (as mt). 1KG is 16MB as an mt. There's clearly a lot of overhead for small datasets.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381
https://github.com/hail-is/hail/pull/5078#issuecomment-452503149:142,Testability,test,test,142,"@tpoterba see changes, I made it backwards compatible by adding a parameter. I'm using the Konrad xmas present now. Let's not approve until I test against local UKBB and test in a cluster (I want to ensure it works at scale too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452503149
https://github.com/hail-is/hail/pull/5078#issuecomment-452503149:170,Testability,test,test,170,"@tpoterba see changes, I made it backwards compatible by adding a parameter. I'm using the Konrad xmas present now. Let's not approve until I test against local UKBB and test in a cluster (I want to ensure it works at scale too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452503149
https://github.com/hail-is/hail/pull/5078#issuecomment-452520165:108,Safety,avoid,avoid,108,"I let ld prune run for a while on a 30GB bgen file. There are known issues here, but I made some changes to avoid some useless work but that included fusing the variant filtering with the ldprune. It's a single stage spark job. It took 1.6 Hours to do a bit more than a third of this file. So that's about 5700 seconds for ~10 GB compared to 160s for 0.7 GB, which is 35:14. I hope to try this after the BGEN fixes to see what the scaling looks like. Anyway, as a part of BlockMatrix IR changes that Daniel will work on, we'll look into why LD Prune isn't within 8x of PLINK.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452520165
https://github.com/hail-is/hail/pull/5078#issuecomment-455017270:63,Testability,test,test,63,"Fixed a bunch of things, improved it a bit, ~30s for the small test now. when combined with the changes from #5107",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-455017270
https://github.com/hail-is/hail/pull/5078#issuecomment-456591269:326,Performance,optimiz,optimizer,326,"I made three more improvements last week (and I don't plan to do anything else now):. - [run hl.sort once](https://github.com/hail-is/hail/pull/5078/commits/57ed10c8d0145f2751dbbbccdfb93f9e983f674b), I realized that if you create an expression that calls `hl.sort(..)` and use it in many places, the sort gets in-lined and no optimizer pass lifts it to a global. I manually lifted it to globals. - [remove an unnecessary per-entry rename](https://github.com/hail-is/hail/pull/5078/commits/9e31a97a8deb0e3b0886f6b20d0d953b9e1b167d). - [remove an unnecessary allocation of a row struct](https://github.com/hail-is/hail/pull/5078/commits/61fd47bf1ee9fc09b9076da3e5acf87c5fc21fd5)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-456591269
https://github.com/hail-is/hail/pull/5081#issuecomment-452769404:29,Testability,test,tests,29,Rebased and added a suite of tests for row annotation/col aggregation.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5081#issuecomment-452769404
https://github.com/hail-is/hail/pull/5082#issuecomment-451950109:99,Testability,log,logic,99,"> They are? Where is the type set?. don't know what you mean here, sorry. I meant that all of this logic exists in functions.py on the expression stuff!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5082#issuecomment-451950109
https://github.com/hail-is/hail/pull/5082#issuecomment-451988168:38,Availability,down,download,38,"Ugh, it is failing because movie lens download is timing out again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5082#issuecomment-451988168
https://github.com/hail-is/hail/pull/5084#issuecomment-453185348:70,Testability,test,test,70,"Does this fix https://github.com/hail-is/hail/blob/master/hail/python/test/hail/expr/test_expr.py#L1829 ?. ```; # FIXME: this next line triggers a bug: None should be sorted last!; # self.assertEqual(hl.sorted([0, 1, 4, hl.null(tint), 3, 2], lambda x: x, reverse=True).collect()[0], [4, 3, 2, 1, 0, None]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5084#issuecomment-453185348
https://github.com/hail-is/hail/pull/5084#issuecomment-453185348:188,Testability,assert,assertEqual,188,"Does this fix https://github.com/hail-is/hail/blob/master/hail/python/test/hail/expr/test_expr.py#L1829 ?. ```; # FIXME: this next line triggers a bug: None should be sorted last!; # self.assertEqual(hl.sorted([0, 1, 4, hl.null(tint), 3, 2], lambda x: x, reverse=True).collect()[0], [4, 3, 2, 1, 0, None]); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5084#issuecomment-453185348
https://github.com/hail-is/hail/pull/5107#issuecomment-453174210:25,Testability,log,logic,25,"The bug was fixed by the logic in `result` that uses UIDs for computed keys, renaming them after aggregation. I'm happy to walk you through this if you want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5107#issuecomment-453174210
https://github.com/hail-is/hail/pull/5113#issuecomment-453836020:41,Testability,test,tested,41,cxx emit is actually fine. Where is that tested?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5113#issuecomment-453836020
https://github.com/hail-is/hail/pull/5113#issuecomment-453836059:14,Testability,test,tested,14,it's actually tested by the same `assertEvalsTo`. nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5113#issuecomment-453836059
https://github.com/hail-is/hail/pull/5113#issuecomment-453836059:34,Testability,assert,assertEvalsTo,34,it's actually tested by the same `assertEvalsTo`. nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5113#issuecomment-453836059
https://github.com/hail-is/hail/pull/5117#issuecomment-455789526:23,Availability,down,down,23,Closing as we're going down a different path.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5117#issuecomment-455789526
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:19,Testability,test,tests,19,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:40,Testability,test,test,40,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:83,Testability,assert,assertEvalsTo,83,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:214,Testability,test,tests,214,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:340,Testability,log,logic,340,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:371,Testability,test,tested,371,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/pull/5127#issuecomment-454913111:402,Testability,test,tests,402,"Ok, I've moved the tests from their own test suite into TableIR, and adjusted the `assertEvalsTo` stuff to go through `SparkBackend`. . @danking I had to add `ArrayFlatMap` to cxx.Emit in order to get the relevant tests to start actually getting compiled through the SparkBackend. It's a pretty straightforward modification of the ArrayMap logic above it, and it's being tested through all the IRSuite tests, but if you'd like me to break it out, please let me know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5127#issuecomment-454913111
https://github.com/hail-is/hail/issues/5128#issuecomment-454219734:85,Deployability,release,release,85,cc: @cseed @tpoterba is this an OK way to track things that should be done before we release 0.3?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5128#issuecomment-454219734
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:131,Deployability,install,installed,131,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:699,Deployability,install,installed,699,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:1118,Deployability,install,install,1118,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:1327,Deployability,install,installed,1327,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:1357,Deployability,deploy,deploy,1357,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:34,Testability,test,testPython,34,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:358,Testability,Test,Tests,358,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:445,Testability,test,tests,445,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:626,Testability,test,test,626,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:690,Testability,test,test,690,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:1315,Testability,test,testing,1315,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:1372,Testability,test,tested,1372,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503
https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:499,Deployability,install,installed,499,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951
https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:56,Testability,test,test,56,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951
https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:202,Testability,test,test,202,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951
https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:232,Testability,test,test,232,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951
https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:284,Testability,test,test,284,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951
https://github.com/hail-is/hail/pull/5130#issuecomment-454466582:53,Deployability,install,install,53,"Yeah you're right about `pyspark` though, if we `pip install pyspark` then we needn't edit PYTHONPATH or set `SPARK_HOME` afaik.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454466582
https://github.com/hail-is/hail/pull/5130#issuecomment-454467812:9,Deployability,install,install,9,"re: `pip install -e`, yeah, I should probably understand how all this python packaging stuff works so I can make an informed decision about that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454467812
https://github.com/hail-is/hail/pull/5130#issuecomment-454592988:12,Deployability,update,updated,12,@tpoterba I updated the main message with a better explainer on the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988
https://github.com/hail-is/hail/pull/5130#issuecomment-454592988:29,Integrability,message,message,29,@tpoterba I updated the main message with a better explainer on the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988
https://github.com/hail-is/hail/pull/5132#issuecomment-454277066:5,Security,expose,exposes,5,This exposes a bug in the pruner in test_tdt that I'm still investigating.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5132#issuecomment-454277066
https://github.com/hail-is/hail/pull/5138#issuecomment-454386209:125,Deployability,pipeline,pipelines,125,"This will prevent optimization around the filter intervals. Given the prevalence of filter intervals in the biggest, baddest pipelines, this is a concern.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209
https://github.com/hail-is/hail/pull/5138#issuecomment-454386209:18,Performance,optimiz,optimization,18,"This will prevent optimization around the filter intervals. Given the prevalence of filter intervals in the biggest, baddest pipelines, this is a concern.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:116,Availability,error,error,116,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:165,Deployability,install,installed,165,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:28,Testability,test,tests,28,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:257,Testability,test,tests,257,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:332,Testability,test,tests,332,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:431,Testability,test,tested,431,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536
https://github.com/hail-is/hail/pull/5139#issuecomment-454422936:47,Availability,failure,failures,47,This looks good to me. There's a bunch of test failures mainly in TextTableSuite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-454422936
https://github.com/hail-is/hail/pull/5139#issuecomment-454422936:42,Testability,test,test,42,This looks good to me. There's a bunch of test failures mainly in TextTableSuite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-454422936
https://github.com/hail-is/hail/pull/5139#issuecomment-455036052:16,Availability,down,down,16,Finally tracked down on the failures. This should be ready to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052
https://github.com/hail-is/hail/pull/5139#issuecomment-455036052:28,Availability,failure,failures,28,Finally tracked down on the failures. This should be ready to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052
https://github.com/hail-is/hail/pull/5141#issuecomment-454422240:116,Modifiability,extend,extends,116,"er, actually it's impossible to generate these nodes:. ```; final case class Literal(_typ: Type, value: Annotation) extends IR {; require(!CanEmit(_typ)); require(value != null); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5141#issuecomment-454422240
https://github.com/hail-is/hail/pull/5142#issuecomment-454423215:22,Testability,test,tests,22,"I expect this to fail tests, then I'll go debug and fix up the IRs that serialize children.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5142#issuecomment-454423215
https://github.com/hail-is/hail/pull/5149#issuecomment-454835409:286,Availability,error,error,286,"> Also, it looks like we didn't define the operator syntax. Sounds like an easy PR to farm out to someone else!. @danking I intentionally didn't add this yet - there is a case I am worried about:. ```; mt.filter_rows(mt.pass & mt.variant_qc.AF[1] > 0.01); ```. Right now this is a type error. With bit operators, this is the same as:. ```; mt.filter_rows((hl.bit_and(hl.int(mt.pass), mt.variant_qc.AF[1]) > 0.01); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5149#issuecomment-454835409
https://github.com/hail-is/hail/pull/5149#issuecomment-454862527:66,Testability,log,logical,66,Good comments. Addressed:. - renamed bit_flip to bit_not; - added logical flag to bit_rshift; - added more tests with negative numbers; - expanded docs around differences between Python and Hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5149#issuecomment-454862527
https://github.com/hail-is/hail/pull/5149#issuecomment-454862527:107,Testability,test,tests,107,Good comments. Addressed:. - renamed bit_flip to bit_not; - added logical flag to bit_rshift; - added more tests with negative numbers; - expanded docs around differences between Python and Hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5149#issuecomment-454862527
https://github.com/hail-is/hail/pull/5150#issuecomment-454993793:36,Testability,Test,TestUtilsSuite,36,Just a heads up this is failing in `TestUtilsSuite.testDataProviders` but I can't reproduce it locally. I'll let you know once I figure out what's going on.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-454993793
https://github.com/hail-is/hail/pull/5150#issuecomment-454993793:51,Testability,test,testDataProviders,51,Just a heads up this is failing in `TestUtilsSuite.testDataProviders` but I can't reproduce it locally. I'll let you know once I figure out what's going on.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-454993793
https://github.com/hail-is/hail/pull/5150#issuecomment-456211174:78,Integrability,wrap,wraps,78,"Will look at this asap; focused on notebook service, can this wait until that wraps up?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456211174
https://github.com/hail-is/hail/pull/5150#issuecomment-456682522:425,Performance,perform,performance,425,"Great questions. The larger change here is that we're trying to decouple the Python front end from Scala/Java. Therefore, instead of calling directly into Python (e.g. getReferenceGenome), we're constructing a MatrixRead IR, which can either be passed to a service (ServiceBackend) or passed to Java as we did before (SparkBackend). I should also remark, py4j is incredibly slow, so batching calls to Python greatly improves performance in the current setup. So, for example, building an IR in Python, serializing it and parsing on Java is much faster than a series of py4j calls that builds the corresponding objects on the Java side one at a time. Some functions which used to be called from Python like importBgens are no longer used, but are still used by the Scala tests. Those got moved to TestUtils and are being phased out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456682522
https://github.com/hail-is/hail/pull/5150#issuecomment-456682522:770,Testability,test,tests,770,"Great questions. The larger change here is that we're trying to decouple the Python front end from Scala/Java. Therefore, instead of calling directly into Python (e.g. getReferenceGenome), we're constructing a MatrixRead IR, which can either be passed to a service (ServiceBackend) or passed to Java as we did before (SparkBackend). I should also remark, py4j is incredibly slow, so batching calls to Python greatly improves performance in the current setup. So, for example, building an IR in Python, serializing it and parsing on Java is much faster than a series of py4j calls that builds the corresponding objects on the Java side one at a time. Some functions which used to be called from Python like importBgens are no longer used, but are still used by the Scala tests. Those got moved to TestUtils and are being phased out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456682522
https://github.com/hail-is/hail/pull/5150#issuecomment-456682522:796,Testability,Test,TestUtils,796,"Great questions. The larger change here is that we're trying to decouple the Python front end from Scala/Java. Therefore, instead of calling directly into Python (e.g. getReferenceGenome), we're constructing a MatrixRead IR, which can either be passed to a service (ServiceBackend) or passed to Java as we did before (SparkBackend). I should also remark, py4j is incredibly slow, so batching calls to Python greatly improves performance in the current setup. So, for example, building an IR in Python, serializing it and parsing on Java is much faster than a series of py4j calls that builds the corresponding objects on the Java side one at a time. Some functions which used to be called from Python like importBgens are no longer used, but are still used by the Scala tests. Those got moved to TestUtils and are being phased out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5150#issuecomment-456682522
https://github.com/hail-is/hail/pull/5152#issuecomment-454915985:53,Usability,clear,clear,53,You're also removing Interval.point_type (just to be clear what the breaking changes are).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-454915985
https://github.com/hail-is/hail/pull/5152#issuecomment-455711797:21,Usability,clear,clearly,21,"I'm not either. It's clearly the right thing. We can write them in terms of `hl.eval`, but I don't think they will work quite the same since we don't have the point type and type inference could fail (e.g. the endpoints are None).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-455711797
https://github.com/hail-is/hail/pull/5152#issuecomment-455895729:69,Integrability,depend,depend,69,I'll put point_type back in for the sake of compatibility. This will depend on https://github.com/hail-is/hail/pull/5138 so I'm waiting for that to go in first.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-455895729
https://github.com/hail-is/hail/pull/5152#issuecomment-456684801:172,Testability,test,tests,172,"I put point_type back. Should be ready to go now. Also, fixed the close definition, good catch. I also removed an additional use of _convert_to_j in import_bgen to get the tests to pass from this PR (sorry my stacking got a bit mixed up): https://github.com/hail-is/hail/pull/5150/files#diff-36d21c1427efe06a781cd36ef5aa8678R961. You can also wait for that to go in and I'll rebase if you're worried about the change. Finally, the imports are a bit of a mess since I wanted to use hail_type in interval.py which is also imported by the types and expr files. @tpoterba I think we should remove types from expr and remove java from utils (we're confusing user utils like hadoop_* and Interval from internal utils like Env and java stuff which don't seem related) and have a clear ""import"" graph: javautils > types > utils > expr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456684801
https://github.com/hail-is/hail/pull/5152#issuecomment-456684801:772,Usability,clear,clear,772,"I put point_type back. Should be ready to go now. Also, fixed the close definition, good catch. I also removed an additional use of _convert_to_j in import_bgen to get the tests to pass from this PR (sorry my stacking got a bit mixed up): https://github.com/hail-is/hail/pull/5150/files#diff-36d21c1427efe06a781cd36ef5aa8678R961. You can also wait for that to go in and I'll rebase if you're worried about the change. Finally, the imports are a bit of a mess since I wanted to use hail_type in interval.py which is also imported by the types and expr files. @tpoterba I think we should remove types from expr and remove java from utils (we're confusing user utils like hadoop_* and Interval from internal utils like Env and java stuff which don't seem related) and have a clear ""import"" graph: javautils > types > utils > expr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456684801
https://github.com/hail-is/hail/pull/5152#issuecomment-456802543:1,Usability,clear,clear,1,"`clear ""import"" graph: javautils > types > utils > expr` . Sounds good to me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456802543
https://github.com/hail-is/hail/pull/5154#issuecomment-454854904:171,Usability,clear,clear,171,I'm a bit worried about confusing people with two (almost) identical methods named differently. . What do you think about something like `rbind` for right-bind? Then it's clear they're in the same family,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5154#issuecomment-454854904
https://github.com/hail-is/hail/pull/5155#issuecomment-454898806:120,Testability,test,test,120,"This bug is caught in expr.test_array_methods, on the `group_by` line with the additions I made in this PR; I think any test that I would add would essentially just be a duplicate of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5155#issuecomment-454898806
https://github.com/hail-is/hail/pull/5161#issuecomment-455034946:108,Testability,test,tests,108,"Sorry, I jumped the gun on this PR. It will need https://github.com/hail-is/hail/pull/5152 to in before the tests will pass.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5161#issuecomment-455034946
https://github.com/hail-is/hail/pull/5162#issuecomment-455362548:117,Deployability,release,released,117,"for future pull requests, can you open a PR from your forked repository? We like to keep the main repo free from non-released branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455362548
https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:12,Availability,robust,robust,12,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:19,Security,authenticat,authentication,19,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:127,Usability,clear,clearing,127,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
https://github.com/hail-is/hail/pull/5162#issuecomment-456601345:140,Integrability,depend,dependency,140,@akotlar re: `package-lock.json` is there an explanation somewhere of why `package-lock.json` recursively includes the description of every dependency? More specifically why isn't the contents of `devDependencies` plus the resolved address and the SHA256 sufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456601345
https://github.com/hail-is/hail/pull/5162#issuecomment-456603534:42,Integrability,depend,dependencies,42,"Disregard, makes sense because two of our dependencies might specify a shared dependency which is not directly our dependency. That dependency will be resolved in some manner and we want to track exactly what it was resolved to. This actually seems like the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534
https://github.com/hail-is/hail/pull/5162#issuecomment-456603534:78,Integrability,depend,dependency,78,"Disregard, makes sense because two of our dependencies might specify a shared dependency which is not directly our dependency. That dependency will be resolved in some manner and we want to track exactly what it was resolved to. This actually seems like the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534
https://github.com/hail-is/hail/pull/5162#issuecomment-456603534:115,Integrability,depend,dependency,115,"Disregard, makes sense because two of our dependencies might specify a shared dependency which is not directly our dependency. That dependency will be resolved in some manner and we want to track exactly what it was resolved to. This actually seems like the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534
https://github.com/hail-is/hail/pull/5162#issuecomment-456603534:132,Integrability,depend,dependency,132,"Disregard, makes sense because two of our dependencies might specify a shared dependency which is not directly our dependency. That dependency will be resolved in some manner and we want to track exactly what it was resolved to. This actually seems like the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456603534
https://github.com/hail-is/hail/pull/5162#issuecomment-456642632:443,Energy Efficiency,reduce,reduce,443,"Yeah, that's a good question, and probably something I should research, address on a Thursday. It would be nice if the structure were flatter. There is an open issue related to this: https://github.com/npm/npm/issues/19770. The file is a bit ridiculous; I should explore using npm 5.5.1 or yarn at some point. Not sure if yarn behavior is better; avoided yarn in this pull request because I want to minimize our use of third party packages to reduce complexity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632
https://github.com/hail-is/hail/pull/5162#issuecomment-456642632:347,Safety,avoid,avoided,347,"Yeah, that's a good question, and probably something I should research, address on a Thursday. It would be nice if the structure were flatter. There is an open issue related to this: https://github.com/npm/npm/issues/19770. The file is a bit ridiculous; I should explore using npm 5.5.1 or yarn at some point. Not sure if yarn behavior is better; avoided yarn in this pull request because I want to minimize our use of third party packages to reduce complexity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632
https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:102,Availability,avail,available,102,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812
https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:184,Availability,error,error,184,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812
https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:88,Performance,optimiz,optimizations,88,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812
https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:26,Testability,log,logic,26,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812
https://github.com/hail-is/hail/pull/5162#issuecomment-460040357:638,Security,authenticat,authentication,638,"@cseed: One thing to note. Safari currently has an issue with silent refresh of auth token, due to the way it prevents cross-site tracking. This can be disabled (we could certainly mention this to participants), and other browsers don't appear to have this problem. The solution I recommend for the moment is to pay auth0 ~$15-20/month for a custom domain (we get some additional features as well: https://auth0.com/pricing, including built-in account linking across social providers, which would be nice to not have to write ourselves) ... which will mean that no cross-site cookies are needed. The other approaches involve putting more authentication functionality on our servers, which will be more work for us, and cost much more than $250/year in developer time. Auth0 is also working on a more community-friendly fix (an authentication standard that is exempt from this kind of cross-site cookie block). I've also posted a request for their opinion on the matter in our case specifically.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460040357
https://github.com/hail-is/hail/pull/5162#issuecomment-460040357:827,Security,authenticat,authentication,827,"@cseed: One thing to note. Safari currently has an issue with silent refresh of auth token, due to the way it prevents cross-site tracking. This can be disabled (we could certainly mention this to participants), and other browsers don't appear to have this problem. The solution I recommend for the moment is to pay auth0 ~$15-20/month for a custom domain (we get some additional features as well: https://auth0.com/pricing, including built-in account linking across social providers, which would be nice to not have to write ourselves) ... which will mean that no cross-site cookies are needed. The other approaches involve putting more authentication functionality on our servers, which will be more work for us, and cost much more than $250/year in developer time. Auth0 is also working on a more community-friendly fix (an authentication standard that is exempt from this kind of cross-site cookie block). I've also posted a request for their opinion on the matter in our case specifically.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460040357
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:1557,Integrability,rout,routes,1557,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:693,Performance,perform,performance,693,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:1467,Performance,response time,response time,1467,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:602,Security,attack,attacks,602,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:434,Testability,log,login,434,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:507,Testability,log,login,507,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:280,Availability,avail,available,280,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:178,Integrability,rout,routing,178,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:1294,Integrability,rout,routing,1294,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:72,Performance,load,loading,72,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:117,Performance,load,loading,117,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:311,Performance,load,loading,311,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:339,Performance,load,loading,339,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:546,Performance,load,loading,546,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:703,Performance,load,loading,703,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:1384,Performance,load,loading,1384,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:1621,Performance,load,loading,1621,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:333,Safety,avoid,avoid,333,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:1480,Usability,responsiv,responsive,1480,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:1511,Usability,feedback,feedback,1511,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
https://github.com/hail-is/hail/issues/5168#issuecomment-456618542:92,Safety,unsafe,unsafely,92,"Root cause found. Each time a batch test runs it generates a bunch of garbage because batch unsafely handles `cancel`. Here's the bad sequence:. - batch adds a job to `job_id_job`; - batch makes an HTTP request to k8s to create the pod, THREAD IS NOW PAUSED WAITING FOR RESULT; - flask handles a new request to cancel said job, tries to delete the pod; - [_delete_pod says: if `_pod_name` is `None`, don't do anything](https://github.com/hail-is/hail/blob/master/batch/batch/server/server.py#L83), so it does nothing but tells the client 200 OK!; - THREAD WAITING ON k8s WAKES UP: oh good, pod created. I think the fix is to check after pod creation if our state was set to canceled. If yes, delete said pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168#issuecomment-456618542
https://github.com/hail-is/hail/issues/5168#issuecomment-456618542:36,Testability,test,test,36,"Root cause found. Each time a batch test runs it generates a bunch of garbage because batch unsafely handles `cancel`. Here's the bad sequence:. - batch adds a job to `job_id_job`; - batch makes an HTTP request to k8s to create the pod, THREAD IS NOW PAUSED WAITING FOR RESULT; - flask handles a new request to cancel said job, tries to delete the pod; - [_delete_pod says: if `_pod_name` is `None`, don't do anything](https://github.com/hail-is/hail/blob/master/batch/batch/server/server.py#L83), so it does nothing but tells the client 200 OK!; - THREAD WAITING ON k8s WAKES UP: oh good, pod created. I think the fix is to check after pod creation if our state was set to canceled. If yes, delete said pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168#issuecomment-456618542
https://github.com/hail-is/hail/issues/5168#issuecomment-456618542:251,Usability,PAUSE,PAUSED,251,"Root cause found. Each time a batch test runs it generates a bunch of garbage because batch unsafely handles `cancel`. Here's the bad sequence:. - batch adds a job to `job_id_job`; - batch makes an HTTP request to k8s to create the pod, THREAD IS NOW PAUSED WAITING FOR RESULT; - flask handles a new request to cancel said job, tries to delete the pod; - [_delete_pod says: if `_pod_name` is `None`, don't do anything](https://github.com/hail-is/hail/blob/master/batch/batch/server/server.py#L83), so it does nothing but tells the client 200 OK!; - THREAD WAITING ON k8s WAKES UP: oh good, pod created. I think the fix is to check after pod creation if our state was set to canceled. If yes, delete said pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168#issuecomment-456618542
https://github.com/hail-is/hail/issues/5168#issuecomment-456621353:27,Testability,log,logic,27,Actual root cause: the new logic to handle triggering a pod when a parent finished incorrectly started a pod even if the job was cancelled,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5168#issuecomment-456621353
https://github.com/hail-is/hail/pull/5172#issuecomment-455698384:291,Usability,simpl,simplifies,291,"It wasn't scanning the full dataset anymore, but:. table.head().flatten() was generating a TableOrderBy(TableKeyBy(TableHead)). There was no way to remove this node, even if the table was already keyed by the sort fields, so we ended up doing an extra scan and possibly shuffle. This change simplifies the whole thing, and emits the correct IR from the beginning",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5172#issuecomment-455698384
https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:580,Availability,error,error,580,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308
https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:586,Integrability,message,message,586,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308
https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:118,Testability,test,test,118,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308
https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:187,Performance,Load,Loading,187,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267
https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:268,Performance,Load,Loading,268,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267
https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:327,Performance,Load,Loading,327,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267
https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:386,Performance,Load,Loading,386,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267
https://github.com/hail-is/hail/issues/5177#issuecomment-455882267:39,Testability,test,test,39,"```; In [4]: ht = hl.import_table('src/test/resources/variantAnnotations.alternateformat.tsv', min_partitions=100); 2019-01-20 11:36:31 Hail: INFO: Reading table with no type imputation; Loading column 'Chromosome:Position:Ref:Alt' as type 'str' (type not specified); Loading column 'Rand1' as type 'str' (type not specified); Loading column 'Rand2' as type 'str' (type not specified); Loading column 'Gene' as type 'str' (type not specified). In [5]: ht.n_partitions(); Out[5]: 1. In [6]: ht.repartition(100).n_partitions(); Out[6]: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5177#issuecomment-455882267
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:353,Integrability,interface,interface,353,"Some initial thoughts:; * I'm not sure we should have implicitly broadcasting operations in the IR. It seems simpler to make broadcast an explicit operation, which we make sure to deforest. In fact, broadcast is a special case of the generic tensor index operation I'll describe below. Implicitly broadcasting operations could be provided in the Python interface, making broadcasts explicit when constructing the IR.; * I'm also not sure how much special treatment we should give to block matrices in the IR. I now like to think of block matrices as just 4-tensors, with matrix operations like matrix multiplication lowering to operations on 4-tensors. If we allow tensors to have some distributed dimensions and some ""small"" dimensions, then at least in the backend we might not need special handling of block structures. It may still be helpful to have a special block matrix/tensor representation at the top level IR, or maybe that should only live in Python—I'm not sure. Here's a proposal for a set of primitive tensor operations. * Outer product: Takes two tensors, T1 and T2, with shapes [n1, ..., ni] and [m1, ..., mj], and entry types t1 and t2, and makes a tensor Out with shape [n1, ..., ni, m1, ..., mj] and entry type (t1, t2). If we want to support sparse tensors, this should take a flag specifying how the sparse structure of the output is determined from those of the inputs. I'll call the possible flags ""and"", ""or"", and ""true"". The ""and"" flag says that Out(n, m) is defined iff T1(n) AND T2(m) are both defined. If we will be multiplying the pairs, or applying any other operation with our default missingness semantics, this is the appropriate setting.; The ""or"" flag says Out(n, m) is defined iff T1(n) OR T2(m) is defined, as is appropriate if we are adding the pairs.; ""true"" just means make Out dense, regardless of the sparsity of the inputs. * Map. I don't think there's much to say here. * Generic index operations (not sure what to call these). I'll first give some example",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3583,Integrability,inject,injective,3583,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3771,Integrability,inject,injective,3771,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:2995,Modifiability,variab,variables,2995,"eneric index operations (not sure what to call these). I'll first give some examples using informal notation, then I'll give a proposal for an IR representation. I'll say ""sum"" everywhere, but that could be any aggregation. Let T be a 1-tensor. O represents the output of the operation. Then. * `T(i) -> O()` is the sum of T. * `T(i) -> O(i)` is identity. * `T(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3045,Modifiability,variab,variable,3045,"roposal for an IR representation. I'll say ""sum"" everywhere, but that could be any aggregation. Let T be a 1-tensor. O represents the output of the operation. Then. * `T(i) -> O()` is the sum of T. * `T(i) -> O(i)` is identity. * `T(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3409,Modifiability,variab,variables,3409,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3583,Security,inject,injective,3583,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3771,Security,inject,injective,3771,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:109,Usability,simpl,simpler,109,"Some initial thoughts:; * I'm not sure we should have implicitly broadcasting operations in the IR. It seems simpler to make broadcast an explicit operation, which we make sure to deforest. In fact, broadcast is a special case of the generic tensor index operation I'll describe below. Implicitly broadcasting operations could be provided in the Python interface, making broadcasts explicit when constructing the IR.; * I'm also not sure how much special treatment we should give to block matrices in the IR. I now like to think of block matrices as just 4-tensors, with matrix operations like matrix multiplication lowering to operations on 4-tensors. If we allow tensors to have some distributed dimensions and some ""small"" dimensions, then at least in the backend we might not need special handling of block structures. It may still be helpful to have a special block matrix/tensor representation at the top level IR, or maybe that should only live in Python—I'm not sure. Here's a proposal for a set of primitive tensor operations. * Outer product: Takes two tensors, T1 and T2, with shapes [n1, ..., ni] and [m1, ..., mj], and entry types t1 and t2, and makes a tensor Out with shape [n1, ..., ni, m1, ..., mj] and entry type (t1, t2). If we want to support sparse tensors, this should take a flag specifying how the sparse structure of the output is determined from those of the inputs. I'll call the possible flags ""and"", ""or"", and ""true"". The ""and"" flag says that Out(n, m) is defined iff T1(n) AND T2(m) are both defined. If we will be multiplying the pairs, or applying any other operation with our default missingness semantics, this is the appropriate setting.; The ""or"" flag says Out(n, m) is defined iff T1(n) OR T2(m) is defined, as is appropriate if we are adding the pairs.; ""true"" just means make Out dense, regardless of the sparsity of the inputs. * Map. I don't think there's much to say here. * Generic index operations (not sure what to call these). I'll first give some example",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1576,Deployability,integrat,integrate,1576,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120
https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:497,Integrability,interface,interface,497,"I think the current proposal is to have distributed and ""small"" dimensions, but the distributed dimensions specify their blocking (rather than just being marked ""distributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120
https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1576,Integrability,integrat,integrate,1576,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120
https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1164,Performance,perform,performance,1164,"r than just being marked ""distributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile),",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120
https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1621,Performance,perform,performance,1621,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120
https://github.com/hail-is/hail/pull/5190#issuecomment-461585416:26,Deployability,update,updated,26,Will open a dev post with updated thoughts when I get to updating my thoughts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-461585416
https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:162,Deployability,configurat,configuration,162,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381
https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:73,Modifiability,variab,variables,73,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381
https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:162,Modifiability,config,configuration,162,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:440,Safety,safe,safe,440,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:21,Security,Secur,Security,21,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:495,Security,Secur,SecurityContext,495,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:582,Security,secur,securitycontext-,582,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:832,Security,access,access,832,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:917,Security,access,access,917,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1101,Security,access,access,1101,"docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1364,Security,access,accessible,1364,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1652,Security,expose,expose,1652,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1928,Security,access,access,1928,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:2019,Security,access,access,2019,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:2121,Security,access,access-authn-authz,2121,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:2300,Security,access,access,2300,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:202,Availability,down,downstream,202,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:232,Deployability,Pipeline,Pipeline,232,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:998,Deployability,Pipeline,Pipeline,998,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1048,Deployability,pipeline,pipeline-controlled,1048,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1083,Deployability,Pipeline,Pipeline,1083,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1129,Deployability,Pipeline,Pipeline,1129,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1422,Deployability,configurat,configuration,1422,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1661,Deployability,update,update,1661,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1422,Modifiability,config,configuration,1422,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739
https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:87,Integrability,depend,dependencies,87,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502
https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:440,Integrability,depend,dependsOn,440,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502
https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:973,Modifiability,variab,variables,973,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502
https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:321,Testability,test,test,321,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502
https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:489,Testability,test,tests-using-input-jar,489,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502
https://github.com/hail-is/hail/pull/5194#issuecomment-457196817:23,Availability,error,error,23,"Great change. Still an error related to hail_pip_version in the tests, though. Can you make a discuss post when this goes in to alert people compiling their own builds?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817
https://github.com/hail-is/hail/pull/5194#issuecomment-457196817:64,Testability,test,tests,64,"Great change. Still an error related to hail_pip_version in the tests, though. Can you make a discuss post when this goes in to alert people compiling their own builds?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:363,Deployability,install,install,363,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:489,Integrability,depend,dependencies,489,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:719,Integrability,depend,dependency,719,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:629,Modifiability,parameteriz,parameterized,629,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:454,Performance,cache,cache,454,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:414,Safety,avoid,avoid,414,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333
https://github.com/hail-is/hail/issues/5199#issuecomment-506360273:0,Testability,test,testing,0,"testing is changing a lot, wont fix",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5199#issuecomment-506360273
https://github.com/hail-is/hail/pull/5200#issuecomment-457222830:178,Testability,test,testing,178,@danking I still think the name of 'jobs' should be 'job_states' and 'jobs' should be a list of the job ids. But we can figure that out in a separate PR. At least this will make testing easier for me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5200#issuecomment-457222830
https://github.com/hail-is/hail/pull/5201#issuecomment-459189555:12,Testability,test,tests,12,@cseed what tests do you want to see?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5201#issuecomment-459189555
https://github.com/hail-is/hail/pull/5201#issuecomment-459357328:20,Testability,test,tests,20,pushed a bunch more tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5201#issuecomment-459357328
https://github.com/hail-is/hail/issues/5207#issuecomment-457700818:2,Testability,test,tested,2,"I tested this on the latest version of master, and it seems it has been fixed since the docker image with the workshop tutorial was created.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5207#issuecomment-457700818
https://github.com/hail-is/hail/issues/5207#issuecomment-457740960:13,Availability,error,error,13,what was the error you saw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5207#issuecomment-457740960
https://github.com/hail-is/hail/pull/5208#issuecomment-457761043:40,Security,access,access,40,"Right, but my concern is that relies on access to the original dataset. And who knows if columns got filtered or what now. I think something that preserves the state in the directory is the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-457761043
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:35,Testability,test,test,35,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:56,Testability,test,test,56,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:97,Testability,test,test,97,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:138,Testability,test,test,138,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:179,Testability,test,test,179,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:220,Testability,test,test,220,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:261,Testability,test,test,261,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:302,Testability,test,test,302,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:343,Testability,test,test,343,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:384,Testability,test,test,384,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/pull/5208#issuecomment-458989639:425,Testability,test,test,425,"Added index file:. ```; $ cat /tmp/test/index.tsv; /tmp/test/part-00.tsv.bgz	{""col_idx"":0}; /tmp/test/part-01.tsv.bgz	{""col_idx"":1}; /tmp/test/part-02.tsv.bgz	{""col_idx"":2}; /tmp/test/part-03.tsv.bgz	{""col_idx"":3}; /tmp/test/part-04.tsv.bgz	{""col_idx"":4}; /tmp/test/part-05.tsv.bgz	{""col_idx"":5}; /tmp/test/part-06.tsv.bgz	{""col_idx"":6}; /tmp/test/part-07.tsv.bgz	{""col_idx"":7}; /tmp/test/part-08.tsv.bgz	{""col_idx"":8}; /tmp/test/part-09.tsv.bgz	{""col_idx"":9}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-458989639
https://github.com/hail-is/hail/issues/5212#issuecomment-457887049:0,Performance,optimiz,optimizer,0,optimizer / compiler bug. Shouldn't be too hard to fix!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5212#issuecomment-457887049
https://github.com/hail-is/hail/pull/5215#issuecomment-458849293:95,Security,authoriz,authorization,95,Added the verification function. Allows a single auth_request to handle both user and notebook authorization checks. . Obviously the MySQL connection handling will change. Relates to https://github.com/hail-is/hail/pull/5162/commits/3114234f51002ce6c477cca40a28c3ebb6ebe759,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-458849293
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:591,Availability,error,errors,591,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:654,Availability,failure,failure,654,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:670,Availability,failure,failure,670,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2567,Availability,outage,outages,2567,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:3119,Availability,error,error,3119,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1958,Energy Efficiency,efficient,efficient,1958,"rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:893,Integrability,contract,contract,893,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1713,Modifiability,config,configurable,1713,"8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2079,Performance,Perform,Performance,2079,"eleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillmen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:396,Security,access,access,396,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2423,Security,access,accessed,2423,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1015,Testability,log,log,1015,"estion: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or shou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1612,Testability,log,log,1612,"8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290
https://github.com/hail-is/hail/pull/5215#issuecomment-459208736:886,Security,hash,hash,886,"@cseed One small annoyance is that k8 has restrictions on characterset in labels tighter than ascii. Affects user ids: auth0 concatenates id and social service with a pipe, ex: google-oauth2|something. Also affects notebook naming (smaller concern): I want people to be able to name notebooks, with some human-readable default value, because strings of random numbers and non-alphanumeric characters feel intimidating to non-cs / data-science / similar people, and I don't want them to feel intimidated in anything so trivial, since those intimidated will project that the core product is inaccessible. I can obviously str.replace unwanted characters, and do the transformation on the other end, but this will be fragile and a bit awkward. If we want this kind of thing and want to stay with k8 label-based storage for such things. Not a concern for the demo. Wondering what is better, hash, base64, or move to sql. Regarding hash, auth0 user names are globally unique, so I imagine, but am not certain, that a so-called ""cryptographically secure"" algorithm would be exceedingly unlikely to result in name collisions. For demo, this isn't a problem; I will just str.replace(|, '--_--'), collisions won't happen unintentionally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459208736
https://github.com/hail-is/hail/pull/5215#issuecomment-459208736:926,Security,hash,hash,926,"@cseed One small annoyance is that k8 has restrictions on characterset in labels tighter than ascii. Affects user ids: auth0 concatenates id and social service with a pipe, ex: google-oauth2|something. Also affects notebook naming (smaller concern): I want people to be able to name notebooks, with some human-readable default value, because strings of random numbers and non-alphanumeric characters feel intimidating to non-cs / data-science / similar people, and I don't want them to feel intimidated in anything so trivial, since those intimidated will project that the core product is inaccessible. I can obviously str.replace unwanted characters, and do the transformation on the other end, but this will be fragile and a bit awkward. If we want this kind of thing and want to stay with k8 label-based storage for such things. Not a concern for the demo. Wondering what is better, hash, base64, or move to sql. Regarding hash, auth0 user names are globally unique, so I imagine, but am not certain, that a so-called ""cryptographically secure"" algorithm would be exceedingly unlikely to result in name collisions. For demo, this isn't a problem; I will just str.replace(|, '--_--'), collisions won't happen unintentionally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459208736
https://github.com/hail-is/hail/pull/5215#issuecomment-459208736:1040,Security,secur,secure,1040,"@cseed One small annoyance is that k8 has restrictions on characterset in labels tighter than ascii. Affects user ids: auth0 concatenates id and social service with a pipe, ex: google-oauth2|something. Also affects notebook naming (smaller concern): I want people to be able to name notebooks, with some human-readable default value, because strings of random numbers and non-alphanumeric characters feel intimidating to non-cs / data-science / similar people, and I don't want them to feel intimidated in anything so trivial, since those intimidated will project that the core product is inaccessible. I can obviously str.replace unwanted characters, and do the transformation on the other end, but this will be fragile and a bit awkward. If we want this kind of thing and want to stay with k8 label-based storage for such things. Not a concern for the demo. Wondering what is better, hash, base64, or move to sql. Regarding hash, auth0 user names are globally unique, so I imagine, but am not certain, that a so-called ""cryptographically secure"" algorithm would be exceedingly unlikely to result in name collisions. For demo, this isn't a problem; I will just str.replace(|, '--_--'), collisions won't happen unintentionally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459208736
https://github.com/hail-is/hail/pull/5215#issuecomment-459225923:91,Safety,safe,safe,91,"Eh, you can't directly use base64 because of the restrictions, either. You can use the URL-safe variant if you bracket it with alphanumeric characters and use . instead of = for the pad.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459225923
https://github.com/hail-is/hail/pull/5215#issuecomment-459425712:22,Usability,feedback,feedback,22,@cseed thanks for the feedback; I'll try urlsafe_b64encode,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459425712
https://github.com/hail-is/hail/pull/5215#issuecomment-459535569:239,Deployability,update,update,239,"@danking, I've addressed your comments. Unfortunately w.r.t your time, I've removed all mysql components. Once I get websocket code working (some issue right now, 402, gevent ws doens't like my headers, maybe cors related), I will push an update. The update will revert all changes to notebook.py, and add a completely separate folder, notebook-api, in which all further changes will be made. I want to keep your (working) notebook code completely distinct, so that Cotton has a fallback for Feb 5 if needed. After Wed we can remove the old code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459535569
https://github.com/hail-is/hail/pull/5215#issuecomment-459535569:251,Deployability,update,update,251,"@danking, I've addressed your comments. Unfortunately w.r.t your time, I've removed all mysql components. Once I get websocket code working (some issue right now, 402, gevent ws doens't like my headers, maybe cors related), I will push an update. The update will revert all changes to notebook.py, and add a completely separate folder, notebook-api, in which all further changes will be made. I want to keep your (working) notebook code completely distinct, so that Cotton has a fallback for Feb 5 if needed. After Wed we can remove the old code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459535569
https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:774,Availability,error,errors,774,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071
https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:69,Deployability,deploy,deployment,69,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071
https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:916,Integrability,synchroniz,synchronizes,916,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071
https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:2219,Modifiability,config,config,2219,"2/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us to use one function to serialize most (any?) kubernetes resource. Some small perf hit of course for an added function call, and tail recursion. Formatting around comments sucks; if you have a standard python formatting config, please share; using vs code default for autopep8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071
https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:507,Security,authenticat,authenticated,507,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071
https://github.com/hail-is/hail/pull/5215#issuecomment-459893109:549,Security,authenticat,authentication,549,"> > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > ; > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py. There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459893109
https://github.com/hail-is/hail/pull/5215#issuecomment-459893784:295,Security,expose,expose,295,"Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what https://github.com/hail-is/hail/pull/5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459893784
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:499,Integrability,rout,route,499,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:277,Security,password,password,277,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:393,Security,access,access,393,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:134,Testability,test,test,134,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:229,Testability,log,logging,229,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:419,Testability,log,login,419,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:428,Testability,Log,Login,428,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:650,Deployability,update,updates,650,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:622,Energy Efficiency,consumption,consumption,622,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:813,Energy Efficiency,Green,Greenfield,813,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:1147,Integrability,interface,interface,1147,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:417,Safety,avoid,avoid,417,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:261,Security,expose,expose,261,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:1161,Security,authoriz,authorization,1161,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:1048,Testability,log,logic,1048,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641
https://github.com/hail-is/hail/pull/5215#issuecomment-460065939:1207,Deployability,deploy,deployments,1207,"> > > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > > ; > > ; > > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py; > ; > There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook. They are different enough at this point that tying one to the other doesn't make much sense to me. But the bigger reason is that if needed, it will be easier to restore notebook from a pristine file, then look back through git commit history to the first breaking change. The goal should always be to introduce as few breaking changes as possible before a public demonstration. Once that is done, I don't mind doing something else. I've had the unfortunate pleasure of doing this with large-ish user-facing deployments (thousands of users), and it's not a fun experience. Git works well, but under the pressure of public-facing issues, entropy is not a friend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939
https://github.com/hail-is/hail/pull/5215#issuecomment-460065939:569,Security,authenticat,authentication,569,"> > > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > > ; > > ; > > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py; > ; > There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook. They are different enough at this point that tying one to the other doesn't make much sense to me. But the bigger reason is that if needed, it will be easier to restore notebook from a pristine file, then look back through git commit history to the first breaking change. The goal should always be to introduce as few breaking changes as possible before a public demonstration. Once that is done, I don't mind doing something else. I've had the unfortunate pleasure of doing this with large-ish user-facing deployments (thousands of users), and it's not a fun experience. Git works well, but under the pressure of public-facing issues, entropy is not a friend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939
https://github.com/hail-is/hail/pull/5215#issuecomment-460099264:189,Safety,safe,safely,189,"Everything now works, well enough for a demo. Outstanding issues:; 1) auth_request during redirect. currently. disabled as. it doesn't handle post-redirect requests. handle these requests. safely; 2) watch MODIFIED events. in. a finer grained way: need to inspect the container for readiness; status.phase, does not provide a sufficient aggregate picture.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460099264
https://github.com/hail-is/hail/pull/5215#issuecomment-463037660:68,Testability,test,tested,68,@danking @cseed I think all comments addressed. . This has all been tested on the production cluster. up to https://github.com/hail-is/hail/pull/5215/commits/d2fa301644eda2434f9cfa679a452b8e24117a04,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-463037660
https://github.com/hail-is/hail/pull/5215#issuecomment-463477132:233,Integrability,wrap,wrapped,233,"@danking Thanks for the great comments! I've responded to all of them, and have begun addressing them. The rest I will either need to do a bit more research on, or potentially speak with you in person about. I think we can have this wrapped up tomorrow though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-463477132
https://github.com/hail-is/hail/pull/5215#issuecomment-463477507:35,Deployability,deploy,deployment,35,"btw, I think we can get rid of the deployment's notebook-secrets, wanted to double check (I believe it just handled basic auth).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-463477507
https://github.com/hail-is/hail/pull/5215#issuecomment-464203521:91,Testability,test,tested,91,"> It looks like this deletes notebook. Please do NOT delete notebook. Right now we have no tested alternative and @tpoterba and I need to use it for a workshop in two weeks. I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. What do you mean by tested? I thought we established that the version of notebook I made worked... it worked for the Feb 5th demo, it worked for Jackie's demo?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464203521
https://github.com/hail-is/hail/pull/5215#issuecomment-464203521:494,Testability,test,tested,494,"> It looks like this deletes notebook. Please do NOT delete notebook. Right now we have no tested alternative and @tpoterba and I need to use it for a workshop in two weeks. I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. What do you mean by tested? I thought we established that the version of notebook I made worked... it worked for the Feb 5th demo, it worked for Jackie's demo?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464203521
https://github.com/hail-is/hail/pull/5215#issuecomment-464205479:126,Testability,log,login,126,"Yeah this is my bad, I thought we were good to go with the new UI @cseed ? Are you referring to a workshop mode wherein users login via OAuth but and are permitted despite not being on the whitelist if they provide a special passphrase?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464205479
https://github.com/hail-is/hail/pull/5215#issuecomment-464226078:125,Testability,test,tested,125,"> Yeah this is my bad, I thought we were good to go with the new UI @cseed ?. Yes, eventually. But the old code works and is tested. The new stuff isn't done, reviewed or in yet. There's no sense deleting the working thing until the new thing is ready to replace it, esp. with a workshop coming up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464226078
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:846,Availability,failure,failures,846,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:894,Availability,avail,available,894,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:527,Deployability,integrat,integrated,527,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:597,Deployability,deploy,deployments,597,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:527,Integrability,integrat,integrated,527,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:874,Performance,latency,latency,874,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:1208,Security,password,password,1208,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:758,Testability,test,tested,758,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:1138,Testability,log,login,1138,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:1179,Testability,log,login,1179,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358
https://github.com/hail-is/hail/pull/5215#issuecomment-464232124:308,Security,validat,validated,308,"@danking Could you please look at the comments and changes made to this repo? I would also like to focus on making the smallest set of changes necessary. Some of the comments appear to be better suited for future PRs, for instance differences in architectural preferences (whether or not a user_id should be validated at this layer, whether the marshaling function could be written to more resemble something you find idiomatic) that don't affect the ability of the web client to consume a valid response. . This PR is getting quite hard to follow, so I'd like to get a summary of what remaining needs to be addressed for you to accept.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464232124
https://github.com/hail-is/hail/issues/5221#issuecomment-459112810:30,Availability,error,error,30,"Both these fail with the same error:; ```; broken_ht = hl.import_table('../data/bikes.csv'); broken_ht = hl.import_table('../data/bikes.csv', delimiter=';'); ```. I think this is an encoding issue. This file is encoded with latin-1 and contains French diacritics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221#issuecomment-459112810
https://github.com/hail-is/hail/issues/5221#issuecomment-459112814:37,Integrability,message,message,37,looks like this is a weird character message maybe? There are some non-ascii chars in there,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5221#issuecomment-459112814
https://github.com/hail-is/hail/pull/5228#issuecomment-461226020:17,Availability,ping,ping,17,@patrick-schultz ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5228#issuecomment-461226020
https://github.com/hail-is/hail/issues/5236#issuecomment-459844874:9,Usability,clear,clear,9,It's not clear we should do this instead of just making the service work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236#issuecomment-459844874
https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:120,Availability,error,errors,120,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115
https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:369,Integrability,interface,interface,369,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115
https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:151,Testability,benchmark,benchmarks,151,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115
https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:409,Testability,benchmark,benchmarks,409,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115
https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:444,Testability,test,test,444,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:568,Modifiability,variab,variables,568,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:173,Performance,perform,performance,173,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:833,Performance,queue,queues,833,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:1796,Performance,cache,cache,1796,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:1865,Performance,queue,queue,1865,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:1558,Testability,benchmark,benchmarks,1558,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:2075,Testability,test,test,2075,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:230,Usability,simpl,simpler,230,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:2115,Usability,simpl,simple,2115,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:294,Performance,perform,performs,294,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082
https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:319,Performance,latency,latency,319,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082
https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:332,Performance,throughput,throughput,332,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082
https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:120,Testability,benchmark,benchmarks,120,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082
https://github.com/hail-is/hail/pull/5242#issuecomment-461202032:66,Performance,queue,queue,66,"> 2. It felt outside of the scope of my PR to change that to some queue solution. OK, great, thanks for clarifying!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461202032
https://github.com/hail-is/hail/pull/5242#issuecomment-461210710:68,Performance,queue,queue,68,"> > 1. It felt outside of the scope of my PR to change that to some queue solution.; > ; > OK, great, thanks for clarifying!. Sure, this is something I am focusing on improving moving forward. Thanks for your feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710
https://github.com/hail-is/hail/pull/5242#issuecomment-461210710:209,Usability,feedback,feedback,209,"> > 1. It felt outside of the scope of my PR to change that to some queue solution.; > ; > OK, great, thanks for clarifying!. Sure, this is something I am focusing on improving moving forward. Thanks for your feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3069,Availability,error,errors,3069,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4083,Availability,error,errors,4083,"t look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4548,Availability,error,errors,4548,"y curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, tim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5068,Availability,error,errors,5068,"nect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg St",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5532,Availability,error,errors,5532,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5884,Availability,error,errors,5884,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6236,Availability,error,errors,6236,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3625,Deployability,upgrade,upgraded,3625,"http library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 72",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2934,Energy Efficiency,power,power,2934,"d about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6360,Energy Efficiency,reduce,reduce,6360,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:314,Performance,perform,performs,314,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:339,Performance,latency,latency,339,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:352,Performance,throughput,throughput,352,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:735,Performance,perform,performance,735,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:804,Performance,perform,performance,804,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:816,Performance,bottleneck,bottleneck,816,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1458,Performance,perform,performance,1458," a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference stand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1562,Performance,perform,performance,1562,". The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1705,Performance,perform,performance,1705,"eators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1750,Performance,perform,performance,1750,"y key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2136,Performance,perform,performance,2136," <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2451,Performance,perform,performance,2451,"n's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/respo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2578,Performance,perform,performance,2578,"bility and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2613,Performance,perform,performance,2613,"bility and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3960,Performance,Latency,Latency,3960,"u linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4425,Performance,Latency,Latency,4425,"ams that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4945,Performance,Latency,Latency,4945,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5412,Performance,Latency,Latency,5412,"alhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5761,Performance,Latency,Latency,5761,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6113,Performance,Latency,Latency,6113,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3017,Safety,timeout,timeouts,3017,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3163,Safety,timeout,timeouts,3163,"vicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3760,Safety,timeout,timeouts,3760,"loop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3795,Safety,timeout,timeouts,3795,"wer than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4122,Safety,timeout,timeout,4122,"t look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4274,Safety,timeout,timeout,4274,"ams that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4587,Safety,timeout,timeout,4587,"y curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, tim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4794,Safety,timeout,timeout,4794,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5107,Safety,timeout,timeout,5107,"nect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg St",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5261,Safety,timeout,timeout,5261,"alhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5571,Safety,timeout,timeout,5571,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5923,Safety,timeout,timeout,5923,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6275,Safety,timeout,timeout,6275,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:128,Testability,benchmark,benchmarks,128,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2324,Testability,test,tested,2324,"me time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that funda",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2380,Testability,benchmark,benchmarks,2380,"me time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that funda",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2425,Testability,benchmark,benchmark,2425,"n's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/respo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2511,Testability,benchmark,benchmarks,2511,"n I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aioh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2889,Testability,benchmark,benchmark,2889,"d about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2917,Testability,benchmark,benchmarking,2917,"d about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3132,Testability,test,tests,3132,"rly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3678,Testability,test,test,3678," and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large ba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3856,Testability,test,test,3856,"u linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4321,Testability,test,test,4321,"ams that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4841,Testability,test,test,4841,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5308,Testability,test,test,5308,"alhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5657,Testability,test,test,5657,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6009,Testability,test,test,6009,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1474,Usability,user experience,user experience,1474," a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference stand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1631,Usability,Usab,Usability,1631,"azing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp libra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2654,Usability,simpl,simple,2654,"bility and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:174,Availability,down,download,174,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:330,Availability,down,download,330,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:918,Availability,avail,available,918,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:390,Deployability,install,installing,390,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:661,Deployability,install,installs,661,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:744,Deployability,install,install,744,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:833,Deployability,install,installed,833,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:949,Deployability,install,installation,949,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:802,Integrability,depend,depends,802,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2146,Integrability,rout,router,2146,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1234,Modifiability,config,configured,1234,"pistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1300,Modifiability,config,config,1300,"pistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1750,Modifiability,config,configure,1750,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:382,Performance,cache,caches,382,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:573,Performance,Perform,Performance,573,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:790,Performance,perform,performance,790,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:854,Performance,optimiz,optimizations,854,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1027,Performance,optimiz,optimized,1027,"ding. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but canno",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1394,Performance,perform,performance,1394,"difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bott",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2164,Performance,cache,caches,2164,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2366,Performance,cache,cache,2366,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2411,Performance,optimiz,optimization,2411,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2466,Performance,bottleneck,bottleneck,2466,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2482,Performance,optimiz,optimization,2482,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1880,Testability,Benchmark,Benchmark,1880,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1919,Testability,benchmark,benchmarks,1919,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1954,Testability,test,test,1954,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:2059,Testability,test,tests,2059,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:201,Availability,error,error,201,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:474,Performance,load,load,474,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:562,Safety,timeout,timeouts,562,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:180,Testability,test,test,180,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:609,Testability,benchmark,benchmark,609,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:902,Testability,test,tests,902,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:1011,Testability,benchmark,benchmarks,1011,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481
https://github.com/hail-is/hail/pull/5242#issuecomment-461451746:6,Usability,clear,clear,6,"To be clear, I'm working on *aiohttp* project, not *Sanic*.; Regarding TechEmpower -- I did not investigate.; Maybe the problem is trivial, maybe it is fixed on master. ; IIRC Sanic has a partial Flow-Control/HTTP-Pipelining implementation now but I'm not 100% sure.; I have many points to apply my spare time, Sanic problems are not in my TOP-10 personal list. I hope you understand my position.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461451746
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:696,Integrability,rout,router,696,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:1211,Integrability,rout,route,1211,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:153,Performance,perform,performance,153,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:714,Performance,cache,caches,714,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:916,Performance,cache,cache,916,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:1021,Performance,perform,performance,1021,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:1081,Performance,bottleneck,bottleneck,1081,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:1137,Performance,bottleneck,bottlenecked,1137,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:941,Usability,simpl,simple,941,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5242#issuecomment-461455096:1239,Usability,clear,clear,1239,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:903,Availability,down,down,903,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:1871,Availability,down,down,1871,Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6Ik16YzNRekpFUXpWRk5VSXdPRE0yTmpJMFF6VkZPVVk1TkRZME9UZzJOa00xUkRBek1ERTJOZyJ9.eyJpc3MiOiJodHRwczovL2hhaWwuYXV0aDAuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTEwNzI2NTIxOTIxMjQ5NDQzNzYwIiwiYXVkIjpbImhhaWwiLCJodHRwczovL2hhaWwuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTU0OTIzMDI2MSwiZXhwIjoxNTQ5MjM3NDYxLCJhenAiOiJURDc4azIzQ2NkTTRwTVdvWVp3WXdLSmJRUEJqMDZqWSIsInNjb3BlIjoib3BlbmlkIHByb2ZpbGUifQ.p3HjkP5t3xrGMGOG8kkCocRCg6BRSrGiO_ymwjqQt-omgk55KnObZCJXFX20BM6n6azzNvF_8EpruB3iSRAFiuhwVvHyabwvRpSZAy3giOpYyxgnj4mPlphdAF9c0yduIU-VpLA6ifaqF9Tj69pfMlFfdjo5ku1tkJnRIkysWYB58bXCqRp9dYSYxZZ45X52YOoP_VrnyyIWX4AvZnp-1Cy9nssFV6l6j2PJmvqkMPLR0suS-lR6NK6PMRRiOessKZy3SXwLJv1oLhJW7qFFEb8kP9pG7zoW0v-TpP9f-XBH0UE9WNaIyur0QOU80qsUa7CmjMdLoi7klDBqdfx-Mg&token=ef8707c52ba1439e9b7ebf78e136075d (10.32.13.94) 0.85ms; alexkotlar:~/projects/hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:327,Deployability,update,update,327,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:1363,Performance,load,loaded,1363,) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6Ik16YzNRekpFUXpWRk5VSXdPRE0yTmpJMFF6VkZPVVk1TkRZME9UZzJOa00xUkRBek1ERTJOZyJ9.eyJpc3MiOiJodHRwczovL2hhaWwuYXV0aDAuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTEwNzI2NTIxOTIxMjQ5NDQzNzYwIiwiYXVkIjpbImhhaWwiLCJodHRwczovL2hhaWwuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTU0OTIzMDI2MSwiZXhwIjoxNTQ5MjM3NDYxLCJhenAiOiJURDc4azI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:134,Testability,Test,Tested,134,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:413,Testability,log,logs,413,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:255,Usability,clear,clearly,255,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
https://github.com/hail-is/hail/pull/5243#issuecomment-460094726:200,Security,authoriz,authorization,200,"Ok, figured out why Dan's image wasn't wholly working; the auth_request works, but then there are series of other requests, which lose the access_token. Will figure this out; Ideally once the initial authorization is made, requests for the nth css file don't require it. I've also enabled ssl for app.hail.is, and notebook-api.hail.is, so any issues due to crossing https to http (and vice versa boundaries), won't crop up (if they didn't exist in an existing implementation, since all existing hail services are behind ssl).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460094726
https://github.com/hail-is/hail/pull/5243#issuecomment-460095986:26,Availability,error,error,26,"I don't think the browser error should pose a problem, it just failed to open a browser. Jupyter should still be running. I run it with `--no-browser`. I'm not sure how Dan's image managed that without the option, I will investigate. You can't connect to my image because it is binding localhost. You can pass `--ip 0.0.0.0` or I can figure out how Dan's image is doing it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460095986
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1564,Deployability,install,installed,1564,"-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1706,Deployability,update,update,1706," image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1724,Deployability,install,install,1724," image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1817,Deployability,install,install,1817,"d6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:2002,Deployability,install,install,2002,"ng Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:2070,Deployability,install,install,2070,"ng Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:4291,Deployability,install,install,4291,"ser move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS; ); pod_spec = kube.client.V1PodSpec(; containers=[; kube.client.V1Container(; command=[; 'jupyter',; 'notebook',; f'--NotebookApp.token={jupyter_token}',; f'--NotebookApp.base_url=/instance/{svc.metadata.name}/'; ],; name='default',; image=image,; ports=[kube.client.V1ContainerPort(container_port=8888)],; resources=kube.client.V1ResourceRequirements(; requests={'cpu': '1.601', 'memory': '1.601G'}),; readiness_probe=kube.client.V1Probe(; http_get=kube.client.V1HTTPGetAction(; path=f'/instance/{svc.metadata.name}/login',; port=8888)))]); pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; 'svc_name': svc.metadata.name,; **labels; },),; spec=pod_spec); pod = k8s.create_namespaced_pod(; 'default',; pod_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS,; ). return svc, pod; ``` . In the pod definition he runs `jupyter notebook --NotebookApp.token={jupyter_token} --NotebookApp.base_url=/instance/{svc.metadata.name}/` which overrides yours. I'm going to try to run headless mode, and if not, maybe we should install jupyterlab?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:208,Energy Efficiency,Schedul,Scheduled,208,"My concern was that not finding a browser when expected would prevent the server from accepting connections, but you may be right. Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 1m default-scheduler Successfully assigned notebook-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:229,Energy Efficiency,schedul,scheduler,229,"My concern was that not finding a browser when expected would prevent the server from accepting connections, but you may be right. Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 1m default-scheduler Successfully assigned notebook-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:161,Integrability,Message,Message,161,"My concern was that not finding a browser when expected would prevent the server from accepting connections, but you may be right. Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 1m default-scheduler Successfully assigned notebook-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1830,Performance,cache,cache-dir,1830,"d6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1279,Testability,log,login,1279,"book-worker-9szt8 to gke-vdc-non-preemptible-pool-0106a51b-pgxq; Normal SuccessfulMountVolume 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq MountVolume.SetUp succeeded for volume ""default-token-xl2w9""; Normal Pulling 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:3634,Testability,log,login,3634,"ser move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS; ); pod_spec = kube.client.V1PodSpec(; containers=[; kube.client.V1Container(; command=[; 'jupyter',; 'notebook',; f'--NotebookApp.token={jupyter_token}',; f'--NotebookApp.base_url=/instance/{svc.metadata.name}/'; ],; name='default',; image=image,; ports=[kube.client.V1ContainerPort(container_port=8888)],; resources=kube.client.V1ResourceRequirements(; requests={'cpu': '1.601', 'memory': '1.601G'}),; readiness_probe=kube.client.V1Probe(; http_get=kube.client.V1HTTPGetAction(; path=f'/instance/{svc.metadata.name}/login',; port=8888)))]); pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; 'svc_name': svc.metadata.name,; **labels; },),; spec=pod_spec); pod = k8s.create_namespaced_pod(; 'default',; pod_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS,; ). return svc, pod; ``` . In the pod definition he runs `jupyter notebook --NotebookApp.token={jupyter_token} --NotebookApp.base_url=/instance/{svc.metadata.name}/` which overrides yours. I'm going to try to run headless mode, and if not, maybe we should install jupyterlab?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218
https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:317,Deployability,update,updates,317,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832
https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:82,Modifiability,config,config,82,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832
https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:259,Security,EXPOSE,EXPOSE,259,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832
https://github.com/hail-is/hail/pull/5244#issuecomment-460333195:232,Deployability,upgrade,upgrade,232,"I'm losing track of all the threads. Trying to summarize:; - Leave the CORS stuff, I don't understand it well enough to have an opinion anyway.; - Global gzip settings with gzip_min_length.; - Back out Docker changes, separately PR upgrade to nginx on Debian (would be my preference).; - Leave auth notebook commented out (although it makes me uncomfortable) and let's keep discussing how to solve it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460333195
https://github.com/hail-is/hail/pull/5244#issuecomment-460338546:244,Deployability,upgrade,upgrade,244,"> I'm losing track of all the threads. Trying to summarize:; > ; > * Leave the CORS stuff, I don't understand it well enough to have an opinion anyway.; > * Global gzip settings with gzip_min_length.; > * Back out Docker changes, separately PR upgrade to nginx on Debian (would be my preference).; > * Leave auth notebook commented out (although it makes me uncomfortable) and let's keep discussing how to solve it. Got it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460338546
https://github.com/hail-is/hail/pull/5244#issuecomment-460363335:274,Modifiability,config,config,274,"> I'm happy to switch to debian:9.6 (that's the same as debian:stretch). But as far as I can tell, it has 1.10.3. Why not use the official docker image nginx:1.15.8? It does everything our custom Dockerfile does, pins the version, and removes 50% of the lines in our custom config. The only thing that I see it not doing that we may want is the jwt auth request module, but that isn't needed currently. https://github.com/nginxinc/docker-nginx/blob/baa050df601b5e798431a9db458e16f53b1031f6/mainline/stretch/Dockerfile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460363335
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:996,Availability,error,error,996,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:541,Deployability,update,updates,541,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:699,Deployability,update,update,699,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:723,Deployability,install,install,723,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:618,Modifiability,config,config,618,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:946,Security,access,access,946,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:936,Testability,log,log,936,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:953,Testability,log,log,953,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:986,Testability,log,log,986,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:1002,Testability,log,log,1002,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467
https://github.com/hail-is/hail/pull/5244#issuecomment-460386902:41,Deployability,update,updates,41,"> I understand the value of conservative updates before public demonstrations. I don't see this as conservative, and I don't think it has to do with the demo. It is to make incremental changes and disentangle unrelated changes. You should think of a PR as being lightweight. There's no reason not to create lots of them. In fact, by creating lots of them, you increase your velocity by removing false dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902
https://github.com/hail-is/hail/pull/5244#issuecomment-460386902:401,Integrability,depend,dependencies,401,"> I understand the value of conservative updates before public demonstrations. I don't see this as conservative, and I don't think it has to do with the demo. It is to make incremental changes and disentangle unrelated changes. You should think of a PR as being lightweight. There's no reason not to create lots of them. In fact, by creating lots of them, you increase your velocity by removing false dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902
https://github.com/hail-is/hail/pull/5245#issuecomment-464144563:42,Energy Efficiency,power,powerful,42,the two rewrites have made this much more powerful and understandable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563
https://github.com/hail-is/hail/pull/5245#issuecomment-464144563:8,Modifiability,rewrite,rewrites,8,the two rewrites have made this much more powerful and understandable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563
https://github.com/hail-is/hail/pull/5247#issuecomment-461285927:48,Usability,guid,guide,48,"> what's the argument for a Notes. [Numpy style guide](https://numpydoc.readthedocs.io/en/latest/format.html) indicates that the notes section is the appropriate place for detail on the algorithm. In particular, the types of the column and entry fields produced aren't directly related to their names, and having them in the parameter description is unintuitive. This style is also inconsistent with the rest of our documentation. In reading the above link, I've realized that all our docs sections are out of order - the Parameters section should come first, then Returns, then Notes, then Examples. We were hoodwinked by the [Sphinx example page](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html). I also strongly reject a warning about a possible future scaling limitation of the function that might never even exist in the life of 0.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5247#issuecomment-461285927
https://github.com/hail-is/hail/pull/5249#issuecomment-460502414:54,Testability,test,tests,54,"You're the lucky winner @jbloom22 . ~Also, weird, the tests are failing, but in code I didn't write...~",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5249#issuecomment-460502414
https://github.com/hail-is/hail/pull/5250#issuecomment-460729545:20,Testability,test,test,20,@tpoterba I added a test and an issue that described the problem. I verified the test fails on `dcf43490c` a recent `master` commit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-460729545
https://github.com/hail-is/hail/pull/5250#issuecomment-460729545:81,Testability,test,test,81,@tpoterba I added a test and an issue that described the problem. I verified the test fails on `dcf43490c` a recent `master` commit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-460729545
https://github.com/hail-is/hail/pull/5250#issuecomment-461123275:100,Availability,toler,tolerances,100,@jbloom22 can you give the latest commit on this PR a second set of eyes? I had to fiddle with some tolerances due to https://storage.googleapis.com/hail-ci-0-1/ci/7a0732726e6873e2c0d85fed5183324ac9441d52/194ea22cd9f744a5463340130e799c8a65ca885e/index.html . I expected the test I added (`test_pcrelate_issue_5263`) to be exactly the same but it differed out at the 10th or so position.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461123275
https://github.com/hail-is/hail/pull/5250#issuecomment-461123275:274,Testability,test,test,274,@jbloom22 can you give the latest commit on this PR a second set of eyes? I had to fiddle with some tolerances due to https://storage.googleapis.com/hail-ci-0-1/ci/7a0732726e6873e2c0d85fed5183324ac9441d52/194ea22cd9f744a5463340130e799c8a65ca885e/index.html . I expected the test I added (`test_pcrelate_issue_5263`) to be exactly the same but it differed out at the 10th or so position.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461123275
https://github.com/hail-is/hail/pull/5250#issuecomment-461544717:7,Performance,cache,cache,7,"if you cache the dataset (or set the global seed and generate it, both for the actual and the expected), do you get exactly the same result in your new test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461544717
https://github.com/hail-is/hail/pull/5250#issuecomment-461544717:152,Testability,test,test,152,"if you cache the dataset (or set the global seed and generate it, both for the actual and the expected), do you get exactly the same result in your new test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461544717
https://github.com/hail-is/hail/pull/5251#issuecomment-461245241:144,Usability,clear,clear,144,"I agree completely. I certainly don't think we can hide or replace Bokeh (I hope the explicit emphasis on Bokeh in the documentation makes this clear), but I think we should continue to add common-case utilities to `hl.plot` life easier for users (and ourselves).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5251#issuecomment-461245241
https://github.com/hail-is/hail/issues/5262#issuecomment-555090024:113,Availability,error,error,113,"This particular example works for me today in 0.2.27. When I upped M to 350, I instead got a Java stack overflow error like:. ```; java.lang.StackOverflowError: null; 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:98); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:204); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:35); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024
https://github.com/hail-is/hail/issues/5262#issuecomment-555090024:2491,Availability,error,error,2491,"tension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:204); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:35); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); ```; and keeps going for a while. So maybe this Python thing is fixed, and we need a new ticket for this Java error?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024
https://github.com/hail-is/hail/issues/5266#issuecomment-460788621:28,Testability,test,tests,28,I believe it is. I did some tests taking that part of my script out and they didn't get generated,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5266#issuecomment-460788621
https://github.com/hail-is/hail/issues/5269#issuecomment-461105202:134,Availability,avail,available,134,`kubectl describe pod POD_NAME` will tell you there reasons the pod could not be scheduled. I often see this issue when we run out of available CPU.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202
https://github.com/hail-is/hail/issues/5269#issuecomment-461105202:81,Energy Efficiency,schedul,scheduled,81,`kubectl describe pod POD_NAME` will tell you there reasons the pod could not be scheduled. I often see this issue when we run out of available CPU.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202
https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1019,Availability,alive,alive,1019,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857
https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1038,Availability,down,download,1038,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857
https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1256,Availability,down,download,1256,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857
https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1074,Energy Efficiency,schedul,schedule,1074,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857
https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1165,Performance,cache,cached,1165,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:214,Performance,load,load,214,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:388,Performance,load,load,388,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:219,Safety,predict,predictor,219,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:380,Safety,predict,predict,380,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:594,Safety,predict,predictive-auto-scaling-engine-part-,594,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:354,Usability,simpl,simple,354,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:361,Usability,learn,learning,361,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:134,Deployability,configurat,configuration,134,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913
https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:198,Deployability,deploy,deployment,198,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913
https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:134,Modifiability,config,configuration,134,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913
https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:634,Testability,test,test,634,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913
https://github.com/hail-is/hail/issues/5282#issuecomment-463364346:48,Deployability,deploy,deployed,48,"Gah, caching. The changes haven't actually been deployed by ci yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463364346
https://github.com/hail-is/hail/pull/5283#issuecomment-467525836:77,Testability,test,test,77,"@tpoterba last time I looked at this it was good to go, but failing on a bad test that I'm fixing with #5442. If you rebase I'll approve this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5283#issuecomment-467525836
https://github.com/hail-is/hail/pull/5305#issuecomment-462478878:14,Availability,toler,tolerance,14,I lowered the tolerance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462478878
https://github.com/hail-is/hail/pull/5305#issuecomment-462482330:20,Performance,cache,cache,20,why do you need the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462482330
https://github.com/hail-is/hail/pull/5307#issuecomment-462502047:35,Testability,log,logic,35,I'm also missing agglet extraction logic in StagedExtractAggs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5307#issuecomment-462502047
https://github.com/hail-is/hail/pull/5308#issuecomment-462480832:52,Testability,test,tested,52,The bigger problem is that this functionality isn't tested. Do we have a tiny dummy reference we can make a fasta for?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462480832
https://github.com/hail-is/hail/pull/5308#issuecomment-462494294:24,Testability,test,test,24,It's already there: src/test/resources/fake_reference.fasta,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462494294
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:19,Testability,test,test,19,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:168,Testability,assert,assertEqual,168,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:219,Testability,assert,assertFalse,219,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:421,Testability,assert,assertTrue,421,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:458,Testability,assert,assertTrue,458,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:565,Testability,assert,assertTrue,565,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/pull/5308#issuecomment-462500751:691,Testability,assert,assertTrue,691,"It looks like this test just needed to be modified:. ```; def test_reference_genome_sequence(self):; gr3 = ReferenceGenome.read(resource(""fake_ref_genome.json"")); self.assertEqual(gr3.name, ""my_reference_genome""); self.assertFalse(gr3.has_sequence()). gr4 = ReferenceGenome.from_fasta_file(""test_rg"", resource(""fake_reference.fasta""),; resource(""fake_reference.fasta.fai""),; mt_contigs=[""b"", ""c""], x_contigs=[""a""]); self.assertTrue(gr4.has_sequence()); self.assertTrue(gr4.x_contigs == [""a""]). t = hl.import_table(resource(""fake_reference.tsv""), impute=True); self.assertTrue(hl.eval(t.all(hl.get_sequence(t.contig, t.pos, reference_genome=gr4) == t.base))). l = hl.locus(""a"", 7, gr4); self.assertTrue(hl.eval(l.sequence_context(before=3, after=3) == ""TTTCGAA"")); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5308#issuecomment-462500751
https://github.com/hail-is/hail/issues/5320#issuecomment-479696405:132,Availability,down,downstream,132,I'm going to close this issue because I feel like we've moved past the sticking point that this issue is referring to. @konradjk if downstream operations are still having problems please feel free to open issues for them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5320#issuecomment-479696405
https://github.com/hail-is/hail/pull/5323#issuecomment-463207883:32,Deployability,install,install,32,Seems to be issue at end:. pip2 install ./; hail-ci-build.sh: line 27: pip2: command not found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5323#issuecomment-463207883
https://github.com/hail-is/hail/pull/5326#issuecomment-467585906:93,Deployability,configurat,configuration,93,"There's something wrong here and I think it has to do with how I'm passing around the hadoop configuration. Closing this since I'm in the process of changing how spark is called, and I'll reopen once I've fixed this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906
https://github.com/hail-is/hail/pull/5326#issuecomment-467585906:93,Modifiability,config,configuration,93,"There's something wrong here and I think it has to do with how I'm passing around the hadoop configuration. Closing this since I'm in the process of changing how spark is called, and I'll reopen once I've fixed this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906
https://github.com/hail-is/hail/pull/5330#issuecomment-463317409:9,Testability,Test,Tests,9,@danking Tests are passing!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5330#issuecomment-463317409
https://github.com/hail-is/hail/pull/5331#issuecomment-462992065:102,Usability,feedback,feedback,102,"I assigned @catoverdrive, but @cseed @tpoterba @chrisvittal may also be interested and/or have useful feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-462992065
https://github.com/hail-is/hail/pull/5331#issuecomment-463907719:280,Integrability,depend,dependencies,280,"After looking more at the build logs here, as I was curious as to how the tests passed, when `make` was run in compilation, it ran the target for `simd/simd.h`, which ran the `$(LIBSIMDPP)` target, which ran `tar xzf $(LIBSIMDPP).tar.gz`. Thus accidentally creating the necessary dependencies before the `test` target is run later in the CI build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719
https://github.com/hail-is/hail/pull/5331#issuecomment-463907719:32,Testability,log,logs,32,"After looking more at the build logs here, as I was curious as to how the tests passed, when `make` was run in compilation, it ran the target for `simd/simd.h`, which ran the `$(LIBSIMDPP)` target, which ran `tar xzf $(LIBSIMDPP).tar.gz`. Thus accidentally creating the necessary dependencies before the `test` target is run later in the CI build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719
https://github.com/hail-is/hail/pull/5331#issuecomment-463907719:74,Testability,test,tests,74,"After looking more at the build logs here, as I was curious as to how the tests passed, when `make` was run in compilation, it ran the target for `simd/simd.h`, which ran the `$(LIBSIMDPP)` target, which ran `tar xzf $(LIBSIMDPP).tar.gz`. Thus accidentally creating the necessary dependencies before the `test` target is run later in the CI build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719
https://github.com/hail-is/hail/pull/5331#issuecomment-463907719:305,Testability,test,test,305,"After looking more at the build logs here, as I was curious as to how the tests passed, when `make` was run in compilation, it ran the target for `simd/simd.h`, which ran the `$(LIBSIMDPP)` target, which ran `tar xzf $(LIBSIMDPP).tar.gz`. Thus accidentally creating the necessary dependencies before the `test` target is run later in the CI build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-463907719
https://github.com/hail-is/hail/pull/5331#issuecomment-464057989:51,Integrability,depend,dependency,51,"Response:. 1. `simdpp/simd.h`, I changed to a user dependency *and* changed the makefile to grab system dependencies, I agree this seems like a good idea (but I'm not sure, so we'll try it and find out!); 2. `mkdir -p`, I changed them all to `@`, but can `mkdir -p` fail (except for, say, filesystem being full)?; 3. `.DEFAULT_GOAL` good catch, I think we don't want `.DEFAULT` at all, I've removed it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-464057989
https://github.com/hail-is/hail/pull/5331#issuecomment-464057989:104,Integrability,depend,dependencies,104,"Response:. 1. `simdpp/simd.h`, I changed to a user dependency *and* changed the makefile to grab system dependencies, I agree this seems like a good idea (but I'm not sure, so we'll try it and find out!); 2. `mkdir -p`, I changed them all to `@`, but can `mkdir -p` fail (except for, say, filesystem being full)?; 3. `.DEFAULT_GOAL` good catch, I think we don't want `.DEFAULT` at all, I've removed it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-464057989
https://github.com/hail-is/hail/pull/5332#issuecomment-462999269:50,Testability,test,testing,50,"IMO, it's better to get this in with unsatisfying testing than to let it bit rot in a branch on my computer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-462999269
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:230,Availability,error,error,230,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:734,Availability,error,error,734,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:1002,Availability,error,error,1002,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:1066,Availability,error,error,1066,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:159,Performance,perform,performance,159,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:418,Safety,sanity check,sanity check,418,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:121,Testability,benchmark,benchmarking,121,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:620,Testability,benchmark,benchmarks,620,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:778,Testability,test,test,778,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:1017,Testability,benchmark,benchmarks,1017,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:1343,Testability,test,test,1343,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:387,Usability,simpl,simple,387,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:1310,Usability,simpl,simple,1310,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
https://github.com/hail-is/hail/issues/5340#issuecomment-463308529:29,Security,Hash,HashMap,29,"The reason we use `java.util.HashMap` there is that the [py4j](https://www.py4j.org/) library will automatically convert Python dicts to `java.util.HashMap` objects. . I don't think you should call this method, though. We are in the process of completely separating the front end Python from the back end execution engine. Soon, all IR execution from Python will be done by calling [`Backend.execute`](https://github.com/hail-is/hail/blob/master/hail/python/hail/backend/backend.py). The `SparkBackend.executeJSON` should be the target for IR execution right now, I think. *Edit: s/LocalBackend/SparkBackend/*",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463308529
https://github.com/hail-is/hail/issues/5340#issuecomment-463308529:148,Security,Hash,HashMap,148,"The reason we use `java.util.HashMap` there is that the [py4j](https://www.py4j.org/) library will automatically convert Python dicts to `java.util.HashMap` objects. . I don't think you should call this method, though. We are in the process of completely separating the front end Python from the back end execution engine. Soon, all IR execution from Python will be done by calling [`Backend.execute`](https://github.com/hail-is/hail/blob/master/hail/python/hail/backend/backend.py). The `SparkBackend.executeJSON` should be the target for IR execution right now, I think. *Edit: s/LocalBackend/SparkBackend/*",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463308529
https://github.com/hail-is/hail/issues/5340#issuecomment-463377623:185,Security,Hash,HashMap,185,Are you saying that a future version of `SparkBackend.executeJSON()` will support the substitution that currently happens through e.g. `parse_table_ir()` without expecting a `java.util.HashMap` to map symbols to Java IR objects?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463377623
https://github.com/hail-is/hail/issues/5340#issuecomment-463825757:23,Integrability,interface,interface,23,"Is there any way the R interface can do things the new way, or do things; need to happen on the Scala side first? Perhaps someone could notify us; when the API has relatively stabilized?. On Thu, Feb 14, 2019, 6:53 AM Tim Poterba <notifications@github.com wrote:. > Yes. We intend to have the Python frontend and scala backend communicating; > only through the methods on the Backend class in backend.py; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/5340#issuecomment-463655588>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AAJp7o6ZVZCdyRH4ixYHFdCItQ-wCy9kks5vNXg3gaJpZM4a583l>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463825757
https://github.com/hail-is/hail/issues/5340#issuecomment-463827896:4,Integrability,interface,interface,4,"The interface needs some work, first, but this is probably a ~3 month timeline (the outstanding calls into java are for things like maximal_independent_set, the BlockMatrix linear algebra stuff, and a few utility functions). I'm also happy to take PRs now to change the java.util.HashMaps to java.util.Map",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896
https://github.com/hail-is/hail/issues/5340#issuecomment-463827896:280,Security,Hash,HashMaps,280,"The interface needs some work, first, but this is probably a ~3 month timeline (the outstanding calls into java are for things like maximal_independent_set, the BlockMatrix linear algebra stuff, and a few utility functions). I'm also happy to take PRs now to change the java.util.HashMaps to java.util.Map",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896
https://github.com/hail-is/hail/issues/5345#issuecomment-466819238:143,Integrability,wrap,wrapper,143,"@cseed looking at the code it appears this aggregator doesn't support primitive types. This is intentional, right? Should we just add a little wrapper to Python to wrap the thing in a tuple or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466819238
https://github.com/hail-is/hail/issues/5345#issuecomment-466819238:164,Integrability,wrap,wrap,164,"@cseed looking at the code it appears this aggregator doesn't support primitive types. This is intentional, right? Should we just add a little wrapper to Python to wrap the thing in a tuple or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466819238
https://github.com/hail-is/hail/issues/5345#issuecomment-466821036:86,Energy Efficiency,efficient,efficient,86,"Ah, no, sorry, I just haven't written the other ones (which can be significantly more efficient) while working on the joint caller. Wrapping primitive types is an easy temporary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036
https://github.com/hail-is/hail/issues/5345#issuecomment-466821036:132,Integrability,Wrap,Wrapping,132,"Ah, no, sorry, I just haven't written the other ones (which can be significantly more efficient) while working on the joint caller. Wrapping primitive types is an easy temporary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036
https://github.com/hail-is/hail/pull/5354#issuecomment-463870141:703,Testability,test,tests,703,"Re, my review above, the PR builder uses . ```; # g++ --version; g++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516; Copyright (C) 2016 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Whereas my machine uses:; ```; $ g++ --version; g++ (Ubuntu 8.2.0-7ubuntu1) 8.2.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. ~~So there was a change in behavior between GCC versions. :man_shrugging:~~ EDIT: Or not. The tests passing is an artifact of the exact order commands are executed in the CI build, rather than the makefile working with older versions of make and g++. It should still work for all versions we would reasonably expect people to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-463870141
https://github.com/hail-is/hail/pull/5354#issuecomment-464034964:64,Integrability,depend,dependencies,64,What’s the deal with the ordering? I want to eliminate implicit dependencies,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464034964
https://github.com/hail-is/hail/pull/5354#issuecomment-464148672:63,Deployability,patch,patch,63,"Ah shit, that's in `-Wextra`? @chrisvittal do you know if that patch also passes the tests? I was worried that change would break something.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464148672
https://github.com/hail-is/hail/pull/5354#issuecomment-464148672:85,Testability,test,tests,85,"Ah shit, that's in `-Wextra`? @chrisvittal do you know if that patch also passes the tests? I was worried that change would break something.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464148672
https://github.com/hail-is/hail/pull/5354#issuecomment-464160988:59,Testability,test,test,59,"I think you can push to this branch directly, then CI will test for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464160988
https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:96,Availability,failure,failures,96,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754
https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:31,Performance,race condition,race conditions,31,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754
https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:64,Testability,test,test,64,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754
https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:128,Testability,test,test,128,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754
https://github.com/hail-is/hail/pull/5354#issuecomment-464188941:21,Deployability,install,install,21,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941
https://github.com/hail-is/hail/pull/5354#issuecomment-464188941:3,Usability,clear,clearly,3,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941
https://github.com/hail-is/hail/pull/5354#issuecomment-464203094:29,Usability,simpl,simple,29,Ok I think this works and is simple. @chrisvittal let me know what you think,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464203094
https://github.com/hail-is/hail/pull/5354#issuecomment-465246727:87,Testability,test,tests,87,We can't add it to CFLAGS because Hail expects it to be set to 1024. We use 256 in the tests to make the test cases easier to write.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-465246727
https://github.com/hail-is/hail/pull/5354#issuecomment-465246727:105,Testability,test,test,105,We can't add it to CFLAGS because Hail expects it to be set to 1024. We use 256 in the tests to make the test cases easier to write.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-465246727
https://github.com/hail-is/hail/pull/5354#issuecomment-465251564:55,Testability,test,testing,55,This is a special case target anyway for an old way of testing so I special cased it even further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-465251564
https://github.com/hail-is/hail/pull/5355#issuecomment-463775654:32,Testability,test,tests,32,@patrick-schultz I can add more tests if you think we need it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5355#issuecomment-463775654
https://github.com/hail-is/hail/issues/5358#issuecomment-526203548:117,Testability,test,tests,117,"Rmoving `prio:high` because I believe this is fixed (has been for a while), but I don't believe there are sufficient tests for the interaction of scans and aggregations yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5358#issuecomment-526203548
https://github.com/hail-is/hail/pull/5359#issuecomment-464059636:7,Testability,test,tests,7,@cseed tests are failing because you didn't check that the indexee was type tarray,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5359#issuecomment-464059636
https://github.com/hail-is/hail/pull/5361#issuecomment-463885950:41,Energy Efficiency,efficient,efficient,41,array_agg with hl.agg.sum is probably as efficient as this?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5361#issuecomment-463885950
https://github.com/hail-is/hail/pull/5363#issuecomment-479897542:5,Integrability,depend,depends,5,This depends on #5479 now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5363#issuecomment-479897542
https://github.com/hail-is/hail/pull/5369#issuecomment-465230568:71,Testability,test,tests,71,"@danking Do you want dockerfile, makefile in this pr? Do you want unit tests in this PR? Or in separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465230568
https://github.com/hail-is/hail/pull/5369#issuecomment-465392000:23,Deployability,integrat,integration,23,"@danking I have unit + integration tests for 2 methods in this package. Do you want me to add them here, or in a separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000
https://github.com/hail-is/hail/pull/5369#issuecomment-465392000:23,Integrability,integrat,integration,23,"@danking I have unit + integration tests for 2 methods in this package. Do you want me to add them here, or in a separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000
https://github.com/hail-is/hail/pull/5369#issuecomment-465392000:35,Testability,test,tests,35,"@danking I have unit + integration tests for 2 methods in this package. Do you want me to add them here, or in a separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000
https://github.com/hail-is/hail/pull/5369#issuecomment-465624040:42,Testability,test,tests,42,"@danking Comments addressed. I also added tests for the getAuthToken function. I was on the fence about adding that to this pr, but thought it would make it easier for you to verify that substring is acting appropriately, and to see what I had in mind for query handling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465624040
https://github.com/hail-is/hail/pull/5369#issuecomment-466130520:0,Availability,ping,ping,0,ping @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-466130520
https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:0,Availability,Down,Downgrading,0,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703
https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:134,Availability,down,downloading,134,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703
https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:1052,Energy Efficiency,allocate,allocated,1052,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703
https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:1203,Energy Efficiency,allocate,allocate,1203,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703
https://github.com/hail-is/hail/issues/5371#issuecomment-690852316:200,Availability,down,downloading,200,"#9435 may at long last, finally be a solution to this. We currently keep a static map per jvm that maps general FASTA paths to a local fasta path that we copy the FASTA into. My change serializes the downloading of these files (using a lock), and we never remove keys from the map, so we should finally have 1 FASTA file per executor. There are still circumstances that can blow up on us like restarting executors (like when yarn shuts them down) and then starts them up again. The broadcast changes fixed some of the issues, and I hope that my change can finally fix this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-690852316
https://github.com/hail-is/hail/issues/5371#issuecomment-690852316:441,Availability,down,down,441,"#9435 may at long last, finally be a solution to this. We currently keep a static map per jvm that maps general FASTA paths to a local fasta path that we copy the FASTA into. My change serializes the downloading of these files (using a lock), and we never remove keys from the map, so we should finally have 1 FASTA file per executor. There are still circumstances that can blow up on us like restarting executors (like when yarn shuts them down) and then starts them up again. The broadcast changes fixed some of the issues, and I hope that my change can finally fix this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-690852316
https://github.com/hail-is/hail/pull/5375#issuecomment-464193120:27,Usability,Clear,Clearly,27,"Talked with jon, summary:. Clearly there are people who need larger block sizes due to data size and partition limits. There are also people who try to use the default block size with smaller data sets and without highmem machines. These people usually have a bad time. We should pay attention to reports of this and figure out how we can better educate people on using smaller block sizes for smaller data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5375#issuecomment-464193120
https://github.com/hail-is/hail/pull/5383#issuecomment-464850638:45,Testability,test,tests,45,"This is overly aggressive until the relevant tests have been ported to Python. It is an easy change, and I'm going to back off for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5383#issuecomment-464850638
https://github.com/hail-is/hail/pull/5385#issuecomment-464850674:45,Testability,test,tests,45,"This is overly aggressive until the relevant tests have been ported to Python. It is an easy change, and I'm going to back off for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5385#issuecomment-464850674
https://github.com/hail-is/hail/pull/5385#issuecomment-464851281:45,Testability,test,tests,45,"This is overly aggressive until the relevant tests have been ported to Python. It is an easy change, and I'm going to back off for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5385#issuecomment-464851281
https://github.com/hail-is/hail/pull/5386#issuecomment-466480945:85,Integrability,depend,depends,85,I have one thing left to do: I need to rebuild pr-builder because the apiserver test depends on Flask. I'll do that shortly. Rest of it is ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386#issuecomment-466480945
https://github.com/hail-is/hail/pull/5386#issuecomment-466480945:80,Testability,test,test,80,I have one thing left to do: I need to rebuild pr-builder because the apiserver test depends on Flask. I'll do that shortly. Rest of it is ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5386#issuecomment-466480945
https://github.com/hail-is/hail/issues/5390#issuecomment-542853488:48,Testability,test,tests,48,This is a bump. . Also need to write AS Parsing tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5390#issuecomment-542853488
https://github.com/hail-is/hail/issues/5406#issuecomment-466484251:47,Integrability,interface,interfaces,47,I think there's another way in which the union interfaces are bad - they don't reorder top-level struct fields. This isn't especially hard to do and would be a huge QoL improvement. We could also reorder column values automatically.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5406#issuecomment-466484251
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1116,Availability,ERROR,ERROR,1116,"happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1847,Availability,Error,Error,1847,"col.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1931,Availability,Error,Error,1931,"GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; retur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3928,Availability,error,error,3928,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:793,Integrability,Protocol,Protocol,793,"(This definitely feels like a problem. Took three minutes to serialize an array with 10 million ints and with 40 million ints, this happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:812,Integrability,Protocol,Protocol,812,"(This definitely feels like a problem. Took three minutes to serialize an array with 10 million ints and with 40 million ints, this happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:841,Integrability,Protocol,Protocol,841,"(This definitely feels like a problem. Took three minutes to serialize an array with 10 million ints and with 40 million ints, this happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:860,Integrability,Protocol,Protocol,860,"(This definitely feels like a problem. Took three minutes to serialize an array with 10 million ints and with 40 million ints, this happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1386,Integrability,protocol,protocol,1386,"on in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1904,Integrability,protocol,protocol,1904,"mmand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:2194,Integrability,wrap,wrapper,2194,"b/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:2513,Integrability,wrap,wrapper,2513,"park-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3855,Integrability,protocol,protocol,3855,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3905,Integrability,protocol,protocol,3905,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184
https://github.com/hail-is/hail/pull/5412#issuecomment-466476158:120,Usability,clear,clear,120,"@danking this should be good to go. Works. In future PR, should we place move gateway to last line of projects.txt? Not clear to me if CI is enforcing gateway-last in a different way",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5412#issuecomment-466476158
https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:18,Deployability,deploy,deploy,18,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158
https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:84,Deployability,configurat,configuration,84,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158
https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:84,Modifiability,config,configuration,84,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158
https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:213,Security,encrypt,encrypt,213,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158
https://github.com/hail-is/hail/pull/5414#issuecomment-466549827:35,Deployability,update,updated,35,Has `_prev_nonnull` in python been updated to use the new `Aggregator2`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466549827
https://github.com/hail-is/hail/pull/5414#issuecomment-466563704:57,Testability,benchmark,benchmark,57,"Nevermind, I can see that it hasn't. How do we expect to benchmark these?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466563704
https://github.com/hail-is/hail/pull/5414#issuecomment-466570246:64,Testability,benchmark,benchmark,64,"Oops, I had meant to include that. Fixed. > How do we expect to benchmark these?. I was planning to measure densify/force_count on your test spare MatrixTable. I was planning to finish the remaining staging improvements first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466570246
https://github.com/hail-is/hail/pull/5414#issuecomment-466570246:136,Testability,test,test,136,"Oops, I had meant to include that. Fixed. > How do we expect to benchmark these?. I was planning to measure densify/force_count on your test spare MatrixTable. I was planning to finish the remaining staging improvements first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466570246
https://github.com/hail-is/hail/pull/5414#issuecomment-466625216:213,Performance,load,loading,213,"Should be good to go. There were two problems:. I needed to make the encoder/decoder `@transient lazy`. The encoder/decoder call generated code but can't be serialized. The make functions handle serialization and loading of the generated code. Also, RegionValueAggregators used in scans can have result called multiple times, so I needed to add a MemoryBuffer.clearPos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216
https://github.com/hail-is/hail/pull/5414#issuecomment-466625216:360,Usability,clear,clearPos,360,"Should be good to go. There were two problems:. I needed to make the encoder/decoder `@transient lazy`. The encoder/decoder call generated code but can't be serialized. The make functions handle serialization and loading of the generated code. Also, RegionValueAggregators used in scans can have result called multiple times, so I needed to add a MemoryBuffer.clearPos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216
https://github.com/hail-is/hail/pull/5416#issuecomment-466480355:62,Performance,perform,performance,62,"Oh, good catch. This is something we can also test for in the performance test suite: compute and use twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5416#issuecomment-466480355
https://github.com/hail-is/hail/pull/5416#issuecomment-466480355:46,Testability,test,test,46,"Oh, good catch. This is something we can also test for in the performance test suite: compute and use twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5416#issuecomment-466480355
https://github.com/hail-is/hail/pull/5416#issuecomment-466480355:74,Testability,test,test,74,"Oh, good catch. This is something we can also test for in the performance test suite: compute and use twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5416#issuecomment-466480355
https://github.com/hail-is/hail/pull/5418#issuecomment-466544014:5,Integrability,message,message,5,That message is caused by returning an `int` from a Flask request handler. Looks like the endpoint is called `/test`. I don't see that in the CI logs. Can you show me on your laptop later?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-466544014
https://github.com/hail-is/hail/pull/5418#issuecomment-466544014:111,Testability,test,test,111,That message is caused by returning an `int` from a Flask request handler. Looks like the endpoint is called `/test`. I don't see that in the CI logs. Can you show me on your laptop later?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-466544014
https://github.com/hail-is/hail/pull/5418#issuecomment-466544014:145,Testability,log,logs,145,That message is caused by returning an `int` from a Flask request handler. Looks like the endpoint is called `/test`. I don't see that in the CI logs. Can you show me on your laptop later?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-466544014
https://github.com/hail-is/hail/pull/5418#issuecomment-467080179:146,Modifiability,refactor,refactor,146,"I thought about this over the weekend, and I think it would be better if we didn't have a separate data structure (JobTask). I'm going to try and refactor this without that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-467080179
https://github.com/hail-is/hail/pull/5421#issuecomment-467106563:67,Deployability,patch,patch-notes-y,67,@danking can you edit the title / commit message to something more patch-notes-y? 🙏,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563
https://github.com/hail-is/hail/pull/5421#issuecomment-467106563:41,Integrability,message,message,41,@danking can you edit the title / commit message to something more patch-notes-y? 🙏,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:0,Deployability,update,update,0,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:492,Deployability,release,release,492,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:216,Testability,test,tests,216,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:387,Testability,test,tests,387,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:437,Testability,test,tests,437,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:475,Testability,test,test,475,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533
https://github.com/hail-is/hail/pull/5424#issuecomment-466820072:80,Deployability,pipeline,pipeline,80,@chrisvittal I verified the partitioner is no longer broadcast for the combiner pipeline. This should be ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466820072
https://github.com/hail-is/hail/pull/5424#issuecomment-467080098:18,Performance,optimiz,optimization,18,"OK, I left in the optimization in multi-way join but also added it to RVD.repartition. This is the best I can see how to do. I realize I wanted `satisfiesAllowedOverlap(key.length - 1)`, not `satisfiesAllowedOverlap(0)`. It's just the case I was working with had one key. Fixed. That might clarify some confusion. Also added a strictify to generate. I think the existing code was wrong without that. @patrick-schultz can you take another look?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467080098
https://github.com/hail-is/hail/pull/5424#issuecomment-467085463:422,Usability,clear,clearer,422,"> I realize I wanted `satisfiesAllowedOverlap(key.length - 1)`, not `satisfiesAllowedOverlap(0)`. Because `allowedOverlap` is tricky to think about, I also gave constructor overloads that take `partitionKey` like in the old style. So this case is equivalent to setting `partitionKey == key` in the constructor `def this(partitionKey: Array[String], kType: TStruct, rangeBounds: IndexedSeq[Interval])`, if you think that's clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467085463
https://github.com/hail-is/hail/pull/5424#issuecomment-467119054:17,Usability,learn,learned,17,I feel that I've learned more about RVDs and partitioners. The front end of this change looks correct. :+1: from me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467119054
https://github.com/hail-is/hail/pull/5424#issuecomment-467130254:19,Usability,learn,learned,19,> I feel that I've learned more about RVDs and partitioners. Same here! Good discussion!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467130254
https://github.com/hail-is/hail/pull/5424#issuecomment-499240905:256,Availability,error,errors,256,"@cseed I'd like to turn on the key checking again for the time being because we were relying on it for some split_multi stuff that's going to take some amount of new infrastructure to fix properly, and in the meantime it's causing some pretty bad/nonsense errors downstream because things are can be out of order (see #6223). Is that going cause any problems?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-499240905
https://github.com/hail-is/hail/pull/5424#issuecomment-499240905:263,Availability,down,downstream,263,"@cseed I'd like to turn on the key checking again for the time being because we were relying on it for some split_multi stuff that's going to take some amount of new infrastructure to fix properly, and in the meantime it's causing some pretty bad/nonsense errors downstream because things are can be out of order (see #6223). Is that going cause any problems?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-499240905
https://github.com/hail-is/hail/pull/5426#issuecomment-467104633:96,Performance,bottleneck,bottleneck,96,"Yes, functions would be great but that is a harder project. I'm skeptical the IR size is a huge bottleneck right now, although I could be wrong. The compiler and serializing many copies of the same bytecode is definitely known to be slow, and this fixes that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467104633
https://github.com/hail-is/hail/pull/5426#issuecomment-467105630:20,Performance,optimiz,optimizer,20,"It is true that the optimizer first runs whole-stage, and that could be killing you, while this caches per-stage. I say we run again with this and see where we are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467105630
https://github.com/hail-is/hail/pull/5426#issuecomment-467105630:96,Performance,cache,caches,96,"It is true that the optimizer first runs whole-stage, and that could be killing you, while this caches per-stage. I say we run again with this and see where we are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467105630
https://github.com/hail-is/hail/pull/5426#issuecomment-467141785:63,Performance,concurren,concurrency,63,I think I understand the problem and I think there is a latent concurrency bug in the handling of randomness. I need to dig in a little more.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467141785
https://github.com/hail-is/hail/pull/5426#issuecomment-467240584:108,Safety,sanity check,sanity check,108,"OK, I think things should be working now. I asked @catoverdrive to take a look at the randomness change for sanity check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467240584
https://github.com/hail-is/hail/pull/5430#issuecomment-467256972:470,Usability,clear,clearly,470,"@danking I think this is set barring change to libsass compilation. If possible, I would like to keep the scss compilation in notebook.py, and create an issue to make a better solution as a step 2. I recognize what you want in broad terms, and am happy to do it, and at the same time the proposed alternative appears more complex, requires me to spend time on research (how to implement auto-reload, not having to retype `make scss` for every style change), and doesn't clearly add value compared to the remaining user-facing work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430#issuecomment-467256972
https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:78,Availability,error,error,78,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718
https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:642,Availability,Error,ErrorHandling,642,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718
https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:668,Availability,Error,ErrorHandling,668,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718
https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:4260,Deployability,update,updated,4260,hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:946); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:946); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1112); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:999); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:806); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1186); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1191); 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```. My updated code is at https://github.com/chrisvittal/hail/commit/92d18b7c5e28db82f2502980b44d48b730d8f000,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718
https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:18,Modifiability,rewrite,rewrite,18,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718
https://github.com/hail-is/hail/pull/5435#issuecomment-467643750:15,Availability,error,error,15,This is a type error: your input argument doesn't match the declared type of the function. Try `assert transform_row_type == mt.row.dtype` before the application of `transform_row_f`. I'll beef up the error reporting of my define_function prototype so the error gets caught before hitting the JVM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750
https://github.com/hail-is/hail/pull/5435#issuecomment-467643750:201,Availability,error,error,201,This is a type error: your input argument doesn't match the declared type of the function. Try `assert transform_row_type == mt.row.dtype` before the application of `transform_row_f`. I'll beef up the error reporting of my define_function prototype so the error gets caught before hitting the JVM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750
https://github.com/hail-is/hail/pull/5435#issuecomment-467643750:256,Availability,error,error,256,This is a type error: your input argument doesn't match the declared type of the function. Try `assert transform_row_type == mt.row.dtype` before the application of `transform_row_f`. I'll beef up the error reporting of my define_function prototype so the error gets caught before hitting the JVM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750
https://github.com/hail-is/hail/pull/5435#issuecomment-467643750:96,Testability,assert,assert,96,This is a type error: your input argument doesn't match the declared type of the function. Try `assert transform_row_type == mt.row.dtype` before the application of `transform_row_f`. I'll beef up the error reporting of my define_function prototype so the error gets caught before hitting the JVM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750
https://github.com/hail-is/hail/pull/5435#issuecomment-467649154:21,Testability,assert,assertion,21,It does though. That assertion returns true.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467649154
https://github.com/hail-is/hail/pull/5435#issuecomment-467652863:15,Testability,assert,assert,15,"I don't see an assert in the linked code. Either way, I'll try to look at this tonight or after my talk in the morning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467652863
https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:28,Availability,error,error,28,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354
https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:107,Deployability,Update,Updated,107,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354
https://github.com/hail-is/hail/pull/5436#issuecomment-468746637:82,Security,access,access,82,"Thanks guys. Does using cloudtools get the latest build from master, or how can I access this to use?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5436#issuecomment-468746637
https://github.com/hail-is/hail/pull/5436#issuecomment-468749967:33,Deployability,deploy,deployed,33,"yep, cloudtools grabs the latest-deployed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5436#issuecomment-468749967
https://github.com/hail-is/hail/pull/5443#issuecomment-467677829:229,Testability,assert,assertions,229,"> I'm not sure what you mean by this. Also, were we just not type checking MatrixIR, etc. before? Was there a reason for this? I thought @patrick-schultz was working on something to do with type checking. Currently, typechecking assertions happen in the constructors of relational IRs. This will consolidate typechecking nicely. The expression problem strikes again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5443#issuecomment-467677829
https://github.com/hail-is/hail/pull/5448#issuecomment-468436879:0,Availability,ping,ping,0,ping @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5448#issuecomment-468436879
https://github.com/hail-is/hail/pull/5451#issuecomment-467611477:98,Deployability,install,installed,98,"> Is this something where you need to add a new docker image to the gcr registry with the package installed? I think for pipeline I had to add new lines to `Dockerfile.pr-builder` and then run `make hail-ci-build-image` and `make push-hail-ci-build-image`. This is just a replacement for notebook, and yeah, basically what you described. This PR adds a dependency (lib sass, corresponding to the `import sass` line in notebook/notebook.py), which will fix the crashloopbackoff currently seen on our cluster (a PR was merged but this was missed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477
https://github.com/hail-is/hail/pull/5451#issuecomment-467611477:121,Deployability,pipeline,pipeline,121,"> Is this something where you need to add a new docker image to the gcr registry with the package installed? I think for pipeline I had to add new lines to `Dockerfile.pr-builder` and then run `make hail-ci-build-image` and `make push-hail-ci-build-image`. This is just a replacement for notebook, and yeah, basically what you described. This PR adds a dependency (lib sass, corresponding to the `import sass` line in notebook/notebook.py), which will fix the crashloopbackoff currently seen on our cluster (a PR was merged but this was missed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477
https://github.com/hail-is/hail/pull/5451#issuecomment-467611477:353,Integrability,depend,dependency,353,"> Is this something where you need to add a new docker image to the gcr registry with the package installed? I think for pipeline I had to add new lines to `Dockerfile.pr-builder` and then run `make hail-ci-build-image` and `make push-hail-ci-build-image`. This is just a replacement for notebook, and yeah, basically what you described. This PR adds a dependency (lib sass, corresponding to the `import sass` line in notebook/notebook.py), which will fix the crashloopbackoff currently seen on our cluster (a PR was merged but this was missed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477
https://github.com/hail-is/hail/pull/5451#issuecomment-467633773:111,Deployability,deploy,deploy,111,Specifically : https://github.com/akotlar/hail/blob/3b639cf77e2ad44c3422b619a36cf33523032953/notebook2/hail-ci-deploy.sh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467633773
https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:86,Deployability,update,updated,86,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462
https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:127,Deployability,configurat,configuration,127,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462
https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:127,Modifiability,config,configuration,127,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462
https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:24,Testability,test,tests,24,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462
https://github.com/hail-is/hail/pull/5452#issuecomment-468711409:300,Testability,log,login,300,"> Can you share the long term vision for this page? Will notebook become a generic `app.hail.is` with many tabs? Changing the index page to a page with no content seems strictly less helpful to the user. They must make an extra click after they type `notebook.hail.is` into their URL bar to get to a login form. If I was the user, I'd want to get dumped right into the log-in form (but maybe I'm a weird user?). Notebook will host the batch stuff. I have no comments about vision longer term than that, it's undefined as far as I know (meaning, I don't know). We can get rid of the home page as well. I ran the home page idea by @cseed but maybe I misunderstood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5452#issuecomment-468711409
https://github.com/hail-is/hail/pull/5452#issuecomment-468711409:369,Testability,log,log-in,369,"> Can you share the long term vision for this page? Will notebook become a generic `app.hail.is` with many tabs? Changing the index page to a page with no content seems strictly less helpful to the user. They must make an extra click after they type `notebook.hail.is` into their URL bar to get to a login form. If I was the user, I'd want to get dumped right into the log-in form (but maybe I'm a weird user?). Notebook will host the batch stuff. I have no comments about vision longer term than that, it's undefined as far as I know (meaning, I don't know). We can get rid of the home page as well. I ran the home page idea by @cseed but maybe I misunderstood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5452#issuecomment-468711409
https://github.com/hail-is/hail/pull/5458#issuecomment-468026163:33,Integrability,depend,dependent,33,Looks good! I'll accept once the dependent PR is accepted,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5458#issuecomment-468026163
https://github.com/hail-is/hail/pull/5465#issuecomment-469782010:174,Performance,optimiz,optimization,174,"it's actually a bit slower (2x) than using f-strings. I could serialize a million-node IR tree in 100ms, though, so I don't think this is the limiting factor compared to the optimization in the backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469782010
https://github.com/hail-is/hail/pull/5465#issuecomment-469789122:1066,Performance,Load,LoadVCF,1066,"For ~1500 joint caller inputs, on what was a frankenbranch (I merged a bunch of stuff together, haven't pushed it). I saw that IR construction in python took around a minute. I'm not entirely sure where the time was being spent though. Also 1 minute is nothing compared to the amount of time we spend actually joint calling. Anyways, `run_combiner.py`; ```python3; def run_combiner(sample_list, json, out_path, tmp_path, summary_path=None, overwrite=False):; # make the temp path a directory, no matter what; tmp_path += f'/combiner-temporary/{uuid.uuid4()}/'; vcfs = [comb.transform_one(vcf); for vcf in hl.import_vcfs(sample_list, json, array_elements_required=False)]; combined = [comb.combine_gvcfs(mts) for mts in chunks(vcfs, MAX_COMBINER_LENGTH)]; if len(combined) == 1:; combined[0].write(out_path, overwrite=overwrite); else:; hl.utils.java.info(f'Writing combiner temporary files to: {tmp_path}'); ... # do more, but this stage isn't huge yet so :man_shrugging: ; ```; Relevant log:; ```; 2019-03-01 22:09:20 DAGScheduler: INFO: Job 0 finished: collect at LoadVCF.scala:1295, took 88.076400 s; 2019-03-01 22:10:19 Hail: INFO: Writing combiner temporary files to: gs://cdv-hail/combiner/tmp//combiner-temporary/dc741728-fdfd-49d9-a66e-94bd7b541879/; ```; Stage zero is tabix reading `sc.parallelize`. The next line is the logging line that I added, almost a minute apart. After that it's 30 seconds for the Optimizer and Lowerer and (printing hundreds of thousands of lines of IR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122
https://github.com/hail-is/hail/pull/5465#issuecomment-469789122:1416,Performance,Optimiz,Optimizer,1416,"For ~1500 joint caller inputs, on what was a frankenbranch (I merged a bunch of stuff together, haven't pushed it). I saw that IR construction in python took around a minute. I'm not entirely sure where the time was being spent though. Also 1 minute is nothing compared to the amount of time we spend actually joint calling. Anyways, `run_combiner.py`; ```python3; def run_combiner(sample_list, json, out_path, tmp_path, summary_path=None, overwrite=False):; # make the temp path a directory, no matter what; tmp_path += f'/combiner-temporary/{uuid.uuid4()}/'; vcfs = [comb.transform_one(vcf); for vcf in hl.import_vcfs(sample_list, json, array_elements_required=False)]; combined = [comb.combine_gvcfs(mts) for mts in chunks(vcfs, MAX_COMBINER_LENGTH)]; if len(combined) == 1:; combined[0].write(out_path, overwrite=overwrite); else:; hl.utils.java.info(f'Writing combiner temporary files to: {tmp_path}'); ... # do more, but this stage isn't huge yet so :man_shrugging: ; ```; Relevant log:; ```; 2019-03-01 22:09:20 DAGScheduler: INFO: Job 0 finished: collect at LoadVCF.scala:1295, took 88.076400 s; 2019-03-01 22:10:19 Hail: INFO: Writing combiner temporary files to: gs://cdv-hail/combiner/tmp//combiner-temporary/dc741728-fdfd-49d9-a66e-94bd7b541879/; ```; Stage zero is tabix reading `sc.parallelize`. The next line is the logging line that I added, almost a minute apart. After that it's 30 seconds for the Optimizer and Lowerer and (printing hundreds of thousands of lines of IR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122
https://github.com/hail-is/hail/pull/5465#issuecomment-469789122:988,Testability,log,log,988,"For ~1500 joint caller inputs, on what was a frankenbranch (I merged a bunch of stuff together, haven't pushed it). I saw that IR construction in python took around a minute. I'm not entirely sure where the time was being spent though. Also 1 minute is nothing compared to the amount of time we spend actually joint calling. Anyways, `run_combiner.py`; ```python3; def run_combiner(sample_list, json, out_path, tmp_path, summary_path=None, overwrite=False):; # make the temp path a directory, no matter what; tmp_path += f'/combiner-temporary/{uuid.uuid4()}/'; vcfs = [comb.transform_one(vcf); for vcf in hl.import_vcfs(sample_list, json, array_elements_required=False)]; combined = [comb.combine_gvcfs(mts) for mts in chunks(vcfs, MAX_COMBINER_LENGTH)]; if len(combined) == 1:; combined[0].write(out_path, overwrite=overwrite); else:; hl.utils.java.info(f'Writing combiner temporary files to: {tmp_path}'); ... # do more, but this stage isn't huge yet so :man_shrugging: ; ```; Relevant log:; ```; 2019-03-01 22:09:20 DAGScheduler: INFO: Job 0 finished: collect at LoadVCF.scala:1295, took 88.076400 s; 2019-03-01 22:10:19 Hail: INFO: Writing combiner temporary files to: gs://cdv-hail/combiner/tmp//combiner-temporary/dc741728-fdfd-49d9-a66e-94bd7b541879/; ```; Stage zero is tabix reading `sc.parallelize`. The next line is the logging line that I added, almost a minute apart. After that it's 30 seconds for the Optimizer and Lowerer and (printing hundreds of thousands of lines of IR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122
https://github.com/hail-is/hail/pull/5465#issuecomment-469789122:1331,Testability,log,logging,1331,"For ~1500 joint caller inputs, on what was a frankenbranch (I merged a bunch of stuff together, haven't pushed it). I saw that IR construction in python took around a minute. I'm not entirely sure where the time was being spent though. Also 1 minute is nothing compared to the amount of time we spend actually joint calling. Anyways, `run_combiner.py`; ```python3; def run_combiner(sample_list, json, out_path, tmp_path, summary_path=None, overwrite=False):; # make the temp path a directory, no matter what; tmp_path += f'/combiner-temporary/{uuid.uuid4()}/'; vcfs = [comb.transform_one(vcf); for vcf in hl.import_vcfs(sample_list, json, array_elements_required=False)]; combined = [comb.combine_gvcfs(mts) for mts in chunks(vcfs, MAX_COMBINER_LENGTH)]; if len(combined) == 1:; combined[0].write(out_path, overwrite=overwrite); else:; hl.utils.java.info(f'Writing combiner temporary files to: {tmp_path}'); ... # do more, but this stage isn't huge yet so :man_shrugging: ; ```; Relevant log:; ```; 2019-03-01 22:09:20 DAGScheduler: INFO: Job 0 finished: collect at LoadVCF.scala:1295, took 88.076400 s; 2019-03-01 22:10:19 Hail: INFO: Writing combiner temporary files to: gs://cdv-hail/combiner/tmp//combiner-temporary/dc741728-fdfd-49d9-a66e-94bd7b541879/; ```; Stage zero is tabix reading `sc.parallelize`. The next line is the logging line that I added, almost a minute apart. After that it's 30 seconds for the Optimizer and Lowerer and (printing hundreds of thousands of lines of IR).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789122
https://github.com/hail-is/hail/pull/5465#issuecomment-469789909:48,Integrability,protocol,protocol,48,This iteration also uses an extremely slow py4j protocol; ```; return [MatrixTable._from_java(jmir) for jmir in jmirs]. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5465#issuecomment-469789909
https://github.com/hail-is/hail/pull/5467#issuecomment-468314466:18,Availability,error,error,18,You still have an error in there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5467#issuecomment-468314466
https://github.com/hail-is/hail/pull/5469#issuecomment-469877688:43,Integrability,depend,depends,43,"@cseed @patrick-schultz @jbloom22 This now depends only on #5463, so I've gone ahead and assigned it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5469#issuecomment-469877688
https://github.com/hail-is/hail/pull/5471#issuecomment-468472896:253,Testability,test,test,253,"@cseed How did you verify only one is created? `eq` is literally the JVM's `if_acmpne`. I include a snippet below. I also checked that if I explicitly define `equals`, I still get `if_acmpne`. At the very least, I'd like the issue to be replicated as a test. I agree that the right solution is for this not to be a case class if we're using reference equality. Maybe we need equals_for_tests or something?. ```; (hail) 130 dking@wmb16-359 # cat foo.scala ; case class Fizzle(x: Int, y: String). class Foo {; def fizzle(x: Fizzle, y: Fizzle): Boolean = {; return x eq y; }; def anyref(x: AnyRef, y: AnyRef): Boolean = {; return x eq y; }; }; (hail) dking@wmb16-359 # scalac foo.scala; (hail) dking@wmb16-359 # javap -c foo; Warning: Binary file foo contains Foo; Compiled from ""foo.scala""; public class Foo {; public boolean fizzle(Fizzle, Fizzle);; Code:; 0: aload_1; 1: aload_2; 2: if_acmpne 9; 5: iconst_1; 6: goto 10; 9: iconst_0; 10: ireturn. public boolean anyref(java.lang.Object, java.lang.Object);; Code:; 0: aload_1; 1: aload_2; 2: if_acmpne 9; 5: iconst_1; 6: goto 10; 9: iconst_0; 10: ireturn. public Foo();; Code:; 0: aload_0; 1: invokespecial #22 // Method java/lang/Object.""<init>"":()V; 4: return; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-468472896
https://github.com/hail-is/hail/pull/5471#issuecomment-469322872:345,Deployability,pipeline,pipeline,345,"> How did you verify only one is created?. I printed in the ReferenceGenome constructor. I guess it could have been serialized, but the issues I was seeing were all on the master, and I don't see how it could have been. I haven't succeeded in making an isolated test case that motivates this change. @chrisvittal has a complicated joint calling pipeline that fails. However, we define equality on ReferenceGenome in terms of value quality, and I think unify should use the same notion of equality so this look like a bug on the face of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872
https://github.com/hail-is/hail/pull/5471#issuecomment-469322872:262,Testability,test,test,262,"> How did you verify only one is created?. I printed in the ReferenceGenome constructor. I guess it could have been serialized, but the issues I was seeing were all on the master, and I don't see how it could have been. I haven't succeeded in making an isolated test case that motivates this change. @chrisvittal has a complicated joint calling pipeline that fails. However, we define equality on ReferenceGenome in terms of value quality, and I think unify should use the same notion of equality so this look like a bug on the face of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872
https://github.com/hail-is/hail/pull/5471#issuecomment-469340908:101,Deployability,pipeline,pipeline,101,"I can stack this change with the change that defines the function, I do test that the `vcf_combiner` pipeline runs in `test_impex.py::VCFTests::test_combiner_works`. That may be sufficient, since it would fail without this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908
https://github.com/hail-is/hail/pull/5471#issuecomment-469340908:72,Testability,test,test,72,"I can stack this change with the change that defines the function, I do test that the `vcf_combiner` pipeline runs in `test_impex.py::VCFTests::test_combiner_works`. That may be sufficient, since it would fail without this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908
https://github.com/hail-is/hail/pull/5471#issuecomment-469341849:84,Deployability,pipeline,pipeline,84,"Strange, I couldn't get a similar example to fail, either through `hl.eval` or in a pipeline:. ```; def test_define_function_locus(self):; contig2 = hl.experimental.define_function(; lambda l: l.contig, hl.tlocus(hl.get_reference('GRCh38'))); t = hl.utils.range_table(1); t = t.annotate(locus = hl.locus('chr22', 123, 'GRCh38')); t = t.annotate(contig = contig2(t.locus)); self.assertEqual(t.collect()[0]['contig'], 'chr22'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849
https://github.com/hail-is/hail/pull/5471#issuecomment-469341849:378,Testability,assert,assertEqual,378,"Strange, I couldn't get a similar example to fail, either through `hl.eval` or in a pipeline:. ```; def test_define_function_locus(self):; contig2 = hl.experimental.define_function(; lambda l: l.contig, hl.tlocus(hl.get_reference('GRCh38'))); t = hl.utils.range_table(1); t = t.annotate(locus = hl.locus('chr22', 123, 'GRCh38')); t = t.annotate(contig = contig2(t.locus)); self.assertEqual(t.collect()[0]['contig'], 'chr22'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849
https://github.com/hail-is/hail/pull/5471#issuecomment-469341988:54,Testability,test,test,54,I say we just merge it and then your change (with the test) will go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469341988
https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:1098,Deployability,update,updates,1098,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208
https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:414,Integrability,rout,routes,414,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208
https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:318,Modifiability,refactor,refactor,318,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208
https://github.com/hail-is/hail/pull/5476#issuecomment-468858796:23,Safety,unsafe,unsafe,23,"@danking addressed the unsafe issue, sha224 (largest that results in a <63 character hex digest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468858796
https://github.com/hail-is/hail/pull/5479#issuecomment-479660187:44,Modifiability,rewrite,rewrite,44,"OK, I have a better answer for why we can't rewrite the AggLets in place -- . we can have IRs like:. ```; TableAggregate; AggLet foo; row; ApplyAggOp(... Ref row); ```. If we extract in place, the `AggLet` becomes a `Let` in the local post-aggregation operation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5479#issuecomment-479660187
https://github.com/hail-is/hail/pull/5485#issuecomment-468401938:72,Availability,error,error,72,"Happy to commit this if it passes tests. Looks like you've got a rebase error, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938
https://github.com/hail-is/hail/pull/5485#issuecomment-468401938:34,Testability,test,tests,34,"Happy to commit this if it passes tests. Looks like you've got a rebase error, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938
https://github.com/hail-is/hail/issues/5486#issuecomment-469299860:348,Availability,avail,available,348,"Interesting, this directly contradicts the [k8s documentation on pod lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/):; > Running | The Pod has been bound to a node, and all of the Containers have been created. At least one Container is still running, or is in the process of starting or restarting. However, given the available statuses, Running seems like the most reasonable one to describe a pod in the process of shutting down. Shall we close the issue now that we've understood the semantics or is there further action for us to take?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860
https://github.com/hail-is/hail/issues/5486#issuecomment-469299860:456,Availability,down,down,456,"Interesting, this directly contradicts the [k8s documentation on pod lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/):; > Running | The Pod has been bound to a node, and all of the Containers have been created. At least one Container is still running, or is in the process of starting or restarting. However, given the available statuses, Running seems like the most reasonable one to describe a pod in the process of shutting down. Shall we close the issue now that we've understood the semantics or is there further action for us to take?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860
https://github.com/hail-is/hail/pull/5489#issuecomment-468433049:79,Deployability,deploy,deploy,79,"> Unfortunately, the only way to check if this actually works right now is for deploy to succeed. 😞. Can't you check by building the docker image?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468433049
https://github.com/hail-is/hail/pull/5489#issuecomment-468433413:21,Testability,test,tested,21,"@akotlar is right, I tested this locally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468433413
https://github.com/hail-is/hail/pull/5489#issuecomment-468434781:180,Testability,test,test,180,"> #5482 :P. Haha man we're on the same wavelength today: https://github.com/akotlar/hail/commit/c0c3751d9de9008b1ec4d0281afa77c5dbd7d186; (g++ is a neater solution, I was going to test that next)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434781
https://github.com/hail-is/hail/pull/5489#issuecomment-468434816:113,Availability,error,error,113,"Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816
https://github.com/hail-is/hail/pull/5489#issuecomment-468434816:61,Deployability,deploy,deployed,61,"Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816
https://github.com/hail-is/hail/pull/5489#issuecomment-468434816:124,Deployability,deploy,deploy,124,"Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816
https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:115,Availability,error,error,115,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191
https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:63,Deployability,deploy,deployed,63,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191
https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:126,Deployability,deploy,deploy,126,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191
https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:392,Deployability,deploy,deployed,392,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191
https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:282,Testability,log,log,282,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191
https://github.com/hail-is/hail/pull/5496#issuecomment-473080271:0,Deployability,update,updated,0,updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5496#issuecomment-473080271
https://github.com/hail-is/hail/issues/5500#issuecomment-468696410:163,Deployability,update,updated,163,Good point. I'm going to first make sure that it would have failed on a sparse matrix with the `.get` and then show that the same test (hopefully!) passes for the updated PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410
https://github.com/hail-is/hail/issues/5500#issuecomment-468696410:130,Testability,test,test,130,Good point. I'm going to first make sure that it would have failed on a sparse matrix with the `.get` and then show that the same test (hopefully!) passes for the updated PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410
https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:92,Deployability,deploy,deploy-svc,92,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890
https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:205,Deployability,deploy,deploy-svc,205,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890
https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:4,Testability,test,tested,4,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890
https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:117,Testability,test,test,117,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890
https://github.com/hail-is/hail/pull/5503#issuecomment-468828579:26,Availability,fault,fault,26,"LOL yeah right u l33t. My fault, should have caught that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468828579
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:190,Availability,ERROR,ERROR,190,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:303,Availability,Error,Error,303,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:326,Availability,ERROR,ERROR,326,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:60,Integrability,rout,route,60,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:819,Safety,timeout,timeout,819,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:832,Safety,timeout,timeout,832,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:953,Safety,timeout,timeout,953,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:587,Availability,Error,Error,587,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:169,Deployability,deploy,deployment,169,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:259,Deployability,deploy,deployment,259,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:445,Deployability,deploy,deploy-svc,445,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:552,Deployability,deploy,deploy,552,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:579,Deployability,deploy,deploy,579,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:667,Deployability,deploy,deploy-svc,667,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:2,Testability,test,tested,2,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:331,Testability,test,test,331,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:512,Testability,test,test,512,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:732,Testability,test,test,732,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:336,Deployability,deploy,deploy-svc-list-test-pvc,336,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:420,Deployability,deploy,deploy-svc,420,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:80,Security,authoriz,authorization,80,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:294,Security,authoriz,authorization,294,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:514,Security,authoriz,authorization,514,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:739,Security,authoriz,authorization,739,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:127,Testability,test,test,127,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:144,Testability,test,test-pvc,144,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:352,Testability,test,test-pvc,352,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:373,Testability,test,test,373,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:488,Testability,test,test-pvc,488,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:786,Testability,test,test,786,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:805,Testability,test,test-pvc,805,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060
https://github.com/hail-is/hail/pull/5503#issuecomment-469326697:22,Deployability,deploy,deploy,22,"it appears that batch deploy is still broken due to the issue alex notes, I'll fix that. not sure if that's related to infinite loops.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-469326697
https://github.com/hail-is/hail/pull/5503#issuecomment-469421690:105,Testability,test,test-svc,105,"OK, `batch-svc` has privileges to create/delete/list/get/etc. persistentvolumeclaims in batch-pods now. `test-svc` always had privileges to do everything in the `test` namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-469421690
https://github.com/hail-is/hail/pull/5503#issuecomment-469421690:162,Testability,test,test,162,"OK, `batch-svc` has privileges to create/delete/list/get/etc. persistentvolumeclaims in batch-pods now. `test-svc` always had privileges to do everything in the `test` namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-469421690
https://github.com/hail-is/hail/pull/5503#issuecomment-470258509:266,Testability,test,test,266,"The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second. We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470258509
https://github.com/hail-is/hail/pull/5503#issuecomment-470260178:131,Testability,assert,assert,131,"Ok, what do you think of this?; ```; i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i = i + 1; if i > 14:; break; assert len(output) != 4; ```. We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470260178
https://github.com/hail-is/hail/pull/5503#issuecomment-470260178:373,Testability,assert,assert,373,"Ok, what do you think of this?; ```; i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i = i + 1; if i > 14:; break; assert len(output) != 4; ```. We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470260178
https://github.com/hail-is/hail/pull/5503#issuecomment-470262025:361,Availability,failure,failure,361,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470262025
https://github.com/hail-is/hail/pull/5503#issuecomment-470262025:275,Testability,test,test,275,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470262025
https://github.com/hail-is/hail/pull/5503#issuecomment-470262025:537,Testability,test,test,537,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470262025
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:361,Availability,failure,failure,361,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:1251,Availability,failure,failures,1251,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:1375,Availability,failure,failures,1375,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:275,Testability,test,test,275,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:537,Testability,test,test,537,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:782,Testability,assert,assert,782,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:1028,Testability,assert,assert,1028,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:1246,Testability,test,test,1246,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:1370,Testability,test,test,1370,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:8,Availability,failure,failure,8,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:93,Availability,failure,failures,93,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:108,Availability,failure,failure,108,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:130,Performance,load,load,130,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:38,Safety,safe,safe,38,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:3,Testability,test,test,3,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:221,Testability,test,test,221,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865
https://github.com/hail-is/hail/issues/5504#issuecomment-470199897:153,Integrability,wrap,wraps,153,"Sure thing. Planning on implementing three top-level convenience methods for converting between relational IRs:; - `t.to_matrix_table` which essentially wraps the python approach you laid out in the creation of this issue; - `bm.to_table` which produces a table where each row corresponds to a row of the original BlockMatrix (will do a write and a read to avoid shuffling, actually have to dig into the RDDs for this one); - `bm.to_matrix_table` which will just compose the previous two methods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897
https://github.com/hail-is/hail/issues/5504#issuecomment-470199897:357,Safety,avoid,avoid,357,"Sure thing. Planning on implementing three top-level convenience methods for converting between relational IRs:; - `t.to_matrix_table` which essentially wraps the python approach you laid out in the creation of this issue; - `bm.to_table` which produces a table where each row corresponds to a row of the original BlockMatrix (will do a write and a read to avoid shuffling, actually have to dig into the RDDs for this one); - `bm.to_matrix_table` which will just compose the previous two methods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897
https://github.com/hail-is/hail/issues/5505#issuecomment-468797074:16,Deployability,release,released,16,Tornado 6.0 was released 5 hours ago. https://pypi.org/project/tornado/#history,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505#issuecomment-468797074
https://github.com/hail-is/hail/issues/5505#issuecomment-468807129:13,Deployability,deploy,deployed,13,Fix has been deployed and I confirmed it resolves the issue in a freshly created cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5505#issuecomment-468807129
https://github.com/hail-is/hail/pull/5509#issuecomment-468830620:17,Deployability,update,update,17,"OK, thanks, will update the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620
https://github.com/hail-is/hail/pull/5509#issuecomment-468830620:28,Testability,test,tests,28,"OK, thanks, will update the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620
https://github.com/hail-is/hail/pull/5511#issuecomment-469133387:179,Integrability,depend,depends,179,"Ah ok, I certainly agree then, that `sparsify_blocks` is a more appropriate name, and this should be documented explicitly. It makes me a little bit uncomfortable that the output depends on the block_size parameter that's not set in this function (I understand why that is, it's just a little odd).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469133387
https://github.com/hail-is/hail/pull/5511#issuecomment-469262861:39,Deployability,update,update,39,I will make the sparsifying rename/doc update a separate PR. @konradjk I also added another test along the same vein but for a matrix that has been filtered for a subset of rows and columns.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861
https://github.com/hail-is/hail/pull/5511#issuecomment-469262861:92,Testability,test,test,92,I will make the sparsifying rename/doc update a separate PR. @konradjk I also added another test along the same vein but for a matrix that has been filtered for a subset of rows and columns.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861
https://github.com/hail-is/hail/pull/5512#issuecomment-469069004:433,Deployability,pipeline,pipeline,433,I added a new `trait BroadcastSerializable` that tries to verify classes implementing this trait are only serialized when broadcasting. It works by getting the current stack trace and verifying that serialization only happens within a call to a `broadcast` method on the class. `ReferenceGenome` and `RVDPartitioner` implement `BroadcastSerializable`. @chrisvittal This also reduces the size of the RDD broadcast in the VCF combiner pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004
https://github.com/hail-is/hail/pull/5512#issuecomment-469069004:375,Energy Efficiency,reduce,reduces,375,I added a new `trait BroadcastSerializable` that tries to verify classes implementing this trait are only serialized when broadcasting. It works by getting the current stack trace and verifying that serialization only happens within a call to a `broadcast` method on the class. `ReferenceGenome` and `RVDPartitioner` implement `BroadcastSerializable`. @chrisvittal This also reduces the size of the RDD broadcast in the VCF combiner pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004
https://github.com/hail-is/hail/pull/5512#issuecomment-472909343:35,Testability,test,tests,35,It looks like it's passing all the tests except the new test @chrisvittal added yesterday for testing skat on the cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-472909343
https://github.com/hail-is/hail/pull/5512#issuecomment-472909343:56,Testability,test,test,56,It looks like it's passing all the tests except the new test @chrisvittal added yesterday for testing skat on the cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-472909343
https://github.com/hail-is/hail/pull/5512#issuecomment-472909343:94,Testability,test,testing,94,It looks like it's passing all the tests except the new test @chrisvittal added yesterday for testing skat on the cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-472909343
https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:412,Availability,failure,failure,412,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362
https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:492,Availability,down,down,492,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362
https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:694,Availability,error,error,694,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362
https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:432,Integrability,interface,interface,432,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362
https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:620,Testability,test,tests,620,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362
https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:121,Deployability,configurat,configuration,121,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554
https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:142,Deployability,configurat,configuration,142,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554
https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:121,Modifiability,config,configuration,121,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554
https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:142,Modifiability,config,configuration,142,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554
https://github.com/hail-is/hail/pull/5514#issuecomment-470284163:101,Testability,test,tests,101,This is only enabled if the cpp feature flag is turned on. I don't think it is ever turned on in the tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5514#issuecomment-470284163
https://github.com/hail-is/hail/pull/5514#issuecomment-470285754:64,Performance,queue,queue,64,"ah, OK. I'll un-block tomorrow, since I think there's already a queue of approved PRs and I have at least 2 that will need to go in this evening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5514#issuecomment-470285754
https://github.com/hail-is/hail/pull/5515#issuecomment-469288996:25,Testability,log,logic,25,"ok, the includedVariants logic makes this super hacky and terrible. I'm going to close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5515#issuecomment-469288996
https://github.com/hail-is/hail/pull/5518#issuecomment-469335166:72,Testability,test,test-pvc,72,"Quick check, do we need ""get"" permissions, and should we rename ""delete-test-pvc"" to ""test-pvc""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5518#issuecomment-469335166
https://github.com/hail-is/hail/pull/5518#issuecomment-469335166:86,Testability,test,test-pvc,86,"Quick check, do we need ""get"" permissions, and should we rename ""delete-test-pvc"" to ""test-pvc""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5518#issuecomment-469335166
https://github.com/hail-is/hail/pull/5522#issuecomment-469364826:207,Performance,perform,performance,207,"`ldscore` only has one `sparsify_row_intervals`, and it's used to immediately write and read. We could rework it to use `export_rectangles` and `rectangles_to_numpy` from #5516. Not sure what it would do to performance though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5522#issuecomment-469364826
https://github.com/hail-is/hail/pull/5523#issuecomment-469831308:0,Availability,ping,ping,0,"ping @danking , should be set",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5523#issuecomment-469831308
https://github.com/hail-is/hail/pull/5524#issuecomment-471082610:24,Testability,test,test,24,"address comments, added test, added Expression.summarize()",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5524#issuecomment-471082610
https://github.com/hail-is/hail/pull/5526#issuecomment-470314061:50,Testability,log,log,50,"@danking I think I know what is happening. If you log into a notebook, and then issue a reachability check, you get a 302, else 405. I need to support both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5526#issuecomment-470314061
https://github.com/hail-is/hail/pull/5528#issuecomment-469468966:77,Availability,checkpoint,checkpoint,77,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966
https://github.com/hail-is/hail/pull/5528#issuecomment-469468966:14,Usability,clear,clear,14,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966
https://github.com/hail-is/hail/pull/5528#issuecomment-469475986:62,Availability,down,down,62,that's what ~I suggested~ I almost suggested until I was shot down 😄,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469475986
https://github.com/hail-is/hail/pull/5528#issuecomment-469476636:22,Availability,down,down,22,"Tim, can you shoot me down, too?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469476636
https://github.com/hail-is/hail/pull/5528#issuecomment-469477749:46,Availability,down,down,46,"ugh, I'm still exhausted from shooting Konrad down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469477749
https://github.com/hail-is/hail/pull/5534#issuecomment-472072730:25,Testability,test,tested,25,@patrick-schultz this is tested in the IRSuite tests for ArraySort/ToSet/ToDict. Do you want c++ tests for the ArraySorter class?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5534#issuecomment-472072730
https://github.com/hail-is/hail/pull/5534#issuecomment-472072730:47,Testability,test,tests,47,@patrick-schultz this is tested in the IRSuite tests for ArraySort/ToSet/ToDict. Do you want c++ tests for the ArraySorter class?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5534#issuecomment-472072730
https://github.com/hail-is/hail/pull/5534#issuecomment-472072730:97,Testability,test,tests,97,@patrick-schultz this is tested in the IRSuite tests for ArraySort/ToSet/ToDict. Do you want c++ tests for the ArraySorter class?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5534#issuecomment-472072730
https://github.com/hail-is/hail/pull/5534#issuecomment-472084593:82,Testability,test,tests,82,Ah great. I just wanted to make sure the new code was getting exercised by the IR tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5534#issuecomment-472084593
https://github.com/hail-is/hail/pull/5540#issuecomment-481370104:26,Deployability,update,updates,26,"@danking Still needs some updates. I think it's low priority. Would it be more appropriate to close this (and multiple notebooks pr), and re-issue when ready?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5540#issuecomment-481370104
https://github.com/hail-is/hail/issues/5546#issuecomment-471025684:58,Testability,log,log,58,This means there were no artifacts. What was in the build log?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5546#issuecomment-471025684
https://github.com/hail-is/hail/issues/5546#issuecomment-472479806:25,Security,access,access,25,"I see, so you receive an access denied when there are no artifacts. The build log has a long list of commands, starting with a git clone. It sounds like this isn't an issue then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5546#issuecomment-472479806
https://github.com/hail-is/hail/issues/5546#issuecomment-472479806:78,Testability,log,log,78,"I see, so you receive an access denied when there are no artifacts. The build log has a long list of commands, starting with a git clone. It sounds like this isn't an issue then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5546#issuecomment-472479806
https://github.com/hail-is/hail/issues/5550#issuecomment-471026187:226,Deployability,update,update,226,"It's quite likely CI was restarted, the job ids are fairly low. There's a current bug wherein CI will spin up a bunch of jobs, kill them all then start them again when it first starts (there's a subtle issue WRT to whether we update state from GH or batch first).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026187
https://github.com/hail-is/hail/issues/5550#issuecomment-471026373:72,Testability,test,tested,72,"(on restart, CI doesn't know that feature branches have been previously tested against master, so it tries to get at least one status finished, for developer feedback purposes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373
https://github.com/hail-is/hail/issues/5550#issuecomment-471026373:158,Usability,feedback,feedback,158,"(on restart, CI doesn't know that feature branches have been previously tested against master, so it tries to get at least one status finished, for developer feedback purposes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373
https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:258,Deployability,update,update-alternatives,258,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523
https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:382,Deployability,update,update-alternatives,382,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523
https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:567,Performance,load,load,567,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523
https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:919,Usability,Clear,Clearly,919,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523
https://github.com/hail-is/hail/issues/5559#issuecomment-472956348:40,Usability,clear,clear,40,"# EDIT: DO NOT USE THIS APPROACH. To be clear, a broad user can execute this command on the cluster to fix their environment:. ```; cat >>~/.my.bash.rc <<EOF; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; EOF; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472956348
https://github.com/hail-is/hail/pull/5560#issuecomment-471116569:33,Energy Efficiency,reduce,reduce,33,"@daniel-goldstein sure, functors.reduce with a backwards iterator does the trick. I also added some tests, including one that ensures we do not evaluate arguments after the non-null one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5560#issuecomment-471116569
https://github.com/hail-is/hail/pull/5560#issuecomment-471116569:100,Testability,test,tests,100,"@daniel-goldstein sure, functors.reduce with a backwards iterator does the trick. I also added some tests, including one that ensures we do not evaluate arguments after the non-null one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5560#issuecomment-471116569
https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:36,Availability,error,errors,36,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362
https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:47,Deployability,Pipeline,Pipeline,47,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362
https://github.com/hail-is/hail/pull/5563#issuecomment-471734576:22,Availability,error,errors,22,@jigold So the pylint errors are now resolved?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471734576
https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:30,Availability,error,error,30,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999
https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:90,Deployability,update,update,90,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999
https://github.com/hail-is/hail/pull/5563#issuecomment-473458156:11,Testability,test,tests,11,"So now the tests are passing locally for me, but not the call back test on the cloud...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-473458156
https://github.com/hail-is/hail/pull/5563#issuecomment-473458156:67,Testability,test,test,67,"So now the tests are passing locally for me, but not the call back test on the cloud...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-473458156
https://github.com/hail-is/hail/pull/5563#issuecomment-473473364:21,Testability,test,test,21,I think the callback test is the unreliable one? Looks like it got retested and is fine now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-473473364
https://github.com/hail-is/hail/pull/5563#issuecomment-473569649:28,Testability,test,test,28,I haven’t seen the callback test be flaky. I’m mildly concerned that it’s become flaky.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-473569649
https://github.com/hail-is/hail/issues/5564#issuecomment-471018741:103,Performance,load,loading-a-plink-file,103,I added a discuss post for our users https://discuss.hail.is/t/i-get-a-negativearraysizeexception-when-loading-a-plink-file/899,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471018741
https://github.com/hail-is/hail/issues/5564#issuecomment-471047536:19,Security,hash,hashCode,19,Confirmed that the hashCode solution works for Danfeng.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471047536
https://github.com/hail-is/hail/issues/5564#issuecomment-471050878:34,Security,hash,hashCode,34,"Closing with recommended solution hashCode=0, long term plan: eliminate Spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471050878
https://github.com/hail-is/hail/issues/5565#issuecomment-472066037:114,Security,hash,hash,114,Next steps seem to be:; - [ ] try master SKAT in a debian docker image (i.e. is this a *nix issue?); - [ ] find a hash that SKAT succeeds in the cloud,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5565#issuecomment-472066037
https://github.com/hail-is/hail/issues/5565#issuecomment-473009468:55,Performance,load,load,55,The root issue seems to be that native libraries don't load properly when they are binding to instance-methods as opposed to class-methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5565#issuecomment-473009468
https://github.com/hail-is/hail/issues/5566#issuecomment-471054575:18,Deployability,patch,patching,18,Seems like monkey patching with event can somehow override the timeout. We're not using event though. https://github.com/kennethreitz/requests/issues/3924#issuecomment-307502871,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575
https://github.com/hail-is/hail/issues/5566#issuecomment-471054575:63,Safety,timeout,timeout,63,Seems like monkey patching with event can somehow override the timeout. We're not using event though. https://github.com/kennethreitz/requests/issues/3924#issuecomment-307502871,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575
https://github.com/hail-is/hail/issues/5566#issuecomment-471056112:56,Testability,test,tests,56,"This seems to happen somewhat consistently to the CI PR tests, but I can't replicate locally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471056112
https://github.com/hail-is/hail/issues/5566#issuecomment-471059257:62,Safety,timeout,timeout,62,"Seems to have passed that PR now. I'm really not sure why our timeout isn't respected. I trolled through the requests issue tracker and didn't find anything relevant. Honestly, requests feels like a huge pile of indirection on top of urllib3 and it's really hard to understand. I'd prefer a library that was a bit cleaner.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471059257
https://github.com/hail-is/hail/pull/5572#issuecomment-471669508:13,Testability,test,test,13,"cool -- I'll test that out. IntelliJ kept telling me that wouldn't work, but maybe it's wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471669508
https://github.com/hail-is/hail/pull/5572#issuecomment-471675774:77,Integrability,depend,dependencies,77,I didn't read your comment carefully enough. Do we want to have all of these dependencies for our project? This is a more general question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471675774
https://github.com/hail-is/hail/pull/5572#issuecomment-471690561:0,Integrability,Depend,Depends,0,Depends on #5563,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471690561
https://github.com/hail-is/hail/pull/5572#issuecomment-471733373:48,Integrability,depend,dependencies,48,"@jigold If the concern is number of third party dependencies, I don’t really have an answer (number of third party dependencies last which productivity drops over some timespan). This library is something like 20 lines long, they just wrap __new__ and __init__ to make those awaitable. . I also think your approach is fine!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471733373
https://github.com/hail-is/hail/pull/5572#issuecomment-471733373:115,Integrability,depend,dependencies,115,"@jigold If the concern is number of third party dependencies, I don’t really have an answer (number of third party dependencies last which productivity drops over some timespan). This library is something like 20 lines long, they just wrap __new__ and __init__ to make those awaitable. . I also think your approach is fine!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471733373
https://github.com/hail-is/hail/pull/5572#issuecomment-471733373:235,Integrability,wrap,wrap,235,"@jigold If the concern is number of third party dependencies, I don’t really have an answer (number of third party dependencies last which productivity drops over some timespan). This library is something like 20 lines long, they just wrap __new__ and __init__ to make those awaitable. . I also think your approach is fine!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5572#issuecomment-471733373
https://github.com/hail-is/hail/pull/5574#issuecomment-471113267:56,Testability,test,test,56,"There's currently only one matching secret, it's in the test namespace, and it only has permission to write to gs://hail-ci-0-1-batch-volume-test-bucket",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574#issuecomment-471113267
https://github.com/hail-is/hail/pull/5574#issuecomment-471113267:141,Testability,test,test-bucket,141,"There's currently only one matching secret, it's in the test namespace, and it only has permission to write to gs://hail-ci-0-1-batch-volume-test-bucket",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5574#issuecomment-471113267
https://github.com/hail-is/hail/pull/5578#issuecomment-471601643:66,Testability,test,tests,66,"yes, good idea. I checked OrderingSuite on my machine and all the tests except the first are running at 30ms vs 3-7s again, but it'll be good to double check the CI timings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471601643
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:49,Modifiability,variab,variable,49,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:11,Testability,test,tests,11,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:233,Testability,test,test-report,233,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:400,Testability,test,test-report,400,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:535,Testability,test,tests,535,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:750,Testability,test,tests,750,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174
https://github.com/hail-is/hail/pull/5582#issuecomment-471740579:10,Availability,down,down,10,Hopefully down to ~2.5 minutes now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5582#issuecomment-471740579
https://github.com/hail-is/hail/issues/5589#issuecomment-483794933:226,Performance,perform,performance,226,"@tpoterba you're referring to the numeric restriction? any numeric expression can go to a distributed tensor, though we don't yet have one and zero dimensional distributed tensors. converting a global to a dict. tensor is bad performance but not invalid.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5589#issuecomment-483794933
https://github.com/hail-is/hail/pull/5593#issuecomment-473324852:4,Integrability,depend,dependencies,4,"The dependencies are used in `project_changed.py`. That's how I broke the CI before -- I changed batch, but the CI tests weren't run because we had no notion of project dependency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593#issuecomment-473324852
https://github.com/hail-is/hail/pull/5593#issuecomment-473324852:169,Integrability,depend,dependency,169,"The dependencies are used in `project_changed.py`. That's how I broke the CI before -- I changed batch, but the CI tests weren't run because we had no notion of project dependency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593#issuecomment-473324852
https://github.com/hail-is/hail/pull/5593#issuecomment-473324852:115,Testability,test,tests,115,"The dependencies are used in `project_changed.py`. That's how I broke the CI before -- I changed batch, but the CI tests weren't run because we had no notion of project dependency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5593#issuecomment-473324852
https://github.com/hail-is/hail/pull/5596#issuecomment-473367050:0,Integrability,Depend,Depends,0,"Depends on the partitioning, of course. Currently, I think Chris said ~1m at 20K partitions. I think we should have ~4K, in which case this overhead of this step would be 6%. There's a problem that we want courser partitioning earlier in the merge process, and finer later, but we don't have the ability to repartition dynamically. Note, this isn't 100% overhead, because there was a previous step to compute and collect the partitioning information which is now gone. If that's problematic there are additional ways to speed things up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5596#issuecomment-473367050
https://github.com/hail-is/hail/issues/5599#issuecomment-472976356:115,Availability,checkpoint,checkpoint,115,"I like this in concept, but I'm worried about people messing themselves up by changing upstream code and expecting checkpoint to rerun.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5599#issuecomment-472976356
https://github.com/hail-is/hail/pull/5601#issuecomment-473303829:10,Integrability,interface,interface,10,"Sure, the interface is slightly different though to accommodate for some of the new features. It also doesn't support passing python lists instead of hail expr -- could be trivially added if useful though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473303829
https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:192,Availability,down,downsampling,192,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:296,Availability,down,downsampling,296,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:31,Integrability,interface,interface,31,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:255,Usability,intuit,intuitive,255,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
https://github.com/hail-is/hail/issues/5603#issuecomment-476625941:154,Performance,Load,Loading,154,"can replicate:; ```; /Users/tpoterba/data/variants_out.txt', min_partitions=1000); 2019-03-26 09:13:47 Hail: INFO: Reading table with no type imputation; Loading column '19:60864:GCAGCCTCAGCACT:G' as type 'str' (type not specified). In [3]: ht.count(); [Stage 0:> (0 + 12) / 12]Out[3]: 848893. In [4]: ht.n_partitions(); Out[4]: 12; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5603#issuecomment-476625941
https://github.com/hail-is/hail/pull/5605#issuecomment-473422952:9,Deployability,pipeline,pipeline,9,"I ported pipeline to new batch, and in doing so I ripped out `copy_service_account_name` and just replaced its uses with `user`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5605#issuecomment-473422952
https://github.com/hail-is/hail/pull/5610#issuecomment-473423764:178,Security,access,access,178,"I've repurposed the `TIterable` class so that ""Iterable"" means something you can iterate over (containers and streams) and ""Container"" means something with elements that you can access by index/out of order (arrays, sets, dicts). This should make typing IRs easier if we intend to have a `ToStream` IR to enforce non-instantiation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5610#issuecomment-473423764
https://github.com/hail-is/hail/pull/5613#issuecomment-473453502:115,Modifiability,variab,variables,115,"as I understand, a java object is shared to Python via some reference, and it doesn't actually serialize the class variables. My guess is that the slow part in import_vcfs is actually constructing the MatrixTable objects -- this is quite slow due to all of the type nonsense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5613#issuecomment-473453502
https://github.com/hail-is/hail/pull/5615#issuecomment-475804671:0,Integrability,Depend,Depends,0,Depends on #5679,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615#issuecomment-475804671
https://github.com/hail-is/hail/pull/5615#issuecomment-476464978:25,Testability,test,tests,25,It's finally passing the tests!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615#issuecomment-476464978
https://github.com/hail-is/hail/pull/5615#issuecomment-477358846:95,Deployability,deploy,deploy,95,@jigold can you check batch in an hour and/or tomorrow morning to make sure its all good after deploy too?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5615#issuecomment-477358846
https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:34,Availability,error,errors,34,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743
https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:18,Deployability,pipeline,pipeline,18,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743
https://github.com/hail-is/hail/pull/5616#issuecomment-475252257:249,Deployability,install,installation,249,"This will allow you to go conda-free: https://github.com/hail-is/hail/pull/5655. In particular, I modified batch to be conda-free as part of this PR. I added the common python dependencies (async stuff, including mysql stuff) to the default python3 installation in the image. I think my PR will obviate this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257
https://github.com/hail-is/hail/pull/5616#issuecomment-475252257:176,Integrability,depend,dependencies,176,"This will allow you to go conda-free: https://github.com/hail-is/hail/pull/5655. In particular, I modified batch to be conda-free as part of this PR. I added the common python dependencies (async stuff, including mysql stuff) to the default python3 installation in the image. I think my PR will obviate this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257
https://github.com/hail-is/hail/pull/5617#issuecomment-474522874:1055,Safety,safe,safe,1055,"We don't really have the infrastructure to create arbitrary values in c++ right (without translating into potentially large trees of IR). Since they already exist as Scala annotations in the IR, I decided that it was easier to serialize them in scala and decode them in the function rather than try to create logic and encode them in the function itself (we can't create them at compile-time and pass around naked hail-values, since they're region-backed and the functions can get serialized and shipped to workers, which don't have access to those). I put the decoded literals on a SparkFunctionContext in the hopes that it will be easier to plug in another literal broadcaster if/when the `Literal` IR no longer holds a Scala annotation, but in the meantime this was a reasonably straightforward way to get them into the c++ Emit code. I think we may not need to serialize literals like this for the `cxx.Compile.apply` methods, since they should all execute on the master node, but we don't currently enforce that so I was serializing there just to be safe. The alternative is to enforce execution of these functions on the master node, which I don't see a huge problem with off the top of my head? @cseed do you have feelings about this? All the distributed computation is going to go through `CollectDistributedArray` and friends, which should create their own entrypoints.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874
https://github.com/hail-is/hail/pull/5617#issuecomment-474522874:533,Security,access,access,533,"We don't really have the infrastructure to create arbitrary values in c++ right (without translating into potentially large trees of IR). Since they already exist as Scala annotations in the IR, I decided that it was easier to serialize them in scala and decode them in the function rather than try to create logic and encode them in the function itself (we can't create them at compile-time and pass around naked hail-values, since they're region-backed and the functions can get serialized and shipped to workers, which don't have access to those). I put the decoded literals on a SparkFunctionContext in the hopes that it will be easier to plug in another literal broadcaster if/when the `Literal` IR no longer holds a Scala annotation, but in the meantime this was a reasonably straightforward way to get them into the c++ Emit code. I think we may not need to serialize literals like this for the `cxx.Compile.apply` methods, since they should all execute on the master node, but we don't currently enforce that so I was serializing there just to be safe. The alternative is to enforce execution of these functions on the master node, which I don't see a huge problem with off the top of my head? @cseed do you have feelings about this? All the distributed computation is going to go through `CollectDistributedArray` and friends, which should create their own entrypoints.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874
https://github.com/hail-is/hail/pull/5617#issuecomment-474522874:309,Testability,log,logic,309,"We don't really have the infrastructure to create arbitrary values in c++ right (without translating into potentially large trees of IR). Since they already exist as Scala annotations in the IR, I decided that it was easier to serialize them in scala and decode them in the function rather than try to create logic and encode them in the function itself (we can't create them at compile-time and pass around naked hail-values, since they're region-backed and the functions can get serialized and shipped to workers, which don't have access to those). I put the decoded literals on a SparkFunctionContext in the hopes that it will be easier to plug in another literal broadcaster if/when the `Literal` IR no longer holds a Scala annotation, but in the meantime this was a reasonably straightforward way to get them into the c++ Emit code. I think we may not need to serialize literals like this for the `cxx.Compile.apply` methods, since they should all execute on the master node, but we don't currently enforce that so I was serializing there just to be safe. The alternative is to enforce execution of these functions on the master node, which I don't see a huge problem with off the top of my head? @cseed do you have feelings about this? All the distributed computation is going to go through `CollectDistributedArray` and friends, which should create their own entrypoints.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:425,Availability,down,down,425,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1388,Availability,robust,robust,1388,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:203,Deployability,integrat,integrating,203,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:203,Integrability,integrat,integrating,203,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1647,Security,access,access,1647,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:335,Testability,log,login,335,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:658,Testability,log,login,658,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1371,Testability,log,logic,1371,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1430,Testability,log,logic,1430,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:97,Usability,simpl,simplifies,97,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:163,Usability,simpl,simpler,163,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1089,Usability,usab,usability,1089,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
https://github.com/hail-is/hail/pull/5618#issuecomment-473591074:325,Availability,down,down,325,"@cseed I had to make the CloudSQL instance have a public IP in order for testing locally (not in the cluster) to work. Should I get rid of that option? Or can we have a separate test database? As for permissions for the databases, I couldn't find a way to say a specific user could not create a database. I think we can lock down a database with SQL commands after the database has been created. This will be good to discuss on Monday. See #5615",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473591074
https://github.com/hail-is/hail/pull/5618#issuecomment-473591074:73,Testability,test,testing,73,"@cseed I had to make the CloudSQL instance have a public IP in order for testing locally (not in the cluster) to work. Should I get rid of that option? Or can we have a separate test database? As for permissions for the databases, I couldn't find a way to say a specific user could not create a database. I think we can lock down a database with SQL commands after the database has been created. This will be good to discuss on Monday. See #5615",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473591074
https://github.com/hail-is/hail/pull/5618#issuecomment-473591074:178,Testability,test,test,178,"@cseed I had to make the CloudSQL instance have a public IP in order for testing locally (not in the cluster) to work. Should I get rid of that option? Or can we have a separate test database? As for permissions for the databases, I couldn't find a way to say a specific user could not create a database. I think we can lock down a database with SQL commands after the database has been created. This will be good to discuss on Monday. See #5615",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473591074
https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:45,Security,secur,security,45,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373
https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:619,Security,access,access,619,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373
https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:148,Testability,test,testing,148,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373
https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:186,Testability,test,test,186,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373
https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:299,Testability,test,testing,299,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373
https://github.com/hail-is/hail/pull/5618#issuecomment-477827672:152,Availability,ping,ping,152,"edit:. Issue seems to be something else; may be having issue connecting to sql host on cluster, at least when executed by ci. @jigold I'll fix it, then ping you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-477827672
https://github.com/hail-is/hail/pull/5618#issuecomment-478149347:440,Testability,log,log,440,"@jigold Lets see if it builds. I added a hail-ci-build.sh to vdc, since that is a project root (I don't know whether I can specify in projects vdc/scripts/user , and vdc already existed as project). I am seeing a slightly weird accumulation of gcp service accounts that look like they're being created by my script, but I'm not really sure why that's happening; each run of the script generate 1, and this hasn't been under CI (under build log nothing suggests this script was running)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478149347
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:177,Availability,error,error,177,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:430,Availability,error,error,430,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:8,Testability,test,test,8,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:55,Testability,test,test,55,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:444,Testability,test,test,444,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:153,Usability,simpl,simply,153,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
https://github.com/hail-is/hail/pull/5618#issuecomment-478592569:121,Deployability,install,install,121,"@jigold ; this should be working, not sure why it's stuck in pending. If there is an issue I suspect it's my use of pip3 install. I'm also going to split out the /docker changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478592569
https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:26,Deployability,install,installed,26,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954
https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:40,Modifiability,config,configured,40,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954
https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:331,Modifiability,config,configured,331,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954
https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:378,Modifiability,config,configured,378,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954
https://github.com/hail-is/hail/pull/5622#issuecomment-477363453:2,Deployability,update,updated,2,I updated this one too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5622#issuecomment-477363453
https://github.com/hail-is/hail/pull/5623#issuecomment-473970604:129,Integrability,depend,dependencies,129,"I agree about conda and having a set of docker images for the whole repo. I ran into other problems with conda and inter-project dependencies last week. The solution was to not rely on conda to specify the environment and just use `setup.py`. I can see this approach failing if different projects require different versions of the same package. Also, I think ci is using Python 3.7 while other projects are 3.6. We should probably pick a version for the entire project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-473970604
https://github.com/hail-is/hail/pull/5623#issuecomment-474007645:9,Deployability,upgrade,upgrade,9,"OK, I'll upgrade to 3.7. There is a Python ppa with the latest version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474007645
https://github.com/hail-is/hail/pull/5623#issuecomment-474131577:198,Deployability,upgrade,upgrade,198,"I am (temporarily) defeated. Getting a working python 3.7 with pip on a standard distribution turns out to be non-trivial. Since ci doesn't specifically rely on 3.7 yet, I say we start with 3.6 and upgrade when it isn't too painful. It seems everyone is onboard with this. I am happy to move forward if you are, @jigold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474131577
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1476,Availability,mainten,maintenance,1476,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:360,Deployability,install,install,360,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:414,Deployability,install,install,414,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1166,Deployability,release,releases,1166,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1189,Deployability,release,released,1189,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:517,Modifiability,layers,layers,517,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:147,Performance,cache,cache,147,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:225,Performance,cache,cache,225,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:527,Performance,cache,cache,527,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:746,Performance,cache,cache,746,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:876,Performance,cache,cache-from,876,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:897,Performance,cache,cache,897,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:941,Performance,cache,cache-from,941,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:976,Performance,cache,cache,976,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1047,Performance,cache,cache,1047,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1100,Performance,cache,cache-from,1100,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1235,Performance,cache,cache-from,1235,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1662,Performance,cache,cache,1662,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1719,Performance,cache,cache,1719,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1784,Performance,cache,cache-not-image-itself,1784,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:266,Security,secur,security,266,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:163,Energy Efficiency,efficient,efficient,163,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:157,Performance,cache,cache-efficient,157,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:228,Performance,concurren,concurrently,228,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:347,Testability,log,login,347,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:353,Testability,log,logout,353,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:300,Usability,UX,UX,300,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
https://github.com/hail-is/hail/pull/5623#issuecomment-474448122:257,Security,secur,secure,257,"@cseed that seems good enough for our purposes. We can run it, unprivileged, on a VM outside of k8s. We have a thin service in k8s that has privilege to talk to that VM. It streams build context and docker file to said VM. That VM invokes `img`. That VM is secure as long as `img` doesn't allow builds to escalate their privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474448122
https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:143,Deployability,deploy,deploying,143,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308
https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:80,Integrability,depend,dependency,80,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308
https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:131,Testability,test,testing,131,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308
https://github.com/hail-is/hail/pull/5624#issuecomment-474138152:31,Integrability,depend,depends,31,"OK, should be fixed. apiserver depends on hail, runs its own tests now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474138152
https://github.com/hail-is/hail/pull/5624#issuecomment-474138152:61,Testability,test,tests,61,"OK, should be fixed. apiserver depends on hail, runs its own tests now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474138152
https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:27,Availability,failure,failures,27,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147
https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:57,Availability,failure,failure,57,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147
https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:90,Availability,error,error,90,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147
https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:51,Testability,Test,Test,51,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147
https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:103,Testability,log,log,103,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147
https://github.com/hail-is/hail/pull/5624#issuecomment-474167431:83,Availability,down,download,83,"The movie lens dataset (used by some of the tutorials, not hosted by us) failed to download. It happens sometimes. I pushed an empty commit to have it retest. In general, if a PR looks good but the tests are failing, I approve. The robots will handle the tests, so I don't have to. If fixing a bug requires significant changes, or changes not in the spirit of the original PR, as an author, I dismiss the review and request another one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431
https://github.com/hail-is/hail/pull/5624#issuecomment-474167431:198,Testability,test,tests,198,"The movie lens dataset (used by some of the tutorials, not hosted by us) failed to download. It happens sometimes. I pushed an empty commit to have it retest. In general, if a PR looks good but the tests are failing, I approve. The robots will handle the tests, so I don't have to. If fixing a bug requires significant changes, or changes not in the spirit of the original PR, as an author, I dismiss the review and request another one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431
https://github.com/hail-is/hail/pull/5624#issuecomment-474167431:255,Testability,test,tests,255,"The movie lens dataset (used by some of the tutorials, not hosted by us) failed to download. It happens sometimes. I pushed an empty commit to have it retest. In general, if a PR looks good but the tests are failing, I approve. The robots will handle the tests, so I don't have to. If fixing a bug requires significant changes, or changes not in the spirit of the original PR, as an author, I dismiss the review and request another one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431
https://github.com/hail-is/hail/pull/5624#issuecomment-475123834:132,Deployability,install,installed,132,This should be fixed once https://github.com/hail-is/hail/pull/5655 goes in and the pr-builder has the standard python dependencies installed for the default python3 image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834
https://github.com/hail-is/hail/pull/5624#issuecomment-475123834:119,Integrability,depend,dependencies,119,This should be fixed once https://github.com/hail-is/hail/pull/5655 goes in and the pr-builder has the standard python dependencies installed for the default python3 image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834
https://github.com/hail-is/hail/pull/5626#issuecomment-475318506:24,Testability,test,test,24,@danking looks like the test is still running into GC issues?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5626#issuecomment-475318506
https://github.com/hail-is/hail/pull/5627#issuecomment-474470091:133,Energy Efficiency,allocate,allocated,133,"re: memory allocation --- passing NDArrays around on the stack seems adequate for now given that the data array will still be region-allocated and I don't think we expect the rest of it to ever get large, but we should probably set up a time to talk about a fuller plan for allocation/memory management sooner rather than later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5627#issuecomment-474470091
https://github.com/hail-is/hail/pull/5633#issuecomment-474443321:1066,Deployability,update,update,1066,"I think I should return the google service account email rather than the name. I propose that I return instead of:. ```; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. this:. ```; 'gsa_email': 'user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. This is because, although google service account management occurs by the `name`, acl operations appear to use the `email` or `uniqueId<int>`. Also, it appears that we don't need to specify the projectId to take operations on the [google service account](https://cloud.google.com/iam/docs/creating-managing-service-accounts#deleting_a_service_account). For reference, the google service account creation response. ```; {'name': 'projects/hail-vdc/serviceAccounts/user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'projectId': 'hail-vdc', 'uniqueId': '<some_int_id>', 'email': 'user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'displayName': 'user', 'etag': 'MDEwMjE5MjA=', 'oauth2ClientId': '<some_int_id>'}; ```. I will update this PR to return gsa_email (and optionally gsa_projectId, although omitting for now because we have only 1, and may always have only 1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474443321
https://github.com/hail-is/hail/pull/5633#issuecomment-474453397:13,Safety,safe,safe,13,"I think it's safe to assume there will only be one project for the foreseeable future, and it's the one the cluster is running in. Also, it is in the service account email.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474453397
https://github.com/hail-is/hail/pull/5633#issuecomment-474511788:17,Security,secur,security,17,"@danking : added security flags (httponly, secure, samesite). should be ready +/- if Cotton wants me to add a cookie field that stores the Kubernetes secret path, or whether we can have that stored under the user's Kubernetes namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474511788
https://github.com/hail-is/hail/pull/5633#issuecomment-474511788:43,Security,secur,secure,43,"@danking : added security flags (httponly, secure, samesite). should be ready +/- if Cotton wants me to add a cookie field that stores the Kubernetes secret path, or whether we can have that stored under the user's Kubernetes namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474511788
https://github.com/hail-is/hail/pull/5633#issuecomment-477341107:7,Security,access,access,7,"I need access to the users' GCP SA key file, which I think is most naturally stored as a k8s secret.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-477341107
https://github.com/hail-is/hail/pull/5636#issuecomment-474554637:48,Modifiability,config,configured,48,"argh, apologies for the branch on origin - just configured this hail setup and haven't set the forks properly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5636#issuecomment-474554637
https://github.com/hail-is/hail/pull/5638#issuecomment-474571477:2,Testability,test,test,2,"> test/test-ci.py::test_pull_request_trigger FAILED. This is the flakey one, right? Can we disable it until we get it fixed? Flakey tests are not OK.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474571477
https://github.com/hail-is/hail/pull/5638#issuecomment-474571477:7,Testability,test,test-ci,7,"> test/test-ci.py::test_pull_request_trigger FAILED. This is the flakey one, right? Can we disable it until we get it fixed? Flakey tests are not OK.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474571477
https://github.com/hail-is/hail/pull/5638#issuecomment-474571477:132,Testability,test,tests,132,"> test/test-ci.py::test_pull_request_trigger FAILED. This is the flakey one, right? Can we disable it until we get it fixed? Flakey tests are not OK.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474571477
https://github.com/hail-is/hail/pull/5638#issuecomment-474580215:86,Safety,timeout,timeout,86,"It's flakey when we have 32 open PRs, it's not flakey if we have 10. I can adjust the timeout to be more generous.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474580215
https://github.com/hail-is/hail/pull/5638#issuecomment-474581928:131,Energy Efficiency,schedul,scheduled,131,It's currently waiting 7.5 minutes for batch to finish running a trivial job. I think if we're waiting 7.5 minutes for jobs to get scheduled the problem is that the cluster is too small.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474581928
https://github.com/hail-is/hail/pull/5639#issuecomment-474659262:105,Testability,test,test,105,"While this is an improvement, I don't think it fixes problem, and therefore still advocate disabling the test until we can implement it correctly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474659262
https://github.com/hail-is/hail/pull/5639#issuecomment-474817396:92,Deployability,deploy,deploy,92,"The problem is it takes more than 7 minutes to schedule a trivial CI job and then a trivial deploy job, I could set the retries or try-delay higher, but what else could correct mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396
https://github.com/hail-is/hail/pull/5639#issuecomment-474817396:47,Energy Efficiency,schedul,schedule,47,"The problem is it takes more than 7 minutes to schedule a trivial CI job and then a trivial deploy job, I could set the retries or try-delay higher, but what else could correct mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396
https://github.com/hail-is/hail/pull/5639#issuecomment-474817750:77,Testability,test,test,77,I think we are a very long way from any system that reserves resources for a test job and in the meantime knowing the ci works is valuable.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817750
https://github.com/hail-is/hail/pull/5641#issuecomment-475309489:49,Integrability,depend,dependency,49,"This should start passing when ""add batch docker dependency .."" https://github.com/hail-is/hail/pull/5655/files merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5641#issuecomment-475309489
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:512,Deployability,configurat,configuration,512,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:145,Modifiability,config,config,145,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:379,Modifiability,config,config,379,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:439,Modifiability,config,config,439,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:512,Modifiability,config,configuration,512,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:714,Modifiability,config,config,714,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:769,Modifiability,config,config,769,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:801,Modifiability,Config,Config,801,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:475,Usability,guid,guide,475,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
https://github.com/hail-is/hail/pull/5644#issuecomment-474937917:21,Deployability,deploy,deploy,21,force merging to fix deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5644#issuecomment-474937917
https://github.com/hail-is/hail/pull/5645#issuecomment-475346629:70,Integrability,interface,interface,70,@konradjk This should be good enough for now. I think I need a better interface for resource groups with regards to file extensions for both `read_input_group` and `write_output`. I made an issue for it #5661,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5645#issuecomment-475346629
https://github.com/hail-is/hail/pull/5655#issuecomment-475060566:36,Integrability,depend,dependency,36,"Hmm, there's another issue with our dependency management. If batch changes and docker doesn't, docker won't get rebuilt locally and batch will fail. I made docker touch a build stamp file (so we don't end up building docker multiple times) and make scorecard and batch build it before building themselves.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475060566
https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:129,Deployability,install,installs,129,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687
https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:354,Deployability,deploy,deploy,354,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687
https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:327,Performance,cache,cache-from,327,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687
https://github.com/hail-is/hail/pull/5655#issuecomment-475301184:29,Integrability,depend,dependencies,29,@cseed I added inter-project dependencies to `projects.yaml` does that address the docker-batch dependence?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475301184
https://github.com/hail-is/hail/pull/5655#issuecomment-475301184:96,Integrability,depend,dependence,96,@cseed I added inter-project dependencies to `projects.yaml` does that address the docker-batch dependence?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475301184
https://github.com/hail-is/hail/pull/5655#issuecomment-475308077:92,Deployability,deploy,deploy,92,@cseed I'm also OK approving and addressing these issues in another PR if that unblocks our deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475308077
https://github.com/hail-is/hail/pull/5655#issuecomment-475317040:153,Integrability,depend,dependencies,153,"OK, I think I addressed the comments:; - I'm going to leave the pylint fix for another PR,; - use latest explicitly for remote repositories,; - explicit dependencies on build-stmp instead of find",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475317040
https://github.com/hail-is/hail/pull/5655#issuecomment-475474180:90,Availability,error,errors,90,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
https://github.com/hail-is/hail/pull/5655#issuecomment-475474180:185,Availability,down,down,185,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
https://github.com/hail-is/hail/pull/5655#issuecomment-475474180:299,Usability,clear,clear,299,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
https://github.com/hail-is/hail/pull/5655#issuecomment-475474180:413,Usability,clear,clear,413,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
https://github.com/hail-is/hail/pull/5655#issuecomment-475664544:14,Availability,error,errors,14,What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475664544
https://github.com/hail-is/hail/pull/5655#issuecomment-475664544:95,Testability,log,logs,95,What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475664544
https://github.com/hail-is/hail/pull/5655#issuecomment-475664792:132,Usability,learn,learned,132,I'm worried that we're back into a bad state where we'll keep seeing the same job over and over again and not realize we've already learned from it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475664792
https://github.com/hail-is/hail/pull/5655#issuecomment-475978868:24,Deployability,update,update,24,"This broke CI, it can't update from batch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475978868
https://github.com/hail-is/hail/pull/5655#issuecomment-476227436:16,Availability,error,errors,16,"> What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted. We were setting the Job id attribute to None on deletion. ci then queried various properties of the job, passing None as the job id in the URL. That caused batch to 500 converting the id to an integer (this is itself a bug). Leaving the id in the Job might also fix it, but it seems wrong in the REST setting to delete an object, but still be able to query it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-476227436
https://github.com/hail-is/hail/pull/5655#issuecomment-476227436:97,Testability,log,logs,97,"> What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted. We were setting the Job id attribute to None on deletion. ci then queried various properties of the job, passing None as the job id in the URL. That caused batch to 500 converting the id to an integer (this is itself a bug). Leaving the id in the Job might also fix it, but it seems wrong in the REST setting to delete an object, but still be able to query it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-476227436
https://github.com/hail-is/hail/issues/5657#issuecomment-475096140:120,Availability,error,error,120,"can you try running with the latest version? I fixed this specific issue recently, but it's possible you'll get another error (it's not tested as part of our CI)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657#issuecomment-475096140
https://github.com/hail-is/hail/issues/5657#issuecomment-475096140:136,Testability,test,tested,136,"can you try running with the latest version? I fixed this specific issue recently, but it's possible you'll get another error (it's not tested as part of our CI)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657#issuecomment-475096140
https://github.com/hail-is/hail/issues/5657#issuecomment-476543562:75,Usability,responsiv,responsive,75,"If this is still an issue, please make a post on the forum, OK? We're more responsive there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657#issuecomment-476543562
https://github.com/hail-is/hail/pull/5658#issuecomment-475282912:181,Deployability,release,released,181,"This particular issue is that I recently added a reference to an Array of IR in the JVM that is owned by python. When the owning object is destructed, the reference in Java will be released, enabling it to be garbage collected. Python won't run a GC pass inside a function without requesting it in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5658#issuecomment-475282912
https://github.com/hail-is/hail/issues/5659#issuecomment-475210838:35,Availability,error,error-building-jar-for-hail-,35,moved to https://discuss.hail.is/t/error-building-jar-for-hail-0-2-11/912,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5659#issuecomment-475210838
https://github.com/hail-is/hail/pull/5662#issuecomment-476883070:40,Availability,error,errors,40,@daniel-goldstein still got some parser errors in the python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5662#issuecomment-476883070
https://github.com/hail-is/hail/pull/5662#issuecomment-476883070:61,Testability,test,tests,61,@daniel-goldstein still got some parser errors in the python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5662#issuecomment-476883070
https://github.com/hail-is/hail/pull/5666#issuecomment-475778217:8,Availability,failure,failure,8,"doctest failure from show() changing the width, it seems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5666#issuecomment-475778217
https://github.com/hail-is/hail/pull/5667#issuecomment-475419212:13,Performance,optimiz,optimize,13,"This doesn't optimize anything w.r.t. calculating let values. This is just to ensure that a stray `Let` that hasn't been moved already in an earlier optimization pass doesn't break up array deforestation since the values in those cases aren't calculated in the per-element scope. I think I don't quite understand what you're getting at with those examples---as far as I can tell, the let is defined on the outside and the value is therefore only ever computed once? I haven't changed that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5667#issuecomment-475419212
https://github.com/hail-is/hail/pull/5667#issuecomment-475419212:149,Performance,optimiz,optimization,149,"This doesn't optimize anything w.r.t. calculating let values. This is just to ensure that a stray `Let` that hasn't been moved already in an earlier optimization pass doesn't break up array deforestation since the values in those cases aren't calculated in the per-element scope. I think I don't quite understand what you're getting at with those examples---as far as I can tell, the let is defined on the outside and the value is therefore only ever computed once? I haven't changed that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5667#issuecomment-475419212
https://github.com/hail-is/hail/pull/5670#issuecomment-475490709:38,Performance,cache,cache,38,Pushed another commit: use the gradle cache when building Hail for apiserver.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5670#issuecomment-475490709
https://github.com/hail-is/hail/pull/5676#issuecomment-475758539:23,Testability,test,test,23,I added a BGEN cluster test to this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5676#issuecomment-475758539
https://github.com/hail-is/hail/pull/5677#issuecomment-476797101:6,Usability,clear,clear,6,"to be clear, there is an option to display row fields, but it defaults to `False`. There's a lot of visual noise if you show both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5677#issuecomment-476797101
https://github.com/hail-is/hail/pull/5679#issuecomment-475787151:47,Testability,test,test,47,"hmm seems like something is wrong, maybe flaky test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5679#issuecomment-475787151
https://github.com/hail-is/hail/pull/5681#issuecomment-475979215:5,Deployability,deploy,deploying,5,hand deploying,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5681#issuecomment-475979215
https://github.com/hail-is/hail/pull/5686#issuecomment-477168302:50,Availability,failure,failure,50,@daniel-goldstein looks like there's still a test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477168302
https://github.com/hail-is/hail/pull/5686#issuecomment-477168302:45,Testability,test,test,45,@daniel-goldstein looks like there's still a test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477168302
https://github.com/hail-is/hail/pull/5686#issuecomment-477169272:158,Testability,test,tests,158,Just now? It broke after switching to use `hadoop_copy` but should have fixed that this morning with my commit. I've been checking in and haven't seen it run tests yet.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477169272
https://github.com/hail-is/hail/pull/5686#issuecomment-477170521:107,Testability,test,test,107,Ya I really need to set up running clusters locally. It's a frustrating feedback loop for a single cluster test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477170521
https://github.com/hail-is/hail/pull/5686#issuecomment-477170521:72,Usability,feedback,feedback,72,Ya I really need to set up running clusters locally. It's a frustrating feedback loop for a single cluster test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477170521
https://github.com/hail-is/hail/pull/5690#issuecomment-477173194:0,Testability,Test,Tested,0,"Tested and appears to run on a medium sized dataset (1700 samples), definitely saves a lot of time (no more ~45 minute break between stages while things serialize)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477173194
https://github.com/hail-is/hail/pull/5690#issuecomment-477306260:148,Availability,error,error,148,"I fixed the part where the behavior of locus_windows was changed, and now the behavior should be consistent with the previous version. (Some of the error types were changed, but I don't really see that as a breaking interface change).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260
https://github.com/hail-is/hail/pull/5690#issuecomment-477306260:216,Integrability,interface,interface,216,"I fixed the part where the behavior of locus_windows was changed, and now the behavior should be consistent with the previous version. (Some of the error types were changed, but I don't really see that as a breaking interface change).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260
https://github.com/hail-is/hail/pull/5692#issuecomment-479642895:20,Availability,fault,fault,20,"Heh, this is all my fault: https://github.com/hail-is/hail/pull/5078/files",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479642895
https://github.com/hail-is/hail/pull/5692#issuecomment-479643139:36,Performance,perform,performance,36,"yeah, the `keyed=False` path is for performance and I'm OK leaving that with duplicated nodes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479643139
https://github.com/hail-is/hail/pull/5698#issuecomment-476796791:130,Modifiability,rewrite,rewrite,130,"I think this could interact badly with Spark's partitioning logic. That would appear in the text file imports. We should probably rewrite the text file stuff the same way we rewrote import_bgen to use our own, sensible, partitioning logic. I guess the worst thing that happens is a small text file is broken into one partition per-line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791
https://github.com/hail-is/hail/pull/5698#issuecomment-476796791:60,Testability,log,logic,60,"I think this could interact badly with Spark's partitioning logic. That would appear in the text file imports. We should probably rewrite the text file stuff the same way we rewrote import_bgen to use our own, sensible, partitioning logic. I guess the worst thing that happens is a small text file is broken into one partition per-line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791
https://github.com/hail-is/hail/pull/5698#issuecomment-476796791:233,Testability,log,logic,233,"I think this could interact badly with Spark's partitioning logic. That would appear in the text file imports. We should probably rewrite the text file stuff the same way we rewrote import_bgen to use our own, sensible, partitioning logic. I guess the worst thing that happens is a small text file is broken into one partition per-line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791
https://github.com/hail-is/hail/issues/5700#issuecomment-478923407:9,Availability,error,error,9,the real error is in there at the top: ; ```; ExpressionException: Hail cannot impute the type of 'None'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5700#issuecomment-478923407
https://github.com/hail-is/hail/pull/5710#issuecomment-479908699:33,Integrability,depend,depends,33,`-_______________________-`. Now depends on #5772,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-479908699
https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:456,Availability,avail,available,456,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172
https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:65,Performance,optimiz,optimize,65,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172
https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:370,Performance,optimiz,optimization,370,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172
https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:162,Testability,test,test,162,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172
https://github.com/hail-is/hail/pull/5710#issuecomment-483001807:17,Modifiability,extend,extend,17,"One option is to extend NormalizeNames to take a prefix, and use a uid as a prefix when calling NormalizeNames inside ForwardLets. . Another option is to add an option to NOT run NormalizeNames inside the optimizer, when we known that there cannot be name collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807
https://github.com/hail-is/hail/pull/5710#issuecomment-483001807:205,Performance,optimiz,optimizer,205,"One option is to extend NormalizeNames to take a prefix, and use a uid as a prefix when calling NormalizeNames inside ForwardLets. . Another option is to add an option to NOT run NormalizeNames inside the optimizer, when we known that there cannot be name collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807
https://github.com/hail-is/hail/pull/5710#issuecomment-483514236:19,Modifiability,extend,extend,19,"> One option is to extend NormalizeNames to take a prefix. Yeah, or use generate uids instead of counting up from 0. I think ForwardLets should take a flag for this (or be parameterized with a name generator).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236
https://github.com/hail-is/hail/pull/5710#issuecomment-483514236:172,Modifiability,parameteriz,parameterized,172,"> One option is to extend NormalizeNames to take a prefix. Yeah, or use generate uids instead of counting up from 0. I think ForwardLets should take a flag for this (or be parameterized with a name generator).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236
https://github.com/hail-is/hail/pull/5723#issuecomment-478703255:168,Testability,test,test,168,@catoverdrive This looks like it's been running for 2 hours now. I would have expected it to be merged already as well. Could this cause an infinite loop -- you didn't test it locally correct?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5723#issuecomment-478703255
https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:165,Modifiability,extend,extending,165,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:235,Modifiability,variab,variable,235,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:257,Modifiability,variab,variables,257,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:388,Modifiability,variab,variable,388,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:469,Usability,feedback,feedback,469,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
https://github.com/hail-is/hail/pull/5737#issuecomment-479530162:5,Deployability,update,updated,5,Also updated binary ops to promote arg types to the return type (e.g. int32 -> float64) if the return type is also numeric (not bools though). This eliminates the need to double-check the signature of a division both in the IR and expressions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479530162
https://github.com/hail-is/hail/pull/5738#issuecomment-479527243:128,Performance,perform,perform,128,"So, the deserializer uses camel case due to the underlying YAML-centered focus of k8s. I sympathize that this is confusing. We [perform this mapping explicitly in batch](https://github.com/hail-is/hail/blob/master/batch/batch/client.py#L167-L179). There's a pair of issues that [track the confusing behavior of `to_dict`](https://github.com/kubernetes-client/python/issues/683) and [provide a workaround](https://github.com/kubernetes-client/python/issues/390):. ```; >>> x = kube.client.V1PodSpec(containers=[], service_account_name='foo'); >>> api.sanitize_for_serialization(x); {'containers': [], 'serviceAccountName': 'foo'}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5738#issuecomment-479527243
https://github.com/hail-is/hail/pull/5740#issuecomment-478736550:15,Integrability,depend,depend,15,"ugh. This must depend on another PR I have open, to remove that assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5740#issuecomment-478736550
https://github.com/hail-is/hail/pull/5740#issuecomment-478736550:64,Testability,assert,assertion,64,"ugh. This must depend on another PR I have open, to remove that assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5740#issuecomment-478736550
https://github.com/hail-is/hail/pull/5740#issuecomment-478750679:21,Integrability,depend,depends,21,"yes, looks like this depends on ForwardLets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5740#issuecomment-478750679
https://github.com/hail-is/hail/issues/5744#issuecomment-479056057:152,Availability,error,error,152,"it must be some system- or file-system- specific issue -- is there any possibility you could try to use 0.2.8 to read that file, and see if you get the error (or another one)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744#issuecomment-479056057
https://github.com/hail-is/hail/issues/5744#issuecomment-480405929:79,Deployability,release,release,79,Would moving back to Spark 2.2 fix this issue? Or should I just wait for a new release with the fix?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5744#issuecomment-480405929
https://github.com/hail-is/hail/issues/5745#issuecomment-555097320:39,Safety,Timeout,Timeout,39,"@jigold Can you elaborate on this one? Timeout to me means testing times out, which does happen in our current CI. Should it be closed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5745#issuecomment-555097320
https://github.com/hail-is/hail/issues/5745#issuecomment-555097320:59,Testability,test,testing,59,"@jigold Can you elaborate on this one? Timeout to me means testing times out, which does happen in our current CI. Should it be closed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5745#issuecomment-555097320
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2326,Availability,failure,failureThreshold,2326,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:3079,Availability,toler,tolerations,3079,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:3167,Availability,toler,tolerationSeconds,3167,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:3267,Availability,toler,tolerationSeconds,3267,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2929,Energy Efficiency,schedul,schedulerName,2929,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2952,Energy Efficiency,schedul,scheduler,2952,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2294,Integrability,protocol,protocol,2294,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2478,Safety,timeout,timeoutSeconds,2478,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:259,Security,access,access,259,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:377,Security,authoriz,authorization,377,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:848,Security,access,access,848,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2963,Security,secur,securityContext,2963,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:1010,Testability,test,test,1010,"o create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2406,Testability,log,login,2406,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2591,Testability,log,log,2591,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:220,Usability,clear,clearly,220,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
https://github.com/hail-is/hail/pull/5753#issuecomment-479298393:40,Security,access,accessing,40,"My guess is the launched pod is the one accessing the secret, with its service account. You could check this by removing the user can read secret binding and try launching a pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479298393
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:624,Energy Efficiency,power,powerful,624,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:330,Modifiability,config,config,330,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:10,Security,Authoriz,Authorization,10,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:71,Security,access,access-authn-authz,71,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:90,Security,authoriz,authorization,90,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:165,Security,access,access,165,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:484,Security,authoriz,authorization,484,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:639,Energy Efficiency,power,powerful,639,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:338,Modifiability,config,config,338,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:12,Security,Authoriz,Authorization,12,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:73,Security,access,access-authn-authz,73,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:92,Security,authoriz,authorization,92,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:173,Security,access,access,173,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:492,Security,authoriz,authorization,492,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942
https://github.com/hail-is/hail/pull/5753#issuecomment-479641018:79,Safety,risk,risk,79,"I think the takeaway is: notebook's ability to create pods makes it a security risk, so we gotta treat all this code with care.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018
https://github.com/hail-is/hail/pull/5753#issuecomment-479641018:70,Security,secur,security,70,"I think the takeaway is: notebook's ability to create pods makes it a security risk, so we gotta treat all this code with care.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018
https://github.com/hail-is/hail/pull/5753#issuecomment-479643234:356,Energy Efficiency,reduce,reduce,356,"@danking, @cseed An alternative: [as mentioned in the ticket Dan linked] the acl boundary for pod creation is a namespace. If we scope all user resources to their namespace, and during user resource creation give notebook service account 'create-pod' permissions in the user's namespace, and also remove create pod permissions in the default namespace, we reduce the likelihood that a compromised notebook leader could expose user secrets and other data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234
https://github.com/hail-is/hail/pull/5753#issuecomment-479643234:419,Security,expose,expose,419,"@danking, @cseed An alternative: [as mentioned in the ticket Dan linked] the acl boundary for pod creation is a namespace. If we scope all user resources to their namespace, and during user resource creation give notebook service account 'create-pod' permissions in the user's namespace, and also remove create pod permissions in the default namespace, we reduce the likelihood that a compromised notebook leader could expose user secrets and other data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234
https://github.com/hail-is/hail/pull/5756#issuecomment-479597161:23,Deployability,deploy,deploy,23,"Yeah, is this going to deploy new jars only on 2.4 or also 2.2 for a while? This is good in the long run, but might be nice to keep 2.2 going in case there are any unintended consequences",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756#issuecomment-479597161
https://github.com/hail-is/hail/pull/5756#issuecomment-480369608:48,Testability,test,tests,48,I added a cloudtools version pin in the cluster tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756#issuecomment-480369608
https://github.com/hail-is/hail/pull/5756#issuecomment-480515434:30,Testability,test,tests,30,Something is wrong with batch tests ...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5756#issuecomment-480515434
https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:52,Availability,error,error,52,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017
https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:163,Deployability,pipeline,pipeline-,163,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017
https://github.com/hail-is/hail/issues/5759#issuecomment-479599142:89,Availability,error,error,89,"Argh, sorry, it's actually the parens, not the spaces:; ```; /bin/sh: -c: line 9: syntax error near unexpected token `('; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599142
https://github.com/hail-is/hail/pull/5762#issuecomment-479569572:38,Testability,test,test,38,Activating the service account in the test locally screwed up my local permissions. Need to fix before merging.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479569572
https://github.com/hail-is/hail/pull/5762#issuecomment-479579007:40,Testability,test,test,40,> Activating the service account in the test locally screwed up my local permissions. Need to fix before merging. sounds good,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479579007
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:258,Deployability,pipeline,pipeline-secrets,258,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:275,Deployability,pipeline,pipeline-test-,275,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:327,Deployability,pipeline,pipeline-test-,327,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:402,Deployability,pipeline,pipeline-test-,402,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:284,Testability,test,test-,284,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:336,Testability,test,test-,336,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:411,Testability,test,test-,411,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435
https://github.com/hail-is/hail/pull/5762#issuecomment-479631417:70,Testability,test,test,70,"@akotlar : @danking and I discussed this in person and we can add the test once the service account infrastructure is in place in Batch. Until then, this approach will not work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479631417
https://github.com/hail-is/hail/pull/5762#issuecomment-479632064:72,Testability,test,test,72,"> @akotlar : @danking and I discussed this in person and we can add the test once the service account infrastructure is in place in Batch. Until then, this approach will not work. got it, so ready to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479632064
https://github.com/hail-is/hail/pull/5772#issuecomment-480308468:108,Integrability,wrap,wrap,108,"Had an idea about an easier way to fix this: don't support open terms in interpret, compile, etc., and then wrap the open term in an ArrayAgg to make it closed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5772#issuecomment-480308468
https://github.com/hail-is/hail/pull/5787#issuecomment-480268873:30,Integrability,depend,dependent,30,"you were the reviewer for the dependent PR, so you did fully approve the change. It went in as one commit instead of two, which saves the CI some cycles and slightly obfuscates the git history 🤷‍♂️",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5787#issuecomment-480268873
https://github.com/hail-is/hail/pull/5787#issuecomment-480269446:14,Integrability,depend,dependent,14,"unless -- the dependent PR was fully rebased, and so the SHA of the merged master was identical! That's actually possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5787#issuecomment-480269446
https://github.com/hail-is/hail/pull/5788#issuecomment-480132253:444,Security,access,access,444,"Regarding permissions. No problems except when creating folders. It appears to me that the folder is being created as separate bucket, rather than as an object in the bucket:. """"""; ...; exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to **untitled-folder**.; """""". There is an open issue describing this problem: https://github.com/src-d/jgscm/issues/13. I found an interesting ""solution"": first click on an already-created folder (an `.ipynb_checkpoints` folder is created when you create a python file, that works); this populates the path with /bucket_name/folder. Back up to ../ and create a new folder, voila. . <img width=""1226"" alt=""Screenshot 2019-04-04 22 50 30"" src=""https://user-images.githubusercontent.com/5543229/55601098-1640c880-572d-11e9-8afb-ac000040962d.png"">. So this seems like something that could be fixed in jgscm, or potentially by setting ""--notebook-dir"" in addition to ""GoogleStorageContentManager.default_path"". (""default_path"" is used, because apparently setting [""--notebook-dir"" to set the root directory to the chosen bucket doesn't work])(https://github.com/src-d/jgscm#usage).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480132253
https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:536,Deployability,patch,patch,536,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780
https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:563,Integrability,depend,dependencies,563,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780
https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:130,Modifiability,config,config,130,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780
https://github.com/hail-is/hail/pull/5788#issuecomment-480347270:241,Deployability,install,install,241,"> I think we should defer folder creation as an improvement to jgscm. That's fine with me. As a workaround, users can create folders with gsutil for the time being. I think we should just fork it into the hail-is organization, fix it there, install from that repo in our build (don't need to go so far as to publish), and offer changes upstream, which they can take or not. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480347270
https://github.com/hail-is/hail/pull/5788#issuecomment-480349251:256,Deployability,install,install,256,"> > I think we should defer folder creation as an improvement to jgscm; > ; > That's fine with me. As a workaround, users can create folders with gsutil for the time being.; > ; > I think we should just fork it into the hail-is organization, fix it there, install from that repo in our build (don't need to go so far as to publish), and offer changes upstream, which they can take or not. Sound good?. Yep. Shooting to get the fix PRd today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480349251
https://github.com/hail-is/hail/pull/5788#issuecomment-480364182:18,Modifiability,config,config,18,In the cloudtools config it doesn’t actually appear that a bucket is being mounted.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480364182
https://github.com/hail-is/hail/pull/5788#issuecomment-480372530:155,Security,access,access-control,155,The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480372530
https://github.com/hail-is/hail/pull/5788#issuecomment-480375549:602,Availability,checkpoint,checkpoint,602,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
https://github.com/hail-is/hail/pull/5788#issuecomment-480375549:157,Security,access,access-control,157,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
https://github.com/hail-is/hail/pull/5788#issuecomment-480375549:555,Usability,clear,clearly,555,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
https://github.com/hail-is/hail/pull/5788#issuecomment-480380482:556,Availability,error,error,556,"I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:. > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to untitled-folder. That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482
https://github.com/hail-is/hail/pull/5788#issuecomment-480380482:495,Security,access,access,495,"I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:. > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to untitled-folder. That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482
https://github.com/hail-is/hail/pull/5788#issuecomment-480381353:645,Availability,error,error,645,"> I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:; > ; > > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: [user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com](mailto:user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com) does not have storage.buckets.get access to untitled-folder.; > ; > That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?. Right, I mentioned this above. It appears to be trying to create, or trying to read, untitled_folder as a bucket. If you trick it into using /your_bucket_name as the cwd, folder creation works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353
https://github.com/hail-is/hail/pull/5788#issuecomment-480381353:577,Security,access,access,577,"> I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:; > ; > > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: [user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com](mailto:user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com) does not have storage.buckets.get access to untitled-folder.; > ; > That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?. Right, I mentioned this above. It appears to be trying to create, or trying to read, untitled_folder as a bucket. If you trick it into using /your_bucket_name as the cwd, folder creation works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353
https://github.com/hail-is/hail/pull/5788#issuecomment-480383944:92,Deployability,patch,patch-,92,"I see, so our proposed fix is this? https://github.com/src-d/jgscm/compare/master...danking:patch-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480383944
https://github.com/hail-is/hail/pull/5797#issuecomment-480980256:127,Safety,avoid,avoid,127,"This is approved but needs rebase. I'm making some more API changes. If you can rebase soon, I will wait until this goes in to avoid further conflicts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5797#issuecomment-480980256
https://github.com/hail-is/hail/pull/5812#issuecomment-481006033:398,Performance,cache,cache,398,"That's a good question, do the semantics of the client library intend to support two different users modifying the objects simultaneously. We don't have an intended use case for it, so I'd say no. So `_status` cannot be out of date, it can only be stale. I'm only checking if it is complete. Once a Job is complete, its status cannot change. In the case it was deleted by someone else, if I didn't cache it would return 404, but we ruled that out. So I think this code is currently safe. Does that clarify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033
https://github.com/hail-is/hail/pull/5812#issuecomment-481006033:482,Safety,safe,safe,482,"That's a good question, do the semantics of the client library intend to support two different users modifying the objects simultaneously. We don't have an intended use case for it, so I'd say no. So `_status` cannot be out of date, it can only be stale. I'm only checking if it is complete. Once a Job is complete, its status cannot change. In the case it was deleted by someone else, if I didn't cache it would return 404, but we ruled that out. So I think this code is currently safe. Does that clarify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033
https://github.com/hail-is/hail/pull/5814#issuecomment-481079454:125,Deployability,pipeline,pipeline,125,"OK, I cleaned this up a bit. Now stacked on: https://github.com/hail-is/hail/pull/5826. Summary of changes:; - added heal; - pipeline is now batch rather than job centeric; - batch logs page shows logs for all batch jobs; - GET /batches/{id} endpoint now returns entire array of jobs, instead of the state counter and exit_codes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454
https://github.com/hail-is/hail/pull/5814#issuecomment-481079454:181,Testability,log,logs,181,"OK, I cleaned this up a bit. Now stacked on: https://github.com/hail-is/hail/pull/5826. Summary of changes:; - added heal; - pipeline is now batch rather than job centeric; - batch logs page shows logs for all batch jobs; - GET /batches/{id} endpoint now returns entire array of jobs, instead of the state counter and exit_codes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454
https://github.com/hail-is/hail/pull/5814#issuecomment-481079454:197,Testability,log,logs,197,"OK, I cleaned this up a bit. Now stacked on: https://github.com/hail-is/hail/pull/5826. Summary of changes:; - added heal; - pipeline is now batch rather than job centeric; - batch logs page shows logs for all batch jobs; - GET /batches/{id} endpoint now returns entire array of jobs, instead of the state counter and exit_codes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454
https://github.com/hail-is/hail/pull/5814#issuecomment-481693314:36,Modifiability,config,configured,36,@cseed unrelated but I think you've configured your local git email and name to be hail-ci-leader,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481693314
https://github.com/hail-is/hail/issues/5817#issuecomment-482597121:189,Integrability,interface,interface,189,"The issue is `j.wait()` will trigger when the job is complete which is set before the callback occurs. However, even if we change the order `set_state` and `callback` are called (change in interface), there's still the possibility that the callback won't complete before the wait is terminated. Therefore, the correct solution should wait for `d` to be non-empty with a timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121
https://github.com/hail-is/hail/issues/5817#issuecomment-482597121:370,Safety,timeout,timeout,370,"The issue is `j.wait()` will trigger when the job is complete which is set before the callback occurs. However, even if we change the order `set_state` and `callback` are called (change in interface), there's still the possibility that the callback won't complete before the wait is terminated. Therefore, the correct solution should wait for `d` to be non-empty with a timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121
https://github.com/hail-is/hail/pull/5819#issuecomment-481007218:13,Testability,test,tested,13,"Also, I just tested this out with my own jar/zip and it does exactly what I wanted it to! So at least there's one empirical test",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5819#issuecomment-481007218
https://github.com/hail-is/hail/pull/5819#issuecomment-481007218:124,Testability,test,test,124,"Also, I just tested this out with my own jar/zip and it does exactly what I wanted it to! So at least there's one empirical test",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5819#issuecomment-481007218
https://github.com/hail-is/hail/pull/5828#issuecomment-481241636:4,Performance,perform,performance,4,Any performance numbers?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481241636
https://github.com/hail-is/hail/pull/5828#issuecomment-481320024:59,Testability,test,tests,59,"If there is an improvement, it is very small, running more tests now, but it looks like the new code is in the noise compared to other factors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481320024
https://github.com/hail-is/hail/pull/5828#issuecomment-481367173:64,Deployability,pipeline,pipeline,64,"More extensive cloud tests are showing speedups in the combiner pipeline compared to master, about 15-20 seconds per partition, but that adds up quickly at scale.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173
https://github.com/hail-is/hail/pull/5828#issuecomment-481367173:21,Testability,test,tests,21,"More extensive cloud tests are showing speedups in the combiner pipeline compared to master, about 15-20 seconds per partition, but that adds up quickly at scale.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173
https://github.com/hail-is/hail/pull/5828#issuecomment-481451647:168,Performance,cache,cache,168,"@cseed . ```python3; import hail as hl; hl.import_vcf(PATH, reference_genome='GRCh38').write('/tmp/vcfmt', overwrite=True); ```. On my laptop with a warmish filesystem cache takes 1 minute for this code and 1:20 for current master. That's actually pretty good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481451647
https://github.com/hail-is/hail/pull/5828#issuecomment-482244062:25,Testability,test,tests,25,I added some old vs. new tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-482244062
https://github.com/hail-is/hail/pull/5829#issuecomment-481371995:34,Modifiability,variab,variable,34,What was the issue with the third variable?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5829#issuecomment-481371995
https://github.com/hail-is/hail/pull/5829#issuecomment-481372325:36,Modifiability,variab,variable,36,> What was the issue with the third variable?. Needed the full path: gcr.io/$(PROJECT)/$(1):$$(shell ...),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5829#issuecomment-481372325
https://github.com/hail-is/hail/pull/5835#issuecomment-481736843:21,Testability,test,test,21,I have on my list to test Nirvana but need a couple days so let's get it in now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5835#issuecomment-481736843
https://github.com/hail-is/hail/pull/5837#issuecomment-482289824:77,Testability,test,tests,77,"@daniel-goldstein @danking CI is showing this as failing, but afaict all the tests are passing? is this a problem we've seen?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837#issuecomment-482289824
https://github.com/hail-is/hail/pull/5837#issuecomment-482297784:69,Testability,log,log,69,The artifact thing was only added for the Hail subproject. The build log is the source of truth. Changes in hail trigger a retest of apiserver which is failing due to an issue fixed by https://github.com/hail-is/hail/pull/5869,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5837#issuecomment-482297784
https://github.com/hail-is/hail/pull/5842#issuecomment-487309837:20,Modifiability,config,config,20,Closing to PR build config with enabled ci2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5842#issuecomment-487309837
https://github.com/hail-is/hail/pull/5844#issuecomment-481837110:28,Deployability,deploy,deploy,28,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
https://github.com/hail-is/hail/pull/5844#issuecomment-481837110:23,Testability,test,test-deploy,23,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
https://github.com/hail-is/hail/pull/5844#issuecomment-481837110:45,Usability,simpl,simplify,45,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
https://github.com/hail-is/hail/pull/5844#issuecomment-482728341:42,Deployability,pipeline,pipeline,42,Yes I'm good with this. I just wanted the pipeline changes to be done separately.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482728341
https://github.com/hail-is/hail/pull/5844#issuecomment-482744047:537,Performance,concurren,concurrent,537,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
https://github.com/hail-is/hail/pull/5844#issuecomment-482744047:1055,Performance,queue,queueing,1055,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
https://github.com/hail-is/hail/pull/5844#issuecomment-482744047:273,Testability,log,log,273,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
https://github.com/hail-is/hail/pull/5844#issuecomment-482744047:79,Usability,learn,learning,79,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:561,Performance,concurren,concurrent,561,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:1088,Performance,queue,queueing,1088,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:1543,Performance,perform,performant,1543,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:282,Testability,log,log,282,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:81,Usability,learn,learning,81,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:258,Availability,down,down,258,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:547,Availability,error,error,547,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1535,Availability,error,error,1535,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1314,Modifiability,config,config,1314,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1375,Safety,risk,risk,1375,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1391,Security,attack,attacker,1391,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1470,Security,attack,attacker,1470,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1544,Security,secur,securing,1544,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:429,Usability,learn,learn,429,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:621,Usability,learn,learning,621,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:837,Usability,learn,learn,837,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:928,Usability,learn,learning,928,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1143,Usability,learn,learn,1143,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1234,Usability,learn,learning,1234,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1404,Usability,learn,learn,1404,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:495,Availability,error,error,495,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:268,Modifiability,config,config,268,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:337,Safety,risk,risk,337,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:353,Security,attack,attacker,353,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:430,Security,attack,attacker,430,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:504,Security,secur,securing,504,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:753,Security,attack,attacker,753,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:366,Usability,learn,learn,366,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
https://github.com/hail-is/hail/pull/5844#issuecomment-483833068:72,Integrability,rout,route,72,"Err, not sure what happened, but I noticed we removed refresh_k8s_state route, so I think we need to get rid of the call in the api (since the route doesn't exist). ```py; def refresh_k8s_state(self, url):; self.post(f'{url}/refresh_k8s_state', json_response=False); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483833068
https://github.com/hail-is/hail/pull/5844#issuecomment-483833068:143,Integrability,rout,route,143,"Err, not sure what happened, but I noticed we removed refresh_k8s_state route, so I think we need to get rid of the call in the api (since the route doesn't exist). ```py; def refresh_k8s_state(self, url):; self.post(f'{url}/refresh_k8s_state', json_response=False); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483833068
https://github.com/hail-is/hail/pull/5844#issuecomment-483852673:69,Integrability,interface,interface,69,"FYI, there's a kubernetes_async that I've used twice now, copies the interface of the official library and works like a charm: https://github.com/tomplus/kubernetes_asyncio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483852673
https://github.com/hail-is/hail/pull/5844#issuecomment-483868483:116,Deployability,update,updates,116,Oh nice I had seen that but didn’t know how well it worked. Would be nice for notebook (to watch fine-grained state updates),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483868483
https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:141,Availability,Error,Error,141,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083
https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:70,Testability,test,test,70,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083
https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:79,Testability,test,test-user-token,79,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083
https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:115,Testability,test,test,115,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083
https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:124,Testability,test,test-user-token,124,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083
https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:45,Availability,error,error,45,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312
https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:23,Deployability,install,installs,23,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312
https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:295,Deployability,update,updates,295,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312
https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:123,Modifiability,variab,variable,123,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312
https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:259,Performance,race condition,race condition,259,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312
https://github.com/hail-is/hail/pull/5844#issuecomment-484972045:3,Testability,test,testing,3,"In testing batch, it looks like:. ```; ModuleNotFoundError: No module named 'hailjwt'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484972045
https://github.com/hail-is/hail/issues/5846#issuecomment-483906188:0,Integrability,depend,depends,0,"depends on your definition of ""in"". 0.0 does compare equal to -0.0 with `==`, but not `java.lang.Double.compare`. Anyhow, this can probably be fixed with a hack.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5846#issuecomment-483906188
https://github.com/hail-is/hail/pull/5852#issuecomment-481821270:36,Testability,test,test,36,"All 10 seconds are saved in the one test that uses volumes. That test takes ~2 minutes (four pods, each takes about 30 seconds to mount the volume and execute). The 10 seconds are presumably volume creation time (which happens in the background while non-volume using tests run).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481821270
https://github.com/hail-is/hail/pull/5852#issuecomment-481821270:65,Testability,test,test,65,"All 10 seconds are saved in the one test that uses volumes. That test takes ~2 minutes (four pods, each takes about 30 seconds to mount the volume and execute). The 10 seconds are presumably volume creation time (which happens in the background while non-volume using tests run).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481821270
https://github.com/hail-is/hail/pull/5852#issuecomment-481821270:268,Testability,test,tests,268,"All 10 seconds are saved in the one test that uses volumes. That test takes ~2 minutes (four pods, each takes about 30 seconds to mount the volume and execute). The 10 seconds are presumably volume creation time (which happens in the background while non-volume using tests run).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481821270
https://github.com/hail-is/hail/pull/5852#issuecomment-481823759:27,Testability,log,log,27,I had to change the delete log test in my SQL branch because I changed the meaning of delete. I haven't had problems with that test previously. The only one I have problems with is `test_callback`. I ran `test_callback` 100 times and it seems like it failed approx 2 times locally. So that's going to be a pain to figure out if I can't make it happen more often...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481823759
https://github.com/hail-is/hail/pull/5852#issuecomment-481823759:31,Testability,test,test,31,I had to change the delete log test in my SQL branch because I changed the meaning of delete. I haven't had problems with that test previously. The only one I have problems with is `test_callback`. I ran `test_callback` 100 times and it seems like it failed approx 2 times locally. So that's going to be a pain to figure out if I can't make it happen more often...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481823759
https://github.com/hail-is/hail/pull/5852#issuecomment-481823759:127,Testability,test,test,127,I had to change the delete log test in my SQL branch because I changed the meaning of delete. I haven't had problems with that test previously. The only one I have problems with is `test_callback`. I ran `test_callback` 100 times and it seems like it failed approx 2 times locally. So that's going to be a pain to figure out if I can't make it happen more often...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5852#issuecomment-481823759
https://github.com/hail-is/hail/pull/5855#issuecomment-482689656:40,Testability,test,test,40,@tpoterba is what I changed the current test to sufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5855#issuecomment-482689656
https://github.com/hail-is/hail/pull/5861#issuecomment-482595342:4,Testability,test,test,4,The test for `test_callback` assumes the job state should be complete versus created.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5861#issuecomment-482595342
https://github.com/hail-is/hail/pull/5862#issuecomment-483869816:394,Performance,perform,performance,394,"1. I don't think this should be a user option. Too fine grained. 2. init has an explosion of options. I think we should have a centralized location for all the options and parameters. The feature flag stuff is a step in that direction, but needs to be more pervasive. Keep this all private/experimental for now. 3. I guess just 3 is going to get nearly all the benefit, but we'll need to track performance to know of sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5862#issuecomment-483869816
https://github.com/hail-is/hail/pull/5862#issuecomment-483878748:59,Security,expose,expose,59,I'll prefix the thing with an underscore. I mostly want to expose this as an option so Chris can turn it back to 1 on the combiner,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5862#issuecomment-483878748
https://github.com/hail-is/hail/pull/5864#issuecomment-486190822:132,Testability,test,tests,132,"Randomly assigned @patrick-schultz. This is a beast, but it's not easy to break up into smaller chunks. I'm going to add a few more tests for the matrix stuff with special cases like I've done for table, and reviewing test sufficiency is probably more important than looking at each changed line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5864#issuecomment-486190822
https://github.com/hail-is/hail/pull/5864#issuecomment-486190822:218,Testability,test,test,218,"Randomly assigned @patrick-schultz. This is a beast, but it's not easy to break up into smaller chunks. I'm going to add a few more tests for the matrix stuff with special cases like I've done for table, and reviewing test sufficiency is probably more important than looking at each changed line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5864#issuecomment-486190822
https://github.com/hail-is/hail/pull/5866#issuecomment-482268052:39,Testability,test,test,39,Do you want me to create a user in the test namespace? The user creation script supports arbitrary namespaces.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-482268052
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:75,Performance,race condition,race condition,75,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:301,Safety,avoid,avoid,301,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:162,Testability,log,logs,162,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:235,Testability,log,log,235,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:475,Testability,log,log,475,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:558,Testability,log,log,558,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:728,Testability,test,test,728,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:733,Testability,log,logs,733,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732
https://github.com/hail-is/hail/pull/5866#issuecomment-485461284:103,Testability,log,log,103,"1. instance_id should go away once persistence goes in. Then it should be just be job_id/task_name/job.log. 2. Deleting a job should delete the log, otherwise it is should stay. I think it's fine for now. We're going to want to do some serious cleanup once everything is in a manageable state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485461284
https://github.com/hail-is/hail/pull/5866#issuecomment-485461284:144,Testability,log,log,144,"1. instance_id should go away once persistence goes in. Then it should be just be job_id/task_name/job.log. 2. Deleting a job should delete the log, otherwise it is should stay. I think it's fine for now. We're going to want to do some serious cleanup once everything is in a manageable state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485461284
https://github.com/hail-is/hail/pull/5866#issuecomment-485464925:36,Testability,test,test,36,1. Won't we have conflicts with the test databases all starting their job ids from `1...`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485464925
https://github.com/hail-is/hail/pull/5866#issuecomment-485465773:4,Testability,test,test,4,The test batch needs a test service account. I don't think a test instance of batch should be able to write to a production bucket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485465773
https://github.com/hail-is/hail/pull/5866#issuecomment-485465773:23,Testability,test,test,23,The test batch needs a test service account. I don't think a test instance of batch should be able to write to a production bucket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485465773
https://github.com/hail-is/hail/pull/5866#issuecomment-485465773:61,Testability,test,test,61,The test batch needs a test service account. I don't think a test instance of batch should be able to write to a production bucket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485465773
https://github.com/hail-is/hail/pull/5866#issuecomment-485466724:92,Testability,test,test,92,"I agree it shouldn't be able to write to a production bucket. But right now, we only have 1 test service account with 1 bucket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485466724
https://github.com/hail-is/hail/pull/5866#issuecomment-485469027:106,Testability,test,tests,106,Agreed. So we leave the instance_id in until persistence is in and we have alternate service accounts for tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485469027
https://github.com/hail-is/hail/pull/5871#issuecomment-482346641:45,Testability,test,test,45,"this seems like a lot of hassle, and doesn't test an important piece - whether idempotent initialization actually works from Python. We can test creation of a JVM much more easily from Python than from Scala.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482346641
https://github.com/hail-is/hail/pull/5871#issuecomment-482346641:140,Testability,test,test,140,"this seems like a lot of hassle, and doesn't test an important piece - whether idempotent initialization actually works from Python. We can test creation of a JVM much more easily from Python than from Scala.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482346641
https://github.com/hail-is/hail/pull/5871#issuecomment-482348287:39,Availability,robust,robustly,39,#5872 fixes the problem and tests more robustly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287
https://github.com/hail-is/hail/pull/5871#issuecomment-482348287:28,Testability,test,tests,28,#5872 fixes the problem and tests more robustly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287
https://github.com/hail-is/hail/pull/5872#issuecomment-484569009:92,Usability,clear,clear,92,@danking ready for another review. This PR has expanded because we needed to properly store/clear session-specific stuff in a bunch of places.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484569009
https://github.com/hail-is/hail/pull/5872#issuecomment-484583174:86,Testability,test,test,86,It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484583174
https://github.com/hail-is/hail/pull/5872#issuecomment-484583174:196,Testability,test,tests,196,It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484583174
https://github.com/hail-is/hail/pull/5872#issuecomment-484602941:88,Testability,test,test,88,"> It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests. We actually do that -- this test_context runs in the same python process as the other tests. I also think we don't really guarantee you can stop and restart, but this at least makes it work with our tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941
https://github.com/hail-is/hail/pull/5872#issuecomment-484602941:198,Testability,test,tests,198,"> It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests. We actually do that -- this test_context runs in the same python process as the other tests. I also think we don't really guarantee you can stop and restart, but this at least makes it work with our tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941
https://github.com/hail-is/hail/pull/5872#issuecomment-484602941:291,Testability,test,tests,291,"> It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests. We actually do that -- this test_context runs in the same python process as the other tests. I also think we don't really guarantee you can stop and restart, but this at least makes it work with our tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941
https://github.com/hail-is/hail/pull/5872#issuecomment-484602941:404,Testability,test,tests,404,"> It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests. We actually do that -- this test_context runs in the same python process as the other tests. I also think we don't really guarantee you can stop and restart, but this at least makes it work with our tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941
https://github.com/hail-is/hail/pull/5873#issuecomment-482439395:137,Integrability,rout,router,137,"I pushed a few more changes:. I added a nginx server block to handle proxying connections to other namespaces. To map a namespace to the router service IP in gateway, I wrote a new server (router-resolver) that looks up the IP via kubernetes and call it with nginx auth_request. So now you can hit, e.g., blah.cseed.internal.hail.is and it gets forwarded to the cseed:router service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873#issuecomment-482439395
https://github.com/hail-is/hail/pull/5873#issuecomment-482439395:189,Integrability,rout,router-resolver,189,"I pushed a few more changes:. I added a nginx server block to handle proxying connections to other namespaces. To map a namespace to the router service IP in gateway, I wrote a new server (router-resolver) that looks up the IP via kubernetes and call it with nginx auth_request. So now you can hit, e.g., blah.cseed.internal.hail.is and it gets forwarded to the cseed:router service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873#issuecomment-482439395
https://github.com/hail-is/hail/pull/5873#issuecomment-482439395:368,Integrability,rout,router,368,"I pushed a few more changes:. I added a nginx server block to handle proxying connections to other namespaces. To map a namespace to the router service IP in gateway, I wrote a new server (router-resolver) that looks up the IP via kubernetes and call it with nginx auth_request. So now you can hit, e.g., blah.cseed.internal.hail.is and it gets forwarded to the cseed:router service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5873#issuecomment-482439395
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1028,Availability,ERROR,ERRORS,1028,"priate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1203,Availability,ERROR,ERROR,1203,"priate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:3870,Availability,error,error,3870,"nt = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def get_return_value(answer, gateway_client, target_id=None, name=None):; """"""Converts an answer received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCom",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:3982,Availability,error,error,3982,"e):; """"""Converts an answer received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protoc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4109,Availability,error,error,4109,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1888,Integrability,wrap,wrapper,1888,"est_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class jav...java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.lang.Thread.run(Thread.java:748)\\n'; gateway_client = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def ge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:2035,Integrability,wrap,wrapper,2035,"===================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class jav...java:79)\\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.lang.Thread.run(Thread.java:748)\\n'; gateway_client = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def get_return_value(answer, gateway_client, target_id=None, name=None):; """"""Converts an answer received from the Java gateway into a Python object.; ; F",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4086,Integrability,protocol,protocol,4086,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4972,Integrability,protocol,protocol,4972,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:816,Modifiability,plugin,plugins,816,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:148,Testability,Test,Tests,148,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:479,Testability,test,test,479,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:577,Testability,test,test,577,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:904,Testability,test,test,904,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1221,Testability,Test,Tests,1221,"priate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1631,Testability,test,test,1631,"==============================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928
https://github.com/hail-is/hail/pull/5878#issuecomment-485139453:90,Availability,error,error,90,@cseed surely now it will pass; set --ignore fs/google_fs.py in the doctest run. Previous error was caused by the testing of this file (since CI doesn't yet have gcsfs). https://storage.googleapis.com/hail-ci-0-1/ci/4d17fb5a7df0bb9d766eb80f4a2926b3ed7bbb70/564ab40014a5aa349a517a8353d09eac4a5273f7/index.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485139453
https://github.com/hail-is/hail/pull/5878#issuecomment-485139453:114,Testability,test,testing,114,@cseed surely now it will pass; set --ignore fs/google_fs.py in the doctest run. Previous error was caused by the testing of this file (since CI doesn't yet have gcsfs). https://storage.googleapis.com/hail-ci-0-1/ci/4d17fb5a7df0bb9d766eb80f4a2926b3ed7bbb70/564ab40014a5aa349a517a8353d09eac4a5273f7/index.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485139453
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:56,Availability,error,error,56,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:308,Availability,ERROR,ERROR,308,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:4,Testability,test,test,4,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:181,Testability,test,test-,181,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:220,Testability,test,test-,220,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:396,Testability,test,test-,396,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266
https://github.com/hail-is/hail/pull/5878#issuecomment-485522267:6,Availability,failure,failure,6,"> The failure isn't the dataproc delete, that's just what you get when you try to delete a cluster that wasn't created. You had an import error:; > ; > ```; > File ""/hail/repo/hail/python/hail/fs/google_fs.py"", line 3, in <module>; > import gcsfs; > ModuleNotFoundError: No module named 'gcsfs'; > ```. I see. gcsfs is already dynamically imported. If the apiserver tests are being run by CI, it needs to have the gcsfs module, unless we want to make fs lazy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485522267
https://github.com/hail-is/hail/pull/5878#issuecomment-485522267:138,Availability,error,error,138,"> The failure isn't the dataproc delete, that's just what you get when you try to delete a cluster that wasn't created. You had an import error:; > ; > ```; > File ""/hail/repo/hail/python/hail/fs/google_fs.py"", line 3, in <module>; > import gcsfs; > ModuleNotFoundError: No module named 'gcsfs'; > ```. I see. gcsfs is already dynamically imported. If the apiserver tests are being run by CI, it needs to have the gcsfs module, unless we want to make fs lazy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485522267
https://github.com/hail-is/hail/pull/5878#issuecomment-485522267:366,Testability,test,tests,366,"> The failure isn't the dataproc delete, that's just what you get when you try to delete a cluster that wasn't created. You had an import error:; > ; > ```; > File ""/hail/repo/hail/python/hail/fs/google_fs.py"", line 3, in <module>; > import gcsfs; > ModuleNotFoundError: No module named 'gcsfs'; > ```. I see. gcsfs is already dynamically imported. If the apiserver tests are being run by CI, it needs to have the gcsfs module, unless we want to make fs lazy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485522267
https://github.com/hail-is/hail/pull/5890#issuecomment-484914074:73,Testability,test,tested,73,"Added the non-blocked buffer spec to `baseBufferSpecs` which now all get tested in `NativeDecoderSuite` and `NativeEncoderSuite`. `NativeEncoderSuite` used to assert that the compressed bytes produced by the Scala and C++ buffer specs were the same, which is not necessarily true. Changed it so it uses Scala buffer spec to decompress both the Scala compressed and C++ compressed outputs to assert they decompress to the same thing. The decoder suite already asserts that something encoded through Scala is decoded correctly in C++",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5890#issuecomment-484914074
https://github.com/hail-is/hail/pull/5890#issuecomment-484914074:159,Testability,assert,assert,159,"Added the non-blocked buffer spec to `baseBufferSpecs` which now all get tested in `NativeDecoderSuite` and `NativeEncoderSuite`. `NativeEncoderSuite` used to assert that the compressed bytes produced by the Scala and C++ buffer specs were the same, which is not necessarily true. Changed it so it uses Scala buffer spec to decompress both the Scala compressed and C++ compressed outputs to assert they decompress to the same thing. The decoder suite already asserts that something encoded through Scala is decoded correctly in C++",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5890#issuecomment-484914074
https://github.com/hail-is/hail/pull/5890#issuecomment-484914074:391,Testability,assert,assert,391,"Added the non-blocked buffer spec to `baseBufferSpecs` which now all get tested in `NativeDecoderSuite` and `NativeEncoderSuite`. `NativeEncoderSuite` used to assert that the compressed bytes produced by the Scala and C++ buffer specs were the same, which is not necessarily true. Changed it so it uses Scala buffer spec to decompress both the Scala compressed and C++ compressed outputs to assert they decompress to the same thing. The decoder suite already asserts that something encoded through Scala is decoded correctly in C++",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5890#issuecomment-484914074
https://github.com/hail-is/hail/pull/5890#issuecomment-484914074:459,Testability,assert,asserts,459,"Added the non-blocked buffer spec to `baseBufferSpecs` which now all get tested in `NativeDecoderSuite` and `NativeEncoderSuite`. `NativeEncoderSuite` used to assert that the compressed bytes produced by the Scala and C++ buffer specs were the same, which is not necessarily true. Changed it so it uses Scala buffer spec to decompress both the Scala compressed and C++ compressed outputs to assert they decompress to the same thing. The decoder suite already asserts that something encoded through Scala is decoded correctly in C++",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5890#issuecomment-484914074
https://github.com/hail-is/hail/pull/5891#issuecomment-484330528:6,Usability,feedback,feedback,6,"Great feedback, thanks @danking. I think I address or responded to all the comments. I also changed the behavior of batch not to run any node until all its ancestors have completed running, which could happen if something failed which caused something else to get cancelled that was always_run. It fixes: https://github.com/hail-is/hail/issues/5903.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891#issuecomment-484330528
https://github.com/hail-is/hail/pull/5893#issuecomment-483923112:9,Testability,test,test,9,We don't test apiserver? `hail-ci-build.sh` runs `make test` which invokes `hail/apiserver/test-apiserver.sh` which starts an apiserver and hits it with a couple of Python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-483923112
https://github.com/hail-is/hail/pull/5893#issuecomment-483923112:55,Testability,test,test,55,We don't test apiserver? `hail-ci-build.sh` runs `make test` which invokes `hail/apiserver/test-apiserver.sh` which starts an apiserver and hits it with a couple of Python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-483923112
https://github.com/hail-is/hail/pull/5893#issuecomment-483923112:91,Testability,test,test-apiserver,91,We don't test apiserver? `hail-ci-build.sh` runs `make test` which invokes `hail/apiserver/test-apiserver.sh` which starts an apiserver and hits it with a couple of Python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-483923112
https://github.com/hail-is/hail/pull/5893#issuecomment-483923112:172,Testability,test,tests,172,We don't test apiserver? `hail-ci-build.sh` runs `make test` which invokes `hail/apiserver/test-apiserver.sh` which starts an apiserver and hits it with a couple of Python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-483923112
https://github.com/hail-is/hail/pull/5893#issuecomment-484171424:164,Testability,test,tests,164,"@cseed @akotlar, ok comments addressed, thanks for the improvements. re: the env var, there is now just one env var, HAIL_APISERVER_URL. no special env var for the tests. I kept the backend test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-484171424
https://github.com/hail-is/hail/pull/5893#issuecomment-484171424:190,Testability,test,test,190,"@cseed @akotlar, ok comments addressed, thanks for the improvements. re: the env var, there is now just one env var, HAIL_APISERVER_URL. no special env var for the tests. I kept the backend test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5893#issuecomment-484171424
https://github.com/hail-is/hail/pull/5895#issuecomment-483942926:13,Testability,test,test,13,I will add a test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5895#issuecomment-483942926
https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:165,Availability,Error,Error,165,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046
https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:11,Testability,test,testing,11,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046
https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:50,Testability,test,test,50,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046
https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:62,Testability,test,test,62,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046
https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:158,Testability,test,test,158,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046
https://github.com/hail-is/hail/pull/5897#issuecomment-484122661:62,Testability,test,test,62,Ok. @akotlar this is why the secrets need to be duplicated in test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122661
https://github.com/hail-is/hail/issues/5898#issuecomment-484217879:59,Performance,queue,queue,59,One option is to treat IR execution as a job pushed into a queue; would have the nice property of persistence in the case that apiserver dies before request is fulfilled,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484217879
https://github.com/hail-is/hail/issues/5898#issuecomment-484219314:94,Integrability,message,messages,94,"It would lessen the load on aiohttp, to something that may be better suited to handling large messages. Then store the result in gcs, and respond with the address of the object",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314
https://github.com/hail-is/hail/issues/5898#issuecomment-484219314:20,Performance,load,load,20,"It would lessen the load on aiohttp, to something that may be better suited to handling large messages. Then store the result in gcs, and respond with the address of the object",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5898#issuecomment-484219314
https://github.com/hail-is/hail/pull/5906#issuecomment-484622404:31,Testability,test,test,31,Is there an option to build an test assembly jar? I want to be able to run the Scala tests with just a assembly test jar.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484622404
https://github.com/hail-is/hail/pull/5906#issuecomment-484622404:85,Testability,test,tests,85,Is there an option to build an test assembly jar? I want to be able to run the Scala tests with just a assembly test jar.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484622404
https://github.com/hail-is/hail/pull/5906#issuecomment-484622404:112,Testability,test,test,112,Is there an option to build an test assembly jar? I want to be able to run the Scala tests with just a assembly test jar.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484622404
https://github.com/hail-is/hail/pull/5906#issuecomment-484685379:28,Testability,test,tests,28,To run this without running tests: `mvn -Dmaven.test.skip=true package assembly:single`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685379
https://github.com/hail-is/hail/pull/5906#issuecomment-484685379:48,Testability,test,test,48,To run this without running tests: `mvn -Dmaven.test.skip=true package assembly:single`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685379
https://github.com/hail-is/hail/pull/5906#issuecomment-484685725:101,Integrability,depend,dependencies,101,@cseed Should be done now. `assembly:single` will build both jars into `target/hail-0.2-sha-jar-with-dependencies.jar` and `target/hail-0.2-sha-test-jar-with-dependencies.jar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685725
https://github.com/hail-is/hail/pull/5906#issuecomment-484685725:158,Integrability,depend,dependencies,158,@cseed Should be done now. `assembly:single` will build both jars into `target/hail-0.2-sha-jar-with-dependencies.jar` and `target/hail-0.2-sha-test-jar-with-dependencies.jar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685725
https://github.com/hail-is/hail/pull/5906#issuecomment-484685725:144,Testability,test,test-jar-with-dependencies,144,@cseed Should be done now. `assembly:single` will build both jars into `target/hail-0.2-sha-jar-with-dependencies.jar` and `target/hail-0.2-sha-test-jar-with-dependencies.jar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-484685725
https://github.com/hail-is/hail/pull/5906#issuecomment-487407741:5,Deployability,integrat,integrate,5,Will integrate with new build system and re-pull.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741
https://github.com/hail-is/hail/pull/5906#issuecomment-487407741:5,Integrability,integrat,integrate,5,Will integrate with new build system and re-pull.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741
https://github.com/hail-is/hail/pull/5910#issuecomment-484684296:40,Modifiability,extend,extend,40,"the script may be clearer and easier to extend if written in python, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296
https://github.com/hail-is/hail/pull/5910#issuecomment-484684296:18,Usability,clear,clearer,18,"the script may be clearer and easier to extend if written in python, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296
https://github.com/hail-is/hail/pull/5910#issuecomment-484684741:79,Deployability,install,installed,79,"I can move some of it over to a python diagnose, but I'm also targeting the ""I installed hail, but import hail fails"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684741
https://github.com/hail-is/hail/pull/5910#issuecomment-484750191:179,Deployability,install,installed,179,"@tpoterba I added `hl.debug_info()` which is maybe helpful?. I guess we're really talking about something that sanity checks the environment but assuming we already have hail pip-installed. What would you want to do? I guess we could check some spark stuff, try to create a context, if that fails tell the user its a spark problem? We could just do that in init though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191
https://github.com/hail-is/hail/pull/5910#issuecomment-484750191:111,Safety,sanity check,sanity checks,111,"@tpoterba I added `hl.debug_info()` which is maybe helpful?. I guess we're really talking about something that sanity checks the environment but assuming we already have hail pip-installed. What would you want to do? I guess we could check some spark stuff, try to create a context, if that fails tell the user its a spark problem? We could just do that in init though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191
https://github.com/hail-is/hail/pull/5913#issuecomment-486809766:284,Modifiability,variab,variable,284,"@tpoterba @patrick-schultz Ok, I switched to elif style and made fixes so that the surrounding code also followed the elif style. I'd like to merge this and push off any further discussions to another PR. IMO, the no-else-return style is almost always nicer when I want to:; - bind a variable half way through an if chain, or; - have a complex condition (see the one with a while loop around 3400) that demands a nested if (I have to duplicate the common fall-through case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5913#issuecomment-486809766
https://github.com/hail-is/hail/pull/5915#issuecomment-484732538:169,Deployability,release,release,169,"I guess I was afraid of somebody copying and pasting from the README. But if they're going to do that, they're not going to get right version anyway. You don't know the release hash until it goes in, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538
https://github.com/hail-is/hail/pull/5915#issuecomment-484732538:177,Security,hash,hash,177,"I guess I was afraid of somebody copying and pasting from the README. But if they're going to do that, they're not going to get right version anyway. You don't know the release hash until it goes in, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538
https://github.com/hail-is/hail/pull/5915#issuecomment-484747010:35,Deployability,install,install,35,"@cseed got you covered, from a JAR install:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/commit/8269129095ea}}; }; ```; from pip:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/releases/tag/0.2.13}}; }. In [4]: print(hl.cite_hail()) ; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484747010
https://github.com/hail-is/hail/pull/5915#issuecomment-484747010:387,Deployability,release,releases,387,"@cseed got you covered, from a JAR install:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/commit/8269129095ea}}; }; ```; from pip:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/releases/tag/0.2.13}}; }. In [4]: print(hl.cite_hail()) ; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484747010
https://github.com/hail-is/hail/pull/5915#issuecomment-484747010:514,Deployability,release,releases,514,"@cseed got you covered, from a JAR install:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/commit/8269129095ea}}; }; ```; from pip:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/releases/tag/0.2.13}}; }. In [4]: print(hl.cite_hail()) ; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484747010
https://github.com/hail-is/hail/pull/5929#issuecomment-485140905:69,Security,access,access,69,@cseed an issue remains: not sure if this is the cause:. `ls: cannot access 'PARK_HOME/python/lib/py4j-*-src.zip': No such file or directory` as a result of `make test`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5929#issuecomment-485140905
https://github.com/hail-is/hail/pull/5929#issuecomment-485140905:163,Testability,test,test,163,@cseed an issue remains: not sure if this is the cause:. `ls: cannot access 'PARK_HOME/python/lib/py4j-*-src.zip': No such file or directory` as a result of `make test`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5929#issuecomment-485140905
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:170,Availability,FAILURE,FAILURES,170,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:732,Availability,echo,echo,732,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:762,Availability,echo,echo,762,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:5,Testability,test,test,5,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:69,Testability,test,tested,69,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:412,Testability,test,tests,412,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:440,Testability,test,tests,440,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:522,Testability,test,tests,522,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:549,Testability,test,tests,549,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:725,Testability,test,test,725,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:749,Testability,test,test,749,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:779,Testability,test,test,779,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:814,Testability,test,test,814,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:935,Testability,test,test,935,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:956,Testability,test,test,956,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:1019,Testability,test,test,1019,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:1075,Testability,assert,assert,1075,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:1087,Testability,log,log,1087,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864
https://github.com/hail-is/hail/pull/5934#issuecomment-485610023:4,Testability,test,tests,4,Now tests are passing...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485610023
https://github.com/hail-is/hail/pull/5934#issuecomment-486009826:208,Performance,perform,performance,208,"@danking I addressed your comments that weren't major changes. I see making three issues -- fix the __init__ methods for Job and Batch, fix the startup polling inconsistent state issues, and fix the database performance issues. Let me know if any of these still need to be addressed in this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486009826
https://github.com/hail-is/hail/pull/5934#issuecomment-486414885:55,Performance,perform,performance,55,I cleaned some things up in this PR. I think the major performance issue left with `list_jobs` and `list_batches` will be solved with an attributes table.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486414885
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:36,Availability,ERROR,ERROR,36,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:169,Integrability,message,message,169,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:375,Modifiability,config,config,375,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:426,Security,authenticat,authenticated,426,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:21,Testability,Log,Log,21,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:266,Testability,log,login,266,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:325,Testability,log,logged,325,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019
https://github.com/hail-is/hail/pull/5951#issuecomment-486458681:104,Security,expose,exposed,104,"I had to rip out the SQLContext because its deprecated, but it does not appear to be used and was never exposed in our public python API. It's been replaced with SparkSession.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5951#issuecomment-486458681
https://github.com/hail-is/hail/pull/5955#issuecomment-486799272:26,Deployability,deploy,deploy,26,Got rid of the cleanup on deploy. Still fixed the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5955#issuecomment-486799272
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:0,Availability,Error,Error,0,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:448,Availability,Failure,Failure,448,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:6,Integrability,message,message,6,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:458,Integrability,message,message,458,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:175,Security,Audit,Audit-Id,175,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:577,Testability,test,test,577,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273
https://github.com/hail-is/hail/pull/5956#issuecomment-487081808:59,Availability,error,error,59,Something must actually be wrong... Shouldn't get the same error when just adding test files to a directory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808
https://github.com/hail-is/hail/pull/5956#issuecomment-487081808:82,Testability,test,test,82,Something must actually be wrong... Shouldn't get the same error when just adding test files to a directory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808
https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:251,Availability,error,errors,251,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526
https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:334,Deployability,install,installed,334,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526
https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:370,Testability,test,test,370,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526
https://github.com/hail-is/hail/pull/5959#issuecomment-488392810:10,Testability,test,tests,10,There are tests for listing batches by attributes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5959#issuecomment-488392810
https://github.com/hail-is/hail/pull/5959#issuecomment-490237506:17,Safety,avoid,avoids,17,"Agreed, triggers avoids the need for the join. In the meantime I fixed the joins (maybe).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5959#issuecomment-490237506
https://github.com/hail-is/hail/pull/5960#issuecomment-487212648:40,Testability,log,log,40,I'm a bit dubious on editing the change log. Seems like it should really be an append only log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5960#issuecomment-487212648
https://github.com/hail-is/hail/pull/5960#issuecomment-487212648:91,Testability,log,log,91,I'm a bit dubious on editing the change log. Seems like it should really be an append only log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5960#issuecomment-487212648
https://github.com/hail-is/hail/pull/5960#issuecomment-487214751:34,Testability,log,log,34,"The git history is an append only log. The changelog is a human-generated, human-readable digest. I'm fine with editing it to fix problems like this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5960#issuecomment-487214751
https://github.com/hail-is/hail/pull/5962#issuecomment-488291829:38,Deployability,pipeline,pipeline,38,We should wait for this to go in once pipeline is actually being tested. Will work on that now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962#issuecomment-488291829
https://github.com/hail-is/hail/pull/5962#issuecomment-488291829:65,Testability,test,tested,65,We should wait for this to go in once pipeline is actually being tested. Will work on that now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962#issuecomment-488291829
https://github.com/hail-is/hail/pull/5962#issuecomment-490193339:6,Testability,test,testing,6,Added testing build steps as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5962#issuecomment-490193339
https://github.com/hail-is/hail/pull/5966#issuecomment-487177448:38,Testability,test,test,38,Still need to get rid of fisher exact test and logistic regression tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5966#issuecomment-487177448
https://github.com/hail-is/hail/pull/5966#issuecomment-487177448:47,Testability,log,logistic,47,Still need to get rid of fisher exact test and logistic regression tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5966#issuecomment-487177448
https://github.com/hail-is/hail/pull/5966#issuecomment-487177448:67,Testability,test,tests,67,Still need to get rid of fisher exact test and logistic regression tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5966#issuecomment-487177448
https://github.com/hail-is/hail/pull/5970#issuecomment-487193594:23,Integrability,message,messages,23,can we make the commit messages more descriptive?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5970#issuecomment-487193594
https://github.com/hail-is/hail/pull/5979#issuecomment-490912792:70,Testability,test,tests,70,Randomly assigned Patrick. This is a big PR but about half of that is tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5979#issuecomment-490912792
https://github.com/hail-is/hail/pull/5980#issuecomment-487741501:50,Testability,Log,LogColorMapper,50,This is what it looks like with default fonts and LogColorMapper / LogTicker; ![image](https://user-images.githubusercontent.com/3502465/56926554-ff796180-6a9f-11e9-9d8a-99a1befd298f.png),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5980#issuecomment-487741501
https://github.com/hail-is/hail/pull/5980#issuecomment-487741501:67,Testability,Log,LogTicker,67,This is what it looks like with default fonts and LogColorMapper / LogTicker; ![image](https://user-images.githubusercontent.com/3502465/56926554-ff796180-6a9f-11e9-9d8a-99a1befd298f.png),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5980#issuecomment-487741501
https://github.com/hail-is/hail/pull/5981#issuecomment-488026847:51,Testability,test,test,51,"Comments should be addressed, but I wasn't able to test it locally so there could still be problems...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5981#issuecomment-488026847
https://github.com/hail-is/hail/pull/5985#issuecomment-489738406:36,Availability,failure,failure,36,"Failed due to intermittent dataproc failure (sign), this just needs a bump.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5985#issuecomment-489738406
https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:47,Availability,failure,failures,47,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769
https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:127,Availability,avail,available,127,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769
https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:110,Deployability,deploy,deployment,110,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769
https://github.com/hail-is/hail/pull/6007#issuecomment-488146766:5,Performance,queue,queue,5,I'll queue up type hints. > add the timestamp of the last status change?. Can you be a bit more specific? You want the time on the status posted to go PR statuses?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6007#issuecomment-488146766
https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:0,Availability,Failure,Failure,0,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064
https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:46,Availability,error,error,46,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064
https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:59,Availability,ERROR,ERROR,59,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064
https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:90,Availability,ERROR,ERROR,90,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064
https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:25,Availability,failure,failure,25,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265
https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:268,Availability,ERROR,ERROR,268,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265
https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:364,Availability,error,error,364,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265
https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:60,Testability,test,test-,60,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265
https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:93,Testability,test,tests,93,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265
https://github.com/hail-is/hail/pull/6015#issuecomment-488307839:41,Testability,test,test-locally,41,"This should be using `pytest`, just like test-locally: `python3 -m pytest test`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-488307839
https://github.com/hail-is/hail/pull/6015#issuecomment-488307839:74,Testability,test,test,74,"This should be using `pytest`, just like test-locally: `python3 -m pytest test`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-488307839
https://github.com/hail-is/hail/pull/6015#issuecomment-489803786:128,Availability,failure,failure,128,@cseed I think we need to bump the Batch test time limit to 360. The tests passed in 328 seconds but wait-for.py marked it as a failure as it was over 300 seconds.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-489803786
https://github.com/hail-is/hail/pull/6015#issuecomment-489803786:41,Testability,test,test,41,@cseed I think we need to bump the Batch test time limit to 360. The tests passed in 328 seconds but wait-for.py marked it as a failure as it was over 300 seconds.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-489803786
https://github.com/hail-is/hail/pull/6015#issuecomment-489803786:69,Testability,test,tests,69,@cseed I think we need to bump the Batch test time limit to 360. The tests passed in 328 seconds but wait-for.py marked it as a failure as it was over 300 seconds.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-489803786
https://github.com/hail-is/hail/pull/6015#issuecomment-490090711:4,Testability,test,tests,4,The tests are now passing. The thing that failed is `deploy_apiserver`. https://ci2.hail.is/jobs/14871/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-490090711
https://github.com/hail-is/hail/pull/6015#issuecomment-490090711:103,Testability,log,log,103,The tests are now passing. The thing that failed is `deploy_apiserver`. https://ci2.hail.is/jobs/14871/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-490090711
https://github.com/hail-is/hail/pull/6028#issuecomment-491016361:127,Deployability,update,update,127,"It's a successful build for a previous version of master. I'm working on a PR to note when ci2 builds are out of date. We only update on approved PR at a time, so it is in a queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361
https://github.com/hail-is/hail/pull/6028#issuecomment-491016361:174,Performance,queue,queue,174,"It's a successful build for a previous version of master. I'm working on a PR to note when ci2 builds are out of date. We only update on approved PR at a time, so it is in a queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361
https://github.com/hail-is/hail/pull/6028#issuecomment-491025570:77,Testability,log,log,77,There's some change to set ordering in python https://ci2.hail.is/jobs/26259/log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491025570
https://github.com/hail-is/hail/pull/6028#issuecomment-491035884:17,Testability,test,tests,17,I had to fix the tests to test equality rather than the expected set itself.; https://docs.python.org/3.5/library/doctest.html#warnings,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491035884
https://github.com/hail-is/hail/pull/6028#issuecomment-491035884:26,Testability,test,test,26,I had to fix the tests to test equality rather than the expected set itself.; https://docs.python.org/3.5/library/doctest.html#warnings,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491035884
https://github.com/hail-is/hail/pull/6039#issuecomment-490169359:364,Testability,test,tests,364,"> Some small comments, but I'm also not sure I totally understand what's going on with the new pdf function. I can explain in person, it's pretty cool (I think). That might also help me figure out something to say in the documentation, though since it's experimental I don't think it needs complete documentation for now. > Would it be possible to add some python tests for the new stuff?. Do you think tests that just check that the `_compaction_counts` field exists are worth having? Beyond that, everything is non-deterministic and hard to test, and I don't know how to test plotting methods either. But I've been playing with them and they are working extremely well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490169359
https://github.com/hail-is/hail/pull/6039#issuecomment-490169359:403,Testability,test,tests,403,"> Some small comments, but I'm also not sure I totally understand what's going on with the new pdf function. I can explain in person, it's pretty cool (I think). That might also help me figure out something to say in the documentation, though since it's experimental I don't think it needs complete documentation for now. > Would it be possible to add some python tests for the new stuff?. Do you think tests that just check that the `_compaction_counts` field exists are worth having? Beyond that, everything is non-deterministic and hard to test, and I don't know how to test plotting methods either. But I've been playing with them and they are working extremely well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490169359
https://github.com/hail-is/hail/pull/6039#issuecomment-490169359:543,Testability,test,test,543,"> Some small comments, but I'm also not sure I totally understand what's going on with the new pdf function. I can explain in person, it's pretty cool (I think). That might also help me figure out something to say in the documentation, though since it's experimental I don't think it needs complete documentation for now. > Would it be possible to add some python tests for the new stuff?. Do you think tests that just check that the `_compaction_counts` field exists are worth having? Beyond that, everything is non-deterministic and hard to test, and I don't know how to test plotting methods either. But I've been playing with them and they are working extremely well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490169359
https://github.com/hail-is/hail/pull/6039#issuecomment-490169359:573,Testability,test,test,573,"> Some small comments, but I'm also not sure I totally understand what's going on with the new pdf function. I can explain in person, it's pretty cool (I think). That might also help me figure out something to say in the documentation, though since it's experimental I don't think it needs complete documentation for now. > Would it be possible to add some python tests for the new stuff?. Do you think tests that just check that the `_compaction_counts` field exists are worth having? Beyond that, everything is non-deterministic and hard to test, and I don't know how to test plotting methods either. But I've been playing with them and they are working extremely well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490169359
https://github.com/hail-is/hail/pull/6039#issuecomment-490256479:198,Availability,error,error,198,"I think that would probably be fine---I just got worried since `_error_from_cdf` had some non-working code and wasn't being called anywhere explicitly (it's imported in `plots`, but not used in the error calculation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490256479
https://github.com/hail-is/hail/pull/6039#issuecomment-492307141:60,Availability,error,error,60,"I moved the changes to the aggregator into #6076, and added error bounds that apply to all quantiles simultaneously, which is what you really want for the pdf.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-492307141
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:12,Availability,error,error,12,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:479,Availability,error,errors,479,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:686,Availability,error,errors,686,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:183,Integrability,depend,dependency-plugin,183,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:890,Integrability,depend,dependency,890,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:937,Integrability,depend,dependencies,937,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:169,Modifiability,plugin,plugins,169,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:194,Modifiability,plugin,plugin,194,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:607,Testability,log,logging,607,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:923,Testability,test,test-jar-with-dependencies,923,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270
https://github.com/hail-is/hail/pull/6050#issuecomment-490304549:118,Performance,Optimiz,Optimize,118,"Just to clarify, I was seeing an infinite loop in Interpret for a 0-ary function via calls to Interpret => Compile => Optimize => FoldConstants => Interpret.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490304549
https://github.com/hail-is/hail/pull/6050#issuecomment-490304711:34,Performance,optimiz,optimize,34,interpret calling compile calling optimize is a huge bug.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490304711
https://github.com/hail-is/hail/pull/6050#issuecomment-490955457:5,Availability,error,errors,5,"Your errors are a result of -0.0, as https://github.com/hail-is/hail/issues/6086",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490955457
https://github.com/hail-is/hail/pull/6050#issuecomment-491048694:19,Testability,test,tests,19,I disabled the two tests that are failing because of this set/dict comparison issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-491048694
https://github.com/hail-is/hail/issues/6062#issuecomment-490088444:120,Testability,assert,assert,120,"I don't see the issue. ```scala; def stringSampleIds: IndexedSeq[String] = {; val colKeyTypes = typ.colKeyStruct.types; assert(colKeyTypes.length == 1 && colKeyTypes(0).isInstanceOf[TString], colKeyTypes.toSeq); val querier = typ.colType.query(typ.colKey(0)); colValues.value.map(querier(_).asInstanceOf[String]); }. def requireUniqueSamples(method: String) {; val dups = stringSampleIds.counter().filter(_._2 > 1).toArray; if (dups.nonEmpty); fatal(s""Method '$method' does not support duplicate column keys. Duplicates:"" +; s""\n @1"", dups.sortBy(-_._2).map { case (id, count) => s""""""($count) ""$id"""""""" }.truncatable(""\n "")); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6062#issuecomment-490088444
https://github.com/hail-is/hail/issues/6062#issuecomment-490092137:29,Availability,error,error,29,Er sorry needs to be a nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6062#issuecomment-490092137
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:106,Availability,ERROR,ERROR,106,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:202,Availability,error,error,202,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:0,Testability,Test,Test,0,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:252,Testability,test,test-,252,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:343,Testability,test,tests,343,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:392,Testability,test,test-,392,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:986,Testability,test,tests,986,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:1035,Testability,test,test-,1035,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264
https://github.com/hail-is/hail/issues/6071#issuecomment-554499349:76,Integrability,interface,interface,76,"it's not necessary, since we can just name them separate things. The Python interface is the reason this is breaking.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6071#issuecomment-554499349
https://github.com/hail-is/hail/pull/6073#issuecomment-490524909:54,Testability,log,logic,54,"The RVDPartitioner union function contains coarsening logic, so I'm not going to abstract that one right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6073#issuecomment-490524909
https://github.com/hail-is/hail/pull/6075#issuecomment-490967834:36,Availability,failure,failure,36,I'm going to work on making the pvc failure not throw an exception and have a polling loop that tries to recreate the jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-490967834
https://github.com/hail-is/hail/pull/6075#issuecomment-491011207:115,Availability,failure,failure,115,@cseed This is ready for review. Please check the logic for _create_pod. I added a Try/Except that sets the pod to failure if pod_creation fails.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-491011207
https://github.com/hail-is/hail/pull/6075#issuecomment-491011207:50,Testability,log,logic,50,@cseed This is ready for review. Please check the logic for _create_pod. I added a Try/Except that sets the pod to failure if pod_creation fails.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-491011207
https://github.com/hail-is/hail/pull/6078#issuecomment-490631176:47,Deployability,patch,patched,47,"@daniel-goldstein I noticed a bug in table.py, patched that in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078#issuecomment-490631176
https://github.com/hail-is/hail/pull/6078#issuecomment-494399462:4,Availability,failure,failure,4,"The failure is in apiserver somehow, probably trivial. Can't id the problem at the moment because ci is giving 401.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6078#issuecomment-494399462
https://github.com/hail-is/hail/pull/6083#issuecomment-491423332:13,Testability,test,tests,13,Also all the tests need to be fixed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-491423332
https://github.com/hail-is/hail/pull/6083#issuecomment-491423599:73,Availability,failure,failures,73,"> Also all the tests need to be fixed. Yep, working on it. Serialization failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599
https://github.com/hail-is/hail/pull/6083#issuecomment-491423599:15,Testability,test,tests,15,"> Also all the tests need to be fixed. Yep, working on it. Serialization failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599
https://github.com/hail-is/hail/pull/6083#issuecomment-492742054:55,Integrability,wrap,wrapper,55,"@chrisvittal Just to draw your attention: I removed my wrapper for FsDataInputStream (HailInputStream).... it is used prominently in BGzipInputStream, which relies on Hadoop compression libraries. I found it more complicated to avoid FsDataInputStream, with I think little obvious benefit to doing so: the [FSDataInputStream constructor just takes an InputStream](https://hadoop.apache.org/docs/r3.0.3/api/org/apache/hadoop/fs/FSDataInputStream.html), which is Java stdlib, which I assume will be returned by most future FS implementations. Happy to explore a different solution if you think there's a better abstraction here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054
https://github.com/hail-is/hail/pull/6083#issuecomment-492742054:228,Safety,avoid,avoid,228,"@chrisvittal Just to draw your attention: I removed my wrapper for FsDataInputStream (HailInputStream).... it is used prominently in BGzipInputStream, which relies on Hadoop compression libraries. I found it more complicated to avoid FsDataInputStream, with I think little obvious benefit to doing so: the [FSDataInputStream constructor just takes an InputStream](https://hadoop.apache.org/docs/r3.0.3/api/org/apache/hadoop/fs/FSDataInputStream.html), which is Java stdlib, which I assume will be returned by most future FS implementations. Happy to explore a different solution if you think there's a better abstraction here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:32,Availability,error,error,32,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:212,Deployability,install,installed,212,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:272,Security,access,accessing,272,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:80,Testability,test,tests,80,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:286,Testability,log,log,286,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:368,Testability,test,test,368,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:406,Testability,test,testAggregators,406,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:472,Testability,test,test,472,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:510,Testability,test,testForwardingOps,510,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:16,Availability,failure,failure,16,"Confirmed, this failure is not happening on local. . ```sh; (base) alex:~/projects/hail/hail:$ ./gradlew test --tests is.hail.expr.ir.ForwardLetsSuite.testForwardingOps; :checkSettings; check: seed = 1, size = 1000, count = 10; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Hadoop.cpp -MG -M -MF build/Hadoop.d -MT build/Hadoop.o; c++ -o build/Region.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2613,Integrability,Wrap,WrappedArray,2613,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2875,Integrability,Wrap,WrappedArray,2875,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3073,Integrability,Wrap,WrappedArray,3073,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3326,Integrability,Wrap,WrappedArray,3326,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:105,Testability,test,test,105,"Confirmed, this failure is not happening on local. . ```sh; (base) alex:~/projects/hail/hail:$ ./gradlew test --tests is.hail.expr.ir.ForwardLetsSuite.testForwardingOps; :checkSettings; check: seed = 1, size = 1000, count = 10; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Hadoop.cpp -MG -M -MF build/Hadoop.d -MT build/Hadoop.o; c++ -o build/Region.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:112,Testability,test,tests,112,"Confirmed, this failure is not happening on local. . ```sh; (base) alex:~/projects/hail/hail:$ ./gradlew test --tests is.hail.expr.ir.ForwardLetsSuite.testForwardingOps; :checkSettings; check: seed = 1, size = 1000, count = 10; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Hadoop.cpp -MG -M -MF build/Hadoop.d -MT build/Hadoop.o; c++ -o build/Region.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:151,Testability,test,testForwardingOps,151,"Confirmed, this failure is not happening on local. . ```sh; (base) alex:~/projects/hail/hail:$ ./gradlew test --tests is.hail.expr.ir.ForwardLetsSuite.testForwardingOps; :checkSettings; check: seed = 1, size = 1000, count = 10; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Hadoop.cpp -MG -M -MF build/Hadoop.d -MT build/Hadoop.o; c++ -o build/Region.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2082,Testability,Log,Logging,2082,"../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedAr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2480,Testability,test,testClasses,2480,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2494,Testability,test,test,2494,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2508,Testability,test,test,2508,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2514,Testability,Test,Test,2514,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2526,Testability,test,testForwardingOps,2526,"me/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2748,Testability,test,test,2748,"march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2788,Testability,test,testForwardingOps,2788,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2969,Testability,test,test,2969,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2975,Testability,Test,Test,2975,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2987,Testability,test,testForwardingOps,2987,"nes/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3200,Testability,test,test,3200,"tus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3240,Testability,test,testForwardingOps,3240,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3412,Testability,test,test,3412,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3418,Testability,Test,Test,3418,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3430,Testability,test,testForwardingOps,3430,"sses; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3596,Testability,test,test,3596,"In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3636,Testability,test,testForwardingOps,3636,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3761,Testability,test,test,3761,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3767,Testability,Test,Test,3767,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3779,Testability,test,testForwardingOps,3779,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:3991,Testability,test,test,3991,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4031,Testability,test,testForwardingOps,4031,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4202,Testability,test,test,4202,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4208,Testability,Test,Test,4208,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4220,Testability,test,testForwardingOps,4220,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4395,Testability,test,test,4395,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:4435,Testability,test,testForwardingOps,4435,"rappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedArray(I32(1), ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))) PASSED; Running test: Test method testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[2](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),If(True(),Ref(x,int32),I32(0)))) PASSED; Running test: Test method testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[3](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Add(),I32(2),Ref(x,int32)),I32(1)))) PASSED; Running test: Test method testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[4](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),ApplyUnaryPrimOp(Negate(),Ref(x,int32)))) PASSED. BUILD SUCCESSFUL. Total time: 2 mins 12.282 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925
https://github.com/hail-is/hail/pull/6083#issuecomment-492897454:1206,Testability,test,tests,1206,"Ah, so I had run make prebuilt in src/main/c after the last master merge (https://github.com/hail-is/hail/pull/6083/commits/acc617674d1e023c7ef514180edf0e9cb86701df), but running it again seemed to change the dylib. Maybe I didn't commit the correct change. Pushing. Also, could this be caused by the fact that this is compiled on OSX?. edit: I see nativeLibPrebuilt just runs make prebuilt, so this is either a platform difference or didn't commit the correct prebuilt files during merge. edit2: maybe not surprisingly, running this changes `prebuilt/lib/darwin/*.dylib` , but not `prebuilt/lib/linux-x86-64/*.so`. edit3: in the makefile:. ```make; UNAME_S :=$(shell uname -s). # ... stuff; ifeq ($(UNAME_S),Linux); LIBFLAGS += -rdynamic -shared; LIBBOOT := lib/linux-x86-64/libboot.so; LIBHAIL := lib/linux-x86-64/libhail.so; ifneq ($(filter %86,$(UNAME_P)),); LIBBOOT := lib/linux-x86/libboot.so; LIBHAIL := lib/linux-x86/libhail.so; endif; endif; ifeq ($(UNAME_S),Darwin); LIBFLAGS += -dynamiclib -Wl,-undefined,dynamic_lookup; LIBBOOT := lib/darwin/libboot.dylib; LIBHAIL := lib/darwin/libhail.dylib; endif; ```. So as it stands, I think we would need to have CI run make prebuilt before running hail tests (since anything I check in won't matter), or cross-compile. I must be wrong...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492897454
https://github.com/hail-is/hail/pull/6083#issuecomment-492911668:385,Deployability,update,updates,385,"Oh right sorry that doesn't make the Linux prebuilt, which you need to build locally on Linux and check in. CI should build those before testing but doesn't yet. The workaround @catoverdrive showed me when I had to remake the prebuilt was to build it in a docker container with the CI image on my machine. Would be happy to help you get them built tomorrow and/or make the appropriate updates to CI and deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668
https://github.com/hail-is/hail/pull/6083#issuecomment-492911668:403,Deployability,deploy,deploy,403,"Oh right sorry that doesn't make the Linux prebuilt, which you need to build locally on Linux and check in. CI should build those before testing but doesn't yet. The workaround @catoverdrive showed me when I had to remake the prebuilt was to build it in a docker container with the CI image on my machine. Would be happy to help you get them built tomorrow and/or make the appropriate updates to CI and deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668
https://github.com/hail-is/hail/pull/6083#issuecomment-492911668:137,Testability,test,testing,137,"Oh right sorry that doesn't make the Linux prebuilt, which you need to build locally on Linux and check in. CI should build those before testing but doesn't yet. The workaround @catoverdrive showed me when I had to remake the prebuilt was to build it in a docker container with the CI image on my machine. Would be happy to help you get them built tomorrow and/or make the appropriate updates to CI and deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668
https://github.com/hail-is/hail/pull/6083#issuecomment-493192123:118,Availability,error,errors,118,"I don't understand why the Python tests are failing yet. They are failing on master as well, and at least some of the errors are identical (NullPointerException in HailContext.apply). Scala tests are passing. Wondering if this is due to py4j version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123
https://github.com/hail-is/hail/pull/6083#issuecomment-493192123:34,Testability,test,tests,34,"I don't understand why the Python tests are failing yet. They are failing on master as well, and at least some of the errors are identical (NullPointerException in HailContext.apply). Scala tests are passing. Wondering if this is due to py4j version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123
https://github.com/hail-is/hail/pull/6083#issuecomment-493192123:190,Testability,test,tests,190,"I don't understand why the Python tests are failing yet. They are failing on master as well, and at least some of the errors are identical (NullPointerException in HailContext.apply). Scala tests are passing. Wondering if this is due to py4j version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:0,Availability,ping,ping,0,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:31,Availability,failure,failure,31,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:193,Availability,failure,failure,193,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2058,Availability,error,error,2058,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2212,Availability,error,error,2212,"e reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## W",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2401,Availability,error,error,2401,"he result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3741,Availability,error,error,3741,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:454,Deployability,Configurat,Configuration,454,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:549,Deployability,Configurat,Configuration,549,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:690,Deployability,Configurat,Configuration,690,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:825,Deployability,Configurat,Configuration,825,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1991,Deployability,Configurat,Configuration,1991,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2470,Deployability,install,install,2470,"he/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3290,Deployability,install,install,3290,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3900,Deployability,configurat,configuration,3900,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:454,Modifiability,Config,Configuration,454,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:549,Modifiability,Config,Configuration,549,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:690,Modifiability,Config,Configuration,690,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:825,Modifiability,Config,Configuration,825,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1622,Modifiability,parameteriz,parameterization,1622,".toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1991,Modifiability,Config,Configuration,1991,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3900,Modifiability,config,configuration,3900,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2272,Performance,CACHE,CACHE,2272,"e reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## W",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3426,Security,access,access,3426,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:220,Testability,test,test,220,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:237,Testability,test,tests,237,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:390,Testability,log,log,390,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1292,Testability,log,log,1292,"FileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or wi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1868,Testability,log,log,1868,"ser) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2108,Testability,test,tests,2108,"FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803
https://github.com/hail-is/hail/pull/6083#issuecomment-496305166:679,Testability,test,testing,679,"I kept having issues with non-local `sc.broadcast`; the @transient conf wouldn't be populated. Serialization worked fine, but this wouldn't. The solution I implemented was to pass a serializableHadoopConfiguration, but it feels like there should be a more elegant solution. However, given that Spark is leaving us forever, I'm not sure it's worth further investigation. Still, if there is a use case for this, I would like to find out why the previous solution was insufficient. . Edit:nvm, not the cause I don’t believe. There’s a bug somewhere, but maybe not serialization. Need to find a better way to replicate locally; has been getting stuck at 0/2 stage. . edit2: resolved testing issue, will find root of bug. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496305166
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:218,Deployability,configurat,configuration,218,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:54,Modifiability,config,configured,54,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:218,Modifiability,config,configuration,218,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:183,Performance,Load,LoadVCF,183,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:336,Performance,Load,LoadVCF,336,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:267,Testability,test,test,267,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496
https://github.com/hail-is/hail/pull/6083#issuecomment-498334543:38,Availability,failure,failure,38,"@chrisvittal This is all garbage, the failure makes little sense. I've started back from scratch with master, and have re-implemented all of the functionality, from this branch, needed to have the Dataproc test run (which relies on LoadVCF). . It works fine. Something else is amiss. I'm going to finish reimplementing everything in that clean slate, and when everything is running close this PR and reissue. The diff between them will show the problem area, which is in some kind of global state affecting sparkContext or RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543
https://github.com/hail-is/hail/pull/6083#issuecomment-498334543:232,Performance,Load,LoadVCF,232,"@chrisvittal This is all garbage, the failure makes little sense. I've started back from scratch with master, and have re-implemented all of the functionality, from this branch, needed to have the Dataproc test run (which relies on LoadVCF). . It works fine. Something else is amiss. I'm going to finish reimplementing everything in that clean slate, and when everything is running close this PR and reissue. The diff between them will show the problem area, which is in some kind of global state affecting sparkContext or RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543
https://github.com/hail-is/hail/pull/6083#issuecomment-498334543:206,Testability,test,test,206,"@chrisvittal This is all garbage, the failure makes little sense. I've started back from scratch with master, and have re-implemented all of the functionality, from this branch, needed to have the Dataproc test run (which relies on LoadVCF). . It works fine. Something else is amiss. I'm going to finish reimplementing everything in that clean slate, and when everything is running close this PR and reissue. The diff between them will show the problem area, which is in some kind of global state affecting sparkContext or RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543
https://github.com/hail-is/hail/pull/6094#issuecomment-491423751:83,Deployability,deploy,deployment,83,Erm. I guess that's OK as long as we ensure the labels are the same as the service/deployment names.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491423751
https://github.com/hail-is/hail/pull/6094#issuecomment-491899622:111,Testability,log,logs,111,"@jigold you won't see the output on this job, this is a change to CI so that every PR that CI builds lists the logs. If the internal links worked and we had a test for ci2, then you'd be able to see it at the newly tested CI's website at: https://ci2.pr-6094-default-q0023anv9fmd.internal.hail.is, but neither do we test ci2 nor do these internal links seem to work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491899622
https://github.com/hail-is/hail/pull/6094#issuecomment-491899622:159,Testability,test,test,159,"@jigold you won't see the output on this job, this is a change to CI so that every PR that CI builds lists the logs. If the internal links worked and we had a test for ci2, then you'd be able to see it at the newly tested CI's website at: https://ci2.pr-6094-default-q0023anv9fmd.internal.hail.is, but neither do we test ci2 nor do these internal links seem to work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491899622
https://github.com/hail-is/hail/pull/6094#issuecomment-491899622:215,Testability,test,tested,215,"@jigold you won't see the output on this job, this is a change to CI so that every PR that CI builds lists the logs. If the internal links worked and we had a test for ci2, then you'd be able to see it at the newly tested CI's website at: https://ci2.pr-6094-default-q0023anv9fmd.internal.hail.is, but neither do we test ci2 nor do these internal links seem to work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491899622
https://github.com/hail-is/hail/pull/6094#issuecomment-491899622:316,Testability,test,test,316,"@jigold you won't see the output on this job, this is a change to CI so that every PR that CI builds lists the logs. If the internal links worked and we had a test for ci2, then you'd be able to see it at the newly tested CI's website at: https://ci2.pr-6094-default-q0023anv9fmd.internal.hail.is, but neither do we test ci2 nor do these internal links seem to work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491899622
https://github.com/hail-is/hail/issues/6097#issuecomment-492704713:94,Performance,perform,performance,94,I expect that supporting missingness on block matrix will make it extremely hard to have good performance and compatibility with other numerical libraries like numpy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492704713
https://github.com/hail-is/hail/issues/6097#issuecomment-492707284:10,Performance,perform,performance,10,"Why would performance be different than just `nan`? Compatibility is understandable (though you could just warn and implicitly convert to `nan` - not a great solution, but it's an option). And I guess `nan` is fine, but then perhaps having the `sparsify_*` methods to be able to convert to `nan` instead of 0 should be supported. You guys had all this logic about why `nan` was different from missing though and I really liked it, so it seems like that should carry forward here",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492707284
https://github.com/hail-is/hail/issues/6097#issuecomment-492707284:352,Testability,log,logic,352,"Why would performance be different than just `nan`? Compatibility is understandable (though you could just warn and implicitly convert to `nan` - not a great solution, but it's an option). And I guess `nan` is fine, but then perhaps having the `sparsify_*` methods to be able to convert to `nan` instead of 0 should be supported. You guys had all this logic about why `nan` was different from missing though and I really liked it, so it seems like that should carry forward here",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492707284
https://github.com/hail-is/hail/issues/6097#issuecomment-492708806:12,Performance,perform,performance,12,"> Why would performance be different than just `nan`?. We can put `nan`s into BLAS, but not `NA`. I'd be fine with sparsify generating `nan`s as an option, though if you do really any operation that's not element-wise on that matrix, you'll get nans everywhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492708806
https://github.com/hail-is/hail/issues/6097#issuecomment-492762915:47,Security,expose,expose,47,"no, it doesn't exist now -- I think we want to expose aggregators on rows/cols of block matrices though. How do we do conditional matrix multiply?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492762915
https://github.com/hail-is/hail/pull/6104#issuecomment-491798256:124,Integrability,message,message,124,There are no commits here. Happy to review a pull request with:; 1) some code changes :); 2) a descriptive title and commit message,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6104#issuecomment-491798256
https://github.com/hail-is/hail/pull/6105#issuecomment-491834040:23,Testability,test,test,23,"we don't have a way to test this right now, right? (we could write something that templates with mock data). I want to next add something that colors the status link - but this will involve a bit of jinja language which I'll probably get wrong at first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6105#issuecomment-491834040
https://github.com/hail-is/hail/pull/6105#issuecomment-491834040:97,Testability,mock,mock,97,"we don't have a way to test this right now, right? (we could write something that templates with mock data). I want to next add something that colors the status link - but this will involve a bit of jinja language which I'll probably get wrong at first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6105#issuecomment-491834040
https://github.com/hail-is/hail/pull/6105#issuecomment-491844758:25,Testability,test,test,25,"> we don't have a way to test this right now, right? (we could write something that templates with mock data); > ; > I want to next add something that colors the status link - but this will involve a bit of jinja language which I'll probably get wrong at first. Yeah, we have no ui-related tests. Jackie had previously made a suggestion to output the rendered web page as a build artifact, which would be nice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6105#issuecomment-491844758
https://github.com/hail-is/hail/pull/6105#issuecomment-491844758:99,Testability,mock,mock,99,"> we don't have a way to test this right now, right? (we could write something that templates with mock data); > ; > I want to next add something that colors the status link - but this will involve a bit of jinja language which I'll probably get wrong at first. Yeah, we have no ui-related tests. Jackie had previously made a suggestion to output the rendered web page as a build artifact, which would be nice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6105#issuecomment-491844758
https://github.com/hail-is/hail/pull/6105#issuecomment-491844758:290,Testability,test,tests,290,"> we don't have a way to test this right now, right? (we could write something that templates with mock data); > ; > I want to next add something that colors the status link - but this will involve a bit of jinja language which I'll probably get wrong at first. Yeah, we have no ui-related tests. Jackie had previously made a suggestion to output the rendered web page as a build artifact, which would be nice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6105#issuecomment-491844758
https://github.com/hail-is/hail/pull/6109#issuecomment-492688221:87,Usability,simpl,simple,87,added disjoint interval. Happy to go over the code with you if you want -- it's pretty simple right now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6109#issuecomment-492688221
https://github.com/hail-is/hail/pull/6110#issuecomment-494143351:17,Deployability,pipeline,pipeline,17,"@cseed Here's my pipeline branch. I'm still thinking about how to pass the bucket around. Will keep iterating, but please let me know if these changes aren't compatible with your proposed changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6110#issuecomment-494143351
https://github.com/hail-is/hail/pull/6118#issuecomment-493502443:137,Availability,avail,available,137,Closing this for now until we have a plan for versioning cloudtools. This change would require not using the hard coded init_notebook.py available on GCS right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6118#issuecomment-493502443
https://github.com/hail-is/hail/pull/6118#issuecomment-493507016:156,Deployability,deploy,deployment,156,"I reassigned you to #6121 -- it shouldn't need much review because it's a copy of cloudtools. Then we can PR this onto that, and I'll get to hacking on the deployment strategy discussed over email yesterday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6118#issuecomment-493507016
https://github.com/hail-is/hail/pull/6128#issuecomment-493586302:19,Testability,test,tests,19,At least the Batch tests are catching things. My change caused always_run jobs to not be run...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6128#issuecomment-493586302
https://github.com/hail-is/hail/pull/6129#issuecomment-493776631:97,Testability,log,log,97,"How do we want to handle the case where one of the tasks does not have timing information?. ```; log.warning(f'job {self.id} has pod {pod.metadata.name} which is '; f'terminated but has no timing information. {pod}'); ```. I think it should be None for all tasks, which is not the behavior I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6129#issuecomment-493776631
https://github.com/hail-is/hail/issues/6132#issuecomment-494049017:68,Deployability,release,releaseJar,68,"@tpoterba We should just make a gradle target for end-users, like: ""releaseJar"" that just depends on the right targets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017
https://github.com/hail-is/hail/issues/6132#issuecomment-494049017:90,Integrability,depend,depends,90,"@tpoterba We should just make a gradle target for end-users, like: ""releaseJar"" that just depends on the right targets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017
https://github.com/hail-is/hail/pull/6134#issuecomment-494054260:2,Testability,test,tested,2,I tested on my local machine and it indeed modified the native lib prebuilt file and did so before running `shadowJar`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134#issuecomment-494054260
https://github.com/hail-is/hail/pull/6134#issuecomment-494062117:48,Deployability,release,releaseJar,48,"this will dirty the working tree, right? should releaseJar clean up?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6134#issuecomment-494062117
https://github.com/hail-is/hail/pull/6136#issuecomment-494081311:9,Deployability,deploy,deploy,9,"I tested deploy locally, and things worked great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311
https://github.com/hail-is/hail/pull/6136#issuecomment-494081311:2,Testability,test,tested,2,"I tested deploy locally, and things worked great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311
https://github.com/hail-is/hail/pull/6136#issuecomment-494892128:361,Deployability,pipeline,pipeline,361,"> It feels a bit odd / bad to copy things into a build directory and build there. the build process drops a bunch of artifacts in the python/ directory (and sub-directories). Isn't it cleaner to do this in a build/ directory?. > It seems like hailctl should be in the hail/python directory?. This seems like a larger organizational decision -- do we want batch/pipeline under there too? It'll be easy to move `hailctl` whenever we want, with just a couple lines of code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494892128
https://github.com/hail-is/hail/issues/6142#issuecomment-494491481:20,Modifiability,config,config,20,Also `WatchedBranch.config`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-494491481
https://github.com/hail-is/hail/issues/6142#issuecomment-497841118:21,Usability,clear,clear,21,"Took a look, and not clear why this is an issue. They seem to track different information, with WatchedBranch relating to one branch/sha combination and PR relating to two (with also a sha for the PR itself). Also Code doesn't appear to define a concrete `code` implementation, at least as of now. If you can help me understand the issue, I'll attempt to address tonight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497841118
https://github.com/hail-is/hail/issues/6142#issuecomment-497843719:234,Availability,error,error,234,"This is a violation of OOP style -- . ```; class A:; def foo(self):; ... class B(A):; def foo(self, bar):; ... class C(A):; def foo(self, bar, baz):; ...; ```. If I have an `A` and call `foo()` (valid signature for an A!) I'll get an error about calling a function with an incorrect signature. I haven't poked around the CI design enough to understand the intention of the code, but maybe we should just remove the definition from the superclass?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497843719
https://github.com/hail-is/hail/issues/6142#issuecomment-497845843:117,Deployability,update,update,117,"Ah I see what you’re talking about; I was focused on the returned object’s schema. Yep I will remove if possible, or update the signature of PR and WartchedBranch implementations",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497845843
https://github.com/hail-is/hail/pull/6149#issuecomment-494519184:50,Deployability,pipeline,pipeline,50,Are you planning on adding the linters to ci2 and pipeline as well?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6149#issuecomment-494519184
https://github.com/hail-is/hail/pull/6149#issuecomment-494557340:23,Deployability,pipeline,pipeline,23,I added to ci2 but not pipeline. https://github.com/hail-is/hail/pull/6150,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6149#issuecomment-494557340
https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:26,Availability,mask,masked,26,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361
https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:218,Availability,error,errors,218,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361
https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:241,Availability,error,errors,241,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361
https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:301,Availability,error,error,301,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361
https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:226,Integrability,Wrap,Wrapping,226,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361
https://github.com/hail-is/hail/pull/6162#issuecomment-497035653:84,Deployability,update,update,84,"@lfrancioli `Slope` was introduced to bokeh in a later version. I'll go through and update some of our python dependencies when I get a chance this week, which should mean this can just go in at that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653
https://github.com/hail-is/hail/pull/6162#issuecomment-497035653:110,Integrability,depend,dependencies,110,"@lfrancioli `Slope` was introduced to bokeh in a later version. I'll go through and update some of our python dependencies when I get a chance this week, which should mean this can just go in at that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653
https://github.com/hail-is/hail/pull/6168#issuecomment-495293829:201,Integrability,message,messages,201,I'm going to start rejecting PRs with non-descriptive names like this (I would have if I caught it!). It makes changelog generation extremely difficult!. I think it's especially important to have good messages when we're force-merging a change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495293829
https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:113,Deployability,deploy,deployment,113,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303
https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:135,Deployability,deploy,deploy,135,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303
https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:170,Integrability,depend,depending,170,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303
https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:197,Testability,test,testing,197,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303
https://github.com/hail-is/hail/pull/6173#issuecomment-496762764:166,Security,access,access,166,"I don't think we should merge this until we make dataproc jupyter notebooks have tokens. We set it to the empty string / no-token because we knew that you could only access the jupyter notebook if you had SSH credentials. I think this approach still left open CSRF attacks (random website tries to POST to localhost, so if people browse untrusted HTML/JS from their SSH tunneled web browsers that's not great). I think we should probably be setting tokens on the jupyter servers in general, but if you also intend to make the leader node's `connect_port` public, then we _definitely_ need to do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6173#issuecomment-496762764
https://github.com/hail-is/hail/pull/6173#issuecomment-496762764:265,Security,attack,attacks,265,"I don't think we should merge this until we make dataproc jupyter notebooks have tokens. We set it to the empty string / no-token because we knew that you could only access the jupyter notebook if you had SSH credentials. I think this approach still left open CSRF attacks (random website tries to POST to localhost, so if people browse untrusted HTML/JS from their SSH tunneled web browsers that's not great). I think we should probably be setting tokens on the jupyter servers in general, but if you also intend to make the leader node's `connect_port` public, then we _definitely_ need to do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6173#issuecomment-496762764
https://github.com/hail-is/hail/pull/6173#issuecomment-496763057:220,Security,secur,security-wise,220,I guess it looks like you're still intending to use the proxy? I guess I'm just not sure what's going on and you're making changes (like the change to `c.NotebookApp.allow_remote_access`) that make me very uncomfortable security-wise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6173#issuecomment-496763057
https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:382,Availability,error,error,382,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954
https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:435,Availability,error,error,435,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954
https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:59,Deployability,update,update,59,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954
https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:492,Deployability,update,update,492,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954
https://github.com/hail-is/hail/pull/6175#issuecomment-496662137:17,Testability,Log,LogStore,17,@jigold I added `LogStore` and `GCS` to manage non-blocking interaction with GCS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-496662137
https://github.com/hail-is/hail/pull/6181#issuecomment-497421947:67,Integrability,depend,dependencies,67,"I fixed something that was definitely broken, and then added scala dependencies that didn't fix scala being missing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6181#issuecomment-497421947
https://github.com/hail-is/hail/pull/6190#issuecomment-498621962:131,Deployability,deploy,deployed,131,this actually isn't broken -- I fixed it (by adding a blank line before the bullet lists) on May 1. The problem is that we haven't deployed the website since then.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6190#issuecomment-498621962
https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:115,Availability,failure,failures,115,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460
https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:22,Integrability,depend,dependencies,22,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460
https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:100,Performance,race condition,race condition,100,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460
https://github.com/hail-is/hail/pull/6194#issuecomment-496358797:79,Testability,log,log,79,Timing:. ```; 15543 | test_hail_java | Complete | Success 🎉 (0) | 13 minutes | log; 15544 | test_hail_java_cpp_codegen | Complete | Success 🎉 (0) | 12 minutes | log; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6194#issuecomment-496358797
https://github.com/hail-is/hail/pull/6194#issuecomment-496358797:161,Testability,log,log,161,Timing:. ```; 15543 | test_hail_java | Complete | Success 🎉 (0) | 13 minutes | log; 15544 | test_hail_java_cpp_codegen | Complete | Success 🎉 (0) | 12 minutes | log; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6194#issuecomment-496358797
https://github.com/hail-is/hail/pull/6194#issuecomment-496482191:19,Testability,test,tests,19,"in other news, our tests have gotten slower again? I thought they were at about 15m...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6194#issuecomment-496482191
https://github.com/hail-is/hail/pull/6194#issuecomment-496509626:73,Testability,test,test,73,"Approving since the semantics didn't change, but we're not running the R test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6194#issuecomment-496509626
https://github.com/hail-is/hail/issues/6197#issuecomment-496521985:58,Modifiability,rewrite,rewrite,58,"actually, hold off on this for a couple days -- I want to rewrite the logic in the whole file to support a feature I need to lower MatrixMapCols",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6197#issuecomment-496521985
https://github.com/hail-is/hail/issues/6197#issuecomment-496521985:70,Testability,log,logic,70,"actually, hold off on this for a couple days -- I want to rewrite the logic in the whole file to support a feature I need to lower MatrixMapCols",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6197#issuecomment-496521985
https://github.com/hail-is/hail/pull/6208#issuecomment-497540673:112,Deployability,pipeline,pipeline,112,"Great question. The need for two clients is we have an asynchronous one that ci uses and a synchronous one that pipeline uses. The client code in `aioclient.py` is the asynchronous one and the code in `client.py` is for the synchronous one. Rather than duplicating the code as was done before, I either had to make the synchronous client use the asynchronous code or vice versa. It was easier to make the asynchronous code synchronous by using the `run_until_complete` function and wrapping calls to the asynchronous classes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673
https://github.com/hail-is/hail/pull/6208#issuecomment-497540673:482,Integrability,wrap,wrapping,482,"Great question. The need for two clients is we have an asynchronous one that ci uses and a synchronous one that pipeline uses. The client code in `aioclient.py` is the asynchronous one and the code in `client.py` is for the synchronous one. Rather than duplicating the code as was done before, I either had to make the synchronous client use the asynchronous code or vice versa. It was easier to make the asynchronous code synchronous by using the `run_until_complete` function and wrapping calls to the asynchronous classes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673
https://github.com/hail-is/hail/pull/6214#issuecomment-497073906:21,Deployability,update,update,21,"If this passes, I'll update the setup.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6214#issuecomment-497073906
https://github.com/hail-is/hail/pull/6214#issuecomment-497901535:4,Deployability,update,updated,4,"ok, updated setup.py. ready.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6214#issuecomment-497901535
https://github.com/hail-is/hail/pull/6215#issuecomment-497099235:46,Integrability,interface,interface,46,Does it have to be in `Gi` not `G`? Otherwise interface looks good (don't know much about batch internals so can't comment on those),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6215#issuecomment-497099235
https://github.com/hail-is/hail/pull/6215#issuecomment-497102468:82,Energy Efficiency,schedul,scheduling,82,https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md#resource-quantities,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6215#issuecomment-497102468
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:260,Modifiability,parameteriz,parameterize,260,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:29,Testability,test,tests,29,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:201,Testability,test,test,201,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:218,Testability,test,tests,218,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:335,Testability,test,test,335,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:22,Usability,simpl,simple,22,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
https://github.com/hail-is/hail/pull/6222#issuecomment-497511305:48,Testability,log,log,48,"No, don't want to do that. I just looked at the log and it did what I expected https://ci.hail.is/jobs/29829/log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6222#issuecomment-497511305
https://github.com/hail-is/hail/pull/6222#issuecomment-497511305:109,Testability,log,log,109,"No, don't want to do that. I just looked at the log and it did what I expected https://ci.hail.is/jobs/29829/log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6222#issuecomment-497511305
https://github.com/hail-is/hail/issues/6223#issuecomment-497480960:23,Availability,error,error,23,WTF - I'm getting this error in a PR - https://ci.hail.is/jobs/30344/log. also see this message at the bottom: . 2019-05-30 19:47:48 Hail: WARN: struct{idx: int32} has no field row_idx at <root>.<array>.end for value JInt(10),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960
https://github.com/hail-is/hail/issues/6223#issuecomment-497480960:88,Integrability,message,message,88,WTF - I'm getting this error in a PR - https://ci.hail.is/jobs/30344/log. also see this message at the bottom: . 2019-05-30 19:47:48 Hail: WARN: struct{idx: int32} has no field row_idx at <root>.<array>.end for value JInt(10),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960
https://github.com/hail-is/hail/issues/6223#issuecomment-497480960:69,Testability,log,log,69,WTF - I'm getting this error in a PR - https://ci.hail.is/jobs/30344/log. also see this message at the bottom: . 2019-05-30 19:47:48 Hail: WARN: struct{idx: int32} has no field row_idx at <root>.<array>.end for value JInt(10),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960
https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:76,Availability,error,error,76,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292
https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:479,Availability,error,error,479,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292
https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:581,Availability,error,error,581,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292
https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:116,Performance,perform,perform,116,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292
https://github.com/hail-is/hail/issues/6223#issuecomment-498871032:88,Availability,error,error,88,The *really* weird thing is that I think even a write/read/collect doesn't trigger this error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498871032
https://github.com/hail-is/hail/issues/6223#issuecomment-498872887:361,Availability,error,error,361,"to reproduce:. ```; l = hl.Locus('1', 1); rows = [; 	hl.Struct(locus=l, alleles=[""A"", ""T""]),; 	hl.Struct(locus=l, alleles=[""T"", ""T""]),; 	hl.Struct(locus=l, alleles=[""A"", ""CC"", ""TT""])]. t = hl.Table.parallelize(rows, hl.tstruct(locus=hl.tlocus(), alleles=hl.tarray(hl.tstr)), ['locus', 'alleles']); split = hl.split_multi(t); print(split.collect()); ```; should error, but does not. Writing/reading/showing `split` should also error, but instead prints:; ```; 2019-06-04 18:55:11 Hail: INFO: wrote table with 4 rows in 1 partition to foo; +---------------+------------+---------+-----------+---------------+-----------------+; | locus | alleles | a_index | was_split | old_locus | old_alleles |; +---------------+------------+---------+-----------+---------------+-----------------+; | locus<GRCh37> | array<str> | int32 | bool | locus<GRCh37> | array<str> |; +---------------+------------+---------+-----------+---------------+-----------------+; | 1:1 | [""A"",""T""] | 1 | false | 1:1 | [""A"",""T""] |; | 1:1 | [""T"",""T""] | 1 | false | 1:1 | [""T"",""T""] |; | 1:1 | [""A"",""CC""] | 1 | true | 1:1 | [""A"",""CC"",""TT""] |; | 1:1 | [""A"",""TT""] | 2 | true | 1:1 | [""A"",""CC"",""TT""] |; +---------------+------------+---------+-----------+---------------+-----------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887
https://github.com/hail-is/hail/issues/6223#issuecomment-498872887:426,Availability,error,error,426,"to reproduce:. ```; l = hl.Locus('1', 1); rows = [; 	hl.Struct(locus=l, alleles=[""A"", ""T""]),; 	hl.Struct(locus=l, alleles=[""T"", ""T""]),; 	hl.Struct(locus=l, alleles=[""A"", ""CC"", ""TT""])]. t = hl.Table.parallelize(rows, hl.tstruct(locus=hl.tlocus(), alleles=hl.tarray(hl.tstr)), ['locus', 'alleles']); split = hl.split_multi(t); print(split.collect()); ```; should error, but does not. Writing/reading/showing `split` should also error, but instead prints:; ```; 2019-06-04 18:55:11 Hail: INFO: wrote table with 4 rows in 1 partition to foo; +---------------+------------+---------+-----------+---------------+-----------------+; | locus | alleles | a_index | was_split | old_locus | old_alleles |; +---------------+------------+---------+-----------+---------------+-----------------+; | locus<GRCh37> | array<str> | int32 | bool | locus<GRCh37> | array<str> |; +---------------+------------+---------+-----------+---------------+-----------------+; | 1:1 | [""A"",""T""] | 1 | false | 1:1 | [""A"",""T""] |; | 1:1 | [""T"",""T""] | 1 | false | 1:1 | [""T"",""T""] |; | 1:1 | [""A"",""CC""] | 1 | true | 1:1 | [""A"",""CC"",""TT""] |; | 1:1 | [""A"",""TT""] | 2 | true | 1:1 | [""A"",""CC"",""TT""] |; +---------------+------------+---------+-----------+---------------+-----------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887
https://github.com/hail-is/hail/issues/6223#issuecomment-500771034:66,Availability,error,error,66,This is definitely no longer high-prio -- we'll throw the correct error message now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034
https://github.com/hail-is/hail/issues/6223#issuecomment-500771034:72,Integrability,message,message,72,This is definitely no longer high-prio -- we'll throw the correct error message now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034
https://github.com/hail-is/hail/pull/6231#issuecomment-497760334:171,Performance,optimiz,optimizer,171,"Good example. You can store physical key as a field, but you can also think of it as the result of an analysis, like prune and nesting depth, which you can compute in the optimizer as needed. Just another organizational option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6231#issuecomment-497760334
https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:9,Availability,failure,failure,9,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414
https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:0,Deployability,Pipeline,Pipeline,0,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414
https://github.com/hail-is/hail/pull/6238#issuecomment-498043583:14,Integrability,message,message,14,"Disregard the message above. The auto increment would not work. Now the critical check to make sure no duplicates are added is this line:. ```python3; job_id = parameters.get('job_id'); has_record = await db.jobs.has_record(batch_id, job_id); if has_record:; log.info(f""database has record for ({batch_id}, {job_id})""); abort(400, f'invalid request: batch {batch_id} already has a job_id={job_id}'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583
https://github.com/hail-is/hail/pull/6238#issuecomment-498043583:320,Safety,abort,abort,320,"Disregard the message above. The auto increment would not work. Now the critical check to make sure no duplicates are added is this line:. ```python3; job_id = parameters.get('job_id'); has_record = await db.jobs.has_record(batch_id, job_id); if has_record:; log.info(f""database has record for ({batch_id}, {job_id})""); abort(400, f'invalid request: batch {batch_id} already has a job_id={job_id}'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583
https://github.com/hail-is/hail/pull/6238#issuecomment-498043583:259,Testability,log,log,259,"Disregard the message above. The auto increment would not work. Now the critical check to make sure no duplicates are added is this line:. ```python3; job_id = parameters.get('job_id'); has_record = await db.jobs.has_record(batch_id, job_id); if has_record:; log.info(f""database has record for ({batch_id}, {job_id})""); abort(400, f'invalid request: batch {batch_id} already has a job_id={job_id}'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583
https://github.com/hail-is/hail/pull/6238#issuecomment-498324907:87,Integrability,interface,interface,87,Please keep in mind this PR is part of a much larger refactor of the code and this bad interface will go away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907
https://github.com/hail-is/hail/pull/6238#issuecomment-498324907:53,Modifiability,refactor,refactor,53,Please keep in mind this PR is part of a much larger refactor of the code and this bad interface will go away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907
https://github.com/hail-is/hail/pull/6238#issuecomment-498333308:828,Usability,simpl,simply,828,"> The check for duplicate key is necessary temporarily to make sure someone doesn't do this.; > ; > ```; > b = batch_client.create_batch(); > j = batch.create_job(); > b2 = batch_client.get_batch(b.id); > j2 = b2.create_job(); > ```; > ; > `j` and `j2` will have the same batch id and job id. Checking the job_id in some capacity is totally necessary, since you're no longer using an auto increment id for the job_id (since you're really emulating one job table per batch, by sticking into a denormalized table containing both batch and job information). However, in the time it takes you to check that the job_id doesn't exist, another process could issue that job_id, invalidating your request, and causing you to fail *anyhow*, despite your check. . I would use the table as the origin of truth of the value, and have python simply read that resulting value. We give MySQL back the control for incrementing the id (by using a nested query, checking the max id in a select statement, and then using +1 that as your id). This will guarantee the behavior that you want, because InnoDB will lock the table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498333308
https://github.com/hail-is/hail/pull/6238#issuecomment-498350998:183,Integrability,interface,interface,183,Awesome! I propose leaving this PR unapproved for now. Please look over the parent_ids PR but don't approve. Let me get a working version of my third PR which is to change the client interface to support atomic batch creation. And then finally the fourth PR will atomically create the batch in the database. That way I'll have 4 working PRs -- easier to debug and review. And we can make sure the final product is what we want. Sound good?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498350998
https://github.com/hail-is/hail/pull/6243#issuecomment-498329233:29,Energy Efficiency,reduce,reduce,29,It was just a way to try and reduce the duplication in the code. The correct thing to do is to use requests and not have the overhead of an asynchronous library for a simple client. We can have this discussion in #6244.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233
https://github.com/hail-is/hail/pull/6243#issuecomment-498329233:167,Usability,simpl,simple,167,It was just a way to try and reduce the duplication in the code. The correct thing to do is to use requests and not have the overhead of an asynchronous library for a simple client. We can have this discussion in #6244.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233
https://github.com/hail-is/hail/pull/6245#issuecomment-498401222:38,Modifiability,refactor,refactor,38,"After talking to Cotton, I'm going to refactor the client code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498401222
https://github.com/hail-is/hail/pull/6245#issuecomment-498414073:2,Modifiability,refactor,refactored,2,I refactored the client code. Should be better now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498414073
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:197,Deployability,deploy,deploy,197,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:43,Integrability,depend,dependencies,43,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:310,Performance,load,load,310,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:183,Testability,log,log,183,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:271,Testability,log,log,271,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:325,Testability,test,tests,325,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:356,Testability,benchmark,benchmarking,356,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082
https://github.com/hail-is/hail/pull/6248#issuecomment-498742903:224,Availability,down,down,224,"Hmm... shadowJar is building at about 2 minutes on my computer, 1 min 20s of which is compileScala. Master is building at about 1 min 40s (not sure how much scala compile is taking, forgot to check), and I managed to get it down to like 1 min 45s by not bundling some of the dependencies that we weren't bundling before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903
https://github.com/hail-is/hail/pull/6248#issuecomment-498742903:275,Integrability,depend,dependencies,275,"Hmm... shadowJar is building at about 2 minutes on my computer, 1 min 20s of which is compileScala. Master is building at about 1 min 40s (not sure how much scala compile is taking, forgot to check), and I managed to get it down to like 1 min 45s by not bundling some of the dependencies that we weren't bundling before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903
https://github.com/hail-is/hail/pull/6248#issuecomment-498797460:68,Testability,log,log,68,Indeed:. > 17 | build_hail | Complete | Success 🎉 (0) | 4 minutes | log. Nice work! I'll read over in more detail soon.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498797460
https://github.com/hail-is/hail/pull/6249#issuecomment-498796054:152,Availability,Error,Error,152,"Great, perfect, I just wanted to make sure there wasn't another higher level issue to fix. This is great for now, but I just want to reiterate need for Error state and reason_for_error in the batch schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6249#issuecomment-498796054
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:59,Availability,failure,failures,59,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:111,Availability,FAILURE,FAILURES,111,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:584,Availability,echo,echo,584,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1889,Availability,echo,echo,1889,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1954,Availability,echo,echo,1954,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:2036,Availability,echo,echo,2036,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:2118,Availability,echo,echo,2118,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1615,Integrability,rout,route,1615,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:54,Testability,test,test,54,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:186,Testability,Test,Test,186,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:247,Testability,test,test,247,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:263,Testability,Test,Test,263,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:268,Testability,test,testMethod,268,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:592,Testability,test,test,592,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:841,Testability,test,tests,841,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:929,Testability,assert,assertEqual,929,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1106,Testability,test,test,1106,"ILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', comman",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1213,Testability,test,test,1213,"__________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); bat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1262,Testability,assert,assertEqual,1262,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1295,Testability,Assert,AssertionError,1295,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1580,Testability,test,test-client,1580,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1623,Testability,test,test,1623,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1653,Testability,test,test,1653,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1832,Testability,test,test,1832,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:2280,Testability,assert,assert,2280,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:2307,Testability,assert,assert,2307,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506
https://github.com/hail-is/hail/pull/6261#issuecomment-499215870:95,Testability,assert,assert-a-spy-is-invoked-with-event-on-click-using-jasmine,95,You could take a look at Mocha or Jasmine. https://stackoverflow.com/questions/11529158/how-to-assert-a-spy-is-invoked-with-event-on-click-using-jasmine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6261#issuecomment-499215870
https://github.com/hail-is/hail/pull/6262#issuecomment-498911743:19,Availability,down,down,19,"we recently locked down our CI to run PRs only from the development team -- we intend to add a feature to our CI to let us run the test suite on a specific PR and SHA, but don't have that feature yet... 😦 . @cseed 	@danking this isn't hard, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743
https://github.com/hail-is/hail/pull/6262#issuecomment-498911743:131,Testability,test,test,131,"we recently locked down our CI to run PRs only from the development team -- we intend to add a feature to our CI to let us run the test suite on a specific PR and SHA, but don't have that feature yet... 😦 . @cseed 	@danking this isn't hard, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743
https://github.com/hail-is/hail/pull/6262#issuecomment-500610415:90,Testability,test,test,90,"Hey @tmwong2003, we recently changed our CI setup and there's a little more work to do to test external PRs. I'll try to have someone finish this in the next couple of days. Thanks for your patience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-500610415
https://github.com/hail-is/hail/pull/6262#issuecomment-500639087:344,Testability,test,test,344,"Thanks Cotton. CI management is never easy at the best of times so no worries. - T. -- ; Theodore Wong <tmw@tmwong.org>; http://www.tmwong.org. > On Jun 10, 2019, at 2:58 PM, cseed <notifications@github.com> wrote:; > ; > Hey @tmwong2003 <https://github.com/tmwong2003>, we recently changed our CI setup and there's a little more work to do to test external PRs. I'll try to have someone finish this in the next couple of days. Thanks for your patience.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/pull/6262?email_source=notifications&email_token=ABF7I6IGGFIJYRP2T6EIMSDPZ3E7HA5CNFSM4HTJ3TF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXLLK3Y#issuecomment-500610415>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF7I6NP2HBAOUHJR4HOWIDPZ3E7HANCNFSM4HTJ3TFQ>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-500639087
https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:379,Availability,failure,failures,379,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788
https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:614,Integrability,message,message,614,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788
https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:184,Testability,test,tests,184,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788
https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:268,Testability,log,logs,268,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788
https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:370,Testability,log,logs,370,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788
https://github.com/hail-is/hail/pull/6263#issuecomment-498938995:17,Testability,test,tests,17,@chrisvittal all tests passing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-498938995
https://github.com/hail-is/hail/pull/6263#issuecomment-500844860:0,Availability,Ping,Ping,0,Ping @chrisvittal. Could you pass this off or review?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500844860
https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:1099,Integrability,interface,interfaces,1099,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573
https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:48,Modifiability,refactor,refactor,48,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573
https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:519,Performance,perform,perform,519,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573
https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:169,Testability,test,tests,169,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573
https://github.com/hail-is/hail/pull/6263#issuecomment-501361010:17,Availability,ping,ping,17,This now passes. ping @chrisvittal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-501361010
https://github.com/hail-is/hail/pull/6264#issuecomment-499467470:22,Availability,failure,failure,22,Our CI had a sporadic failure here -- you can retest again by pushing an empty commit to the branch:. git commit --allow-empty -m bump && git push,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6264#issuecomment-499467470
https://github.com/hail-is/hail/pull/6265#issuecomment-499142223:55,Testability,log,log,55,@akotlar where are the other whitelists? I'll have him log into auth0 now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6265#issuecomment-499142223
https://github.com/hail-is/hail/pull/6265#issuecomment-499142882:37,Security,authoriz,authorized-users,37,Stored under notebook-secrets : data.authorized-users,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6265#issuecomment-499142882
https://github.com/hail-is/hail/pull/6266#issuecomment-504217666:17,Testability,test,tests,17,"I ran the python tests with my latest changes and they all passed. I'll work on addressing comments and adding tests, but this should be ready for another round of code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-504217666
https://github.com/hail-is/hail/pull/6266#issuecomment-504217666:111,Testability,test,tests,111,"I ran the python tests with my latest changes and they all passed. I'll work on addressing comments and adding tests, but this should be ready for another round of code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-504217666
https://github.com/hail-is/hail/pull/6266#issuecomment-506857050:17,Testability,benchmark,benchmarks,17,Also need to see benchmarks.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-506857050
https://github.com/hail-is/hail/pull/6266#issuecomment-507079121:20,Testability,benchmark,benchmarks,20,"Still need to write benchmarks, but I wrote a new type to hold the options for the indexed reads, please take a look.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-507079121
https://github.com/hail-is/hail/pull/6266#issuecomment-507403990:4,Testability,benchmark,benchmarks,4,"Ok, benchmarks are pretty bad, these are the range table benchmarks from #6529. #### No Index; ```; running write_range_table_p1000...; Mean, Median: 11.79s, 11.18s; running write_range_table_p100...; Mean, Median: 4.76s, 4.67s; running write_range_table_p10...; Mean, Median: 3.28s, 3.03s; ```; #### Index; ```; running write_range_table_p1000...; Mean, Median: 28.60s, 28.88s; running write_range_table_p100...; Mean, Median: 10.17s, 10.20s; running write_range_table_p10...; Mean, Median: 9.52s, 9.44s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-507403990
https://github.com/hail-is/hail/pull/6266#issuecomment-507403990:57,Testability,benchmark,benchmarks,57,"Ok, benchmarks are pretty bad, these are the range table benchmarks from #6529. #### No Index; ```; running write_range_table_p1000...; Mean, Median: 11.79s, 11.18s; running write_range_table_p100...; Mean, Median: 4.76s, 4.67s; running write_range_table_p10...; Mean, Median: 3.28s, 3.03s; ```; #### Index; ```; running write_range_table_p1000...; Mean, Median: 28.60s, 28.88s; running write_range_table_p100...; Mean, Median: 10.17s, 10.20s; running write_range_table_p10...; Mean, Median: 9.52s, 9.44s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6266#issuecomment-507403990
https://github.com/hail-is/hail/pull/6267#issuecomment-499296452:174,Availability,error,error,174,"oh, crap - the lowering rule for MatrixEntriesTable has the same problem:; ```; E Current key: [1:10000,[A,G],sample_001]; E Previous key: [1:10000,[A,G],sample_500]; E This error can occur after a split_multi if the dataset; E contains both multiallelic variants and duplicated loci.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499296452
https://github.com/hail-is/hail/pull/6267#issuecomment-499585600:133,Performance,perform,performance,133,"@catoverdrive I pushed a fix on here. I'll let Patrick review I think, unless you want to review my piece. Unfortunately, this kills performance. The no-key benchmark is about 5-6x faster (20s) than the keyed benchmark (110s)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499585600
https://github.com/hail-is/hail/pull/6267#issuecomment-499585600:157,Testability,benchmark,benchmark,157,"@catoverdrive I pushed a fix on here. I'll let Patrick review I think, unless you want to review my piece. Unfortunately, this kills performance. The no-key benchmark is about 5-6x faster (20s) than the keyed benchmark (110s)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499585600
https://github.com/hail-is/hail/pull/6267#issuecomment-499585600:209,Testability,benchmark,benchmark,209,"@catoverdrive I pushed a fix on here. I'll let Patrick review I think, unless you want to review my piece. Unfortunately, this kills performance. The no-key benchmark is about 5-6x faster (20s) than the keyed benchmark (110s)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499585600
https://github.com/hail-is/hail/pull/6268#issuecomment-499306521:23,Availability,Error,Error,23,"> Pending -> Ready -> (Error, Running -> (Failed, Success)) . Does Running mean the pod has been created, or it is verified to be running? In the former case, is it Pending -> Ready -> Running -> (Error, Failed, Success)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499306521
https://github.com/hail-is/hail/pull/6268#issuecomment-499306521:197,Availability,Error,Error,197,"> Pending -> Ready -> (Error, Running -> (Failed, Success)) . Does Running mean the pod has been created, or it is verified to be running? In the former case, is it Pending -> Ready -> Running -> (Error, Failed, Success)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499306521
https://github.com/hail-is/hail/pull/6268#issuecomment-499331021:130,Availability,Error,Error,130,"I set it to Running at the end of `_create_pod`. Maybe that's not correct. It should probably be this then:. Pending -> Ready -> (Error, Created -> Running -> (Failed, Success))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499331021
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:47,Availability,recover,recoverable,47,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:59,Availability,error,error,59,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:107,Availability,Error,Error,107,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:154,Availability,Error,Error,154,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:393,Deployability,update,update,393,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:47,Safety,recover,recoverable,47,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:503,Safety,safe,safe,503,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683
https://github.com/hail-is/hail/pull/6268#issuecomment-499339906:61,Availability,Error,Error,61,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906
https://github.com/hail-is/hail/pull/6268#issuecomment-499339906:95,Availability,Error,Error,95,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906
https://github.com/hail-is/hail/pull/6268#issuecomment-499339906:11,Usability,clear,clear,11,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906
https://github.com/hail-is/hail/pull/6268#issuecomment-499651541:278,Availability,Error,Error,278,"> But I'm confused because I thought that was the same as my initial implementation. Sorry, don't read too much into my first few comments, I was trying to understand the constraints. I think my last comment is very close to your original proposal, but also includes Running -> Error and Running -> Ready as possible transitions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499651541
https://github.com/hail-is/hail/pull/6270#issuecomment-499320804:39,Safety,safe,safe,39,"This is untested, but almost certainly safe. Waiting on CI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6270#issuecomment-499320804
https://github.com/hail-is/hail/pull/6272#issuecomment-499524823:55,Integrability,message,message,55,I'm pretty sure GitHub will merge this with the commit message (instead of PR title/comments) since it's a single commit. oh well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6272#issuecomment-499524823
https://github.com/hail-is/hail/pull/6272#issuecomment-499532296:47,Integrability,message,message,47,"@tpoterba sorry, may bad, I amended the commit message and re-pushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6272#issuecomment-499532296
https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:14,Availability,error,error,14,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220
https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:21,Availability,Error,Error,21,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220
https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:237,Availability,error,error,237,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220
https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:106,Integrability,message,message,106,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220
https://github.com/hail-is/hail/pull/6276#issuecomment-499694445:14,Integrability,message,message,14,"> print out a message and don't label but still create. That's what I meant, yeah. But I'll do the munging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694445
https://github.com/hail-is/hail/pull/6279#issuecomment-499672902:91,Safety,unsafe,unsafe,91,The sun lint thing is irrelevant in Scala because we have a Java-based shim around all the unsafe calls.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6279#issuecomment-499672902
https://github.com/hail-is/hail/pull/6284#issuecomment-499938113:17,Deployability,install,install,17,Trying to do dev install deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113
https://github.com/hail-is/hail/pull/6284#issuecomment-499938113:25,Deployability,deploy,deploy,25,Trying to do dev install deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113
https://github.com/hail-is/hail/pull/6288#issuecomment-501727081:35,Deployability,pipeline,pipeline,35,@cseed What are you thinking of in pipeline? Both CI and pipeline should use the batch client and thus be abstracted from changes in URLs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6288#issuecomment-501727081
https://github.com/hail-is/hail/pull/6288#issuecomment-501727081:57,Deployability,pipeline,pipeline,57,@cseed What are you thinking of in pipeline? Both CI and pipeline should use the batch client and thus be abstracted from changes in URLs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6288#issuecomment-501727081
https://github.com/hail-is/hail/pull/6291#issuecomment-500138300:22,Testability,benchmark,benchmark,22,hold up -- we need to benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500138300
https://github.com/hail-is/hail/pull/6291#issuecomment-500153683:26,Testability,benchmark,benchmarking,26,"If only we had, y'know, a benchmarking system that could, y'know, run benchmarks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500153683
https://github.com/hail-is/hail/pull/6291#issuecomment-500153683:70,Testability,benchmark,benchmarks,70,"If only we had, y'know, a benchmarking system that could, y'know, run benchmarks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500153683
https://github.com/hail-is/hail/pull/6291#issuecomment-500453080:10,Testability,test,tests,10,"I ran two tests. The one you suggested, 10k/10k, and one with a better work-to-partition ratio: 1M/1k. I was a bit surprised to see that the cost ratio is larger with fewer partitions and more work. We range from 15x to 3x slower. . This branch:; ```python; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 17 ms, sys: 6.36 ms, total: 23.4 ms; Wall time: 1min 16s. In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.42 s, sys: 303 ms, total: 1.73 s; Wall time: 15.2 s. ```. Master `0.2.14-4da055db5a7b`; ```python; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500453080
https://github.com/hail-is/hail/pull/6291#issuecomment-500455081:97,Modifiability,parameteriz,parameterize,97,"And this is on a laptop with an SSD, right? it'll be even worse on the cloud, I think. Should we parameterize this behavior?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500455081
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:7,Deployability,install,installing,7,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:94,Deployability,deploy,deploy,94,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:473,Deployability,install,install,473,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:25,Integrability,depend,dependencies,25,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:41,Integrability,inject,injecting,41,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:72,Integrability,depend,dependencies,72,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:144,Integrability,depend,dependencies,144,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:414,Integrability,depend,dependencies,414,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:41,Security,inject,injecting,41,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:543,Availability,error,error,543,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:651,Availability,avail,available,651,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:41,Deployability,install,install,41,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:174,Deployability,install,installing,174,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:431,Deployability,install,install,431,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:706,Deployability,install,install,706,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:716,Deployability,upgrade,upgrade,716,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508
https://github.com/hail-is/hail/issues/6299#issuecomment-500772207:4,Deployability,install,installed,4,"You installed Hail using pip, right? You're running locally?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-500772207
https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:364,Availability,error,error,364,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374
https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:39,Deployability,install,installed,39,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374
https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:179,Deployability,install,installed,179,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374
https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:337,Integrability,protocol,protocol,337,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374
https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:213,Performance,load,load,213,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374
https://github.com/hail-is/hail/issues/6299#issuecomment-516543503:25,Deployability,install,install,25,"@zillurbmb51 You need to install Java 8, you have Java 12.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-516543503
https://github.com/hail-is/hail/issues/6299#issuecomment-516552799:47,Deployability,install,install-and-use-java-,47,https://discuss.hail.is/t/on-mac-os-x-how-do-i-install-and-use-java-8-if-i-already-have-a-different-version-of-java-installed/831/2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-516552799
https://github.com/hail-is/hail/issues/6299#issuecomment-516552799:116,Deployability,install,installed,116,https://discuss.hail.is/t/on-mac-os-x-how-do-i-install-and-use-java-8-if-i-already-have-a-different-version-of-java-installed/831/2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-516552799
https://github.com/hail-is/hail/pull/6302#issuecomment-500597278:18,Testability,log,logic,18,we need to add CI logic to prevent merging things that are stacked. This should have gone in after #6298,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6302#issuecomment-500597278
https://github.com/hail-is/hail/pull/6304#issuecomment-500933118:152,Energy Efficiency,Schedul,SchedulerBackend,152,"Yes, DistributedBackend vs. LocalBackend with the assumption that the generic terms would identify our native implementations. I didn't want to call it SchedulerBackend because hopefully eventually everything (shuffle, etc) will tie in here, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6304#issuecomment-500933118
https://github.com/hail-is/hail/pull/6308#issuecomment-500908659:126,Safety,safe,safer,126,"`db` sounds like a connection. I used dbpool. Added a check in _start_build. I had debated about this, but I agree, certainly safer. I put the yaml fields in alphabetical order, which is what the K8s docs usually do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6308#issuecomment-500908659
https://github.com/hail-is/hail/pull/6312#issuecomment-500791514:73,Deployability,pipeline,pipelines,73,would prefer if #6313 went in first -- don't want to deoptimize anyone's pipelines!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-500791514
https://github.com/hail-is/hail/pull/6312#issuecomment-501000473:32,Availability,failure,failures,32,thank goodness for random batch failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-501000473
https://github.com/hail-is/hail/pull/6312#issuecomment-501314794:20,Deployability,deploy,deployment,20,"don't approve until deployment is working again, please",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-501314794
https://github.com/hail-is/hail/pull/6318#issuecomment-501295237:146,Modifiability,parameteriz,parameterizable,146,"Oh yeah, sorry, this is confusing. . #6322 (which went in yesterday) changed the `pip` definition to `PIP` to indicate it's supposed to be a user-parameterizable variable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501295237
https://github.com/hail-is/hail/pull/6318#issuecomment-501295237:162,Modifiability,variab,variable,162,"Oh yeah, sorry, this is confusing. . #6322 (which went in yesterday) changed the `pip` definition to `PIP` to indicate it's supposed to be a user-parameterizable variable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501295237
https://github.com/hail-is/hail/pull/6318#issuecomment-501311985:100,Deployability,install,install-deps,100,"Alright, rebased and made the requested change. Noticed one strange thing though. When I do ; `make install-deps`, the print out starts with:. ```; cat: /secrets//pypi-username: No such file or directory; cat: /secrets//pypi-password: No such file or directory; python3 -m pip install -U -r python/requirements.txt -r python/dev-requirements.txt; ```; Obviously that third line makes sense but I don't see why first 2 happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985
https://github.com/hail-is/hail/pull/6318#issuecomment-501311985:277,Deployability,install,install,277,"Alright, rebased and made the requested change. Noticed one strange thing though. When I do ; `make install-deps`, the print out starts with:. ```; cat: /secrets//pypi-username: No such file or directory; cat: /secrets//pypi-password: No such file or directory; python3 -m pip install -U -r python/requirements.txt -r python/dev-requirements.txt; ```; Obviously that third line makes sense but I don't see why first 2 happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985
https://github.com/hail-is/hail/pull/6318#issuecomment-501311985:225,Security,password,password,225,"Alright, rebased and made the requested change. Noticed one strange thing though. When I do ; `make install-deps`, the print out starts with:. ```; cat: /secrets//pypi-username: No such file or directory; cat: /secrets//pypi-password: No such file or directory; python3 -m pip install -U -r python/requirements.txt -r python/dev-requirements.txt; ```; Obviously that third line makes sense but I don't see why first 2 happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985
https://github.com/hail-is/hail/pull/6318#issuecomment-501313676:174,Deployability,deploy,deploy,174,"ah, annoying. It's hard to figure out when stuff is actually going to be executed in make. We can fix this by moving those definitions inside the target, I think:; ```; pypi-deploy: check-pypi wheel; 	 TWINE_USERNAME=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-username) \ ; 	 TWINE_PASSWORD=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-password) \; 	 twine upload build/deploy/dist/*; ```. mind making that change in a separate branch?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676
https://github.com/hail-is/hail/pull/6318#issuecomment-501313676:370,Deployability,deploy,deploy,370,"ah, annoying. It's hard to figure out when stuff is actually going to be executed in make. We can fix this by moving those definitions inside the target, I think:; ```; pypi-deploy: check-pypi wheel; 	 TWINE_USERNAME=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-username) \ ; 	 TWINE_PASSWORD=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-password) \; 	 twine upload build/deploy/dist/*; ```. mind making that change in a separate branch?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676
https://github.com/hail-is/hail/pull/6318#issuecomment-501313676:336,Security,password,password,336,"ah, annoying. It's hard to figure out when stuff is actually going to be executed in make. We can fix this by moving those definitions inside the target, I think:; ```; pypi-deploy: check-pypi wheel; 	 TWINE_USERNAME=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-username) \ ; 	 TWINE_PASSWORD=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-password) \; 	 twine upload build/deploy/dist/*; ```. mind making that change in a separate branch?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676
https://github.com/hail-is/hail/pull/6333#issuecomment-501729022:15,Availability,failure,failure,15,"Fixed a rebase failure, deleted some unnecessary whitespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501729022
https://github.com/hail-is/hail/pull/6333#issuecomment-501732176:16,Integrability,interface,interface,16,"Erm. There's an interface issue here. I need an output stream that can tell me the file position, but I don't know of any standard library interface that exposes that, so I just hardcoded the `FSDataOutputStream`. Is this going to be a problem? Are all `FS` implementations Hadoop file systems? cc: @akotlar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176
https://github.com/hail-is/hail/pull/6333#issuecomment-501732176:139,Integrability,interface,interface,139,"Erm. There's an interface issue here. I need an output stream that can tell me the file position, but I don't know of any standard library interface that exposes that, so I just hardcoded the `FSDataOutputStream`. Is this going to be a problem? Are all `FS` implementations Hadoop file systems? cc: @akotlar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176
https://github.com/hail-is/hail/pull/6333#issuecomment-501732176:154,Security,expose,exposes,154,"Erm. There's an interface issue here. I need an output stream that can tell me the file position, but I don't know of any standard library interface that exposes that, so I just hardcoded the `FSDataOutputStream`. Is this going to be a problem? Are all `FS` implementations Hadoop file systems? cc: @akotlar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176
https://github.com/hail-is/hail/issues/6339#issuecomment-501674970:10,Modifiability,extend,extend,10,"we should extend the valid locus range, I think. For now you can import with `reference_genome=None`, which will import the chr/pos as a string/int instead of a Hail locus type.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6339#issuecomment-501674970
https://github.com/hail-is/hail/pull/6341#issuecomment-502415859:195,Performance,queue,queueing,195,"Will accept this now to prevent perfect from being enemy of good. I do think some of the points raised should be considered (indent or replace result line, whether to check n_cancelled before re-queueing a job, whether to drop batch.state).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6341#issuecomment-502415859
https://github.com/hail-is/hail/issues/6342#issuecomment-658296305:55,Testability,test,test,55,Can I close this and create an asana task to add a BOM test?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342#issuecomment-658296305
https://github.com/hail-is/hail/issues/6342#issuecomment-1265654206:37,Usability,simpl,simple,37,"I looked into this a bit and its not simple. There's a [BOMInputStream](https://commons.apache.org/proper/commons-io/javadocs/api-2.5/org/apache/commons/io/input/BOMInputStream.html) and [definitions of the BOMs](https://commons.apache.org/proper/commons-io/javadocs/api-2.5/org/apache/commons/io/ByteOrderMark.html) in commons. After handling compression and if we are looking at the first byte in the file, we need to check if the first 2-4 bytes are one of the magic BOM constants, if yes we should drop that. Unfortunately, the BOMInputStream doesn't propagate position information along. We probably need a PositionedBOMInputStream and I'm not exactly sure what getPosition should return.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342#issuecomment-1265654206
https://github.com/hail-is/hail/pull/6344#issuecomment-501853609:73,Testability,log,log,73,"You ran out of disk, apparently. https://ci.hail.is/batches/1181/jobs/25/log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6344#issuecomment-501853609
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:132,Energy Efficiency,schedul,scheduler,132,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:172,Energy Efficiency,schedul,scheduler,172,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:271,Energy Efficiency,schedul,scheduler,271,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:369,Energy Efficiency,schedul,scheduler,369,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:623,Energy Efficiency,schedul,scheduler,623,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:704,Energy Efficiency,schedul,scheduler,704,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:810,Energy Efficiency,schedul,scheduler,810,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:960,Energy Efficiency,schedul,scheduler,960,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1049,Energy Efficiency,schedul,scheduler,1049,e a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1147,Energy Efficiency,schedul,scheduler,1147,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1243,Energy Efficiency,schedul,scheduler,1243,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:88); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.Interp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1408,Energy Efficiency,schedul,scheduler,1408,.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:88); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$7.apply(InterpretNonCompilable.scala:19); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$7.apply(InterpretNonCompilable.scala:19); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:6939,Energy Efficiency,schedul,scheduler,6939,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:7011,Energy Efficiency,schedul,scheduler,7011,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:7296,Performance,concurren,concurrent,7296,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:7381,Performance,concurren,concurrent,7381,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:303,Safety,abort,abortStage,303,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:401,Safety,abort,abortStage,401,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:646,Safety,abort,abortStage,646,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307
https://github.com/hail-is/hail/pull/6346#issuecomment-501841086:3,Testability,log,logic,3,"CI logic is around the assignee, not requested reviewer (still not sure what that means / how the API looks)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6346#issuecomment-501841086
https://github.com/hail-is/hail/pull/6349#issuecomment-504085816:12,Availability,failure,failure,12,random test failure...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816
https://github.com/hail-is/hail/pull/6349#issuecomment-504085816:7,Testability,test,test,7,random test failure...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816
https://github.com/hail-is/hail/pull/6366#issuecomment-502703328:32,Energy Efficiency,monitor,monitor,32,I also added a PV and PVC count monitor to Grafana,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6366#issuecomment-502703328
https://github.com/hail-is/hail/pull/6377#issuecomment-503584053:3,Availability,recover,recovered,3,CI recovered once the PR was closed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053
https://github.com/hail-is/hail/pull/6377#issuecomment-503584053:3,Safety,recover,recovered,3,CI recovered once the PR was closed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053
https://github.com/hail-is/hail/pull/6414#issuecomment-504040126:11,Availability,error,error,11,"Fixes this error msg:. ```; jinja2.exceptions.TemplateSyntaxError: Encountered unknown tag 'endfor'. You probably made a nesting mistake. Jinja is expecting this tag, but currently looking for 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6414#issuecomment-504040126
https://github.com/hail-is/hail/pull/6423#issuecomment-504153212:146,Deployability,install,install,146,I duplicated globals.py so I can get the tests going. I'll think about how to organize sharing between the client in the server. Might just be to install the client on the server image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212
https://github.com/hail-is/hail/pull/6423#issuecomment-504153212:41,Testability,test,tests,41,I duplicated globals.py so I can get the tests going. I'll think about how to organize sharing between the client in the server. Might just be to install the client on the server image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212
https://github.com/hail-is/hail/pull/6425#issuecomment-509820038:102,Testability,test,tests,102,rebased. one merge change in `object TableValue` `apply` I wasn't sure about (a constructor deleted). tests likely failing. will follow up on both tonight.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6425#issuecomment-509820038
https://github.com/hail-is/hail/pull/6430#issuecomment-504241795:25,Availability,down,downloading,25,I tested this locally by downloading the HTML for the UI page and tinkering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795
https://github.com/hail-is/hail/pull/6430#issuecomment-504241795:2,Testability,test,tested,2,I tested this locally by downloading the HTML for the UI page and tinkering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795
https://github.com/hail-is/hail/pull/6432#issuecomment-507093890:2,Security,authoriz,authorized,2,I authorized it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6432#issuecomment-507093890
https://github.com/hail-is/hail/pull/6437#issuecomment-504535236:28,Testability,test,test,28,@tpoterba Is there a way to test the pip package still works?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504535236
https://github.com/hail-is/hail/pull/6437#issuecomment-504542922:6,Deployability,install,install-wheel,6,"`make install-wheel` and then `PYTHONPATH="""" python -c ""import hail; hail.init()""`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504542922
https://github.com/hail-is/hail/pull/6437#issuecomment-504543247:33,Testability,log,logic,33,you need to fix up some makefile logic,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504543247
https://github.com/hail-is/hail/pull/6437#issuecomment-504548967:6,Deployability,install,install-wheel,6,`make install-wheel` changes my local environment?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504548967
https://github.com/hail-is/hail/pull/6437#issuecomment-504549671:61,Deployability,install,install,61,"Wait, how's that? Looks like it runs `pip uninstall hail ... install hail`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504549671
https://github.com/hail-is/hail/pull/6437#issuecomment-504550511:7,Deployability,install,install-wheel,7,"> make install-wheel and then PYTHONPATH="""" python -c ""import hail; hail.init()"". Should this be an automated test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504550511
https://github.com/hail-is/hail/pull/6437#issuecomment-504550511:110,Testability,test,test,110,"> make install-wheel and then PYTHONPATH="""" python -c ""import hail; hail.init()"". Should this be an automated test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504550511
https://github.com/hail-is/hail/pull/6438#issuecomment-504570971:42,Testability,test,tests,42,"Oof, I'm failing one of the docs Pedigree tests. Surprising, but glad we have those. I'll investigate",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6438#issuecomment-504570971
https://github.com/hail-is/hail/pull/6444#issuecomment-504634470:104,Integrability,message,message,104,I'm reverting this to 1 so Konrad can get some work done this weekend -- will figure out why the client message size is still failing next week.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6444#issuecomment-504634470
https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:11,Deployability,pipeline,pipeline,11,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295
https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:95,Deployability,pipeline,pipeline,95,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295
https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:159,Deployability,pipeline,pipeline,159,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295
https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:194,Deployability,pipeline,pipeline,194,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295
https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:75,Testability,test,tests,75,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295
https://github.com/hail-is/hail/pull/6449#issuecomment-505070267:210,Testability,test,test,210,"> Maybe we should also use this opportunity to add hailctl to scorecard / remove cloudtools?. yeah, you could replace cloudtools with hailctl dataproc as well. Especially since you need to bump to fix a random test ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6449#issuecomment-505070267
https://github.com/hail-is/hail/pull/6454#issuecomment-505178184:118,Testability,assert,asserting,118,> It's calling into the db once per job? That's bad. Why does it even need to do that?. That's an easy fix. It's just asserting the job wasn't seen before. Remnants of debugging probably. We can also have a callback that starts all of the jobs not in the create_batch call.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6454#issuecomment-505178184
https://github.com/hail-is/hail/pull/6454#issuecomment-505182631:134,Testability,test,test,134,As I briefly mentioned over Zulip we’d may want to increase the proxy_timeout above 60s default (it appears you hit the limit in your test). There may be some other parameters to address as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6454#issuecomment-505182631
https://github.com/hail-is/hail/pull/6454#issuecomment-505216094:26,Testability,test,tests,26,"@jigold I removed all the tests, so I think this is good to go in for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6454#issuecomment-505216094
https://github.com/hail-is/hail/pull/6456#issuecomment-505908840:6,Testability,test,tests,6,added tests and narrowed the match. I had a bug that generated invalid IR if the `insertedChild` here wasn't a ref.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6456#issuecomment-505908840
https://github.com/hail-is/hail/issues/6458#issuecomment-505215908:22,Availability,error,error,22,This should be a type error. I don't think you should be able to use comparison operators on non-primitive types,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458#issuecomment-505215908
https://github.com/hail-is/hail/issues/6458#issuecomment-505219406:165,Security,expose,expose,165,"what about `array<set<dict<str, str>>>` < `array<set<dict<str, str>>>`? I agree that Locus comparisons should be specially supported, but I think that the change to expose comparisons on **any** types was wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6458#issuecomment-505219406
https://github.com/hail-is/hail/pull/6462#issuecomment-506361818:85,Testability,test,tests,85,"Thanks, @tpoterba! I wasn't done so I hadn't assigned anyone, just wanted to run the tests, but it should be good to go now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6462#issuecomment-506361818
https://github.com/hail-is/hail/pull/6462#issuecomment-506362677:130,Usability,simpl,simplified,130,"Note, @jigold gets the credit for this, ExportBGEN.scala was basically taken from an old PR of hers that never made it in. I just simplified it to only support the 8-bit case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6462#issuecomment-506362677
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:5313,Availability,toler,tolerations,5313,"_message_path': '/dev/termination-log',; 'termination_message_policy': 'File',; 'tty': None,; 'volume_devices': None,; 'volume_mounts': [{'mount_path': '/gsa-key',; 'mount_propagation': None,; 'name': 'gsa-key',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/io',; 'mount_propagation': None,; 'name': 'batch-2554-job-4-8vvgl',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',; 'mount_propagation': None,; 'name': 'default-token-8h99c',; 'read_only': True,; 'sub_path': None}],; 'working_dir': None}],; 'dns_config': None,; 'dns_policy': 'ClusterFirst',; 'enable_service_links': True,; 'host_aliases': None,; 'host_ipc': None,; 'host_network': None,; 'host_pid': None,; 'hostname': None,; 'image_pull_secrets': None,; 'init_containers': None,; 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-4gq2',; 'node_selector': None,; 'priority': 500000,; 'priority_class_name': 'user',; 'readiness_gates': None,; 'restart_policy': 'Never',; 'runtime_class_name': None,; 'scheduler_name': 'default-scheduler',; 'security_context': {'fs_group': None,; 'run_as_group': None,; 'run_as_non_root': None,; 'run_as_user': None,; 'se_linux_options': None,; 'supplemental_groups': None,; 'sysctls': None},; 'service_account': 'default',; 'service_account_name': 'default',; 'share_process_namespace': None,; 'subdomain': None,; 'termination_grace_period_seconds': 30,; 'tolerations': [{'effect': None,; 'key': 'preemptible',; 'operator': None,; 'toleration_seconds': None,; 'value': 'true'},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/not-ready',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/unreachable',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None}],; 'volumes': [{'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11043,Availability,error,error,11043,"mespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instan",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14642,Availability,toler,tolerations,14642,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14765,Availability,toler,tolerationSeconds,14765,"P; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradj",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14865,Availability,toler,tolerationSeconds,14865,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:18859,Availability,Toler,Tolerations,18859,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:218,Deployability,update,update,218,"I deleted the pod; ```; # k delete pod batch-2554-job-4-main-cc8d4 -n batch-pods; ```; Batch logs when batch discovered 2554 task 4 ""failed"":; ```; INFO | 2019-06-25 12:37:07,611 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:989 | job (2554, 4) mark complete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipelin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:347,Deployability,update,update,347,"I deleted the pod; ```; # k delete pod batch-2554-job-4-main-cc8d4 -n batch-pods; ```; Batch logs when batch discovered 2554 task 4 ""failed"":; ```; INFO | 2019-06-25 12:37:07,611 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:989 | job (2554, 4) mark complete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipelin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1677,Deployability,pipeline,pipeline,1677,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1686,Deployability,pipeline,pipeline-,1686,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1749,Deployability,pipeline,pipeline,1749,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1758,Deployability,pipeline,pipeline-,1758,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1825,Deployability,pipeline,pipeline,1825,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1834,Deployability,pipeline,pipeline-,1834,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1912,Deployability,pipeline,pipeline,1912,"e.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1921,Deployability,pipeline,pipeline-,1921,"e.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1994,Deployability,pipeline,pipeline,1994,"te_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2003,Deployability,pipeline,pipeline-,2003,"te_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2078,Deployability,pipeline,pipeline,2078,"batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2087,Deployability,pipeline,pipeline-,2087,"batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2156,Deployability,pipeline,pipeline,2156,"batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2165,Deployability,pipeline,pipeline-,2165,"batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2320,Deployability,pipeline,pipeline,2320,"_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': Non",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2329,Deployability,pipeline,pipeline-,2329,"_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': Non",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2423,Deployability,pipeline,pipeline,2423,"{'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2432,Deployability,pipeline,pipeline-,2432,"{'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2502,Deployability,pipeline,pipeline,2502,"{'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2511,Deployability,pipeline,pipeline-,2511,"{'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11276,Deployability,update,update,11276,"y"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a8",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11405,Deployability,update,update,11405,"ython3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RES",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11719,Deployability,update,update,11719,"ent/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12392,Deployability,pipeline,pipeline,12392,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12401,Deployability,pipeline,pipeline-,12401,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12460,Deployability,pipeline,pipeline,12460,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12469,Deployability,pipeline,pipeline-,12469,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12533,Deployability,pipeline,pipeline,12533,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12542,Deployability,pipeline,pipeline-,12542,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12617,Deployability,pipeline,pipeline,12617,"hon/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12626,Deployability,pipeline,pipeline-,12626,"hon/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12696,Deployability,pipeline,pipeline,12696,"h_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12705,Deployability,pipeline,pipeline-,12705,"h_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12777,Deployability,pipeline,pipeline,12777,"h-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12786,Deployability,pipeline,pipeline-,12786,"h-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12852,Deployability,pipeline,pipeline,12852,"h-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12861,Deployability,pipeline,pipeline-,12861,"h-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13013,Deployability,pipeline,pipeline,13013,"batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13022,Deployability,pipeline,pipeline-,13022,"batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13113,Deployability,pipeline,pipeline,13113,"Version: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13122,Deployability,pipeline,pipeline-,13122,"Version: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13189,Deployability,pipeline,pipeline,13189,"Version: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
