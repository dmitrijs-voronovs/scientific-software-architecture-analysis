quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3636:246,error,error,246,https://hail.is,https://github.com/hail-is/hail/pull/3636,1,['error'],['error']
Availability,"Depends on #3992. Resolves https://github.com/hail-is/hail/issues/878. @tpoterba @catoverdrive Could you please double check I handled the joins, indices, and aggregations properly? Everything I tried besides just copying the indices and aggregations from `agg_expr` resulted in an index mismatch error. My current code generates the right output. I'm just skeptical whether it is robust to other faulty inputs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4004:297,error,error,297,https://hail.is,https://github.com/hail-is/hail/pull/4004,3,"['error', 'fault', 'robust']","['error', 'faulty', 'robust']"
Availability,"Depends on #4049 and #4121. . @danking I'm concerned I changed the meaning of your `file_row_index` field and the code you specialized for Caitlin is broken with this PR. That field is always sorted now from 0 to nVariants. Before, the indices were the order in the BGEN file. Now they're the variant index into the actual index file. I had to change/delete some of the Python tests because they didn't make sense anymore. Next PR will support push down by locus, alleles rather than row index and this shouldn't be a concern anymore (and we can delete file_row_index). If you think the change in behavior will cause problems, I'll close this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4102:449,down,down,449,https://hail.is,https://github.com/hail-is/hail/pull/4102,1,['down'],['down']
Availability,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:166,failure,failure,166,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904,3,"['down', 'failure']","['down', 'failure', 'failures']"
Availability,Did this finish today? It is the same error as the last issue: it actually succeeded and shouldn't happen again for new jobs. ```; -rw-r--r-- 2 aganna supergroup 0 2016-04-19 00:00 /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds/rdd.parquet/_SUCCESS; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309#issuecomment-211989236:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/issues/309#issuecomment-211989236,1,['error'],['error']
Availability,Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006:128,failure,failure,128,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006,2,"['down', 'failure']","['down', 'failure']"
Availability,Dir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:19); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:19); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:244); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:77); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at __C1310collect_distributed_array_table_native_writer.apply_region1_87(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C1310collect_distributed_array_table_native_writer.apply(Unknown,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:6052,Error,ErrorHandling,6052,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Error'],['ErrorHandling']
Availability,"Dir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3488,failure,failure,3488,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['failure'],['failure']
Availability,"Discussion in a different forum sounds good. This came up with the GPU branch as there was a question on whether we can trust what we are parsing as a resource or did we need to hardcode the SKUs explicitly. This was my proposed solution for us at least knowing that something has changed with lots of driver error messages and to not try and do any updates if the invariants of the ""SKU"" don't hold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579:309,error,error,309,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579,1,['error'],['error']
Availability,Do I need to add resource requests to the build.yaml file and CI build.py? I'm worried about things getting out of memory errors now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7583#issuecomment-557524946:122,error,errors,122,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557524946,1,['error'],['errors']
Availability,"Do I need to do something on my end in order to test copying from my computer rather than through CI? I keep getting a 400 regardless of whether I use my bucket `hail-jigold` or the test bucket `hail-test-dmk9z`. The only information I can get is ""Bad Request"" which I think is from this:. > The request cannot be completed based on your current Cloud Storage settings. For example, you cannot lock a retention policy if the requested bucket doesn't have a retention policy, and you cannot set ACLs if the requested bucket has Bucket Policy Only enabled. I don't think this is the real error as we know copying works fine with the Hail test bucket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10218#issuecomment-813514852:586,error,error,586,https://hail.is,https://github.com/hail-is/hail/pull/10218#issuecomment-813514852,1,['error'],['error']
Availability,"Do a tree reduce instead of a linear reduce. This means that the java; stack depth is log2(N) instead of N, and prevents stack overflow errors; when unioning hundreds of tables together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6943:136,error,errors,136,https://hail.is,https://github.com/hail-is/hail/pull/6943,1,['error'],['errors']
Availability,Do we need to manually delete the stopped but not deleted instances for batch to recover?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614:81,recover,recover,81,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614,1,['recover'],['recover']
Availability,"Do you have an error message other than that failed post? I'm not seeing why that would stop a deploy. In #13115, the PR healing code is after the `_heal_deploy` code, so I don't know why an exception when healing a PR would stop a deploy from occurring. This POST error is also occurring on GCP. Definitely something to fix but I'm not sure why it's related. > This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. Unfortunately I'm not sure if this is relevant in Azure. Azure CI thinks about merge candidate when it comes to testing PRs, but it doesn't merge any PRs and whether or not it does a deploy just depends if there's a new commit on `main`, it shouldn't have to do with PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065,2,['error'],['error']
Availability,"Do you mean when showing the log in the UI or the final upload to blob storage?. I think that's a great idea for the UI, where we show a truncated view in the page and the download button provides a way to stream the log file without loading it into memory on the front-end. In terms of the final upload, I'm a little wary about making a breaking change like that. It's probably true that if you're spewing tons of logs as a user you probably want to not do that. But if we move later to hosting logs in user-provided buckets instead of our own bucket there's no reason why they shouldn't be able to write large logs if they want to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197:172,down,download,172,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197,1,['down'],['download']
Availability,Do you want me to adopt this and make sure it doesn't break anything / see if the error messages disappear?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555,1,['error'],['error']
Availability,Docker also reports transient DNS resolution failure as 500.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8189:45,failure,failure,45,https://hail.is,https://github.com/hail-is/hail/pull/8189,1,['failure'],['failure']
Availability,"Docker jobs were cleaning up after themselves by including the entire directory for that job which removed the old container files. This PR makes it so `Container.remove()` removes the directory for the container. Before we merge this, I want to check the logs for this PR and make sure there aren't error messages that don't show up in tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12397:300,error,error,300,https://hail.is,https://github.com/hail-is/hail/pull/12397,1,['error'],['error']
Availability,Docker just always returns 500s for images that don't exist and has a slightly different error message for each thing that could go wrong (project / repository / image / tag don't exist) and that message varies across registries.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12594:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/pull/12594,1,['error'],['error']
Availability,Docs formatting error in multi-way-zip-join,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7945:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/issues/7945,1,['error'],['error']
Availability,Documentation for this field's use is availible at the end of the linked; section. We can maybe version the docs url in order to host the docs to; the released versions. https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8549:38,avail,availible,38,https://hail.is,https://github.com/hail-is/hail/pull/8549,1,['avail'],['availible']
Availability,"Does the version of spark matter? such as apache spark 2.0.2 and the cloudera spark?; We use the cloudera hadoop,but for hail, the cloudera'spark can't work,so in the configuration we replaced the cloudera spark with the apache spark2.0.2,and this works in local mode,but have errors in cluster mode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969:277,error,errors,277,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969,2,['error'],['errors']
Availability,Doing and administrator merge override because I'm afraid CI won't be alive long enough to successfully test this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198:70,alive,alive,70,https://hail.is,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198,1,['alive'],['alive']
Availability,"Don't commit this yet. I intentionally introduced an error to make sure it fails. Although it failed for a different reason, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/540#issuecomment-237322879:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/540#issuecomment-237322879,1,['error'],['error']
Availability,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:76,failure,failure,76,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['failure'],['failure']
Availability,"Don't know what's going on with `test_linreg` failing. It passes locally, and the error ""java.io.FileNotFoundException: /tmp/blockmgr-0a5af284-ccc3-4893-bfcc-bf840e7b1973/1e/broadcast_3344 (No space left on device)"" looks like a CI issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4189#issuecomment-415088122:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/4189#issuecomment-415088122,1,['error'],['error']
Availability,Don't know why the diff looks so weird -- I just dropped benchmark down one level into `benchmark/run`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521:67,down,down,67,https://hail.is,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521,1,['down'],['down']
Availability,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:1077,error,errors,1077,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883,1,['error'],['errors']
Availability,Double check that the test is what you want. I made the bounds pretty wide to make sure we never had an unlucky sporadic test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10748#issuecomment-893816262:126,failure,failure,126,https://hail.is,https://github.com/hail-is/hail/pull/10748#issuecomment-893816262,1,['failure'],['failure']
Availability,Down with Travis. Up with Jenkins!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/118#issuecomment-208690564:0,Down,Down,0,https://hail.is,https://github.com/hail-is/hail/issues/118#issuecomment-208690564,1,['Down'],['Down']
Availability,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:0,Down,Downgrading,0,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703,2,"['Down', 'down']","['Downgrading', 'downloading']"
Availability,Downsample Aggregator Needs Examples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8241:0,Down,Downsample,0,https://hail.is,https://github.com/hail-is/hail/issues/8241,1,['Down'],['Downsample']
Availability,DownsampleAggregator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4010:0,Down,DownsampleAggregator,0,https://hail.is,https://github.com/hail-is/hail/pull/4010,1,['Down'],['DownsampleAggregator']
Availability,DownsampleAggregator with label,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4090:0,Down,DownsampleAggregator,0,https://hail.is,https://github.com/hail-is/hail/pull/4090,1,['Down'],['DownsampleAggregator']
Availability,"Due to a typo I tried to read a vds that doesn't exist. I got the following error message:; hail: fatal: read: corrupt VDS: no metadata.ser file. Recreate VDS. It should probably say something more specific, ideally:; hail: fatal: read: VDS does not exist. Or at least:; hail: fatal: read: non-existent or corrupt VDS: no metadata.ser file. Please (re)create VDS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/327:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/327,1,['error'],['error']
Availability,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958:605,recover,recover,605,https://hail.is,https://github.com/hail-is/hail/pull/8958,1,['recover'],['recover']
Availability,Due to most of these failures being due to service tests. Feel free to revert when you see fit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10178:21,failure,failures,21,https://hail.is,https://github.com/hail-is/hail/pull/10178,1,['failure'],['failures']
Availability,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1069,error,error,1069,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,1,['error'],['error']
Availability,"EDIT: this was an issue with the input data. When using Hail 0.2.85, getting the following error (with code that worked with 0.2.78). ; Wondering what changes have been made between these versions that can help point to a solution?; Thanks!. code:; ```; phenotypes = phenotype_df.columns[1:]. y = list(map(lambda p: hl.float64(mt[p]), phenotypes)). covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])). x = mt.GT.n_alt_alleles(). hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs). hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); ```. error. `is.hail.utils.HailException: 1 samples and 12 covariates (including x) implies -11 degrees of freedom.; `. stacktrace. ```; Py4JJavaError Traceback (most recent call last); <[command-1581524108362637]()> in <module>; 4 covs = list(map(lambda cov: mt[cov], covariate_df.columns[1:])); 5 x = mt.GT.n_alt_alleles(); ----> 6 hl_res = hl.methods.linear_regression_rows(y,x,[1]+covs); 7 hl_res.checkpoint(quantitative_gwas_results_chk, overwrite=True); 8 n_filtered_variants = hl_res.count(). <decorator-gen-1734> in linear_regression_rows(y, x, covariates, block_size, pass_through, weights). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /databricks/python/lib/python3.8/site-packages/hail/methods/statgen.py in linear_regression_rows(y, x, covariates, block_size, pass_through, weights); 374 ht_result = ht_result.annotate(**{f: ht_result[f][0] for f in fields}); 375 ; --> 376 return ht_result.persist(); 377 ; 378 . <decorator-gen-1132> in persist(self, storage_level). /databricks/python/lib/python3.8/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413:91,error,error,91,https://hail.is,https://github.com/hail-is/hail/issues/11413,3,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"ERROR: org.broadinstitute.hail.annotations.AnnotationPathException,throw new AnnotationPathException()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/454:0,ERROR,ERROR,0,https://hail.is,https://github.com/hail-is/hail/issues/454,1,['ERROR'],['ERROR']
Availability,ETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2451,echo,echo,2451,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"E_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resour",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1097,echo,echo,1097,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['echo'],['echo']
Availability,"Each webpage should have a single `h1` matching the `title` text. Individual command documentation begins with `#`/`h1`. The single doc page has a `title`/`h1` Hail Documentation, so headings in individual command docs should get shifted down a level, `hi => h(i+1)`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/635:238,down,down,238,https://hail.is,https://github.com/hail-is/hail/issues/635,1,['down'],['down']
Availability,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:347,down,downsampled,347,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307,15,['down'],"['downed', 'downsample', 'downsampled']"
Availability,Either is fine with me -- just ping me when you are ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9934#issuecomment-776937414:31,ping,ping,31,https://hail.is,https://github.com/hail-is/hail/pull/9934#issuecomment-776937414,1,['ping'],['ping']
Availability,Either sphinx cached incorrectly or it's not writing the added text: https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/18456:id/www/hail/expr/hail.expr.TInt.html#hail.expr.TInt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806:99,down,download,99,https://hail.is,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806,1,['down'],['download']
Availability,"Environment:; - Spark 3.2.0; - Scala 2.12.15. Running: ; ```; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.2.0; ```; I get the error:; ```BUILD SUCCESSFUL in 2m 5s; 3 actionable tasks: 3 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; make: *** No rule to make target 'check-pip-lockfiles', needed by 'install-on-cluster'. Stop.; ```. Issue is fixed for me by renaming `install-on-cluster: $(WHEEL) check-pip-lockfiles` -> `install-on-cluster: $(WHEEL) check-pip-lockfile` on line 344 of hail/Makefile. Many thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12568:167,error,error,167,https://hail.is,https://github.com/hail-is/hail/issues/12568,1,['error'],['error']
Availability,"Envoy by default responds with a 403 if it fails to make an authorization check. We should treat the failure to contact `auth` as a transient error, so that clients can know to retry whatever request they were trying to make instead of failing. In particular, in dev namespaces the current behavior can trigger a QoB client to cancel an ongoing pipeline while polling for completion. [status_on_error](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/ext_authz/v3/ext_authz.proto#extensions-filters-http-ext-authz-v3-extauthz) allows us to configure that default behavior to return a 503 unavailable. I deployed this in Azure by running `make -C gateway deploy NAMESPACE=default`. I poked around to see that I could access my dev namespace and production pages through the browser but did not otherwise try to prove that this failure mode no longer exists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13735:101,failure,failure,101,https://hail.is,https://github.com/hail-is/hail/pull/13735,3,"['error', 'failure']","['error', 'failure']"
Availability,Er sorry needs to be a nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6062#issuecomment-490092137:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/6062#issuecomment-490092137,1,['error'],['error']
Availability,Error building jar for Hail 0.2.11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['Error'],['Error']
Availability,Error compiling HAIL on Mac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/1274,1,['Error'],['Error']
Availability,"Error estimates for approx quantiles, better PDF plots.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/6039,1,['Error'],['Error']
Availability,Error for importvcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Error'],['Error']
Availability,Error if something goes wrong generating build info,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1994:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/1994,1,['Error'],['Error']
Availability,"Error in ""transmute_entries"" method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11660:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/11660,1,['Error'],['Error']
Availability,Error in hl.split_multi_hts documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10676:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/10676,1,['Error'],['Error']
Availability,Error in import vcf/write vds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/913:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/913,1,['Error'],['Error']
Availability,"Error in parsing vcf files to hail (""cannot set missing field for required type +PFloat64"")",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Error'],['Error']
Availability,Error is rather obvious as is fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8403:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/8403,1,['Error'],['Error']
Availability,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2881:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/2881,1,['Error'],['Error']
Availability,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Error message was this:. ```; + kubectl -n pr-12053-default-chqnmsebv8fq scale deployment batch-driver --replicas=0; Error from server (Forbidden): deployments.apps ""batch-driver"" is forbidden: User ""system:serviceaccount:pr-12053-default-chqnmsebv8fq:admin"" cannot get resource ""deployments"" in API group ""apps"" in the namespace ""pr-12053-default-chqnmsebv8fq""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12059:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/12059,2,['Error'],['Error']
Availability,Error message: hail: variantqc: caught exception: org.apache.spark.SparkException,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/660,1,['Error'],['Error']
Availability,"Error messages from GCR and AR are different, and sometimes it looks like this from GCR:. ```; ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12708:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/12708,1,['Error'],['Error']
Availability,"Error occurs in 0.2.18 and 0.2.17, but not in 0.2.16",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634#issuecomment-511214882:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/6634#issuecomment-511214882,1,['Error'],['Error']
Availability,Error of using Nirvana in the HAIL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5657:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5657,1,['Error'],['Error']
Availability,Error on spark-submit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['Error'],['Error']
Availability,"Error page was too aggressive: it would delete any pod even if there was just a temporary network hiccup. Error page means a connection to the notebook failed. That means the state isn't Ready anymore, it is at most initializing (but maybe the pod is gone). In that case, probe k8s to figure out the current state, but don't probe the notebook, let /notebook call /wait to poll for the notebook to come back up (or report whatever state is there)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7158:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/7158,2,['Error'],['Error']
Availability,Error reading negative integer in CN field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Error'],['Error']
Availability,Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}},MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['Error'],['Error']
Availability,"Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. ```; In [4]: tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-3a6cee08b392> in <module>; ----> 1 tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['Error'],['Error']
Availability,Error using approx_median aggregation on column grouped matrix table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7824,1,['Error'],['Error']
Availability,Error when executing operations using entry fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['Error'],['Error']
Availability,Error when exporting variants,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/367:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/367,1,['Error'],['Error']
Availability,Error when importing /broad/hptmp/tpoterba/dbNSFP_3.2a_variant.filtered.all.txt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/391:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/391,1,['Error'],['Error']
Availability,Error when importing large WGS study (merck),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/301:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/301,1,['Error'],['Error']
Availability,Error when iterating over a set literal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4078:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/4078,1,['Error'],['Error']
Availability,Error when running `hl.agg.hist` with value `-0.0`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5846:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5846,1,['Error'],['Error']
Availability,Error when running filter_intervals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12920:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/12920,1,['Error'],['Error']
Availability,Error when running variant annotation pipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/320:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/320,1,['Error'],['Error']
Availability,Error when subsetting newly imported ExAC file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/309,1,['Error'],['Error']
Availability,Error when trying to subset the newly imported ExAC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/304:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/304,1,['Error'],['Error']
Availability,Error when union'ing tabes from order_by,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4080:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/4080,1,['Error'],['Error']
Availability,Error when using `agg.group_by` with `agg.filter`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5296:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/5296,1,['Error'],['Error']
Availability,Error when using hail in spark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['Error'],['Error']
Availability,Error when variantsqc and exporting to plink,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/325:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/325,1,['Error'],['Error']
Availability,"Error with export(), write(), to_pandas() when using spark-submit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['Error'],['Error']
Availability,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738,3,"['Error', 'error']","['Error', 'errors']"
Availability,Error：Execution failed for task ':compileScala',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/453,1,['Error'],['Error']
Availability,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:192,ERROR,ERROR,192,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417,2,['ERROR'],['ERROR']
Availability,"Every suite is allocated by the gradle test framework, and then only those matching the requested filters are executed. Ergo, non-lazy fields on a suite will be executed (and may trigger errors) even though the suite wasn't requested by the gradle user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2262:187,error,errors,187,https://hail.is,https://github.com/hail-is/hail/pull/2262,1,['error'],['errors']
Availability,"Ex.v7.max_expression_per_gene_per_tissue.031318.kt""). <decorator-gen-1046> in read_table(path). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. /home/hail/hail.zip/hail/methods/impex.py in read_table(path); 1865 :class:`.Table`; 1866 """"""; -> 1867 return Table(Env.hc()._jhc.readTable(path)); 1868 ; 1869 @typecheck(t=Table,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.types.TableType.<init>(TableType.scala:15); 	at is.hail.expr.Parser$$anonfun$table_type_expr$7.apply(Parser.scala:308); 	at is.hail.expr.Parser$$anonfun$table_type_expr$7.apply(Parser.scala:307); 	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:137); 	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136); 	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237); 	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237); 	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4325:1830,Error,Error,1830,https://hail.is,https://github.com/hail-is/hail/issues/4325,1,['Error'],['Error']
Availability,"Example VCF parse error now looks like:. ```; Error summary: HailException: sample.vcf:column 1862: invalid character 'x' in integer literal; ... :80,0:80:13:0,13,2219 0/1:65,19:94:9x9:233,0,1732 0/0:34,3:45:74:0,74,12 ...; ^; offending line: 20	10273694	.	CT	C	29059.60	VQSRTrancheINDEL97.00to99.00	HWP...; see the Hail log for the full offending line; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2421#issuecomment-343983297:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/2421#issuecomment-343983297,2,"['Error', 'error']","['Error', 'error']"
Availability,"Example batch job created (1 burn in, 1 iteration, 1 job per benchmark). Failures are legit test failures.; https://batch.hail.is/batches/8181721",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565#issuecomment-2142867787:73,Failure,Failures,73,https://hail.is,https://github.com/hail-is/hail/pull/14565#issuecomment-2142867787,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890,1,['error'],['error']
Availability,Example failure https://ci.azure.hail.is/batches/1675765/jobs/112,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12153:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/12153,1,['failure'],['failure']
Availability,Example failure: https://ci.hail.is/batches/202357/jobs/104,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10153#issuecomment-792776867:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/10153#issuecomment-792776867,1,['failure'],['failure']
Availability,"Example from Zulip:. ```; mt_one.count(); # (239279, 153); mt_two.count(); # (522049, 188); mt_merge = mt_one.union_cols(mt_two); mt_merge.count(); (242190, 341); ```. We should warn or error in this case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5993:186,error,error,186,https://hail.is,https://github.com/hail-is/hail/issues/5993,1,['error'],['error']
Availability,Example stack trace:; ```; 2022-02-08 18:09:30 root: ERROR: IllegalArgumentException: requirement failed; From java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:52); 	at is.hail.rvd.RVDPartitioner.extendKeySamePartitions(RVDPartitioner.scala:141); 	at is.hail.expr.ir.LoweredTableReader$$anon$2.coerce(TableIR.scala:382); 	at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); 	at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1790); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:581); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1304); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1035); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:394); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:547); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:69); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77). ```. called from here:; https://github.com/hail-is/hail/blob/d2f87d81dd1af43617740309e354d4bac8c672e0/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11335:53,ERROR,ERROR,53,https://hail.is,https://github.com/hail-is/hail/issues/11335,1,['ERROR'],['ERROR']
Availability,"Example that currently fails:. ```; >>> mt = hl.import_vcf('sample.vcf'); >>> mt.annotate_entries(PL = hl.cond(mt.DP > 20, mt.PL, hl.map(lambda x: x, mt.PL))); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/cotton/hail/python/hail/matrixtable.py"", line 975, in annotate_entries; return self._select_entries(caller, s=self.entry.annotate(**e)); File ""/home/cotton/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/home/cotton/hail/python/hail/matrixtable.py"", line 2887, in _select_entries; return cleanup(MatrixTable(base._jvds.selectEntries(str(s._ir)))); File ""/home/cotton/opt/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/cotton/hail/python/hail/utils/java.py"", line 212, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed: mismatch:; array<int32>; array<int32>; ApplyComparisonOp(GT(int32,int32),GetField(ArrayRef(GetField(Ref(va,struct{locus: locus<GRCh37>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{NEGATIVE_TRAIN_SITE: bool, HWP: float64, AC: array<int32>, culprit: str, MQ0: int32, ReadPosRankSum: float64, AN: int32, InbreedingCoeff: float64, AF: array<float64>, GQ_STDDEV: float64, FS: float64, DP: int32, GQ_MEAN: float64, POSITIVE_TRAIN_SITE: bool, VQSLOD: float64, ClippingRankSum: float64, BaseQRankSum: float64, MLEAF: array<float64>, MLEAC: array<int32>, MQ: float64, QD: float64, END: int32, DB: bool, HaplotypeScore: float64, MQRankSum: float64, CCC: int32, NCC: int32, DS: bool}, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{GT: call, AD: array<int32>, DP: int32, GQ: int32, PL: array<int32>}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]),Ref(i,int32)),DP),I32(20)). Java stack trace:; java.lang.AssertionError: assertion failed: mismatch:; array<int32>; array<int32>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4134:840,Error,Error,840,https://hail.is,https://github.com/hail-is/hail/issues/4134,1,['Error'],['Error']
Availability,Example transient error:. ```; E hail.utils.java.FatalError: batch id was 2301842; E IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E java.lang.IllegalStateException: Timeout on blocking read for 30000000000 NANOSECONDS; E at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:123); E at reactor.core.publisher.Mono.block(Mono.java:1727); E at com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:130); E at com.azure.storage.blob.specialized.BlobClientBase.downloadStreamWithResponse(BlobClientBase.java:731); E at is.hail.io.fs.AzureStorageFS$$anon$1.fill(AzureStorageFS.scala:152); E at is.hail.io.fs.FSSeekableInputStream.read(FS.scala:141); E at java.io.DataInputStream.read(DataInputStream.java:149); E at is.hail.utils.richUtils.RichInputStream$.readRepeatedly$extension0(RichInputStream.scala:21); E at is.hail.utils.richUtils.RichInputStream$.readFully$extension1(RichInputStream.scala:12); E at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:546); E at is.hail.io.LZ4SizeBasedCompressingInputBlockBuffer.readBlock(InputBuffers.scala:608); E at is.hail.io.BlockingInputBuffer.readBlock(InputBuffers.scala:382); E at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:388); E at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:405); E at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:217); E at is.hail.io.LEB128InputBuffer.readInt(InputBuffers.scala:223); E at __C173548Compiled.__m174060INPLACE_DECODE_r_binary_TO_r_binary(Emit.scala); E at __C173548Compiled.__m174059DECODE_r_struct_of_r_binaryEND_TO_SBaseStructPointer(Emit.scala); E at __C173548Compiled.__m174058begin_group_0(Emit.scala); E at __C173548Compiled.__m174057begin_group_0(Emit.scala); E at __C173548Compiled.__m173566split_Let(Emit.scala); E at __C173548Compiled.apply(Emit.scala); E at is.hail.expr.ir.lowering.LowerToCDA$.$anonfun$lower$2(LowerToCDA.scala:51,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12261:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/12261,2,"['down', 'error']","['downloadStreamWithResponse', 'error']"
Availability,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3545:300,error,errors,300,https://hail.is,https://github.com/hail-is/hail/pull/3545,3,"['error', 'failure']","['errors', 'failures']"
Availability,"Execute TableWriter and MatrixWriter via lowering pipeline instead of spark execution.; Removed support for checkpoint files for now - plan is to implement something more general purpose somewhat akin to call-caching. Plot of top 20 affected benchmarks, none of which use writing, interestingly...; ![image](https://user-images.githubusercontent.com/8223952/231855633-84ddbe64-1dba-4e62-bfa0-b9e2b041d588.png); You can view these results yourself in [benchmarks.zip](https://github.com/hail-is/hail/files/11225644/benchmarks.zip)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12879:108,checkpoint,checkpoint,108,https://hail.is,https://github.com/hail-is/hail/pull/12879,1,['checkpoint'],['checkpoint']
Availability,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:15638,Error,Error,15638,https://hail.is,https://github.com/hail-is/hail/issues/8944,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"ExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9645,failure,failure,9645,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['failure'],['failure']
Availability,"Existing tables and matrix tables that we don't own/didn't create but are accessible via the datasets API are not always available on both GCS and S3 (e.g. pan UKB datasets are only on AWS S3), or are not available in an `eu` bucket on GCS (e.g. gnomAD datasets). . This just adds a column `cloud: [regions]` to the tables on the datasets API and annotation DB docs pages to be more explicit about what datasets/versions are available on which cloud platform. . So, for example, if a version of a dataset is in `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1`, then under `cloud: [regions]` we would see `gcp: [eu,us], aws: [us]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10358:121,avail,available,121,https://hail.is,https://github.com/hail-is/hail/pull/10358,3,['avail'],['available']
Availability,Expands the interface to permit importing with no entry fields (previously was error),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3449:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/pull/3449,1,['error'],['error']
Availability,"Export_elasticsearch encounter ""Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]]""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/5643,1,['error'],['error']
Availability,Extending the idea of better error messages from #9398 to include `NDArrayRef`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9444:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/9444,1,['error'],['error']
Availability,"F; ```; produces:; ```; Hail version: 0.2.11-cf54f08305d1; Error summary: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; ```; I expect it to say something like ""yo dawg, you forgot to have entries, maybe you actually want import_table"". full output:; ```; Initializing Spark and Hail with default parameters...; using hail jar at /Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 19/03/27 18:27:22 WARN Utils: Your hostname, wmb16-359 resolves to a loopback address: 127.0.0.1; using 10.1.1.163 instead (on interface en0); 19/03/27 18:27:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Running on Apache Spark version 2.2.3; SparkUI available at http://10.1.1.163:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /Users/dking/hail-20190327-1827-0.2.11-cf54f08305d1.log; Traceback (most recent call last):; File ""<stdin>"", line 4, in <module>; File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2371, in count; return (self.count_rows(), self.count_cols()); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/matrixtable.py"", line 2331, in count_rows; TableCount(MatrixRowsTable(self._mir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/dking/anaconda2/envs/hail/lib/python3.6/site-packages/hail/utils/java.py"", line ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:1182,avail,available,1182,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['avail'],['available']
Availability,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-cc8d4 (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Warning FailedMount 2m59s (x248 over 9h) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; # k get pod batch-2554-job-4-main-cc8d4 -n batch-pods -o yaml ; apiV",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:2890,Toler,Tolerations,2890,https://hail.is,https://github.com/hail-is/hail/issues/6466,1,['Toler'],['Tolerations']
Availability,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:18859,Toler,Tolerations,18859,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['Toler'],['Tolerations']
Availability,"F>\""]},\""includeStart\"":true,\""includeEnd\"":true},{\""start\"":{\""locus\"":{\""contig\"":\""chr20\"",\""position\"":17994753},\""alleles\"":[\""A\"",\""<NON_REF>\""]},\""end\"":{\""locus\"":{\""cont...""] ; !s20 = ToStream(%29) [False]; !s21 = StreamMap(!s20) { (%elt10) =>; SelectFields(%elt10) [; (partitionCounts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15665,Error,ErrorHandling,15665,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['Error'],['ErrorHandling']
Availability,"FO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:1867,failure,failure,1867,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['failure'],['failure']
Availability,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197,2,['error'],['error']
Availability,"FWIW, cutting it down to 2446 aggregators work (sat in code generation for about a minute but then ran in 18 seconds)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516#issuecomment-428425318:17,down,down,17,https://hail.is,https://github.com/hail-is/hail/issues/4516#issuecomment-428425318,1,['down'],['down']
Availability,"FWIW, in my current run I've successfully run 14440 jobs and seen 36 deadlock errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568581842:78,error,errors,78,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568581842,1,['error'],['errors']
Availability,"FWIW, it is currently available at www.hail.is but we'll fix the top level domain",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14616#issuecomment-2230929637:22,avail,available,22,https://hail.is,https://github.com/hail-is/hail/issues/14616#issuecomment-2230929637,1,['avail'],['available']
Availability,"FWIW, the per-row copy was super painful. Once I filtered down to the only the things I needed, I saw ~10X speedups in some cases. Will be good to get rid of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3421#issuecomment-383730398:58,down,down,58,https://hail.is,https://github.com/hail-is/hail/pull/3421#issuecomment-383730398,1,['down'],['down']
Availability,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:14,echo,echo,14,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414,2,['echo'],['echo']
Availability,"FYI @cseed @danking: I found a bad bug in the Kubernetes deserializer that we use to deserialize the pod spec from the request from the client. Any variable with a compound name like `generate_name` or `cluster_name` was being set to None regardless of what the input was. This is because their code has a mapping from the Python style with underscores to camel case and it was using the camel case to look up the attributes instead of the underscore names. @akotlar was going to make a PR to correct it in their repo. For now, I'm including the correct code in our repo. Without it, I was getting errors trying to deserialize pod templates with metadata from the MySQL database back to Python. The other horrible behavior I found with the deserializer is if you pass `None` to the deserialize function, you get a dictionary with all attributes set to `None` instead of just `None`. Something to be aware of or we should fix it in this PR to return None instead. @akotlar: I tried to make everything just adhere to Python3, but can you make sure I did the conversions correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5738:598,error,errors,598,https://hail.is,https://github.com/hail-is/hail/pull/5738,1,['error'],['errors']
Availability,"FYI @daniel-goldstein . I created three new abstract classes that act as the interface between different cloud compute implementations: `BaseZoneMonitor`, `BaseActivityMonitor`, and `BaseDiskMonitor`. There's a new `BaseComputeManager` that wraps the different monitors and also provides an interface for creating, deleting, and getting instances. I added an `InstanceState` that represents a common instance state between clouds (Running, Creating, Terminating). . I created a new `gcp` module that mirrors the structure of the batch module. I put all of the GCP specific implementations in there. In a future PR, I'll add the WorkerConfig for GCP and all of the GCP cost utility functions. I tested everything by hand looking for errors in the Logs Viewer. I'd like to do more testing of the disk monitor if you are good with the structure of this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10860:732,error,errors,732,https://hail.is,https://github.com/hail-is/hail/pull/10860,1,['error'],['errors']
Availability,"FYI @danking . I wasn't sure if removing a previously deleted user should be an error. I think it should be, but I left it as idempotent for now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9941:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/pull/9941,1,['error'],['error']
Availability,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:172,avail,available,172,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706,1,['avail'],['available']
Availability,"FYI @konradjk . I want to do a bit more testing but this should be ready later today for you to play with. I will ping you when it is ready and send instructions and some potential gotchas. For the most part it is self-explanatory:. ```; from hailtop import pipeline. p = pipeline.Pipeline(; backend=pipeline.GoogleBackend(; service_account='...',; scratch_dir='gs://hail-cseed/pipeline/tmp',; worker_cores=1,; worker_disk_size_gb='20',; pool_size=3,; max_instances=1000),; default_image='ubuntu:18.04'). input = p.read_input('gs://hail-cseed/cs-hack/input.txt'). t1 = p.new_task('concat'); t1.command(f'cp {input} {t1.ofile} && echo ""end"" >> {t1.ofile}'). t2 = p.new_task('sum'); t2.command(f'sum {t1.ofile} > {t2.sum}'). p.write_output(t2.sum, 'gs://hail-cseed/cs-hack/sum.txt'). p.run(); ```. You have to run this in a VM with a custom image. pool_size is the (maximum) number of active workers. max_instances is a cap on running instances to avoid blowing out CPU quota if you're close to the limit and we're creating new instances while others are shutting down. @jigold I'm not 100% sure this should go in. It stores resources in gs://hail-common (worker startup scripts, etc.) and to do this right we'll need to test deployments, version files, etc. It might make sense just to keep this as a reference and steal what you can from it for batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6772:114,ping,ping,114,https://hail.is,https://github.com/hail-is/hail/pull/6772,3,"['down', 'echo', 'ping']","['down', 'echo', 'ping']"
Availability,"FYI @tpoterba. I have a few pending PRs now. I will run through this when they are all in. If anything else changes, we should probably do it again a couple days before the workshop and fix issues. Remaining workshop todo items:; - My error handling is too aggressive and frozen pod (2.B) won't work right now; - Still need to add memory/CPU settings to the workshop",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7156:235,error,error,235,https://hail.is,https://github.com/hail-is/hail/pull/7156,1,['error'],['error']
Availability,"FYI I've made 1000 genomes phase 3 MTs publicly available here: ; ```; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh37.mt; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh38.liftover.mt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4160#issuecomment-413959206:48,avail,available,48,https://hail.is,https://github.com/hail-is/hail/issues/4160#issuecomment-413959206,1,['avail'],['available']
Availability,"FYI It failed due to transient connection reset by peer, I added ECONNRESET to the list of transient OS errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7631#issuecomment-559284193:104,error,errors,104,https://hail.is,https://github.com/hail-is/hail/pull/7631#issuecomment-559284193,1,['error'],['errors']
Availability,FYI The last build failed due to a transient error (aiohttp.ServerDisconnectedError). I added it to the retry logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128,1,['error'],['error']
Availability,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:47,failure,failures,47,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769,2,"['avail', 'failure']","['available', 'failures']"
Availability,"FYI, git clone can clone a single branch: https://stackoverflow.com/questions/1778088/how-do-i-clone-a-single-branch-in-git. > k run. Might be worth benchmarking in batch2, since that is where it will run. (k run! Shame on you!). > But the download drops from 4.7 to ~1.5. I don't understand this. Drops compared to what? Where'd the 4.7 come from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660:240,down,download,240,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660,1,['down'],['download']
Availability,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:115,failure,failures,115,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460,1,['failure'],['failures']
Availability,"Failed due to intermittent dataproc failure (sign), this just needs a bump.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5985#issuecomment-489738406:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/5985#issuecomment-489738406,1,['failure'],['failure']
Availability,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:203,Error,Error,203,https://hail.is,https://github.com/hail-is/hail/issues/303,1,['Error'],['Error']
Availability,Failed with a 500 error from batch 2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-542216720:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-542216720,1,['error'],['error']
Availability,Failing due to some mypy silliness but only in the pip installed images. Need to somehow modify this to make sure we have all appropriate stubs available and/or ignore missing stubs?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772:144,avail,available,144,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772,1,['avail'],['available']
Availability,"Failing pipeline with data available here: gs://hail-jigold/ibd-exomes-part4471.mt. [Zulip conversation](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/Working.20on.20translating.20a.20QC.20step.20from.200.2E1.20to.200.2E2). ```; import hail as hl. mt = hl.read_matrix_table('/Users/jigold/ibd-exomes-part4471.mt'). vars_of_interest = hl.set([""frameshift_variant"", ""inframe_deletion"", ""inframe_insertion"", ""stop_lost"", ""stop_gained"", ""start_lost"",\; ""splice_acceptor_variant"", ""splice_donor_variant"", ""splice_region_variant"", ""missense_variant"", ""synonymous_variant""]). mt = mt.annotate_globals(x = vars_of_interest); filtered = mt.filter_rows(mt.x.contains(mt.vep.most_severe_consequence)). filtered.write('/tmp/guhan.mt', overwrite=True); ```. This causes a segmentation fault that can be replicated with the first variant only `head(1)`. Filtering out rows where `mt.vep.most_severe_consequence` is missing will make the pipeline succeed. Tried this IR in IRSuite to replicate error with no success:; ```; @Test def debugGuhan() {; val s = ToSet(MakeArray(FastIndexedSeq(Str(""frameshift_variant""), Str(""inframe_deletion""), Str(""inframe_insertion""),; Str(""stop_lost""), Str(""stop_gained""), Str(""start_lost""), Str(""splice_acceptor_variant""), Str(""splice_donor_variant""),; Str(""splice_region_variant""), Str(""missense_variant""), Str(""synonymous_variant"")), TArray(TString()))). assertEvalsTo(LowerBoundOnOrderedCollection(s, NA(TString()), onKey = false), 11); }; ```. Separately, @tpoterba thinks the IR needs to have a Let statement for `set` in the IR implementation. I tried this, but it didn't fix the segmentation fault.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522:27,avail,available,27,https://hail.is,https://github.com/hail-is/hail/issues/4522,4,"['avail', 'error', 'fault']","['available', 'error', 'fault']"
Availability,"Fails with:; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 491 in stage 1.0 failed 20 times, most recent failure: Lost task 491.19 in stage 1.0 (TID 17951, exomes-sw-gf9k.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:95,failure,failure,95,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,2,['failure'],['failure']
Availability,Failure during installing Hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/14235,1,['Failure'],['Failure']
Availability,"Failure in image_fetcher_image:. ```; Traceback (most recent call last):; File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 279, in run; await docker_call_retry(self.container.start); File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 86, in docker_call_retry; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start; data=kwargs; File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'read unix @->@/containerd-shim/moby/7f911041e4fcc5ea78b6b3979d1232d0954193bced0e1e85acd2133b80cc463e/shim.sock: read: connection reset by peer: unknown'); ```. I will add this to the retry list. Another reason to drop docker ASAP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320,1,['Failure'],['Failure']
Availability,Failure in vep annotation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/303,1,['Failure'],['Failure']
Availability,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064,4,"['ERROR', 'Failure', 'error']","['ERROR', 'Failure', 'error']"
Availability,Failure to annotate variants with dbNSFP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/issues/317,1,['Failure'],['Failure']
Availability,Failures:; - a Java test I created that should fail to make sure the memory checks are happening; - Python test_ld_score; - A docs test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858:0,Failure,Failures,0,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858,1,['Failure'],['Failures']
Availability,Fatal error when using `hl.sum` over Array of Double,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3436:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/issues/3436,1,['error'],['error']
Availability,"FatalError: IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 21, ap01-sw-wdp1.c.topmed-kathiresan-lipids-wgs.internal, executor 7): java.lang.IllegalArgumentException: Zero-length interval cannot be lifted over. Interval: null; at htsjdk.samtools.liftover.LiftOver.liftOver(LiftOver.java:137); at is.hail.io.reference.LiftOver.queryInterval(LiftOver.scala:61); at is.hail.variant.ReferenceGenome.liftoverLocusInterval(ReferenceGenome.scala:435); at is.hail.codegen.generated.C11.method1(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.codegen.generated.C11.apply(Unknown Source); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:627); at is.hail.expr.ir.TableMapRows$$anonfun$21$$anonfun$apply$11.apply(TableIR.scala:626); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1264); at is.hail.rvd.RVD$$anonfun$apply$25$$anon$3.next(RVD.scala:1258); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$JoinIterator.next(Iterator.scala:232); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1138); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1137); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174:175,failure,failure,175,https://hail.is,https://github.com/hail-is/hail/issues/5174,2,['failure'],['failure']
Availability,"File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:49038,error,error,49038,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['error'],['error']
Availability,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:576,avail,available,576,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,2,"['avail', 'error']","['available', 'error']"
Availability,Finally tracked down on the failures. This should be ready to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052:16,down,down,16,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052,2,"['down', 'failure']","['down', 'failures']"
Availability,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:412,failure,failure,412,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362,3,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,"First error was we were not cancelling the batch on TimeoutErrors. I kept the interface to timeout on the client side rather than passing the timeout to the service spec. We can revisit this design at another point. I also added another layer of tasks to make sure we were cancelling batches in the case that a submit failed. Some coroutines may have already succeeded and created a BatchPoolFuture, which we need to cancel. We need to use tasks instead of coroutines as the input to `gather` because on at least one failure we need to know if the submit task was completed successfully to extract the BatchPoolFuture to cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10696:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/10696,2,"['error', 'failure']","['error', 'failure']"
Availability,First step towards relative error approx cdf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6025:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/6025,1,['error'],['error']
Availability,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445:108,error,errors,108,https://hail.is,https://github.com/hail-is/hail/pull/7445,2,['error'],['errors']
Availability,"First, this actually succeeded:. ```; $ hdfs dfs -ls /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; -rw-r--r-- 2 aganna supergroup 0 2016-04-18 20:14 /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; ```. It crashed while the program was shutting down. I know what caused it and it has been fixed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303#issuecomment-211716001:250,down,down,250,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211716001,1,['down'],['down']
Availability,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8819:456,down,downstream,456,https://hail.is,https://github.com/hail-is/hail/pull/8819,2,['down'],['downstream']
Availability,Fix #3475: Expression error gets output in stacktrace,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3497:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/3497,1,['error'],['error']
Availability,"Fix Die, add better require_biallelic error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4865:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/4865,1,['error'],['error']
Availability,Fix Solr cast error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/560:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/560,1,['error'],['error']
Availability,Fix Type Error When PAR is Empty,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3897:9,Error,Error,9,https://hail.is,https://github.com/hail-is/hail/pull/3897,1,['Error'],['Error']
Availability,Fix VCF error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3076:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/3076,1,['error'],['error']
Availability,Fix aggregate rows key error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8322:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8322,1,['error'],['error']
Availability,Fix an error in the MatrixTable tutorial,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14239:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/14239,1,['error'],['error']
Availability,Fix asm4s test failures on Cray.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1372:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/1372,1,['failure'],['failures']
Availability,Fix bad error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2789:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/2789,2,['error'],['error']
Availability,"Fix bad error message in Table.order_by, extend functionality",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4927:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/4927,1,['error'],['error']
Availability,Fix bad error message in unify,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3801:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/3801,1,['error'],['error']
Availability,"Fix bad error message, make 'analyze' errors better",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4045:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/4045,2,['error'],"['error', 'errors']"
Availability,Fix bgen index error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2499:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/2499,1,['error'],['error']
Availability,Fix downcode docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4802:4,down,downcode,4,https://hail.is,https://github.com/hail-is/hail/pull/4802,1,['down'],['downcode']
Availability,Fix empty struct write error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1272:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/1272,1,['error'],['error']
Availability,Fix error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1040:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/1040,2,['error'],['error']
Availability,Fix error message for import_gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2537:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/2537,1,['error'],['error']
Availability,Fix error message when Code[T] emission happens on worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3525:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/3525,1,['error'],['error']
Availability,Fix error message when indexing into (Matrix)Tables without keys,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6736:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/6736,1,['error'],['error']
Availability,Fix error message when no files found,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2850:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/2850,1,['error'],['error']
Availability,Fix error message with null values for export vcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4156:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/4156,1,['error'],['error']
Availability,Fix error messages for MatrixTable / Table join syntax with __getitem__,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2694:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/2694,1,['error'],['error']
Availability,Fix error messages in union_cols,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5241:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5241,1,['error'],['error']
Availability,Fix error messages related to bad indexing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5171:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5171,1,['error'],['error']
Availability,"Fix error messages to be consistent about using quotes, not backticks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5761:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/5761,1,['error'],['error']
Availability,"Fix error, a bit of cleanup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2889:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/2889,1,['error'],['error']
Availability,Fix errors in FormatParser when array elements are missing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5170:4,error,errors,4,https://hail.is,https://github.com/hail-is/hail/pull/5170,1,['error'],['errors']
Availability,Fix failure after is.hail rename.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1268:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/1268,1,['failure'],['failure']
Availability,Fix manhattan plot downsampling,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4704:19,down,downsampling,19,https://hail.is,https://github.com/hail-is/hail/pull/4704,1,['down'],['downsampling']
Availability,Fix match error on import VCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2232:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/2232,1,['error'],['error']
Availability,Fix merge failure loop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6045:10,failure,failure,10,https://hail.is,https://github.com/hail-is/hail/pull/6045,1,['failure'],['failure']
Availability,Fix method not found error due to relocation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1086:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/1086,1,['error'],['error']
Availability,Fix py4j error handling,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1210:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/1210,1,['error'],['error']
Availability,Fix python error messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1206:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/1206,1,['error'],['error']
Availability,Fix require biallelic error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2742:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/2742,1,['error'],['error']
Availability,Fix serialization error of $MapLike$$ produced by TDict.mapValues,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/613:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/613,1,['error'],['error']
Availability,Fix some aggregator parse errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2953:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/2953,1,['error'],['errors']
Availability,Fix splitmulti error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3433:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/3433,1,['error'],['error']
Availability,Fix struct errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2900:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/2900,1,['error'],['errors']
Availability,Fix tc error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1838:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/1838,1,['error'],['error']
Availability,Fix the `(`. Mention in docs the three available distributions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1318#issuecomment-276210949:39,avail,available,39,https://hail.is,https://github.com/hail-is/hail/pull/1318#issuecomment-276210949,1,['avail'],['available']
Availability,Fix typo in Table.checkpoint docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6431:18,checkpoint,checkpoint,18,https://hail.is,https://github.com/hail-is/hail/pull/6431,1,['checkpoint'],['checkpoint']
Availability,Fix up echoes after Table/MatrixTable/BlockMatrix writes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4801:7,echo,echoes,7,https://hail.is,https://github.com/hail-is/hail/pull/4801,1,['echo'],['echoes']
Availability,Fix warning in Table/MatrixTable checkpoint docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8938:33,checkpoint,checkpoint,33,https://hail.is,https://github.com/hail-is/hail/pull/8938,1,['checkpoint'],['checkpoint']
Availability,"Fixed a rebase failure, deleted some unnecessary whitespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6333#issuecomment-501729022:15,failure,failure,15,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501729022,1,['failure'],['failure']
Availability,Fixed bad error message in Variant ctor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1545:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/1545,1,['error'],['error']
Availability,"Fixed error message in 4d38cca, new issue supercedes in #376",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-218033798:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-218033798,1,['error'],['error']
Availability,Fixed error messages (!),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/324:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/324,1,['error'],['error']
Availability,Fixed error messages caused by invalid VCF input,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/369:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/369,1,['error'],['error']
Availability,Fixed error reporting problem in Jenkinsfile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/540:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/540,1,['error'],['error']
Availability,Fixed failure on bgen import/hardcalls/write [0.1],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2114:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/pull/2114,1,['failure'],['failure']
Availability,Fixed serialization error in binary op,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/940:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/940,1,['error'],['error']
Availability,"Fixes #11335. @tpoterba made the replicating test, with that it was easy to find the source of the bug. The error was in; ```; pkPartitioned; .strictify(); ...; .changePartitionerNoRepartition(partitioner.extendKeySamePartitions(keyType)); ```; where `pkPartitioned` is keyed by the partition key. In the test case, all rows have the same partition key, so the partitioner looks like `[x, x], [x, x], ...`. In that case, `strictify` correctly collapses all those partitions into one, but `partitioner.extendKeySamePartitions(keyType)` tries to extend the key type without changing the partitioning, which in this case creates an invalid partitioner. The fix is to use `pkPartitioned.extendKeyPreservesPartitioning(key)`, which does the `strictify` and creates the correct partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11355:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/11355,1,['error'],['error']
Availability,"Fixes #11899 . To make [GENCODE v35](https://www.gencodegenes.org/human/release_35.html) available via the datasets API, as requested in issue https://github.com/hail-is/hail/issues/11899. . The Hail Table was created from the [comprehensive gene annotation (on the reference chromosomes only) GTF file](https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_35/gencode.v35.annotation.gtf.gz).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11946:89,avail,available,89,https://hail.is,https://github.com/hail-is/hail/pull/11946,1,['avail'],['available']
Availability,"Fixes #12976. I do not fully understand the Azure git tagging scheme, but [this commit](https://github.com/Azure/azure-sdk-for-java/commit/054df3fb74098f4ee30eeb1df70df1e40438d169) appears to have made `close` idempotent. It was merged in June of 2022. That commit resolved [an issue](https://github.com/Azure/azure-sdk-for-java/issues/24782) reporting an error very similar to our own. All the azure version changes update the Azure packages to their latest versions as of 2023-05-09 1713 ET. Unfortunately, newer Azure versions and Spark 3.3.0 have inconsistent `io.netty` requirements. Spark is stuck back in February 2022. Spark 3.4.0 does support a compatible version of `io.netty`, but we're months from seeing that in Dataproc. This change packages up Azure and all its dependencies into one JAR of relocated classes. We must refer to those classes by their relocated names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032:356,error,error,356,https://hail.is,https://github.com/hail-is/hail/pull/13032,1,['error'],['error']
Availability,"Fixes #13346. Another user was confused by this: https://github.com/hail-is/hail/issues/14102. Unfortunately, the world appears to have embraced missing values in VCF array fields even though the single element case is ambiguous. In #13346, I proposed a scheme by which we can disambiguate many of the cases, but implementing it ran into challenges because LoadVCF.scala does not expose whether or not an INFO field was a literal ""."" or elided entirely from that line. Anyway, this error message actually points users to the fix. I also changed some method names such that every method is ArrayType and never TypeArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14105:482,error,error,482,https://hail.is,https://github.com/hail-is/hail/pull/14105,1,['error'],['error']
Availability,"Fixes #13441 . Usage: `python3 devbin/generate_gcp_ar_cleanup_policy.py > my_policy_file.txt`; Reference: https://cloud.google.com/artifact-registry/docs/repositories/cleanup-policy; Note, we have a maximum of 10 policies available.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13489:222,avail,available,222,https://hail.is,https://github.com/hail-is/hail/pull/13489,1,['avail'],['available']
Availability,"Fixes #1368 . Yes, TArray is IndexSeq, I've verified it fixes the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1369#issuecomment-278473040:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/1369#issuecomment-278473040,1,['error'],['error']
Availability,Fixes #13696.; `n_divisions` is a downsampling factor. It conflicts with `collect_all` when `collect_all=True` is specified.; Deprecate `collect_all` and issue warning when users supply a non-`None` `collect_all` argument.; Use `n_divisions=None` to not downsample (implies `collect_all=True`).; Throw when `collect_all=True` and `n_divisions` is not `None` - this is a conflict.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13738:34,down,downsampling,34,https://hail.is,https://github.com/hail-is/hail/pull/13738,2,['down'],"['downsample', 'downsampling']"
Availability,"Fixes #14262. Ever since starting to control job network namespaces ourselves, we run the worker container with `--network host`. But running with the host's network namespace means there's no need (nor meaning) to use port forwarding rules with `-p`. Docker safely ignores this redundant setting but emit some log messages like:. ```; WARNING: Published ports are discarded when using host network mode; ```. The solution here is to just remove the old port publishing settings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14252:279,redundant,redundant,279,https://hail.is,https://github.com/hail-is/hail/pull/14252,1,['redundant'],['redundant']
Availability,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2173:13,Error,Error,13,https://hail.is,https://github.com/hail-is/hail/pull/2173,3,['Error'],"['Error', 'ErrorHandling']"
Availability,"Fixes #2921. Example:. ```; In [3]: mt = mt.annotate_entries(PL=[0]). In [4]: hl.split_multi_hts(mt)._force_count_rows(). ... hundreds of lines of java trace ... Hail version: 0.2.30-bae67f384161; Error summary: HailException: array index out of bounds: index=1, length=1; ----------; Python traceback:; File ""<ipython-input-4-ba11ec1cd68e>"", line 1, in <module>; hl.split_multi_hts(mt)._force_count_rows(). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2246, in split_multi_hts; (hl.range(0, 3).map(lambda i:. File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))). File ""/Users/tpoterba/hail/hail/python/hail/methods/statgen.py"", line 2250, in <lambda>; ).map(lambda j: split.PL[j])))))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7792:197,Error,Error,197,https://hail.is,https://github.com/hail-is/hail/pull/7792,1,['Error'],['Error']
Availability,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3921:630,error,errors,630,https://hail.is,https://github.com/hail-is/hail/pull/3921,1,['error'],['errors']
Availability,Fixes #5293. @tpoterba I decided not to throw an error if the key is null and instead just let the `flatMap` in `restrictTo` take care of it. Let me know if you think we should error out.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5294:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/5294,2,['error'],['error']
Availability,Fixes #6888. Also simplify code to remove redundant checks.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6897:42,redundant,redundant,42,https://hail.is,https://github.com/hail-is/hail/pull/6897,1,['redundant'],['redundant']
Availability,"Fixes #7584. We already require this in Scala, but since we don't require it in python the error message is bad.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7656:91,error,error,91,https://hail.is,https://github.com/hail-is/hail/pull/7656,1,['error'],['error']
Availability,"Fixes #8083 . The format of the status is ; ```; # {; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int; # }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8088:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/8088,2,['error'],['error']
Availability,"Fixes #8343. The Google Storage Hadoop connector introduced; a backwards incompatible change in 2.1.0 which relies on; a new method in Hadoop 2.8.3 that is not present in Hadoop; 2.7.3. There are no Spark releases that include Hadoop 2.8.3; yet, so we choose to downgrade to the last compatible; connector library version. Randomly picked a hail query person.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8344:262,down,downgrade,262,https://hail.is,https://github.com/hail-is/hail/pull/8344,1,['down'],['downgrade']
Availability,"Fixes #8944. CHANGELOG: Fixed crash (error 134 or SIGSEGV) in `MatrixTable.annotate_cols`, `hl.sample_qc`, and more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9169:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/9169,1,['error'],['error']
Availability,Fixes `bash: line 10: yum: command not found` failures in `delete_azure_batch_instances`. ### Security Assessment; This change has no security impact,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14720:46,failure,failures,46,https://hail.is,https://github.com/hail-is/hail/pull/14720,1,['failure'],['failures']
Availability,"Fixes https://github.com/hail-is/hail/issues/14130. We pervasively assume:; 1. That our entire system is used within a single Python thread.; 2. That once an event loop is created that's the only event loop that will exist forever. Pytest (and newer version of IPython, afaict) violate this pretty liberally. ~~pytest_asyncio has [explicit instructions on how to run every test in the same event loop](https://pytest-asyncio.readthedocs.io/en/latest/how-to-guides/run_session_tests_in_same_loop.html). I've implemented those here.~~ [These instructions don't work](https://github.com/pytest-dev/pytest-asyncio/issues/744). It seems that the reliable way to ensure we're using one event loop everywhere is to use pytest-asyncio < 0.23 and to define an event_loop fixture with scope `'session'`. I also switched test_batch.py into pytest-only style. This allows me to use session-scoped fixtures so that they exist exactly once for the entire test suite execution. Also:; - `RouterAsyncFS` methods must either be a static method or an async method. We must not create an FS in a sync method. Both `parse_url` and `copy_part_size` now both do not allocate an FS.; - `httpx.py` now eagerly errors if the running event loop in `request` differs from that at allocation time. Annoying but much better error message than this nonsense about timeout context managers.; - `hail_event_loop` either gets the current thread's event loop (running or not, doesn't matter to us) or creates a fresh event loop and sets it as the current thread's event loop. The previous code didn't guarantee we'd get an event loop b/c `get_event_loop` fails if `set_event_loop` was previously called.; - `conftest.py` is inherited downward, so I lifted fixtures out of test_copy.py and friends and into a common `hailtop/conftest.py`; - I added `make -C hail pytest-inter-cloud` for testing the inter cloud directory. You still need appropriate permissions and authn.; - I removed extraneous pytest.mark.asyncio since we use auto mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097:641,reliab,reliable,641,https://hail.is,https://github.com/hail-is/hail/pull/14097,1,['reliab'],['reliable']
Availability,Fixes the test where 'ERROR: could not find file' is in the message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11692:22,ERROR,ERROR,22,https://hail.is,https://github.com/hail-is/hail/pull/11692,1,['ERROR'],['ERROR']
Availability,"Fixes this assertion error in ci2 logs:; ```; ERROR | 2019-05-21 20:22:56,019 | ci.py | update_loop:239 | hail-is/hail:master update failed due to exception: Traceback (most recent call last):; File ""/ci/ci.py"", line 235, in update_loop; await wb.update(app); File ""/ci/github.py"", line 465, in update; await self._update(app); File ""/ci/github.py"", line 481, in _update; await self._update_github(gh); File ""/ci/github.py"", line 543, in _update_github; await pr._update_github_review_state(gh); File ""/ci/github.py"", line 261, in _update_github_review_state; assert state in ('DISMISSED', 'COMMENTED'), state; AssertionError: PENDING; PENDING; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6152:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/6152,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Fixes this error in a deploy batch:. ```; io/test/test_batch.py::Test::test_authorized_users_only ; -------------------------------- live log setup --------------------------------; 2020-03-03T21:02:22 INFO test.conftest conftest.py:8:log_before_after starting test; FAILED; _______________________ Test.test_authorized_users_only ________________________. self = <test.test_batch.Test testMethod=test_authorized_users_only>. def test_authorized_users_only(self):; endpoints = [; (requests.get, '/api/v1alpha/batches/0/jobs/0', 401),; (requests.get, '/api/v1alpha/batches/0/jobs/0/log', 401),; (requests.get, '/api/v1alpha/batches', 401),; (requests.post, '/api/v1alpha/batches/create', 401),; (requests.post, '/api/v1alpha/batches/0/jobs/create', 401),; (requests.get, '/api/v1alpha/batches/0', 401),; (requests.delete, '/api/v1alpha/batches/0', 401),; (requests.patch, '/api/v1alpha/batches/0/close', 401),; # redirect to auth/login; (requests.get, '/batches', 302),; (requests.get, '/batches/0', 302),; (requests.post, '/batches/0/cancel', 401),; (requests.get, '/batches/0/jobs/0', 302)]; for f, url, expected in endpoints:; full_url = deploy_config.url('batch', url); r = f(full_url, allow_redirects=False); > assert r.status_code == expected, (full_url, r, expected); E AssertionError: ('http://batch.hail/api/v1alpha/batches/0/jobs/0/log', <Response [503]>, 401); E assert 503 == 401; E -503; E +401. io/test/test_batch.py:415: AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8230:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/8230,1,['error'],['error']
Availability,"Fixes this error message in logs:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py"", line 418, in start; resp = await task; File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py"", line 458, in _handle; resp = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py"", line 119, in impl; return await handler(request); File ""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py"", line 152, in factory; response = await handler(request); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 302, in batch_callback; await asyncio.shield(batch_callback_handler(request)); File ""/usr/local/lib/python3.6/dist-packages/ci/ci.py"", line 276, in batch_callback_handler; await wb.notify_batch_changed(); TypeError: notify_batch_changed() missing 1 required positional argument: 'app'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7399:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7399,1,['error'],['error']
Availability,"Fixes this error msg:. ```; jinja2.exceptions.TemplateSyntaxError: Encountered unknown tag 'endfor'. You probably made a nesting mistake. Jinja is expecting this tag, but currently looking for 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6414#issuecomment-504040126:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/6414#issuecomment-504040126,1,['error'],['error']
Availability,Fixes this error: ```except Py4JJavaError as e:; NameError: global name 'Py4JJavaError' is not defined```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1166:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/1166,1,['error'],['error']
Availability,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7660-default-ne0pow1za1v4' '--format=value(name)'; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7661:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7661,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Fixes this error:. ```; + gcloud -q compute instances list --filter 'tags.items=batch2-agent AND labels.namespace=pr-7663-default-ccnvti4s750b' '--format=value(name)' --project hail-vdc; + xargs -r gcloud -q compute instances delete --zone us-central1-a --project hail-vdc; Deleted [https://www.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-o2tmf].; ERROR: (gcloud.compute.instances.delete) Could not fetch resource:; - The resource 'projects/hail-vdc/zones/us-central1-a/instances/batch2-worker-pr-7663-default-ccnvti4s750b-8z4dd' was not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7666:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/7666,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Fixes this error:. ```; Traceback (most recent call last):; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 83, in __del__; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 79, in close; File ""/Users/tpoterba/anaconda3/envs/hail/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 453, in _close; AttributeError: 'ServiceBackend' object has no attribute '_batch_client'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11337:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/11337,1,['error'],['error']
Availability,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7520:80,ERROR,ERROR,80,https://hail.is,https://github.com/hail-is/hail/pull/7520,1,['ERROR'],['ERROR']
Availability,"Fixes this:. ```; WARNING	| 2019-04-30 23:31:34,751 	| server.py 	| handler:818 | callback for batch 33, job 993 failed due to an error, I will not retry. Error: Invalid URL 'ci2/batch_callback': No schema supplied. Perhaps you meant http://ci2/batch_callback?; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6008:130,error,error,130,https://hail.is,https://github.com/hail-is/hail/pull/6008,2,"['Error', 'error']","['Error', 'error']"
Availability,Fixes: #14559; `hl.nd.array`s constructed from stream pipelines can cause out of memory exceptions owing to a limitation in the python CSE algorithm that does not eliminate partially redundant expressions in if-expressions. Explicitly `let`-binding the input collection prevents it from being evaluated twice: once for the flattened data stream and once for the original shape.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14571:183,redundant,redundant,183,https://hail.is,https://github.com/hail-is/hail/pull/14571,1,['redundant'],['redundant']
Availability,Follow-up PRs will use this for better error messages in places like require_biallelic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4845:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/4845,1,['error'],['error']
Availability,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/630:439,down,downloads,439,https://hail.is,https://github.com/hail-is/hail/pull/630,1,['down'],['downloads']
Availability,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396:676,avail,available,676,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396,1,['avail'],['available']
Availability,"For added context see: https://github.com/hail-is/hail/issues/13351. cc: @patrick-schultz @chrisvittal @daniel-goldstein @ehigham @iris-garden . I actually don't think we need to notify anyone because we'll still get errors if something goes wrong in default. If main starts failing, we have the slightly annoying situation of needing to run tests in a one-off manner to verify and we might need to block a release until we can revert.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13353:217,error,errors,217,https://hail.is,https://github.com/hail-is/hail/pull/13353,1,['error'],['errors']
Availability,"For attributes, same as /jobs, and state queries: open, closed, running, success, failure, cancelled, and complete. Also added an `open` batch state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7645:82,failure,failure,82,https://hail.is,https://github.com/hail-is/hail/pull/7645,1,['failure'],['failure']
Availability,"For example, see error messages when batch-driver shuts down: https://cloudlogging.app.goo.gl/kKgJdv34s3a6Vd8n6.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13418:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/13418,2,"['down', 'error']","['down', 'error']"
Availability,"For my SAIGE implementation, it would be nice to be able to use the `{BATCH_TMPDIR}` environment variable within a file name so that I can give user-specified file path names for output files to write that can be used downstream in globs when importing temporary files in Hail without having to localize all of the files (could be 4K+ files). I also thought this could solve Konrad's region bucket request where we copy data from a region-specific location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14340:218,down,downstream,218,https://hail.is,https://github.com/hail-is/hail/pull/14340,1,['down'],['downstream']
Availability,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5200:99,error,error,99,https://hail.is,https://github.com/hail-is/hail/pull/5200,1,['error'],['error']
Availability,"For now, downgrade:; ```; pip3 install 'ipython<8.17'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300:9,down,downgrade,9,https://hail.is,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300,1,['down'],['downgrade']
Availability,"For rrm and grm, I've pushed overall rescaling to the faster block matrix side and thereby removed the action that counts the number of variants after filtering. The behavior is identical except that we no longer warn when constant variants are dropped, but I've clarified the behavior in the docs. In the unfathomable case that all variants are constant, the user will receive the error message `HailException: block matrix must have at least one row` when the BlockMatrix is being created. For rrm, I've also simplified the filtering, as well as the expression algebra for std_dev from; ```; hl.sqrt((mt.__ACsq + (n_samples - mt.__n_called) * mt.__mean_gt ** 2) / n_samples - mt.__mean_gt ** 2); ```; to; ```; hl.sqrt(mt.__ACsq - (mt.__AC ** 2) / mt.__n_called); ```; with the added benefit of combining two annotates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3944:382,error,error,382,https://hail.is,https://github.com/hail-is/hail/pull/3944,1,['error'],['error']
Availability,For some reason either artifact registry or aiodocker returns a 500 instead of a 403 when a service account does not have access to pull an image. Had to add another special case for handling this error. https://console.cloud.google.com/logs/query;query=%22ys6od%22;pinnedLogId=2022-10-03T13:09:09.430766581Z%2Fyw46w2divo5eqk0vv;cursorTimestamp=2022-10-03T13:09:09.430766581Z?project=hail-vdc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12257:197,error,error,197,https://hail.is,https://github.com/hail-is/hail/pull/12257,1,['error'],['error']
Availability,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:96,heartbeat,heartbeat,96,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047,1,['heartbeat'],['heartbeat']
Availability,"For your consideration… We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str(…)`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str(…)` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath` — there it downloads the remote file and returns a local path — which is not at all what Hail needs. However probably this is a theoretical concern and `str(…)` will be fine…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965:308,error,error,308,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965,2,"['down', 'error']","['downloads', 'error']"
Availability,Force pushed to resolve errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1775#issuecomment-302124342:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/1775#issuecomment-302124342,1,['error'],['errors']
Availability,"Found this bug in my dev environment with randomly changing the version names. The error is a catastrophic failure trying to insert duplicate rows, so there shouldn't be any issues with the current database data.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12721:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/pull/12721,2,"['error', 'failure']","['error', 'failure']"
Availability,"Friendly ping, as merging this would help keep our fork in sync.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1143064464:9,ping,ping,9,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1143064464,1,['ping'],['ping']
Availability,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:363,failure,failure,363,https://hail.is,https://github.com/hail-is/hail/issues/3447,2,['failure'],['failure']
Availability,From @berylc - possibly due to empty partitions? This was after 2 import_table's and a join:; ```; Java stack trace:; java.lang.UnsupportedOperationException: empty.min; at scala.collection.TraversableOnce$class.min(TraversableOnce.scala:222); at scala.collection.mutable.ArrayOps$ofRef.min(ArrayOps.scala:186); at is.hail.rvd.OrderedRVD$.calculateKeyRanges(OrderedRVD.scala:615); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:184); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:19); at is.hail.table.Table.repartition(Table.scala:1003); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-3959178; Error summary: UnsupportedOperationException: empty.min; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3515:1279,Error,Error,1279,https://hail.is,https://github.com/hail-is/hail/issues/3515,1,['Error'],['Error']
Availability,"From Akhil:. ```; Traceback (most recent call last):; File ""/tmp/e9b3538af9b14d6ba440edbdf89e1ef1/phewas_rvas_jhs_unrel.py"", line 55, in <module>; print(mt.describe()); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 85, in describe; rowstr = ""\nRows: \n"" + ""\n "".join([""{}: {}"".format(k, v._type) for k, v in self._row_keys.items()]); AttributeError: 'list' object has no attribute 'items'; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [e9b3538af9b14d6ba440edbdf89e1ef1] failed with error:; Job failed with message [AttributeError: 'list' object has no attribute 'items']. Additional details can be found in 'gs://dataproc-37b10a9c-3ebd-47a4-9228-bf38bdfba439-us/google-cloud-dataproc-metainfo/e432509c-1bea-4839-9cb8-4d404d195a35/jobs/e9b3538af9b14d6ba440edbdf89e1ef1/driveroutput'.; ```; ```; mt = hl.read_matrix_table('gs://jhs_data_topmed/HCLOF_JHS_AF_01_unrel.mt'); mt.describe(); print(mt.count()); mt = mt.annotate_rows(genes = mt.vep.transcript_consequences.gene_symbol); mt = mt.group_rows_by(mt.genes); print(mt.describe()); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7410:424,ERROR,ERROR,424,https://hail.is,https://github.com/hail-is/hail/issues/7410,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:821,Error,Error,821,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1983:73,error,error,73,https://hail.is,https://github.com/hail-is/hail/issues/1983,5,"['Error', 'error']","['Error', 'error']"
Availability,"From the logs:; ```; WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9771:160,error,error,160,https://hail.is,https://github.com/hail-is/hail/pull/9771,1,['error'],['error']
Availability,"From triage discussion. What this means: Instance destruction is slower than it needs to be, this can impact throughput but does not really impact reliability, and is uncommon enough to not be high priority at this time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541:147,reliab,reliability,147,https://hail.is,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541,1,['reliab'],['reliability']
Availability,"Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.3.4</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.3...ca4cb2d6a4b95a6925de85a47b323d2235032c74"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Revert latest changes to <code>ThreadedZMQSocketChannel</code> because they break Qtconsole <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/803"">#803</a> (<a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Fix sphinx 5.0 support <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/804"">#804</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/799"">#799</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-07&amp;to=2022-06-08&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Ablink1073+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​blink1073</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Accordoba12+updated%3A2022-06-07..2022-06-08&amp;type=Issues""><code>@​cc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:4002,Mainten,Maintenance,4002,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Mainten'],['Maintenance']
Availability,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/issues/3038,1,['error'],['error']
Availability,"Function2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:1078,error,error,1078,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['error'],['error']
Availability,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:82,down,down,82,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747,1,['down'],['down']
Availability,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:656,error,error,656,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,1,['error'],['error']
Availability,GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:2099,Heartbeat,HeartbeatReceiver,2099,https://hail.is,https://github.com/hail-is/hail/issues/4780,2,['Heartbeat'],['HeartbeatReceiver']
Availability,"GCP Logging attempts to infer the severity of a log entry and defaults to labelling everything written to `stdout` as `INFO` and everything written to `stderr` as `ERROR`. There are some [LogEntry fields](https://cloud.google.com/logging/docs/structured-logging) that can be overwritten by fields in our JSON output, including `severity`. Until now we had been logging the severity level as `levelname`, which is the expectation of `jsonlogger`, but this means GCP's levels and ours do not necessarily match. This adds another `severity` field to the logs so GCP can pick up the level. This should get rid of a swath of non-error log messages written to `stderr` (only JSON though) and lets other levels like `WARNING` get marked as such. GCP does quite literally extract the severity field though, so it does not appear in the `jsonPayload`. I've kept the `levelname` in then as a sanity check but am also happy to try to remove it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9910:164,ERROR,ERROR,164,https://hail.is,https://github.com/hail-is/hail/pull/9910,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"GENCODE GTF files have contigs ""chr1"", ""chr2"", ... , ""chrX"", ""chrY"", ""chrM"". Currently, when `reference_genome` is set to GRCh37, `import_gtf` removes the ""chr"" prefix. This works for chromosomes 1-22, X, and Y. However, for the mitochondrial contig, Hail expects ""MT"" instead of ""M"" and attempting to do anything with the imported table results in an error: `HailException: Invalid interval '[...]' found. Contig 'M' is not in the reference genome 'GRCh37'.`. With this change, `import_gtf` recodes ""M"" and ""chrM"" as ""MT"" so that the intervals are valid for GRCh37.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9694:352,error,error,352,https://hail.is,https://github.com/hail-is/hail/pull/9694,1,['error'],['error']
Availability,"GIAB VCF import fails because htsjdk ""repairs"" header in vcf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:38,repair,repairs,38,https://hail.is,https://github.com/hail-is/hail/issues/6012,1,['repair'],['repairs']
Availability,GRM failure with N >50k,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1004:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/issues/1004,1,['failure'],['failure']
Availability,"GWAS. Everything is OK until the line to do GWAS. The error I have is as follows:. -------------- start of error ------------------; 2021-01-25 12:36:11 Hail: INFO: linear_regression_rows: running on 250 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:44859); Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-40-d6d936b012db>"", line 3, in <module>; covariates=[1.0]); File ""<decorator-gen-1697>"", line 2, in linear_regression_rows; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/hail/methods/statgen.py"", line 370, in linear_regression_rows; return ht_result.persist(); File ""<decorator-gen-1111>"", line 2, in persist; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:1749,Error,Error,1749,https://hail.is,https://github.com/hail-is/hail/issues/9939,1,['Error'],['Error']
Availability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:811,error,error,811,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,1,['error'],['error']
Availability,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8058:210,down,downstream,210,https://hail.is,https://github.com/hail-is/hail/pull/8058,1,['down'],['downstream']
Availability,"GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; retur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1931,Error,Error,1931,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['Error'],['Error']
Availability,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:24,failure,failure,24,https://hail.is,https://github.com/hail-is/hail/issues/3053,2,"['error', 'failure']","['error', 'failure']"
Availability,Getting more failures :(. Let's sit down tomorrow and I'll show you how to build docs locally,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760:13,failure,failures,13,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760,2,"['down', 'failure']","['down', 'failures']"
Availability,Getting some weird CI errors: the hail module isn't found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-669870559:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-669870559,1,['error'],['errors']
Availability,"Getting the following errors when compiling on a Mac. Any suggestions?. ./gradlew shadowJar ; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/farrell/github/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/darwin; c++ -fvisibility=hidden -dynamiclib -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/darwin/libibs.dylib; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:23:no such instruction: `vmovd %xmm0, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:38:no such instruction: `vpextrq $1, %xmm0,%rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:79:no such instruction: `vpcmpeqd %xmm5, %xmm5,%xmm5'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:96:no such instruction: `vpxor %xmm1, %xmm0,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:114:no such instruction: `vpxor %xmm5, %xmm1,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:135:no such instruction: `vpand %xmm3, %xmm2,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:150:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:167:no such instruction: `vpxor %xmm5, %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:193:no such instruction: `vpor LC1(%rip), %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:212:no such instruction: `vpand %xmm1, %xmm0,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:225:no such instruction: `vpandn %xmm4, %xmm3,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:241:no such instruction: `vmovd %xmm4, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:255:no such instruction: `vpxor %xmm1, %xmm0,%xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:277:no such instruction: `v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/issues/1274,1,['error'],['errors']
Availability,"Gidgethub does not properly handle this error response. The right answer is to fix gidgethub, but, alas, too much to do and too little time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8540:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/pull/8540,1,['error'],['error']
Availability,"Good comments, thanks. I think I addressed everything. There were some pending bugs from moving things to workshop.hail.is. My namespace should be up to date now, let me know if you're still seeing 500 errors. I'm still working on the remaining todo items, but those are independent and I think this is ready to go in (when you approve).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535317771:202,error,errors,202,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535317771,1,['error'],['errors']
Availability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:405,echo,echo,405,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,2,['echo'],['echo']
Availability,Good work hunting this down; these messages are always a pain to find.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817,1,['down'],['down']
Availability,Google considers any message written to stderr as an error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9845:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/9845,1,['error'],['error']
Availability,Got it. +1 for a more specific error message if possible.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784,1,['error'],['error']
Availability,"Got this error in a migration step when dev deploying. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 598, in _read_bytes; data = await self._reader.readexactly(num_bytes); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8761:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/8761,1,['error'],['error']
Availability,Got this error message:. [Stage 0:> (0 + 120) / 2941]hail: write: fatal: hdfs://dataflow01.broadinstitute.org:8020/user/aganna/DIABETES.vcf.bgz: caught java.lang.IllegalArgumentException: requirement failed; offending line: 1 23735206 var_1_23735206 C T 4044.57PASS AC=1;AF=1.971e-0… . which according to Tim can be improved (there is no separation between QUAL and INFO filed). My main point however is that for these malformed files we should be able to remove the malformed lines instead of just not allowing to import the .vcf. This might be an important vcf and cannot / or difficult to find a better version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/361,1,['error'],['error']
Availability,"Gotcha. Yes, your test works. I'll throw an error for now, since this sounds like not a use case we want to support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181,1,['error'],['error']
Availability,"Grace requested this for gnomAD purposes. In Azure, we can determine this; using the classpath or envionrment variables (the env var is only available; inside Jupyter). In GCP, I added an environment variable to our Spark env; / Jupyter env.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11230:141,avail,available,141,https://hail.is,https://github.com/hail-is/hail/pull/11230,1,['avail'],['available']
Availability,"Great change. Still an error related to hail_pip_version in the tests, though. Can you make a discuss post when this goes in to alert people compiling their own builds?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817,1,['error'],['error']
Availability,"Great question! I have not thought to run black on query and agree with your point about it maybe not being a good match. The intention here was just for formatting internal python services, not necessarily as part of the CI but for ease of development and not having to fix `pylint` format errors by hand. Even so I don't love quite how restrictive black is, but it does seem like the most popular formatter out there now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-769082082:291,error,errors,291,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-769082082,1,['error'],['errors']
Availability,Great that error's resolved! but you'll need to compile Hail against the version of Spark you downloaded:. ```; ./gradlew -Dspark.version=2.1.1 shadowJar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Great, perfect, I just wanted to make sure there wasn't another higher level issue to fix. This is great for now, but I just want to reiterate need for Error state and reason_for_error in the batch schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6249#issuecomment-498796054:152,Error,Error,152,https://hail.is,https://github.com/hail-is/hail/pull/6249#issuecomment-498796054,1,['Error'],['Error']
Availability,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:104,down,download,104,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090,3,['down'],"['download', 'downstream']"
Availability,HAIL 0.1: export vcf hadoop error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['error'],['error']
Availability,"HON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14211:2790,avail,available,2790,https://hail.is,https://github.com/hail-is/hail/pull/14211,1,['avail'],['available']
Availability,"HON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Generation of Error Message Containing Sensitive Information <br/>[SNYK-PYTHON-JUPYTERSERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `40.5.0 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:2860,avail,available,2860,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability,HTTP 404 error at https://hail.is/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14616:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/14616,1,['error'],['error']
Availability,"HTTPError: 413 Client Error: Request Entity Too Large. FYI, cranking up the relevant setting will only sort of fix the problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5898:22,Error,Error,22,https://hail.is,https://github.com/hail-is/hail/issues/5898,1,['Error'],['Error']
Availability,"HY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:4157,avail,available,4157,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"HY-3315328](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315328) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Timing Attack <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315331](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315331) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315452](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315452) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315972](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315972) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3315975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3315975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-3316038](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-3316038) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:4149,avail,available,4149,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"Had this refractory Dataproc failure, that kind-of pointed to serialization errors, but which @tpoterba clearly saw wasn't due to serialization, as a test in which the HadoopFS class was explicitly serialized and deserialized succeeded. The problem appeared to be in something affecting sparkContext's ability to broadcast, as even the standard SerializableHadoopConfiguration would appear null in map-reduce operations. I therefore created a clean-slate branch from master, and have issued this here. It passes all tests, including a local reproduction of the Dataproc test, by spinning up 1 spark master, 2 workers, and passing initializing hail with master=spark-master:7077 (thanks @cseed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263:29,failure,failure,29,https://hail.is,https://github.com/hail-is/hail/pull/6263,2,"['error', 'failure']","['errors', 'failure']"
Availability,"Had to chase down a latent bug in `StreamAgg`, but I think everything should be sorted now (pun intended).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405#issuecomment-1673830234:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/pull/13405#issuecomment-1673830234,1,['down'],['down']
Availability,Hail Literal Throws Weird Errors on Hail Objects,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:26,Error,Errors,26,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['Error'],['Errors']
Availability,"Hail Version: 0.2.108. Running the line w/ Zstd compressed UKB bgen files:; `ht = hl.experimental.pc_project(; mt.GT,; loadings_ht.loadings,; loadings_ht.pca_af,; ); `; I get the error at the end of spark execution: ; `Exception in thread ""map-output-dispatcher-0"" Exception in thread ""map-output-dispatcher-1"" java.lang.UnsatisfiedLinkError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.recommendedCOutSize()J; `; (full error attached); [error.txt](https://github.com/hail-is/hail/files/10458606/error.txt). Any ideas?; Many Thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12608:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/issues/12608,4,['error'],['error']
Availability,"Hail Version: `0.2.53-8140f17d9262`. I've got a matrix table where I have added a row and col index to, and am trying to select a field from the table using the following syntax and getting the following error:. `mt[1, 1].GT`. ```; The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); <ipython-input-65-47ed69d0881f> in <module>; 1 for header in headers:; ----> 2 ext = ext.annotate(newHeader = filtered[header.row_idx, ext.idx].GT); 3 ext = ext.rename({ 'newHeader': header.header }). /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in __getitem__(self, item); 626 return self.index_entries(row_key, col_key); 627 except TypeError as e:; --> 628 raise invalid_usage from e; 629 raise invalid_usage; 630. TypeError: MatrixTable.__getitem__: invalid index argument(s); Usage 1: field selection: mt['field']; Usage 2: Entry joining: mt[mt2.row_key, mt2.col_key]. To join row or column fields, use one of the following:; rows:; mt.index_rows(mt2.row_key); mt.rows().index(mt2.row_key); mt.rows()[mt2.row_key]; cols:; mt.index_cols(mt2.col_key); mt.cols().index(mt2.col_key); mt.cols()[mt2.col_key]; ```. Please let me know if you need any further information. The zulip topic name is `Error accessing entry by row/col key`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9704:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/issues/9704,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hail appears to have executed the exact same write command twice. The first write driver ends at 2023-10-13T01:17:55Z and the next write driver starts at 2023-10-13T01:18:11Z, just 16 seconds later. Batch: https://batch.hail.is/batches/8058522; Just the drivers: https://batch.hail.is/batches/8058522?q=name%3Dexecute%28...%29_driver. Driver & frontend logs indicate the first driver job completed and was almost immediately followed by a resubmission of the entire pipeline. https://cloudlogging.app.goo.gl/1344nayXTgaqKhCz8. # OLD. ### What happened?. NB: This is a development build 87398e1b514e. I think my comments below might be misleading. We purposely `WriteMetadata` multiple times, but with different `MetadataWriter`s. Unfortunately, this information does not appear in the SSA IR for some reason?. ---. The ""Relevant log output"" contains the last IR printed before the code was executed. The observed error was:. <details>; <summary>Expand me for the full trace. ```; Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```. </summary>. ```; Traceback (most recent call last):; File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 345, in <module>; ); File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 297, in main; mt = mt.filter_rows(mt.locus.in_autosome()); File ""<decorator-gen-1358>"", line 2, in write; File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/typecheck/check.py"", line 587, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/matrixtable.py"", line 2738, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 541, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:913,error,error,913,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['error'],['error']
Availability,Hail context error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3102:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/3102,1,['error'],['error']
Availability,Hail improve error message for pip,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7081:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/7081,1,['error'],['error']
Availability,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7063:657,error,error,657,https://hail.is,https://github.com/hail-is/hail/issues/7063,1,['error'],['error']
Availability,Hail should not signal an error if the PAR is empty. This change provides an explicit type to the PAR before handing it to filter_intervals. . @tpoterba assigning you because I want to make sure I'm doing this right.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3897:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/3897,1,['error'],['error']
Availability,Hail should produce a legible error message when reading an out-of-date VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2159:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/2159,1,['error'],['error']
Availability,Hail should report an error to the user immediately when an input file is malformed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4100:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/4100,1,['error'],['error']
Availability,Hail tests should better describe their failures,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1286:40,failure,failures,40,https://hail.is,https://github.com/hail-is/hail/issues/1286,1,['failure'],['failures']
Availability,Hail throws a confusing error message on a weird CSV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5222:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/5222,1,['error'],['error']
Availability,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:311,error,error,311,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,4,"['error', 'reliab']","['error', 'reliable', 'reliably']"
Availability,"Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 14:4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:32033,AVAIL,AVAILABLE,32033,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,HailContext(local=local[1]) uses all available cores,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1564:37,avail,available,37,https://hail.is,https://github.com/hail-is/hail/issues/1564,1,['avail'],['available']
Availability,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8636:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/pull/8636,1,['error'],['error']
Availability,"HailException: error while checking subtype:; super: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_sexes: int64, n_cases_full_cohort_females: int64, n_cases_full_cohort_males: int64, col_array: tuple(1:struct{n_cases: int32, n_controls: int32, heritability: struct{estimates: struct{ldsc: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; sub: struct{trait_type: str, phenocode: str, pheno_sex: str, coding: str, modifier: str, pheno_data: array<struct{}>, description: str, description_more: str, coding_description: str, category: str, n_cases_full_cohort_both_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10858:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/10858,1,['error'],['error']
Availability,Handle failure in SamplePCA when matrix has fewer than k non-zero singular values,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2008:7,failure,failure,7,https://hail.is,https://github.com/hail-is/hail/pull/2008,1,['failure'],['failure']
Availability,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Ve",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13392,AVAIL,AVAILABLE,13392,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6600,AVAIL,AVAILABLE,6600,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:338,failure,failure,338,https://hail.is,https://github.com/hail-is/hail/issues/430,2,['failure'],['failure']
Availability,"Happy to commit this if it passes tests. Looks like you've got a rebase error, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938:72,error,error,72,https://hail.is,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938,1,['error'],['error']
Availability,"Happy to sit down and go through what this is about. This is current running on the cluster:. ```; $ kubectl get pods; NAME READY STATUS RESTARTS AGE; ...; spark-master-ffcfbf95c-gth5s 1/1 Running 0 4m; spark-worker-699db74c7-lsd9v 1/1 Running 0 11h; spark-worker-699db74c7-plgdd 1/1 Running 0 11h; ```. but I haven't automated deployment yet. I'm currently building the hail image from a distribution I hand built, but I'll switch over to the standard distribution once this goes in: https://github.com/hail-is/hail/pull/4554 (it fixed some bugs that showed up in this deployment). That's the `gs://hail-cseed/hail-test.zip` stuff. This was surprisingly difficult to get working. The main culprit, I think, is that Spark makes it impossible to bind and advertise different addresses for the Spark master. In the end I faked it out with:. > echo ""0.0.0.0 spark-master"" >> /etc/hosts. which works but seems a bit dubious.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4560:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/pull/4560,2,"['down', 'echo']","['down', 'echo']"
Availability,"Has the math never rendered on the CI server? I see MathJax.js downloaded successfully but math isn't rendered, for example, in `lmmreg`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2145#issuecomment-324995288:63,down,downloaded,63,https://hail.is,https://github.com/hail-is/hail/pull/2145#issuecomment-324995288,1,['down'],['downloaded']
Availability,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:47,error,error,47,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734,1,['error'],['error']
Availability,"Having some issues with `map` in `annotateglobal`. The following command:. ```; annotateglobal expr -c 'global.pops = [""AFR"", ""NFE""]' \; annotateglobal expr -c 'global.pop_counts = global.pops.map(x => samples.count(sa.meta_test.POP == x))' \; showglobals; ```. return this error:. ```; hail: fatal: annotateglobal expr: symbol `x' not found; <input>:1:global.nfes = global.pops.map(x => samples.count(sa.meta_test.POP == x)); ```. @tpoterba seemed to suggest aggregators don't work inside map?. Anyway, to provide context: what I'd love to do is eventually be able to iterate through arrays to summarize data (rather than having to specify all my criteria once in each annotate command). Perhaps this is better done in some `group_by` type functionality, but I'm not sure where that is on the roadmap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/549:274,error,error,274,https://hail.is,https://github.com/hail-is/hail/issues/549,1,['error'],['error']
Availability,"Heh, I separately discovered the test failure and fixed at https://github.com/hail-is/hail/pull/13477",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13470#issuecomment-1688883263:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/13470#issuecomment-1688883263,1,['failure'],['failure']
Availability,"Heh, so turns out that `test_weird_urls` is missing the `@pytest.mark.asyncio` decorator, and so it was getting skipped with a warning this whole time. The pytest upgrade added auto-detection of async tests and so it ran this broken test for the first time. I'm PR'ing to treat most warnings as errors in #12322.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498:295,error,errors,295,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498,1,['error'],['errors']
Availability,"Heh, this is all my fault: https://github.com/hail-is/hail/pull/5078/files",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5692#issuecomment-479642895:20,fault,fault,20,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479642895,1,['fault'],['fault']
Availability,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:94,Error,Error,94,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8908:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/issues/8908,4,"['Error', 'down', 'error']","['Error', 'downstream', 'error']"
Availability,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:364,error,error,364,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,1,['error'],['error']
Availability,"Hello, I have installed hail using pycharm not from github. Now getting the attached error. My command was:; ```; import hail as hl; hl.init(); import os; from hail.plot import show; [hail.err.txt](https://github.com/hail-is/hail/files/3570397/hail.err.txt). from pprint import pprint; hl.plot.output_notebook(). hl.utils.get_1kg('data/'); hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). mt = hl.read_matrix_table('data/1kg.mt'); mt.rows().select().show(5); ```; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6982:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/issues/6982,1,['error'],['error']
Availability,"Hello, when I test hail in the spark cluster, there is an error:. bash-4.2$ spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ***/hail-all-spark.jar --master yarn-client importvcf /user/hail/sample.vcf splitmulti write -o /user/hail/sample_1.vds exportvcf -o /user/hail/sample_1.vcf. Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/solr/client/solrj/SolrClient : Unsupported major.minor version 52.0; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:800); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:449); at java.net.URLClassLoader.access$100(URLClassLoader.java:71); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.net.URLClassLoader$1.run(URLClassLoader.java:355); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:354); at java.lang.ClassLoader.loadClass(ClassLoader.java:425); at java.lang.ClassLoader.loadClass(ClassLoader.java:358); at org.broadinstitute.hail.driver.ToplevelCommands$.<init>(Command.scala:62); at org.broadinstitute.hail.driver.ToplevelCommands$.<clinit>(Command.scala); at org.broadinstitute.hail.driver.Main$.main(Main.scala:205); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/issues/825,1,['error'],['error']
Availability,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453:733,error,errors,733,https://hail.is,https://github.com/hail-is/hail/issues/453,2,"['FAILURE', 'error']","['FAILURE', 'errors']"
Availability,"Here are the alternatives that I see:. The original design:; ```scala; object LowerArrayToStream {; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]); streamified = ToArray(streamified). if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]); ToStream(node); else; node; }; }; }. private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; }; }. def apply(node: IR): IR = boundary(node); }; ````. the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. This can be fixed using TContainer instead of TArray. But as soon as you do this, you need to make sure you're never generating ToArray(ToStream(something of type TDict or TSet)), which means you need the if check in the present PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280:1009,error,errors,1009,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280,1,['error'],['errors']
Availability,Here is the problematic command:. `annotateglobal table \; -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/genelists/all_scores.scores \; -r global.all_scores \; annotateglobal expr -c 'global.GWAS_height = global.all_scores.filter(x => x.GWAS_HEIGHT == '1').map(x => x.V1)' \; annotatevariants expr -c 'va.andrea.test = global.GWAS_height.toSet.contains(va.andrea.genename)' \`. The shell was eliding the single quote and we were comparing a String and an Int. That should be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/388#issuecomment-219845067:480,error,error,480,https://hail.is,https://github.com/hail-is/hail/issues/388#issuecomment-219845067,1,['error'],['error']
Availability,"Here is what I get when invoking pyspark. $ pyspark; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/02 13:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/02 13:56:47 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.13 (default, Jul 18 2017 09:16:53); SparkSession available as 'spark'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996:1396,avail,available,1396,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996,1,['avail'],['available']
Availability,Here's a clear instance of buffer corruption after a transient error (in this case an SSLException). https://batch.hail.is/batches/7996481/jobs/182741; ```; 2023-09-13 16:37:36.612 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 2: /batch/1c00c7157d4d41bcbf508f12d75329b1; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 3: /batch/1c00c7157d4d41bcbf508f12d75329b1/log; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:63,error,error,63,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,1,['error'],['error']
Availability,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:126,FAILURE,FAILURE,126,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,2,['FAILURE'],['FAILURE']
Availability,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233012302:655,error,error,655,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302,1,['error'],['error']
Availability,Here's one of the easier solutions: http://discuss.hail.is/t/python-error-importerror-no-module-named-decorator/131/4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1459#issuecomment-283822337:68,error,error-importerror-no-module-named-decorator,68,https://hail.is,https://github.com/hail-is/hail/issues/1459#issuecomment-283822337,1,['error'],['error-importerror-no-module-named-decorator']
Availability,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1151:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1151,5,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:296,redundant,redundant,296,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,1,['redundant'],['redundant']
Availability,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-248645143:242,error,errors,242,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143,3,"['down', 'error']","['download', 'downloaded', 'errors']"
Availability,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/446#issuecomment-234642054:498,error,error,498,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054,2,['error'],['error']
Availability,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492:589,avail,available,589,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492,1,['avail'],['available']
Availability,"Hey @dlcotter ! Thanks for the report. I anticipate a fix in the next version of Hail. For now, I think you can fix with `pip3 install 'parsimonious>=0.9'` or downgrading to Python 3.10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443:159,down,downgrading,159,https://hail.is,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443,1,['down'],['downgrading']
Availability,"Hey @iris-garden !. This was a PR for a new tutorial that our summer intern, Aleisha, worked on during her internship. Could you do a review for me? The goal here is to make sure there's no typos and that each cell executes without error. Can you also build the docs and make sure the rendered notebook looks OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12097#issuecomment-1266105755:232,error,error,232,https://hail.is,https://github.com/hail-is/hail/pull/12097#issuecomment-1266105755,1,['error'],['error']
Availability,Hey @mhebrard !. I'm really sorry Hail has been such a pain to install. This looks to me like a Scala version incompatibility. In your Makefile you specified this:; ```; ... SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```; [EMR's docs](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-691-release.html) confirms that Scala 2.12.15 should be installed. I think my next questions are:; 1. Which `spark-shell` is that?; 2. What latent JVMs are around?; 3. What's the class path and what's on it?; 4. What scala executables are around?. ```; which spark-shell; spark-shell --version; which java; java -version; which scala; scala -version; echo $CLASSPATH; ```. That `SettingsOps` is an implicit nested class of the `MutableSettings` object. It is definitely present in [2.13](https://github.com/scala/scala/blob/2.13.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L70-L88) and [2.12](https://github.com/scala/scala/blob/2.12.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L83-L94). It appears to be missing in [2.11](https://github.com/scala/scala/blob/2.11.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L64-L68). It appears to have arrived in [2.12.14](https://github.com/scala/scala/commit/3bd24299fc34e5c3a480206c9798c055ca3a3439).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525:642,echo,echo,642,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525,1,['echo'],['echo']
Availability,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:621,failure,failure,621,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771,2,['failure'],['failure']
Availability,"Hey @poneill !. `-O`/`PYTHONOPTIMIZE` is explicitly defined as the ""turn off asserts"" option in [the docs](https://docs.python.org/3/using/cmdline.html#cmdoption-O). If you disable asserts, you'll get even more inscrutable errors. I recommend against doing that. If you see any `assert(x, y)` in the code base, please file a PR or a bug. We'll fix it. We will not replace asserts with if-raise. ---. As to the bug you've found: yes this is a bug in Hail. We incorrectly assume that if there is at least one dataset with the right version and at least one dataset with the right reference genome that there's a dataset with the right version *and* reference genome. That logic is obviously false. I'll have someone fix this in the next couple weeks. As to the root issue: the Hail annotation database doesn't have a GRCh38 version of `gnomad_pca_variant_loadings` version 2.1. This is because [gnomAD](https://gnomad.broadinstitute.org/downloads#v2-liftover) hasn't published a GRCh38 version of their 2.1 variant loadings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230:223,error,errors,223,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230,2,"['down', 'error']","['downloads', 'errors']"
Availability,"Hey @seanjosephjurgens !. Sorry you're running into trouble. This error message is bad. See https://github.com/hail-is/hail/issues/13346 for that bug. The real issue here is VCF INFO fields like:; ```; AS_BaseQRankSum=0.000,.,0.100,0.500; ```; The VCF spec doesn't explicitly permit missing values as elements of INFO or FORMAT fields. It does permit the whole field to be missing a la `FIELD=.` but `FIELD=1,.,1` or `FIELD=.,.,.` are not explicitly permitted. In particular, `FIELD=.` could mean ""this field is missing"" or ""this field is not-missing, it is a one-element array containing one missing value"". The fix is to use `hl.import_vcf(..., array_elements_required=False)`. When that is true, Hail will parse `1,.,1` as `[1, NA, 1]`. Be forewarned: Hail treats `FIELD=.` as a missing field, not an array with one missing element.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511,1,['error'],['error']
Availability,"Hey @tpoterba I tried, but am getting ; `remote: Permission to broadinstitute/hail.git denied to bw2.; fatal: unable to access 'https://github.com/broadinstitute/hail/': The requested URL returned error: 403`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248072411:197,error,error,197,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248072411,1,['error'],['error']
Availability,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:587,error,errors,587,https://hail.is,https://github.com/hail-is/hail/issues/1525,2,['error'],['errors']
Availability,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721,1,['error'],['error']
Availability,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/1017,3,['error'],['error']
Availability,"Hi @Sun-shan,. First, I should note that we do not currently test hail against Spark version 2.2.0, I recommend using Spark 2.1.1 or 2.0.2. Spark versions aside, the error you encountered is unrelated to Spark, as far as I know. What version of the `decorator` package is installed on your machine? `decorator` version 4.0.10 should work correctly. Unfortunately, we are still looking for a python dependency management solution. My apologies that you've run into this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534,1,['error'],['error']
Availability,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-240446097:97,down,downloaded,97,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097,5,"['down', 'error']","['download', 'downloaded', 'errors']"
Availability,"Hi @danking, I see the initial CI result failed, but I'm unable to login and see what the failure is. I signed in to google with my popgen account, and get 504 Gateway Time-out on the `auth.hail.is/oauth2callback`, I imagine because I don't have an account there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-784542735:90,failure,failure,90,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-784542735,1,['failure'],['failure']
Availability,"Hi @danking, sorry this took me a little to test. I think there's a problem with the latest changes, in my dev-deploy, it failed on the '`create_certs` and `create_accounts`, with the error:. ```; FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/hailtop/hail_version'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814:184,error,error,184,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814,1,['error'],['error']
Availability,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665:358,error,errors,358,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665,2,['error'],['errors']
Availability,"Hi @eric-czech,. The errors you're running into with aggregating sorted arrays should be fixed in versions 0.2.31 and later; if upgrading the version works for you, please let me know and I will close this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-585938806:21,error,errors,21,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-585938806,1,['error'],['errors']
Availability,"Hi @jmarshall, the team talked about this issue in our standup today. We had some concerns about appropriateness of using this table as a long term storage area for larger metadata, and the likely developer effort and system downtime to perform the migration. So we currently don't plan on prioritizing this in the immediate future, but do let us know if you have any concerns about that - or if it ends up being impossible for you to work around this - and we might be able to reconsider (or maybe come up with alternative solutions). Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647:225,downtime,downtime,225,https://hail.is,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647,1,['downtime'],['downtime']
Availability,"Hi @konradjk Did you ever determine a cause of this? I am seeing very, very similar errors in an unrelated project (but also executing in GCP).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419655426:84,error,errors,84,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419655426,1,['error'],['errors']
Availability,"Hi @mgeaghan, it looks like in this [change from 14 Feb](https://github.com/hail-is/hail/commit/6eacd66b3979bf27982b4d15b5f17c60474148cb) data for US, UK and Europe was moved to regional buckets, however it appears that the Australian data was not moved. At the moment `gs://hail-australia-southeast1-vep` does not exist, yet `gs://hail-aus-sydney-vep` DOES exist. You see the opposite pattern if you try UK, US or EU equivalents. @danking - is this a reasonable explanation? Are you able to move the Australian data, or is there someone else (or a different forum) that we should ask for this?. also pinging @patrick-schultz and @chrisvittal as you both were cc'd in the change referred to above :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513#issuecomment-2092036876:601,ping,pinging,601,https://hail.is,https://github.com/hail-is/hail/issues/14513#issuecomment-2092036876,1,['ping'],['pinging']
Availability,"Hi @rmporsch,. I'm sorry you're experiencing this error. I've resolved this in https://github.com/hail-is/hail/pull/3589 . After that PR is merged, it should be available in the GS bucket no more than an hour later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3585#issuecomment-389245636:50,error,error,50,https://hail.is,https://github.com/hail-is/hail/issues/3585#issuecomment-389245636,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi @tushu1232, can you post the output of:. ```; gcc --version ; g++ --version; ```. We strongly suggest using GCC 4.7 to compile hail because it has [relatively complete feature support](https://gcc.gnu.org/gcc-4.7/cxx0x_status.html) for C++11. If you must use an older version of C++ you can try changing that argument to `-std=c++0x`. If you have a version of GCC >4.7 and are still seeing this issue, I am happy to help determine why you're getting this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276986028:458,error,error,458,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276986028,1,['error'],['error']
Availability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:865,error,error,865,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['error'],['error']
Availability,"Hi Hail developers,; I am a new hail user and was struggling to process my multi-sample vcf file with hail. I first tried to read in my file with the code below and create annotate another column:. **import hail as hl; rt = hl.import_vcf('chr1_biallelic.vcf.gz',force_bgz=True,reference_genome=""GRCh38"",drop_samples=True).rows(); rt = rt.annotate(variant=rt.CHROM + ':' + rt.POSITION + "":"" + rt.REF + "":"" + rt.ALT)''**. However, running the third line gives me the following error:; **AttributeError: Table instance has no field, method, or property 'CHROM'**. Even though the CHROM header is present in my vcf file, it's not being read. Is it because of the metadata headers? Sorry if this is a basic question, I don't have any other resources to rely on and couldn't find any solutions online.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10978:475,error,error,475,https://hail.is,https://github.com/hail-is/hail/issues/10978,1,['error'],['error']
Availability,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240384398:266,avail,available,266,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398,1,['avail'],['available']
Availability,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456:450,down,down,450,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456,1,['down'],['down']
Availability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:357,error,error,357,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['error'],['error']
Availability,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['error'],['error']
Availability,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['error'],['error']
Availability,"Hi all,; I used hail 0.2 and made my own hail referene genome for a plant other than human by faking the X, Y and Par. But I can not find anywhere to setup my own reference genome even I know there is hl.init method which can set different version of human reference genome, and there is no set_reference method which is the counterpart of hl.get_reference().; How can I make my own reference genome available in hail0.2? Thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9837:400,avail,available,400,https://hail.is,https://github.com/hail-is/hail/issues/9837,1,['avail'],['available']
Availability,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:274,ERROR,ERROR,274,https://hail.is,https://github.com/hail-is/hail/issues/1419,12,['ERROR'],['ERROR']
Availability,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:275,error,error,275,https://hail.is,https://github.com/hail-is/hail/issues/6012,2,"['error', 'repair']","['error', 'repair']"
Availability,"Hi guys,; I've cloned the master and rebuilt, but unfortunately I get the same error when annotating the VCF just read.; ```; [Stage 0:====================================================>(2111 + 1) / 2112]hail: info: Coerced sorted dataset. ------------Annotate the VCF file-------------; [Stage 1:> (0 + 160) / 2112]; [Stage 1:> (0 + 160) / 2112]found fatal is.hail.utils.package$FatalExcepti; on: swe.vcf.bgz: invalid AD field `24,0,0': expected 2 values, but got 3.; offending line: 1	65684548	.	G	A	3524.14	VQSRTrancheSNP99.60to99.80	AC=1;AF=...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1415#issuecomment-282545923:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282545923,1,['error'],['error']
Availability,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250746848:264,error,errors,264,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848,1,['error'],['errors']
Availability,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:402,down,download,402,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724,1,['down'],['download']
Availability,"Hi!!!. - I was just delivered a wheel of Hail 0.2.128 loaded with this method from Chris, available at: gs://hail-30-day/hailctl/dataproc/cvittal-dev/0.2.128-2863e0eb192c/hail-0.2.128-py3-none-any.whl . Very helpful for removing those temp methods in the PR!. - it looks like the code will have a home inside of something that looks like the following: ; ```; qc = hl.struct(; **{; ann: hl.agg.filter(; expr,; vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt.variant_ac,; variant_atypes=vmt.variant_atypes,; dp=vmt.DP,; ),; ); for ann, expr in argument_dictionary.items(); }; ); ```; inside of a hl.agg.filter call, and where argument_dictionary looks like {'vep_name':ht.vep.field , ""pass_all_vqc"": hl.len(vmt.filters) == 0} and so on. Just to give an idea what the usage is going to be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641:90,avail,available,90,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641,1,['avail'],['available']
Availability,"Hi!. Trying to calculate polygenic risk score with code from the [Polygenic Score Calculation](https://hail.is/docs/0.2/guides/genetics.html#polygenic-score-calculation), getting error with stacktrace:. `2022-05-14 12:09:07 Hail: INFO: Running Hail version 0.2.94-f0b38d6c436f; 2022-05-14 12:09:08 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.; 2022-05-14 12:09:08 root: INFO: RegionPool: initialized for thread 30: Thread-4; 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 34.3 KiB, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['error'],['error']
Availability,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561#issuecomment-238502640:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640,1,['error'],['error']
Availability,"Hi, . I'm using the concordance function to compare two sets of data, and I feel the n_discordant (last column) is not correct. . For example: ; ```; chr1:930314 [""C"",""T""] {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} {""locus"":{""contig"":""chr1"",""position"":930314},""alleles"":[""C"",""T""]} [[0,0,0,0,0],[0,0,21,1,0],[0,0,2057,0,0],[0,0,0,91,0],[0,0,0,0,3]] 2172; chr1:946538 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946538},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,5,3,0],[0,0,1868,1,1],[0,0,0,279,0],[0,0,0,0,16]] 2170; chr1:946653 [""G"",""A""] {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} {""locus"":{""contig"":""chr1"",""position"":946653},""alleles"":[""G"",""A""]} [[0,0,0,0,0],[0,0,856,275,66],[0,0,386,74,7],[0,0,16,415,33],[0,0,0,3,42]] 1898; ```. In the first example, I thought the n_discordant should be 0 if the `concordance` field is correct, isn't it?. The code I was using: ; `global_GA_both, samples_GA_both, SNPs_GA_both = hl.concordance(mt_exome, mt_GAsP_ft)`. The Hail version:; ```Running on Apache Spark version 2.4.3; SparkUI available at http://spark-master:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.26-2dcc3d963867; LOGGING: writing to Concordance_2019_11_28_hail.log; ```. When I was using google Terra Hail 0.2.11-daed180b84d8, I didn't have this issue. The output didn't have `left_row` or `right_row`. Cheers,; Qinqin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7632:1137,avail,available,1137,https://hail.is,https://github.com/hail-is/hail/issues/7632,1,['avail'],['available']
Availability,"Hi, @danking ,. Thanks for the fast and detailed answer.; No worry, it didn't disrupt my workflow, but confused me since the pages are still in the search results but not available and I didn't see the note for moving to VDS in the changelogs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377#issuecomment-1667580110:171,avail,available,171,https://hail.is,https://github.com/hail-is/hail/issues/13377#issuecomment-1667580110,1,['avail'],['available']
Availability,"Hi, @danking ; I reconfigurated the spark cluster, with the cloudera spark : version 2.2.0.cloudera1; But I can't import hail this time, How can I fix it?. The test:; ```; >>> spark.sparkContext.master; u'yarn'. bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> spark.sparkContext.master; u'yarn'; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/hail/python/hail/__init__.py"", line 1, in <module>; import hail.expr; File ""/opt/Software/hail/python/hail/expr.py"", line 3, in <module>; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; File ""/opt/Software/hail/python/hail/representation/__init__.py"", line 1, in <module>; from hail.representation.variant import Variant, Locus, AltAllele; File ""/opt/Software/hail/python/hail/representation/variant.py"", line 2, in <module>; from hail.typecheck import *; File ""/opt/Software/hail/python/hail/typecheck/__init__.py"", line 1, in <module>; from check import *; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 1, in <module>; from decorator import decorator, getargspec; ImportError: cannot import name getargspec; >>> ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486:960,avail,available,960,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486,1,['avail'],['available']
Availability,"Hi, I am a staff in DCH. Now, we are testing the hail software and meet some test errors .; 1. How to check the results of some commands? such as, annotatesamples. For example, if you run this command:; hail importvcf sample.vcf annotatesamples expr -c 'sa.nHet = gs.count(g.isHet)‘ exportsamples –c ‘s.id’ –o sample_tmp.tsv ; you can get the sample_tmp file including the names of genes satisfying the screen , but how to check the output as a number in the terminal, like the format shown in showglobals command?. If you run the command:; hail read -i tmp.vds imputesex -m 0.01 exportsamples –o impute_tmp.tsv -c “ID=s.id” exportvcf –o impute_tmp.vcf ; how to obtain the inbreeding coefficient from the impute_tmp file?; 1. The Structure has no filed ***. During the test, there are some similar errors in different modules. For example, if you run the command , ; hail importvcf sample.vcf filtersamples expr --keep -c 'sa.qc.callRate > 0.99' write -o output.vds exportvcf -o sample1.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnotations.anno1,ANNO2=va.MyAnnotations.anno2' -o file.tsv -o sample.tsv ; hail read -i output.vds exportvariants -c 'v,va.pass,va.qc.AF' -o file.tsv ; hail read -i output.vds exportsamples -c 's.id, sa.qc.rTiTv' -o file.tsv; you will get the same fatal error: ‘Struct’ has no field ‘qc’. Is it because the qc isn`t defined in “sa” struct? ; The same problems appeared in sa.pheno, global.genes, va.Myannotations and va.qc . ; hail importvcf sample.vcf annotatevariants expr -c 'va.minorCase = gs.count(sa.pheno.Pheno1 == ""Case"" && g.isHet)’ )‘ exportvcf -o fet_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c ‘global.first10genens = global.genes[:10]‘ exportvcf -o global_tmp.vcf ; hail importvcf sample.vcf annotateglobal expr -c 'global.nCase = samples.count(sa.pheno.isCase)’ exportvcf -o global_tmp.vcf ; hail read -i output.vds exportgenotypes -c 'SAMPLE=s,VARIANT=v,GQ=g.gq,DP=g.dp,ANNO1=va.MyAnnota",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683:82,error,errors,82,https://hail.is,https://github.com/hail-is/hail/issues/683,2,['error'],['errors']
Availability,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:412,echo,echo,412,https://hail.is,https://github.com/hail-is/hail/issues/2076,2,"['echo', 'error']","['echo', 'errors']"
Availability,"Hi, I found hl.init(sc=sc) returns error since hail-0.2.92.; It can reproduce simply run as following.; Is it a bug ??; Or should I run other ways ?. - Environments I tested. ; Hail version : 0.2.92 or later.; Mac book air (M1) , spark local mode; Rocky Linux 8.5 , spark local mode; Rocky Linux 8.5 , spark yarn cluster mode. - how to reproduce; ```; import os; os.environ['PYSPARK_SUBMIT_ARGS'] = ' \; --jars \; /Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/hail/backend/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; pyspark-shell '. from pyspark import SparkContext; sc=SparkContext.getOrCreate(). import hail as hl; hl.init(sc=sc); ```. - Error logs ; ```; 22/05/11 14:31:21 WARN Utils: Your hostname, spacerider.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en6); 22/05/11 14:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/username/miniforge3/envs/hail/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 22/05/11 14:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use set",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/issues/11827,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299:285,down,downgrade,285,https://hail.is,https://github.com/hail-is/hail/issues/7299,2,['down'],"['downgrade', 'downgrades']"
Availability,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:425,ERROR,ERROR,425,https://hail.is,https://github.com/hail-is/hail/issues/1003,5,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Hi, I'm facing an error when importing a bgen file and I was wondering if you can help me. I'd made a small .bgen file using qctools to select only a few samples from the original file and now I'm trying to import this smaller file into hail, like this:. `hl.import_bgen('subsetted_chr22.bgen', entry_fields=['GT', 'GP', 'dosage'], sample_file='samples_test.sample').write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/8545,2,['error'],['error']
Availability,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229750270:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270,1,['error'],['error']
Availability,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252404979:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979,2,['error'],['error']
Availability,"Hi, not sure if this is the right avenue, but I'd also like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:330,error,error,330,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi,. I tried to compile Hail version 852f92aac4532abc2fc743e0629840a7f6d86496; but it failed with:. >$ ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++11 -Ilibsimdpp-2.1 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -c NativeBoot.cpp; NativeBoot.cpp:1:0: error: bad value (sandybridge) for -march= switch; #include <jni.h>; ^; make: *** [build/NativeBoot.o] Error 1; :nativeLib FAILED. I'm compiling on Amazon's EMR, emr-5.10.0, where I install miniconda to get python 3.6 installed (as a bootstrap action), and run manually:. > sudo yum update -y; sudo yum install g++ cmake git -y; sudo mkdir -p /etc/alternatives/jre; sudo ln -s /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-7.b10.37.amzn1.x86_64/include /etc/alternatives/jre/include; git clone https://github.com/broadinstitute/hail.git; cd hail/; git checkout 852f92aac4532abc2fc743e0629840a7f6d86496; ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4305:500,error,error,500,https://hail.is,https://github.com/hail-is/hail/issues/4305,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi,. Thanks so much for setting up a tutorial for Batch, and for all your work on Hail as a service - it's a really great project and I'm excited to try it out!. I'm following the tutorial and was wondering if this line is there by mistake?. https://github.com/hail-is/hail/blob/8140f17d926235470b1ed1cdefd591c3b41838a5/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py#L73. It throws `AttributeError: 'InputResourceFile' object has no attribute 'add_extension'`, and the method is not defined for `InputResourceFile` indeed. However it is defined for `JobResourceFile`, which, if I understand, Batch uses to find the job output? If so, I guess, for the input resource with an explicitly defined name, calling `add_extension` is redundant? . I'm on Hail `0.2.59-63cf625e29e5`. Vlad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9645:743,redundant,redundant,743,https://hail.is,https://github.com/hail-is/hail/issues/9645,1,['redundant'],['redundant']
Availability,"Hi,; When I remove plink from my path I get a bit of a different error. Can you rerun `gradle check` with the `--info` argument? It'll vomit a bunch of details, but the output from those tests should tell us what's going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230192783:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230192783,1,['error'],['error']
Availability,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/issues/715,3,"['error', 'failure']","['error', 'failure']"
Availability,"Hi:; I come form DCH in China. Now I try to use the following commands to import table in tutorial;; ```; from hail import *; hs= HailContext(); table=hc.import_table('1000Genomes.sample_annotations',impute=True).key_by('Sample'); ```. However, an error appeared like this :; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'import_table' is not defined. Should I import other modules to use import_table command? Thanks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818:248,error,error,248,https://hail.is,https://github.com/hail-is/hail/issues/1818,1,['error'],['error']
Availability,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:161,error,error,161,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109,2,['error'],['error']
Availability,Hist error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/944:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/pull/944,1,['error'],['error']
Availability,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:871,error,errors,871,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,7,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Hm, weird. When I try these tests out against default I get:. ```; FatalError: batch id was 2271614; HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.ru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:579,Error,ErrorHandling,579,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,4,['Error'],['ErrorHandling']
Availability,"Hmm, I realized one problem with this - the error message will return that of coalesce rather than or_else and might be a little confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355,1,['error'],['error']
Availability,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:280,avail,available,280,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145,1,['avail'],['available']
Availability,"Hmm, idempotency is a bit hard to talk about here. This change makes it impossible to not ""cleanup"" the chunks iterator if you hit an exception midway through the chunks iterator. In particular, this now works:; ```; try:; with chunks(...) as data:; raise ValueError(); except ValueError:; pass; with chunks(...) as data:; ... use data ...; ```. In the current code, that does not work. The second call to `chunks` raises an error unless chunks is empty. ---. But you're probably asking about the code that uses chunks? In the Google case it is idempotent: lines 206-215 construct a new request before iterating chunks. The PUT request includes the specific range of bytes we want to write to, so even if we partially succeeded with a previous PUT, this subsequent PUT should overwrite (or, more likely, error). In practice, I don't think we can partially succeed. I think either we write fully or we terminate the connection early and google drops the data. Summary: I think Google is fine. As for Azure, we use a randomly generated block_id. If we error while inside `stage_block` that block_id is never added to `self.block_ids`. As a result, we can safely make a second attempt to upload the block with a new id.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868:425,error,error,425,https://hail.is,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868,3,['error'],['error']
Availability,"Hmm, so I've been using a branch to run some 10k and 100k scale tests of /bin/true https://github.com/hail-is/hail/pull/7783 and I've found deadlocks to be rather rare?. In that PR, I only changed the known deadlocking calls to be deadlock resilient. However, deadlock errors seem to be a feature of mysql and it seems were always intended to retry them, so I think this PR (7782) is the right solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582:240,resilien,resilient,240,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582,2,"['error', 'resilien']","['errors', 'resilient']"
Availability,"Hmm, so, I see that the py4j backend doesn't error if you `get_flags` for a flag that you haven't previously set. I think it gives you the default value?. The service backend is a bit screwy when it comes to flags because I don't want to run a batch job just to figure out what the Scala-side flags are. I think the right fix is to move all the flag information into Python. What do you think of that @tpoterba ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12307#issuecomment-1276964160:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/12307#issuecomment-1276964160,1,['error'],['error']
Availability,"Hmm. I think in the main pod file exists case, the init container actually needs to exit non-zero. Batch then needs to distinguish initContainer failure from main container failure. If the initContainer exits 0, the main container will run. For output pod, I think we should just use a normal container, not an init container, so that the above issue isn't present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632:145,failure,failure,145,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632,2,['failure'],['failure']
Availability,"Hmm. I trust the code now. I test against several R SKAT runs. I'm not sure I understand how we derive that Q is generalized chi-squared distributed. We use the residual phenotypes in the calculation of Q, but those are inverse-logit transformed normal variables. The derivation for the linear case doesn't apply, as far as I can tell. I assume the residuals are Bernoulli distributed? Maybe not. I guess the phenotypes are Bernoulli but the errors aren't? I'm not sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599:442,error,errors,442,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599,1,['error'],['errors']
Availability,"Hmm. I'm having a real hard time figuring out how the fast path has affected this new test. Checkpointing before the group by makes this pass. Removing the union/filter_partitions line makes it pass.; ```; t = hl.utils.range_table(8, n_partitions=8); t = t._filter_partitions([7]).union(t._filter_partitions([7], keep=False)); t = t.group_by(_key=t.idx).aggregate(t_value=hl.agg.collect(t.row_value)); expected = [; hl.Struct(_key=0, t_value=[hl.Struct()]),; hl.Struct(_key=1, t_value=[hl.Struct()]),; hl.Struct(_key=2, t_value=[hl.Struct()]),; hl.Struct(_key=3, t_value=[hl.Struct()]),; hl.Struct(_key=4, t_value=[hl.Struct()]),; hl.Struct(_key=5, t_value=[hl.Struct()]),; hl.Struct(_key=6, t_value=[hl.Struct()]),; hl.Struct(_key=7, t_value=[hl.Struct()]); ]; actual = t.collect(); assert actual == expected. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11785#issuecomment-1106757367:92,Checkpoint,Checkpointing,92,https://hail.is,https://github.com/hail-is/hail/pull/11785#issuecomment-1106757367,1,['Checkpoint'],['Checkpointing']
Availability,Hmm. Not reliably.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834378001:9,reliab,reliably,9,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834378001,1,['reliab'],['reliably']
Availability,"Hmm. This means every dev deploy will generate a new root key. I'm worried about the derived keys and trust lists. After this runs, any service which was not dev deployed needs to know to reload the trust list and start using the new key. For example, if you dev deploy batch, then separately dev deploy query, the new query will get cert errors when talking to batch, I think. I will give some thought this week to the right long-term certificate strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199:339,error,errors,339,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199,1,['error'],['errors']
Availability,"Hmm... shadowJar is building at about 2 minutes on my computer, 1 min 20s of which is compileScala. Master is building at about 1 min 40s (not sure how much scala compile is taking, forgot to check), and I managed to get it down to like 1 min 45s by not bundling some of the dependencies that we weren't bundling before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903:224,down,down,224,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903,1,['down'],['down']
Availability,Hmmm... we do give these labels to disks. Maybe that's the error when no orphaned disks are found with any labels,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13719#issuecomment-1737802239:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/pull/13719#issuecomment-1737802239,1,['error'],['error']
Availability,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733,2,['error'],['error']
Availability,Hopefully down to ~2.5 minutes now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5582#issuecomment-471740579:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/pull/5582#issuecomment-471740579,1,['down'],['down']
Availability,"Hopefully you don't mind this unrequested review, I noticed this PR and had some thoughts. As a developer, I like the new way of defining available parameters and the greater clarity between group and command options. As a user, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--asyn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:138,avail,available,138,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,1,['avail'],['available']
Availability,Horrible diff but it's mainly just a de-indentation of the `insert_jobs_into_db` function and lifting the try/except block to the callsite of the transaction. By handling that error externally to the transaction we don't stifle errors that would be caught by the `@transaction` retry logic.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14370:176,error,error,176,https://hail.is,https://github.com/hail-is/hail/pull/14370,2,['error'],"['error', 'errors']"
Availability,How does the history slow it down? It's just a wrapper.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662:29,down,down,29,https://hail.is,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662,1,['down'],['down']
Availability,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:94,error,error,94,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642,1,['error'],['error']
Availability,"Huh! I seem to have reliably broken unicode ordering and/or en/de-coding. I've marked as fails for now. Hopefully all the remaining tests will now pass.; ```; _________________________ Tests.test_unicode_ordering __________________________; [gw1] linux -- Python 3.7.12 /usr/bin/python3. self = <test.hail.table.test_table.Tests testMethod=test_unicode_ordering>. def test_unicode_ordering(self):; a = hl.literal([""é"", ""e""]); ht = hl.utils.range_table(1, 1); ht = ht.annotate(fd=hl.sorted(a)); > assert ht.fd.collect()[0] == [""e"", ""é""]; E AssertionError: assert ['?', 'e'] == ['e', 'é']; E At index 0 diff: '?' != 'e'; E Full diff:; E - ['e', 'é']; E + ['?', 'e']. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1034968369:20,reliab,reliably,20,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1034968369,1,['reliab'],['reliably']
Availability,"Huh, checkpointing `sample.vcf` after the filter clears up the issue. This may be something to do with the result of import_vcf being used directly with `_same`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500:5,checkpoint,checkpointing,5,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500,1,['checkpoint'],['checkpointing']
Availability,Huh. I added you back to broad-ctsa. I'm curious to nail down the requester pays issues. Mmm. The billing monitoring is a bit of a mess. The BQ table should really be in hail-vdc and terraform should be able to create the necessary billing sinks to dump billing data into BQ. I'm not exactly sure what the equivalent tools are in Azure to get billing information.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794:57,down,down,57,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794,1,['down'],['down']
Availability,"Huh. Well, this is a terrible error message, but the short answer is that Hail doesn't support reading directly from an HTTP(S) server. You can either download that file or use a dataset that is available in a cloud storage bucket. In general, you'll want to convert to Hail's native MatrixTable format before you do further analysis anyway. I'll fix this to give a more reasonable error message, but, in general, not all HTTP(S) servers support the Range header which means Hail can't efficiently read from all HTTP(S) servers. If you're looking for public datasets to experiment with, I strongly recommend using the Dense Hail MatrixTable of the HGDP+1KG dataset hosted for free by the three major clouds https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614,5,"['avail', 'down', 'error']","['available', 'download', 'downloads', 'error']"
Availability,I accidentally copy pasted the line. If you look a few lines down you can see the actual name/value pair for that environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13628:61,down,down,61,https://hail.is,https://github.com/hail-is/hail/pull/13628,1,['down'],['down']
Availability,I accidentally introduced this in #10172 while removing a templating error that inadvertently masked this problem.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10239#issuecomment-809615637:69,error,error,69,https://hail.is,https://github.com/hail-is/hail/pull/10239#issuecomment-809615637,2,"['error', 'mask']","['error', 'masked']"
Availability,"I accidentally passed a list instead of a string as the hb.Batch name and got this error; ```; Traceback (most recent call last):; File ""outrider_batch_pipeline.py"", line 216, in <module>; main(); File ""outrider_batch_pipeline.py"", line 212, in main; logger.info(f""Output: {output_file}""); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 119, in __exit__; next(self.gen); File ""/Users/weisburd/code/methods/batch/batch_utils.py"", line 66, in run_batch; batch.run(dry_run=args.dry_run, verbose=args.verbose); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/batch.py"", line 423, in run; return self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 435, in _run; bc_batch = bc_batch.submit(disable_progress_bar=disable_progress_bar); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 167, in submit; async_batch = async_to_blocking(self._async_builder.submit(*args, **kwargs)); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/lib/python3.7/site-packages/nest_asyncio.py"", line 63, in run_until_complete; return self._run_until_complete_orig(future); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 492, in submit; batch = aw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9050:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/9050,1,['error'],['error']
Availability,"I added a cheatsheets page to the docs. It has a link to the single current cheat sheet. I just used the fact that it's already on github to create a download link, if that's bad practice I'm happy to do something else.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7570:150,down,download,150,https://hail.is,https://github.com/hail-is/hail/pull/7570,1,['down'],['download']
Availability,I added an optimization so we prefer the default location if we haven't had many failures creating instances. This will resolve my concerns about unnecessarily charging users more with this change without giving them a way to prefer a cheaper region.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342:81,failure,failures,81,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342,1,['failure'],['failures']
Availability,I added the MatrixRead as an input and changed range to take into account the dropRows and dropCols push down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3487#issuecomment-386750335:105,down,down,105,https://hail.is,https://github.com/hail-is/hail/pull/3487#issuecomment-386750335,1,['down'],['down']
Availability,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:576,error,error,576,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['error'],['error']
Availability,"I added the error for each container to the logs output. I think that's fine, but maybe we need separate section for errors?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10545#issuecomment-853153867:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/pull/10545#issuecomment-853153867,2,['error'],"['error', 'errors']"
Availability,"I addressed comments apart from improving the tests on VSM. There are two options regarding plan for writing out a Spark IRM:; 1) just delete it; 2) keep it, pass partStarts through for efficiency, and cut down on code duplication. I tried the latter, creating KeyedIndexedRowMatrix as abstraction to handle both PCA and writing, and pushing common structure to an object WriteBlocksRDD.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2559#issuecomment-351252274:206,down,down,206,https://hail.is,https://github.com/hail-is/hail/pull/2559#issuecomment-351252274,1,['down'],['down']
Availability,"I addressed most of your comments. I also fixed the `downcastToPK` problem by getting rid of it, instead adding a `KeyedOrderedRVD` which has a join key in addition to an ordering key.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3159#issuecomment-374660245:53,down,downcastToPK,53,https://hail.is,https://github.com/hail-is/hail/pull/3159#issuecomment-374660245,1,['down'],['downcastToPK']
Availability,"I agree this is the most compelling critique. I think the Pythonistas would make two points: a) KeyError is the one specific error you get in this case and b) it should be written like this:; ```python3; try:; persisted_bm = self._persisted_locations[bm]; except KeyError as err:; raise ValueError(f'{bm} is not persisted') from err; persisted_bm.__exit__(None, None, None); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864#issuecomment-1516511437:125,error,error,125,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516511437,1,['error'],['error']
Availability,"I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:. > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to untitled-folder. That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482:556,error,error,556,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482,1,['error'],['error']
Availability,"I agree. And parsing strings to figure out what's going on is insanity. Here's a middle-ground: retry unknown 500 errors once. If they really are rare, they won't reoccur (p^2 very small).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783#issuecomment-568585850:114,error,errors,114,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568585850,1,['error'],['errors']
Availability,"I agree. Rate limiting is an expected scenario, not really an error. Do you know by chance if all the rate limiting you're seeing are in fact 429s? I hope that covers most scenarios, but I feel like I've seen some rate limiting use other more generic http error codes, or just reset the connection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14595#issuecomment-2200997075:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/issues/14595#issuecomment-2200997075,2,['error'],['error']
Availability,I also added SocketException and SSLException as transient errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9404:59,error,errors,59,https://hail.is,https://github.com/hail-is/hail/pull/9404,1,['error'],['errors']
Availability,"I also added `batch/ignored-pylint-errors` which we diff the output of pylint against. If pylint differs from the `ignored-pylint-errors`, then either we fixed an ignored error or we added a new unignored error. In either case, the developer should address the issue by changing the code or the `ignored-pylint-errors` file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4789:35,error,errors,35,https://hail.is,https://github.com/hail-is/hail/pull/4789,5,['error'],"['error', 'errors']"
Availability,I also checked and made sure the `$(dir)` function was available on make 3.81.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6940#issuecomment-524914202:55,avail,available,55,https://hail.is,https://github.com/hail-is/hail/pull/6940#issuecomment-524914202,1,['avail'],['available']
Availability,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:120,error,errors,120,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115,1,['error'],['errors']
Availability,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045:112,down,down,112,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045,2,['down'],['down']
Availability,I also switched the `api` argument to `BatchClient` to `default_api` rather than `None`. I wonder if that was somehow creating a muffled error message. It should have erred as soon as someone tried to make a request with `api=None`. Not sure why it looped forever instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373:137,error,error,137,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373,1,['error'],['error']
Availability,"I am a fresh user for hail.; I try this command ""hail importannotations table variantAnnotations.alternateformat.tsv -e Variant --impute write -o consequences.vds"", but I received an error message as follow ""hail: fatal: importannotations table: parse error: ""-e"" is not a valid option"", why?; additionally, I can not find the corresponding test file in the test file of hail download from here and it is really very inconvenient for me to test it!; thanks a lot!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561:183,error,error,183,https://hail.is,https://github.com/hail-is/hail/issues/561,3,"['down', 'error']","['download', 'error']"
Availability,I am convinced this is actually just a bad error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326,1,['error'],['error']
Availability,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/1807,1,['error'],['error']
Availability,I am having the same error on Mac OS 10.12.4 with gcc 4.9.3. How do I move over to Xcode cc?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-295648585:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-295648585,1,['error'],['error']
Availability,I am having this problem when writing to `HDFS` with `write.parquet()` function.; ```java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN.```. Spark 1.6.2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-319521657:175,error,error,175,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319521657,1,['error'],['error']
Availability,"I am not really sure why $? is not set to 1 if these parentheses are absent. Nonetheless,; this fixes CI to actually complain about mypy failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9335:137,failure,failures,137,https://hail.is,https://github.com/hail-is/hail/pull/9335,1,['failure'],['failures']
Availability,"I am not sure what it should look like, but right now trying to show a table with no row fields causes an error. To replicate:. ```; hl.utils.range_table(10).key_by().select().show(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7545:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/issues/7545,1,['error'],['error']
Availability,"I am not sure what persist should mean in the service backend. For some linear; algebra work, persist appears to be used to store the results of an expensive; query. In that setting, we should clearly checkpoint the dataset. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11936:201,checkpoint,checkpoint,201,https://hail.is,https://github.com/hail-is/hail/pull/11936,1,['checkpoint'],['checkpoint']
Availability,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831:317,error,error,317,https://hail.is,https://github.com/hail-is/hail/issues/10831,2,['error'],['error']
Availability,I am so glad I added those requester pays tests. They changed the exception type for requester pays failures and that broke our try-catch. The requester pays situation in GCP is so harebrained.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12555#issuecomment-1397345833:100,failure,failures,100,https://hail.is,https://github.com/hail-is/hail/pull/12555#issuecomment-1397345833,1,['failure'],['failures']
Availability,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354,1,['error'],['error']
Availability,"I am testing hail build on spark3 (v0.2.89, spark 3.1.2) and getting the following error with jinja2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/11705,6,"['Down', 'error']","['Downgrading', 'error']"
Availability,"I am trying to convert the matrix table output from `vcf_combiner `to vcf format. ; I get the following error on running `export_vcf `: ; ```; Hail version: 0.2.64-1ef70187dc78; Error summary: HailException: Invalid type for format field 'gvcf_info'. Found 'struct{BaseQRankSum: float64, ExcessHet: float64, InbreedingCoeff: float64, MLEAC: array<int32>, MLEAF: array<float64>, MQRankSum: float64, RAW_MQandDP: array<int32>, ReadPosRankSum: float64}'.; ```; Description of the matrix table : ; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'LA': array<int32>; 'LGT': call; 'LAD': array<int32>; 'LPGT': call; 'LPL': array<int32>; 'RGQ': int32; 'END': int32; 'gvcf_info': struct {; BaseQRankSum: float64, ; ExcessHet: float64, ; InbreedingCoeff: float64, ; MLEAC: array<int32>, ; MLEAF: array<float64>, ; MQRankSum: float64, ; RAW_MQandDP: array<int32>, ; ReadPosRankSum: float64; }; 'DP': int32; 'GP': array<float64>; 'GQ': int32; 'MIN_DP': int32; 'PG': array<float64>; 'PID': str; 'PS': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; ```; The gVCF was produced by GATK4.2 dragen-mode.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10644:104,error,error,104,https://hail.is,https://github.com/hail-is/hail/issues/10644,2,"['Error', 'error']","['Error', 'error']"
Availability,"I am using Hail 0.2.54. However, I also tested with the latest build.gradle file. I run the following make install command:; `make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.2`. However, I got this error message which did not appear before. ` > Could not resolve org.scalanlp:breeze-natives_2.11:+.; Required by:; project :; > Failed to list versions for org.scalanlp:breeze-natives_2.11.; > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error; * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.; * Get more help at https://help.gradle.org. BUILD FAILED in 29s; make: *** [build/libs/hail-all-spark.jar] Error 1`. It seems that is caused by https://repo.hortonworks.com/content/repositories/releases/ server is done.; I am wondering whether there is any maven substitute can be used temporarily to compile hail.jar?. Thanks in advance for your help.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419:226,error,error,226,https://hail.is,https://github.com/hail-is/hail/issues/9419,3,"['Error', 'error']","['Error', 'error']"
Availability,"I apologize for not responding sooner to this. I've been mulling over what to do here as it's been over 4 years since I wrote the first interface. I think your changes are fine, but I need to go through the tests again and figure out what `_mentioned` was originally intended for to make sure this change doesn't break anything subtle. I'm going to have our CI run this SHA so I can see what the failures are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953:396,failure,failures,396,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953,1,['failure'],['failures']
Availability,"I argue `fatalIf(p, msg)` is less readable than `if (p) fatal(msg)` and not any shorter. It also causes an error since you can't control which fatal to call, e.g., `Utils.fatal` vs `Line.fatal`. @tpoterba Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/262:107,error,error,107,https://hail.is,https://github.com/hail-is/hail/issues/262,1,['error'],['error']
Availability,I assume the local file reads are somehow more tolerant to being closed? I don't know why this doesn't fail like every single matrix read test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3512:47,toler,tolerant,47,https://hail.is,https://github.com/hail-is/hail/pull/3512,1,['toler'],['tolerant']
Availability,I believe I've addressed all of your comments now. The test failure is some spurious batch thing. I'll rerun when it's approved.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339:60,failure,failure,60,https://hail.is,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339,1,['failure'],['failure']
Availability,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275,1,['error'],['error']
Availability,"I believe that regardless of chunking, the total body of the request is not to exceed aiohttp's [client_max_size](https://docs.aiohttp.org/en/stable/web_reference.html?highlight=client_max_size#application). Internal-gateway will stream transparently to aiohttp which will buffer the chunks [here](https://github.com/hail-is/hail/blob/235d2bcba1d4594a27a3dea6947c91cc4043de72/memory/memory/memory.py#L61) and blow up. It could be that adding chunking will help java to better see the 429 instead of erroring with `Connection closed` or hanging, but that is just wishful thinking, it could be happening at a deeper level of the network stack.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12691#issuecomment-1432381359:499,error,erroring,499,https://hail.is,https://github.com/hail-is/hail/pull/12691#issuecomment-1432381359,1,['error'],['erroring']
Availability,"I believe the code is behaving as designed. The error message, however, leaves a lot to be desired. A few take-aways:; 1. Hail doesn't support heterogeneous arrays. In situations like these, using an array of tuples has the desired outcome.; 2. The variable `x` in `lambda x:` is already a hail expression and so you don't need to explictly capture it as a `literal`.; a. While support for using hail expressions with `literal` was added in https://github.com/hail-is/hail/pull/4086 (see the issue for motivation), it can only be used when that expression is self-contained (ie it's not dependent on another hail expression, eg referencing an element of a hail array expression or tuple expression etc).; b. Our evaluation strategy is to `eval` the expression, then broadcast the result in a `literal`.; c. `eval` correctly complains that that expression has free variables and so can't be evaluated.; d. This error is ugly and has little to do with what the user wanted to achieve. Off the top of my head, a couple of ways to proceed:; 1. The hardest (but backwards compatible) fix is to somehow provide a good error message that the `x` in `lambda x:` in this particular context is a hail expression containing a reference that you should not use with `literal`.; 2. Remove support for using hail expressions with literal.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413,3,['error'],['error']
Availability,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:1001,down,down,1001,https://hail.is,https://github.com/hail-is/hail/issues/5564,1,['down'],['down']
Availability,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241806708:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708,1,['error'],['error']
Availability,"I can write an RFC for how to do this with regards to billing updates and the database. I don't think it's too difficult, but it will take a bit of work to add some new metadata that says whether a resources is `by_time` or `by_unit` and compute usage accordingly per billing update. If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. If we have to track by IP address, I don't know how to do that and would have to look into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482:328,down,downloaded,328,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482,1,['down'],['downloaded']
Availability,"I can't figure out why I'm getting an error in one test. But I also am not sure what to do with the `/batches` endpoint. I want the UI default to only show you your batches with the default query string 'user:jigold`. However, what should the REST endpoint be? All batches in all billing projects you have access for?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101,1,['error'],['error']
Availability,"I can't figure out why my out of memory test isn't working. It's reporting exit code 0 and no out of memory error even though when I do the same thing locally on my computer with docker run or on an instance in the cluster, I get exit code 137 and out of memory. I'm limiting the docker run command to the same amount of bytes that the docker command in the worker should be limiting it to (looked at the docker output in the worker logs). I think the next step is to try using curl to run docker containers rather than the docker cli.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216,1,['error'],['error']
Availability,"I can't make create idempotent, it returns a fresh batch id. I did make `jobs/create` idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I only retry transient errors, but I allow the user to say they don't want that. A missing internet connection is apparently considered ""transient"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574394825:241,error,errors,241,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574394825,1,['error'],['errors']
Availability,I cannot enable these tests because both the local and service backend fail due to this error:. ```; E Java stack trace:; E java.lang.AssertionError: assertion failed:; E ir key: [Ljava.lang.String;@28f4484b; E lowered key: WrappedArray(); E 	at scala.Predef$.assert(Predef.scala:223); E 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:1101); E 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1118); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:67); E 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:36); E 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:16); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:75); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); E 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:70); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:88); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:124); E 	at is.hail.backend.local.LocalBac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10379:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/pull/10379,1,['error'],['error']
Availability,I can’t help you without an error message or description of what didn’t work. I recommend waiting for the next release which should come out today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992,1,['error'],['error']
Availability,I changed PCA and toIndexRowMatrix to take a field. Now these all use select_entries so no need to analyze keys or process joins. But I still check that the expression has a matrix table source with good error message using `matrix_table_source`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353,1,['error'],['error']
Availability,"I changed ci.hail.is to point to kubernetes, so this won't work any more. The new web site is ready to go (live at test.hail.is) and I will switch it over hail.is over late tonight. It needs to go down for a short while to get new Let's Encrypt credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4399:197,down,down,197,https://hail.is,https://github.com/hail-is/hail/pull/4399,1,['down'],['down']
Availability,I changed my PR so the tests will run. I narrowed down the change in behavior occurs when removing just the commit #3426. Should we look into this further?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3417#issuecomment-385512355:50,down,down,50,https://hail.is,https://github.com/hail-is/hail/pull/3417#issuecomment-385512355,1,['down'],['down']
Availability,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962:591,checkpoint,checkpoint,591,https://hail.is,https://github.com/hail-is/hail/pull/7962,1,['checkpoint'],['checkpoint']
Availability,"I changed the block_size to default (4096) because a 1024 means 4x the number of open files per task for `BlockMatrix.write_from_entry_expr`. On UKBB, that's about 500 open files, each with its own write buffer (64MB on by default on GCP) so Hadoop fails spectacularly. Liam reports that `ld_prune` succeeds with 50k samples (and ~280k markers), and succeeds but with lots of failures with 75k samples, and fails completely with 100k samples. He just tried again with 459k samples reducing the write buffer size to 1MB, and it still failed there, but I'm hopeful the larger block size will help. We'll try 100k again soon.; ` --properties 'core:fs.gs.io.buffersize.write=1048576,core:fs.gs.io.buffersize=8388608' \`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-393889443:376,failure,failures,376,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-393889443,1,['failure'],['failures']
Availability,"I checked the [logs for this PR](https://console.cloud.google.com/logs/query;query=logName:%22worker%22%0Alabels.namespace:%22pr-10467%22;timeRange=PT6H;cursorTimestamp=2021-05-07T19:35:43.101282634Z?project=hail-vdc&folder=true&organizationId=548622027621&query=%0A), both normal logs and just ERRORs and didn't see anything abnormal. If there's a particular stress test you'd like me to try out I'm happy to test it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834787593:295,ERROR,ERRORs,295,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834787593,1,['ERROR'],['ERRORs']
Availability,I checked the database and was surprised to see the SKUs weren't necessarily unique to a specific region. But it makes sense when I looked at their API here: https://cloud.google.com/billing/docs/reference/rest/v1/services.skus/list#sku. I think we should put this in and address what happens if they change the SKU of a particular region if that occurs in the future. We'll just get a bunch of error messages with no price updates and it shouldn't impact users. ~~I will also manually check this in Azure.~~ I checked in both GCP and Azure and the updates looked fine with no errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597:395,error,error,395,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597,2,['error'],"['error', 'errors']"
Availability,I checked the test failure should be fixed by #8131,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656,1,['failure'],['failure']
Availability,I commented on Zulip about how to make this error the same for every backend. I think it should be a simple change to use `parallelizeAndComputeWithIndex`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242,1,['error'],['error']
Availability,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:319,down,download,319,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,4,['down'],['download']
Availability,"I copied all the secrets from batch-pods into default that (1) didn't already exist (by name) in default, and (2) weren't k8s service account tokens (which are batch-pods specific). I also fixed the remaining test failures so this should be ready to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692:214,failure,failures,214,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692,1,['failure'],['failures']
Availability,I couldn't replicate the error locally. It seems to be transient because at least one of the CI builds succeeded.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1834495397:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1834495397,1,['error'],['error']
Availability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:164,error,errors,164,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,2,['error'],"['error', 'errors']"
Availability,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7647:235,error,error,235,https://hail.is,https://github.com/hail-is/hail/issues/7647,4,"['Error', 'error']","['Error', 'ErrorHandling', 'error']"
Availability,"I created this bug when I added exception handling. I incorrectly put the exception handling *after* the `rmdir`. If an exception occurred while removing one of the children of a directory, in all likelihood, the directory is non-empty. With this change, we raise any exceptions before unlinking the directory. It is the responsibility of the caller to decide what to do if we were unable to remove one of our children. I saw this while debugging another transient error on another PR: https://github.com/hail-is/hail/issues/13361.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13362:465,error,error,465,https://hail.is,https://github.com/hail-is/hail/pull/13362,1,['error'],['error']
Availability,"I decided to break off this chunk from another PR that has stalled. That PR will ultimately build on this to add all developers automatically to dev AND test namespaces, but this should be an improvement for now. A few things in here:. - Deleted all the `DatabaseResource` stuff in the auth driver. Since databases now are created and destroyed with the namespace and not the developer, this is basically dead code.; - Added the ability to add a user for an existing hail identity. This is only permitted in dev namespaces and serves as a way for developers to use the same hail identity across namespaces. There is one caveat here: `create_initial_account.py` tries to copy the `<dev-name>-gsa-key` secret from default into the developer namespace and this code will *not* do that anymore. For the developer to submit jobs to the namespace, they must first manually copy in the secret from `default` if it does not already exist inside the namespace. This is awkward, but IMO acceptable because:; - the copying code in `create_initial_account.py` is already broken anyway because when that script is run in a dev deploy it does not have access to production secrets; - I hope that when we eventually go keyless we can delete the gsa key secrets and this whole problem goes away.; - I feel like it's not too bad to do this manual one time copy as opposed to maintaining code that is privileged enough to reach across namespaces. Seems error prone and like a security headache.; - Deletes `create_initial_account.py` in favor of using our actual API to create the dev user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13180:1435,error,error,1435,https://hail.is,https://github.com/hail-is/hail/pull/13180,1,['error'],['error']
Availability,I dev deployed this and it still is working fine. It's still pretty slow and I was getting rate limit exceeded errors still trying to attch/detach 64 disks. Average operation time was still 15 seconds. I think part of the problem might be the delay starts at 0.1 for retry_transient_errors. We can consider making this a parameter and setting it to a higher number for this use case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838:111,error,errors,111,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838,1,['error'],['errors']
Availability,"I did debug this, though. The failing test (which succeeds in Spark, but fails in local backend) collects a table and asserts that the result is equal to a list of expected rows. The failure is caused by an ordering issue - the rows are the same, but the order is slightly different between the Spark backend (which produces the expected output) and the local backend. However, I think that actually *both* orders are valid under Hail's guarantees. I'll bring this to our next team meeting for group discussion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408:183,failure,failure,183,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408,1,['failure'],['failure']
Availability,"I did make sure it renders as I intended, and the round trip test means it produces valid type grammar. but I'm hesitant to add a test for exact characters, since if we want to change spacing or something cosmetic then we have to change the test. I feel the same way about assertRaisesRegex checks -- we should be able to make error messages nicer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849:327,error,error,327,https://hail.is,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849,1,['error'],['error']
Availability,"I did the following ; ```; import is.hail._; val hc = HailContext(sc); ```. When I did so, I got this error message . ```; java.util.NoSuchElementException: spark.hadoop.io.compression.codecs; at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:235); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.SparkConf.get(SparkConf.scala:235); at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:94); at is.hail.HailContext$.apply(HailContext.scala:171); ... 50 elided; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1447:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/issues/1447,1,['error'],['error']
Availability,I did try it out and the error messages are WAY better!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845,1,['error'],['error']
Availability,"I do not fully understand the Azure git tagging scheme, but [this commit](https://github.com/Azure/azure-sdk-for-java/commit/054df3fb74098f4ee30eeb1df70df1e40438d169) appears to have made `close` idempotent. It was merged in June of 2022. That commit resolved [an issue](https://github.com/Azure/azure-sdk-for-java/issues/24782) reporting an error very similar to our own. All the azure version changes update the Azure packages to their latest versions as of 2023-05-09 1713 ET. Fixes #12976",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13018:342,error,error,342,https://hail.is,https://github.com/hail-is/hail/pull/13018,1,['error'],['error']
Availability,"I do not fully understand why, but I was getting errors about df.columns not accepting string indexes:. ```; IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices. ```. I think `df.columns` is not intended to let us get the Series for a column from the DataFrame. I think we are supposed to use `df` directly for this purpose, which is what I do here. I also noticed an unfortunate issue where `''` is false-y so it is overlooked by the `or` in favor of the default value. It would be nice to have some kind of lazy `None` coalescing operator in Python :/.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12337:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/12337,1,['error'],['errors']
Availability,"I do not fully understand why, but pytest sometimes imports Hail in threads; other than the main thread. Asyncio raises an error if you try to start an; event loop in a non-main thread. This PR only runs nest_asyncio if there is already a running event loop. If; there is no running event loop, we do not need to run nest_asyncio anyway! If; there is a running event loop, we can modify it to be nest-able even from; a non-main thread.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10364:123,error,error,123,https://hail.is,https://github.com/hail-is/hail/pull/10364,1,['error'],['error']
Availability,"I do not know why but /etc triggers errors about:; ```; archive/tar: write too long; ```; Even though /etc is not very large (1.4MB). I suspect there is some symlink; or other nonsense which is breaking Kaniko. The solution, after much trial and error, was simple: copy over directories that do not; cause issues and copy only the necessary files out of etc. A mix of speculation and; binary search lead me to the conclusion that /etc/ld.so.* are the only files necessary; from /etc for python to run correctly. These files tell the kernel how to link python3.7; to the various libraries on which it depends (which live in lib and lib64). Anyway, I've tested that this image can build itself, so it should be good enough for; our purposes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10399:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/10399,2,['error'],"['error', 'errors']"
Availability,"I do not know why the retries setting in pip.conf did not catch https://ci.hail.is/batches/167314/jobs/27, but more retries never hurt anyone. Another CI-related PR. This one changes the base image of everything else: hail-ubuntu. It's an ubuntu image with two scripts that make pip and apt more resilient. Take a look at docker/hail-ubuntu.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906:296,resilien,resilient,296,https://hail.is,https://github.com/hail-is/hail/pull/9906,1,['resilien'],['resilient']
Availability,"I do not think we frequently get errors in `storage.reader`, but I think `storage.writer` was always flaky and we were protected by the `retryTransientErrors` on `createNoCompression`. My change to fix requester pays delayed the error until either the first `write` or the `close` which do not have a `retryTransientErrors` (and it is not obvious to me that it is safe to retry a `flush`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12868:33,error,errors,33,https://hail.is,https://github.com/hail-is/hail/pull/12868,2,['error'],"['error', 'errors']"
Availability,"I don't believe I have access to look at the test failures. If you let me know what failed, I'll do my best to fix it!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490:50,failure,failures,50,https://hail.is,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490,1,['failure'],['failures']
Availability,"I don't disagree. However, we need toString on (scalar) Type because they are used for error messages all over. MatrixTable used to have a bunch of separate types, now it just has a MatrixType. I think some error messages could now use the matrix type. Python also has some matrix type printing logic, these should probably get unified. Once I have printing for the user, it seemed easier to write a (admittedly small) parser than a separate to/from JSON. I admit, apart from user error messages, JSON is natural since that's what we're storing in the metadata file. Do you have a concrete suggestion? I'm not sure quite what to do that's better than this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473:87,error,error,87,https://hail.is,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473,3,['error'],['error']
Availability,"I don't know how to fix the deadlock errors. But for context, the next change Daniel had in mind is to have a table with the n_jobs counts (n_pending, n_failed, etc.) as well as time_completed and then use aggregation queries when the batch state is needed in the Python services. The time the batch is completed is just taken as the latest timestamp from the tokenized table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039620995:37,error,errors,37,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039620995,1,['error'],['errors']
Availability,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362,1,['error'],['errors']
Availability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:170,fault,fault,170,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,1,['fault'],['fault']
Availability,"I don't really put this in the same category as other external libraries. It's just bindings to the C API that `iptables` itself uses. In general, I think we should prefer just directly calling C libraries instead of shelling out and parsing strings, but I doubt this will have a major impact on worker speed. I worry a bit about the reliability of parsing text, but `iptables` seems like the sort of core utility that would be machine-parsable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12542#issuecomment-1371317794:334,reliab,reliability,334,https://hail.is,https://github.com/hail-is/hail/pull/12542#issuecomment-1371317794,1,['reliab'],['reliability']
Availability,I don't really understand the failure. Seems like the stack trace is missing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4243#issuecomment-417655757:30,failure,failure,30,https://hail.is,https://github.com/hail-is/hail/pull/4243#issuecomment-417655757,1,['failure'],['failure']
Availability,"I don't recall why that `isPrimitive` was added to be honest. I remember sitting down with you to write `checkedConvertFrom` and we decided we needed it then, but it needs to go away and be dealt with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-663024852:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663024852,1,['down'],['down']
Availability,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:182,error,error,182,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290,2,['error'],"['error', 'errors']"
Availability,"I don't think I addressed all of your comments, but I'd like you to take another look. I rewrote the `io.py` component and fixed the parallelism and context managers. I also figure out where to write files to based on the checkpoint_dir argument. We need to `results.checkpoint()` at the end because I decided to treat the raw SAIGE output as temp files (or checkpoints) and the results as a hail table is the thing people want to save to an output directory. I can make the output directory optional as well and not checkpoint that. The issue is that if we write the raw SAIGE results to a temp dir, when the context manager exits, the results HailTable won't be able to read the data once it's returned back to the user.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13804#issuecomment-1779917553:267,checkpoint,checkpoint,267,https://hail.is,https://github.com/hail-is/hail/pull/13804#issuecomment-1779917553,3,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,I don't think it is used anymore. Builds are failing because it is returning 500. > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9418:548,Error,Error,548,https://hail.is,https://github.com/hail-is/hail/pull/9418,1,['Error'],['Error']
Availability,"I don't think it makes sense to check explicitly for ""Chromosome"" and not impute that as `Int`. I also don't particularly like implicitly converting `Int` to `String` in the variant constructor -- this could lead to much more indecipherable errors like ""NumberFormatException: cannot convert 'X' to Int in column Chr"". I'm not sure what to do here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520#issuecomment-236406850:241,error,errors,241,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236406850,1,['error'],['errors']
Availability,"I don't think so. The change is clearly fixes an issue and is an improvement. That said, write failures are rare and I just want to flush out any other rare errors so the tests are reliable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666:95,failure,failures,95,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666,3,"['error', 'failure', 'reliab']","['errors', 'failures', 'reliable']"
Availability,"I don't think the browser error should pose a problem, it just failed to open a browser. Jupyter should still be running. I run it with `--no-browser`. I'm not sure how Dan's image managed that without the option, I will investigate. You can't connect to my image because it is binding localhost. You can pass `--ip 0.0.0.0` or I can figure out how Dan's image is doing it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460095986:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460095986,1,['error'],['error']
Availability,"I don't think this is going to get fixed especially soon. There's not an easy way to warn / error given our current infrastructure. If you can convince Laurent to PR a fix for downcode to work when PLs are missing (which is reasonable, I think) that's probably the best short-term bet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458,2,"['down', 'error']","['downcode', 'error']"
Availability,"I don't think this is the correct solution. As you say, we're incorrectly conflating null to mean two things: the value is legitimately null and it has been filtered out. I see two ways to fix this correctly:. - Values being aggregated indicate having been filtered by making them Option[Any] or storing an extra boolean somewhere to indicate they've been filtered out. This has the downside of more allocations.; - Make value being aggregated an Iterator. This makes aggregables pull-based rather than push-based. It allows aggregators to terminate early (e.g. take(5)). On the other hand, it means the genotype stream will be decoded multiple times, once for each aggregator. That's slow. I think the solution is to unpack the genotype stream into an array before running the aggregators. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1209#issuecomment-268822041:383,down,downside,383,https://hail.is,https://github.com/hail-is/hail/pull/1209#issuecomment-268822041,1,['down'],['downside']
Availability,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:59,failure,failures,59,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506,3,"['FAILURE', 'echo', 'failure']","['FAILURES', 'echo', 'failures']"
Availability,"I don't understand why the Python tests are failing yet. They are failing on master as well, and at least some of the errors are identical (NullPointerException in HailContext.apply). Scala tests are passing. Wondering if this is due to py4j version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123:118,error,errors,118,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123,1,['error'],['errors']
Availability,I downloaded from the artifact url ; ```; gsutil cp gs://hail-ci-nt3qc/build/ec2176d5842192a57afea55fe102e32c/www.tar.gz .; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789#issuecomment-517703256:2,down,downloaded,2,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-517703256,1,['down'],['downloaded']
Availability,"I encountered an error in an external package when I used a Hail-generated pandas data frame, which is due to an unsupported dtype [`pandas.StringDtype`](https://pandas.pydata.org/docs/reference/api/pandas.StringDtype.html#pandas.StringDtype).; https://github.com/biocore-ntnu/pyranges/pull/264. Given it's still experimental in pandas, can we have an option to generate a data frame that have `dtype=object` string columns? or maybe, we should make `dtype=object` default.; https://github.com/hail-is/hail/blob/c4b09953f62cea090c8ab2026bc81851b9f4d64a/hail/python/hail/table.py#L3345-L3346",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11738:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/11738,1,['error'],['error']
Availability,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896,1,['error'],['error']
Availability,I feel like rebinding an argument should be a syntax error in scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6984#issuecomment-527605115:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/6984#issuecomment-527605115,1,['error'],['error']
Availability,"I figure we should keep this up to date with our supported python images. AFAIK, we currently only use the mirror of python:3.7. We don't expose any of them as publicly available images, but perhaps we should due to dockerhub rate limits? I suppose that's a question for another day. We probably want to base our python-dill images on these so that docker hub can't just force push a new version of a tag and break our builds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12255:169,avail,available,169,https://hail.is,https://github.com/hail-is/hail/pull/12255,1,['avail'],['available']
Availability,"I find it compelling that this fixed the downloads. But I'd also like to understand why this changed worked. I will approve it to see if it unblocks Lindo while we keep discussing. > We needed to await cancelled tasks to handle the error that was raised inside the task. Right. We want to cancel the task and wait for it to finish, but we don't want any exceptions to be raised out. Your code appears to do that, but so does the previous code. Nothing in the documentation for `asyncio.wait` indicates it will raise exceptions: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait. I also tested a short example:. ```; import asyncio; import sys. async def foo():; try:; print('A'); await asyncio.sleep(5); print('B'); return 5; finally:; print(sys.exc_info()). async def async_main():; print('creating task...'); t = asyncio.ensure_future(foo()); # wait for foo to sleep; await asyncio.sleep(1). # cancel foo in sleep; print('cancelling task...'); t.cancel(). print('waiting for task...'); await asyncio.wait([t]). print('done.'). asyncio.run(async_main()); ```. which prints:. ```; $ python3 foo.py; creating task...; A; cancelling task...; waiting for task...; (<class 'concurrent.futures._base.CancelledError'>, CancelledError(), <traceback object at 0x7f8cdef65e10>); done.; ```. The task is cancelled, and CancelledError is raised, but not propagated out. > 75% of his jobs would fail with this error. I'm actually confused where the cancellation error is coming from in the first place. If the code you're changing is the issue (and I think it is, too) then we only cancel if some other exception was raised, either by a task or in `__aexit__`. What's that exception? Can we print it out (enable more logging) in your test setup?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655:41,down,downloads,41,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655,4,"['down', 'error']","['downloads', 'error']"
Availability,"I fixed some of these with hacks that defer errors beyond the MatrixReader/TableReader constructors, but some remain (native reader, VCF, etc)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5030#issuecomment-462493288:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/issues/5030#issuecomment-462493288,1,['error'],['errors']
Availability,"I fixed the SQL query. I had to use a feature available in MySQL v8.0.14 and later called lateral joins. I believe what this does is it scans the first table for rows that match, then scans the second lateral table for that matching row and applies the filter. So basically we have an iterator of the two tables joined together rather than a realized temporary table with the results of the entire subquery.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748#issuecomment-1583061748:46,avail,available,46,https://hail.is,https://github.com/hail-is/hail/pull/12748#issuecomment-1583061748,1,['avail'],['available']
Availability,I fixed the client not closed errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684971261:30,error,errors,30,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684971261,1,['error'],['errors']
Availability,"I fixed the part where the behavior of locus_windows was changed, and now the behavior should be consistent with the previous version. (Some of the error types were changed, but I don't really see that as a breaking interface change).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260:148,error,error,148,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260,1,['error'],['error']
Availability,I followed the rabbit hole form https://github.com/hail-is/hail/pull/7922 and was a bit concerned that we weren't verifying the stream met our expectations. I don't think we need to handle errors more gracefully (these assertions should only fail on malformed files).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7932:189,error,errors,189,https://hail.is,https://github.com/hail-is/hail/pull/7932,1,['error'],['errors']
Availability,I forgot I turned off the syslog so we won't see errors there from while the worker is running. Up to you on whether you think this change needs more scrutiny.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769117923:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769117923,1,['error'],['errors']
Availability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:828,echo,echo,828,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['echo'],['echo']
Availability,"I found that aggregating sorted arrays gives incorrect results and crashes when trying to do so without converting the array to string. This code explains what I found pretty well:. ```python; > mt = hl.balding_nichols_model(1, 10, 10). # Aggregate concatenated alleles (works fine); > mt.aggregate_rows(hl.agg.counter(hl.delimit(mt.alleles, '|'))); {'A|C': 10}. # Group by the array directly (gives an expected error); > mt.aggregate_rows(hl.agg.counter(mt.alleles)); TypeError: unhashable type: 'list'. # Aggregate sorted arrays (works but gives wrong result); > mt.aggregate_rows(hl.agg.counter(hl.delimit(hl.sorted(mt.alleles), '|'))); {'A|A|A|C|\x0b\x00\x00': 2, 'A|A|A|C|C|C': 8}. # Aggregate the sorted arrays directly (segfault); # *This should probably throw ""unhashable type list"" like it does without the sort*; mt.aggregate_rows(hl.agg.counter(hl.sorted(mt.alleles))); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/opt/conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty; ...; Py4JError: An error occurred while calling o59.executeJSON; ```. Here is the full [stack trace](https://github.com/hail-is/hail/files/4187400/stacktrace.txt) and [core dump](https://github.com/hail-is/hail/files/4187399/coredump.txt). I think some related questions that arise from this are:. 1. What's the best way to group by an array to avoid the conversion to a delimited string? In this case I could do something like ```mt.aggregate_rows(hl.agg.counter(hl.tuple([mt.alleles[0], mt.alleles[1]])))``` but I can't find a solution for getting a tuple from an array without knowing the length of it beforehand for every row. Is there a more fundamental reason why the API doesn't allow aggregation by arrays even if Spark does?; 2. When the Py4J server crashes, it's no longer reachable from the python cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076:412,error,error,412,https://hail.is,https://github.com/hail-is/hail/issues/8076,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:168,error,error,168,https://hail.is,https://github.com/hail-is/hail/issues/8029,2,"['Error', 'error']","['Error', 'error']"
Availability,I get these errors locally with mypy 0.931,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11734#issuecomment-1090453169:12,error,errors,12,https://hail.is,https://github.com/hail-is/hail/pull/11734#issuecomment-1090453169,1,['error'],['errors']
Availability,I get this error trying to install GWASTools,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080,1,['error'],['error']
Availability,"I get this error when using the last hail version built on Spark 1.6. ``` hail: annotatevariants vds: caught exception: java.lang.IllegalArgumentException: requirement failed: nPartitions = 2847, ranges = 2844; 	at scala.Predef$.require(Predef.scala:233); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner.<init>(OrderedPartitioner.scala:27); 	at org.broadinstitute.hail.sparkextras.OrderedPartitioner$.read(OrderedPartitioner.scala:110); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:185); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$8.apply(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.utils.richUtils.RichHadoopConfiguration$.readObjectFile$extension(RichHadoopConfiguration.scala:205); 	at org.broadinstitute.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:184); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$$anonfun$1.apply(Read.scala:41); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:41); 	at org.broadinstitute.hail.driver.Read$.run(Read.scala:9); 	at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:259); 	at org.broadinstitute.hail.driver.Command.run(Command.scala:264); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:61); 	at org.broadinstitute.hail.driver.AnnotateVariantsVDS$.run(AnnotateVariantsVDS.scala:9)```. however, the same commands work on the hail version ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1213:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1213,1,['error'],['error']
Availability,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641,1,['error'],['error']
Availability,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:106,Error,Error,106,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability,"I got an email saying the activity logs are no longer supported after September 30th. Here's the [migration instructions](https://cloud.google.com/compute/docs/logging/migrating-from-activity-logs-to-audit-logs#log_entry_field_mappings). I figured out how to map the fields mostly by trial and error looking at the JSON for an event. The only thing that didn't map at all was the operationType. I hardcoded that as 'insert'. There are different event_subtype names such as 'v1.compute.instances.insert' or 'beta.compute.instances.insert'. So I did what they suggested and looked for a partial match such as 'compute.instances.insert'. I can send you the full JSON for the events if you want to double check anything. I also double checked that the activity logs aren't used anywhere else in the repo, but it might be good for you to confirm that since you wrote a lot of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9439:294,error,error,294,https://hail.is,https://github.com/hail-is/hail/pull/9439,1,['error'],['error']
Availability,"I got sick of having my PRs fail due to these ""rare"" errors. This PR adds; a type of error which we will retry exactly once. Hopefully this; reduces the frequency of these errors sufficiently that we are no; longer plagued by them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11917:53,error,errors,53,https://hail.is,https://github.com/hail-is/hail/pull/11917,3,['error'],"['error', 'errors']"
Availability,I got the error message while importing VCFs in dataflow01. `hail -l /medpop/afib/schoi/projects/TOPMed/Script/log/TopMed.Chr22.QC.vds.test.log \; importvcf file:///medpop/afib/schoi/projects/TOPMed/Data/BROAD/Link/Chr22/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.fold,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['error'],['error']
Availability,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418:359,error,error,359,https://hail.is,https://github.com/hail-is/hail/pull/5418,1,['error'],['error']
Availability,I had done this a while ago in a throwaway after talking to Tim. This causes more failures than just the new test that I added. I'm at a loss for how to proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128:82,failure,failures,82,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128,1,['failure'],['failures']
Availability,I had to change the return type and override delete for Azure as they just return a status code unless there's an error. The `resp.json()` didn't work for the Azure response type.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10936:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/pull/10936,1,['error'],['error']
Availability,"I had to choose rather small upper bounds to avoid a variety of overflow errors in `Genotype`. I imagine these are practically not a problem, but I didn't take a particularly principled approach to choosing upper bounds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241846815:73,error,errors,73,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241846815,1,['error'],['errors']
Availability,I had to put the tolerations back to get it to pass. I'll remove them in a second pass after I remove the preemptible pool taint.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560172025:17,toler,tolerations,17,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560172025,1,['toler'],['tolerations']
Availability,I handled the merge conflicts and a syntax error. Hopefully we can get this merged post-haste!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1335949595:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1335949595,1,['error'],['error']
Availability,"I have a GATK-generated VCF that has been passed through vt normalize. During the vt stage, a ""NaN"" value in the VCF was changed to ""nan"", which causes write to fail:. `2016-11-07 15:13:31 ERROR Hail:101 - hail: fatal: write: file:###.vcf.bgz: variant 5:49429187:G:C,T: INFO field InbreedingCoeff:; unable to convert nan (of class java.lang.String) to Double:; caught java.lang.NumberFormatException: For input string: ""nan""; offending line: 5 49429187 rs59402528 G C,T 2.28455e+07 VQSRTrancheSNP99.90t...`. The original ""NaN"" value is processed fine. As the VCF spec is pretty quiet on exact floating point representation in VCFs, could code to handle ""nan"" be added?. Apologies for edit spam -- clumsy fingers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1065:189,ERROR,ERROR,189,https://hail.is,https://github.com/hail-is/hail/issues/1065,1,['ERROR'],['ERROR']
Availability,I have a subsequent PR that uses it in build.yaml. Using it in batch is going to take a bit more thinking because we need an image with Hail available.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11072#issuecomment-972380804:141,avail,available,141,https://hail.is,https://github.com/hail-is/hail/pull/11072#issuecomment-972380804,1,['avail'],['available']
Availability,"I have a thing today until 12:30, someone else should look at it. Quick fix: revert changes. 404 is an error anyway",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948#issuecomment-446587917:103,error,error,103,https://hail.is,https://github.com/hail-is/hail/issues/4948#issuecomment-446587917,1,['error'],['error']
Availability,I have an error in the SQL,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399#issuecomment-1672022806:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/13399#issuecomment-1672022806,1,['error'],['error']
Availability,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:152,failure,failure,152,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,1,['failure'],['failure']
Availability,"I have looked at https://github.com/agronholm/sphinx-autodoc-typehints, yes. One of the major problems with it right now is that it doesn't work with forward references, like this in matrixtable:; ```python; @typecheck_method(exprs=oneof(str, Expression),; named_exprs=expr_any); def group_rows_by(self,; *exprs: Tuple[Union[Expression, str]],; **named_exprs: NamedExprs) -> 'GroupedMatrixTable':; ```. The reason it doesn't work is the decorator. The forward reference strings are evaluated in the module of the function after the module is fully imported, and in this case the module is the module of the _decorator_, not _group_rows_by_. So GroupedMatrixTable is a name error. There are a few solutions:; - Don't use decorators anywhere. This requires a lot of work to fully port over typechecking to typecheck2, which I'm now not totally sure is even the right thing.; - A bit hacky: import `hail` in the typecheck module and use fully clarified paths in forward references.; - see if we can eagerly evaluate the hints in the typecheck module and set them on the decorated function.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3138#issuecomment-373043373:673,error,error,673,https://hail.is,https://github.com/hail-is/hail/pull/3138#issuecomment-373043373,1,['error'],['error']
Availability,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:404,Down,Downloading,404,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700,1,['Down'],['Downloading']
Availability,I have no idea how this ever worked. It should have triggered UTF-8 decoding errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10381:77,error,errors,77,https://hail.is,https://github.com/hail-is/hail/pull/10381,1,['error'],['errors']
Availability,"I have no idea what this means. ```; Caused by: is.hail.shadedazure.com.azure.storage.blob.models.BlobStorageException: If you are using a StorageSharedKeyCredential, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate method call.; If you are using a SAS token, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate generateSas method call.; Please remember to disable 'Azure-Storage-Log-String-To-Sign' before going to production as this string can potentially contain PII.; Status code 403, (empty body); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073:194,error,error,194,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073,2,['error'],['error']
Availability,I have rerun one of scripts. It ran fine without any errors. Thanks for the fix!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-698963629:53,error,errors,53,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-698963629,1,['error'],['errors']
Availability,"I have some reorganization and better error checking I want to do, but I'll accept this and make that in a separate pull request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/310#issuecomment-212577261:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/310#issuecomment-212577261,1,['error'],['error']
Availability,I have yet to successfully create a VCF that doesn't hit another error before hitting this one. But user hit this here: https://discuss.hail.is/t/assertionerror-exception/1700,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9494:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/pull/9494,1,['error'],['error']
Availability,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:95,error,errors,95,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990,4,['error'],"['error', 'errors']"
Availability,"I haven't tested this error message, as I'm not sure how to replicate the bug scenario, but I think it should work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946,1,['error'],['error']
Availability,I hope this breaks so I can use up some of that downtime budget! Totally worth it to get rid of batch-pods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9718#issuecomment-730020693:48,downtime,downtime,48,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-730020693,1,['downtime'],['downtime']
Availability,"I hope this is the last one. Instead of assert (since the catch doesn't catch assertion errors), I just use an `if`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1383:88,error,errors,88,https://hail.is,https://github.com/hail-is/hail/pull/1383,1,['error'],['errors']
Availability,"I implemented `cancelled` as a join from the batch table rather than storing redundant information for each job. @akotlar The issue was that all jobs were getting set to cancelled at the same time (and thus notifying children), so always run jobs were all getting run at once neglecting the hierarchy of job dependencies. This is the purpose of having the `cancelled` flag as separate from the `Cancelled` state and is checked in `create_if_ready`. @cseed Can you look and see if this solves the cancel problem we discussed earlier? The faulty PR was this one: https://github.com/hail-is/hail/pull/6128/files",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6341:77,redundant,redundant,77,https://hail.is,https://github.com/hail-is/hail/pull/6341,2,"['fault', 'redundant']","['faulty', 'redundant']"
Availability,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1377:131,error,error,131,https://hail.is,https://github.com/hail-is/hail/issues/1377,2,['error'],['error']
Availability,I just added a slightly more informative error message. Do you want something more than this?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9733:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/9733,1,['error'],['error']
Availability,"I just got the same error running my [slightly altered] code again, but I stupidly didn't save the error output. I'm running it again and will let you know if I encounter another error!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387561493:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387561493,3,['error'],['error']
Availability,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229752430:105,error,error,105,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430,1,['error'],['error']
Availability,"I just wanted to let you know that the issue was fixed by running python through Jupyter notebook on my Mac where the HailContext could be created without problems. I never managed to run this through the command line, but as I wanted to use the notebook, anyway, this was ok for me. . However, I encountered a problem within your tutorial with the 'data/1kg.vds', which throws a fatal error ; ``HailException: Invalid VDS: old version [4]; Recreate VDS with current version of Hail.``; However, as this is a separate problem, I'll open up a separate issue for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320149912:386,error,error,386,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320149912,1,['error'],['error']
Availability,"I kinda think this should be an error, actually.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7418#issuecomment-548483578:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548483578,1,['error'],['error']
Availability,"I know I said I thought this was a reasonable approach a while ago, but I’ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something we’re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that I’m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers — right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration — they’ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:256,down,down,256,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249,1,['down'],['down']
Availability,"I know why it happened (was on an earlier version) but this is a general thing I've seen:; ```; Traceback (most recent call last):; File ""saige_pan_ancestry.py"", line 247, in <module>; main(args); File ""saige_pan_ancestry.py"", line 210, in main; n_threads=n_threads); File ""/home/konradk/ukb_common/utils/saige_pipeline.py"", line 238, in load_results_into_hail; load_data_task.always_run().depends_on(*tasks_to_hold); TypeError: 'TaskResourceFile' object is not callable; ```; say you did `Task.always_runn()` - it would give this error bc it creates a file and then can't call it as a function. Not sure what the long term solution is, but just thought I'd raise it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8011:531,error,error,531,https://hail.is,https://github.com/hail-is/hail/issues/8011,1,['error'],['error']
Availability,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:589,redundant,redundantly,589,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['redundant'],['redundantly']
Availability,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321#issuecomment-212885679:222,error,error,222,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679,1,['error'],['error']
Availability,"I like the idea that, in a future PR, we investigate a slightly higher initial back off for disk creation failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872569078:106,failure,failures,106,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872569078,1,['failure'],['failures']
Availability,"I like this in concept, but I'm worried about people messing themselves up by changing upstream code and expecting checkpoint to rerun.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5599#issuecomment-472976356:115,checkpoint,checkpoint,115,https://hail.is,https://github.com/hail-is/hail/issues/5599#issuecomment-472976356,1,['checkpoint'],['checkpoint']
Availability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:202,down,downstream,202,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,1,['down'],['downstream']
Availability,"I like this table much better! However, it's too wide. I don't know exactly the best way to shrink it down, but here's a few off the cuff thoughts:. I don't think I care about ""Live"", I can do that math myself (it's pending + active, right?). Can we shorten Instances to ""I"" and Cores to ""C"" with abbr tags a la `<abbr title=""Instances"">`?. I don't think I care about schedulable instances, for scheduling I really care about cores. I don't think I care about the cores column, right? ~~Isn't that a synonym for ""active cores""?~~ Ah versioning matters. Hmm. Can we maybe just do `XX / YY` and `ZZ%` columns? It's just too wide to quickly scan this table. I think the most important super-heading is ""Schedulable"", what do you think of putting that at the far left of the table?. If we swap ""Spot"" for ""Preemptible"" that will also shrink the width of the table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920:102,down,down,102,https://hail.is,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920,1,['down'],['down']
Availability,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['error'],['error']
Availability,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:261,echo,echo,261,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,2,['echo'],['echo']
Availability,"I looked at this again and I think we can do this with `online: true`. It's a quick enough migration where it shouldn't impact the driver for too long that it's trying to query the long tables. If there's a problem and the driver can't make forward progress once the database migration is done, we can just shut down the deployment to 0 replicas. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153:312,down,down,312,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153,1,['down'],['down']
Availability,I lowered the tolerance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305#issuecomment-462478878:14,toler,tolerance,14,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462478878,1,['toler'],['tolerance']
Availability,"I made any command that is larger than 10KB is written to a file instead and is downloaded as an input file to be run (same as what we do for Python jobs). I also added a new field `user_code` that represents the user's code that they want to run versus the command we have Docker run. I formatted the `user_code` for Python jobs to show the actual function definition and the arguments used to call the function. It's probably not perfect (i.e. JobResourceFile), but better than nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10664:80,down,downloaded,80,https://hail.is,https://github.com/hail-is/hail/pull/10664,1,['down'],['downloaded']
Availability,"I merged main and was forced to fix the pyright errors, which was great! I found two bugs with these new types:. 1. It's possible for a spec file to be missing which means the spec is None which would fail our get job front end code when it tries to fix up the resources.; 2. In the aioclient, we assumed the attributes was present and a dict but that key can be completely missing from the dict. I also noticed `cost_str` has had the wrong type since always.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13500#issuecomment-1701592782:48,error,errors,48,https://hail.is,https://github.com/hail-is/hail/pull/13500#issuecomment-1701592782,1,['error'],['errors']
Availability,I met with Lily and Ruchit. They have more work they need to do to figure out what their use case is and how they want downstream analysts to use the data to figure out how to key their datasets. They'll get back to me once they're ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13864#issuecomment-1785898859:119,down,downstream,119,https://hail.is,https://github.com/hail-is/hail/issues/13864#issuecomment-1785898859,1,['down'],['downstream']
Availability,"I modified the artifacts to include the two uber jars:. ```; +:build/docs => docs; +:build/libs/hail-all-spark.jar; +:build/libs/hail-all-spark-test.jar; ```. The latest successful master build of `hail-all-spark.jar` is available at:. https://ci.hail.is/httpAuth/app/rest/builds/buildType:(id:HailSourceCode_HailCi),count:1,status:SUCCESS/artifacts/content/hail-all-spark.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/754#issuecomment-245692697:221,avail,available,221,https://hail.is,https://github.com/hail-is/hail/issues/754#issuecomment-245692697,1,['avail'],['available']
Availability,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232824524:121,error,error,121,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524,2,['error'],['error']
Availability,"I moved the changes to the aggregator into #6076, and added error bounds that apply to all quantiles simultaneously, which is what you really want for the pdf.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039#issuecomment-492307141:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-492307141,1,['error'],['error']
Availability,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/511#issuecomment-236389770:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770,1,['error'],['error']
Availability,"I need this extra debugging information to understand what is going on in Azure with deleted VMs still showing up in the portal with ResourceNotFound errors. Miah and Greg are running into this same problem in their deployment. My guess is what is happening is the worker is active and working fine, but then the deployment gets ""Canceled"" because the OMSAgent takes too long to deploy. So our loop then cancels the deployment which messes up the state in Azure of the already deployed and running VM. I popped the parameters from the deployment result in case it contains sensitive data (I'm mainly worried about any private SSH keys).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13231:150,error,errors,150,https://hail.is,https://github.com/hail-is/hail/pull/13231,1,['error'],['errors']
Availability,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:34,error,errors,34,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743,1,['error'],['errors']
Availability,"I need to get GKE costs down further. The driver is now 3 CPU anyway. @daniel-goldstein, I am not sure how to modify the Azure terraform. It appears this; is controlled by a variable which is already set to a 4 core machine?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11985:24,down,down,24,https://hail.is,https://github.com/hail-is/hail/pull/11985,1,['down'],['down']
Availability,"I need to investigate further, but now I see a segfault and I think it's coming from the LMM tests. I need to fix the `test-gcp.sh` script so that it looks for the coredump file in the case of a seg fault. ```; [Stage 2225:==========================================> (3 + 1) / 4]2017-08-28 21:47:32 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:32 Hail: INFO: Ordering unsorted dataset with network shuffle; 2017-08-28 21:47:33 Hail: WARN: called redundant split on an already split VDS; 2017-08-28 21:47:33 Hail: INFO: using 2 trios for transmission analysis; [Stage 2229:==========================================> (3 + 1) / 4]2017-08-28 21:47:35 Hail: INFO: while writing:; file:/tmp/hail.16cpq9RzwI7a/out.00000.txt; merge time: 65.459ms; 2017-08-28 21:47:35 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:35 Hail: INFO: Ordering unsorted dataset with network shuffle; [Stage 2234:============================================> (4 + 1) / 5]2017-08-28 21:47:37 Hail: WARN: Found 2 samples with missing sex information (not 1 or 2).; Missing sex identifiers: [ 0 ]; 2017-08-28 21:47:37 Hail: WARN: 2 samples discarded from .fam: sex of child is missing.; 2017-08-28 21:47:38 Hail: INFO: Found 250 samples in fam file.; 2017-08-28 21:47:38 Hail: INFO: Found 2000 variants in bim file.; 2017-08-28 21:47:38 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:38 Hail: INFO: Modified the genotype schema with annotateGenotypesExpr.; Original: Struct{GT:Call}; New: Genotype; 2017-08-28 21:47:38 Hail: INFO: Reading table to impute column types; [Stage 2258:============================> (1 + 1) / 2]2017-08-28 21:47:40 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: Reading table to impute column types; 2017-08-28 21:47:41 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading colu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:199,fault,fault,199,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,2,"['fault', 'redundant']","['fault', 'redundant']"
Availability,I need to make this more robustly ignore migration files,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11231#issuecomment-1016834010:25,robust,robustly,25,https://hail.is,https://github.com/hail-is/hail/pull/11231#issuecomment-1016834010,1,['robust'],['robustly']
Availability,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523:97,resilien,resilient,97,https://hail.is,https://github.com/hail-is/hail/pull/7523,4,"['failure', 'resilien']","['failure', 'resilient']"
Availability,"I needed to make the read function in MatrixRead into a class so I can print/parse as part of the work to replace AST with IR in the Python interface. Also, push down required type into MT decoder. I also use the compiler to build the function to add the entries to the row values (no RegionValueBuilder).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3819:162,down,down,162,https://hail.is,https://github.com/hail-is/hail/pull/3819,1,['down'],['down']
Availability,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402:272,error,error,272,https://hail.is,https://github.com/hail-is/hail/pull/8402,1,['error'],['error']
Availability,"I noticed that jobs in test deployments were deadlocking because we weren't spinning up extra instances (compared to the production version of Batch). Although each job could fit on an open instance, its allocated share is still less than the core request for that job. This PR aims to increase the probability in which we ignore an exceed shares error the more we have these errors such that at a certain point the rate will be 100% and we'll be able to continue scheduling.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9464:347,error,error,347,https://hail.is,https://github.com/hail-is/hail/pull/9464,2,['error'],"['error', 'errors']"
Availability,"I noticed that the gnomAD mitochondria datasets were pointing at the `gs://gnomad-public-requester-pays` bucket. The Google Cloud Public Datasets version should be up to date now. Also updated the documented schema for chrM sites. Some information about those changes is available in the gnomAD change log: https://gnomad.broadinstitute.org/news/2021-08-rename-filter-in-mitochondria-dataset-and-minor-format-changes/. And finally, since these were the only two datasets that reference `gnomad-public-requester-pays`, removed `gnomad-public-requester-pays` from the list of annotation DB buckets used by `hailctl dataproc`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11282:271,avail,available,271,https://hail.is,https://github.com/hail-is/hail/pull/11282,1,['avail'],['available']
Availability,I noticed that this step blew up in the benchmarking PR and thought I'd provide a better error message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12866:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/pull/12866,1,['error'],['error']
Availability,"I now pass the scores_table through as a Table rather than localizing and passing through colKeys, colKeyType, and scores annotations. The column key can now be any type. Both string and integer keys are tested from Python. However, `requireUniqueSamples` still requires a single string ID (this was the remaining problem of going generic), so I've removed this check and would appreciate feedback on the best approach to checking uniqueness, preferably on the localized `keys` in PCRelate so as not to trigger additional actions. I could use keyType.valuesSimilar to compare any two elements...it's a bit weird to have a tolerance on floats here. As noted, I'm also a bit wary that I'm relying on `scores` from `pca` to be in the same order as the columns on the matrix table. This is currently true, but could change. @danking I think the joins in `fuse` should also be zipPartitions, I've noted it in a FIXME. I'm also concerned that the number of diagonal blocks is an upper bound on parallelism for the matrix multiply. We should be able to fix that by immediately writing and then reading phi.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065:622,toler,tolerance,622,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065,1,['toler'],['tolerance']
Availability,"I observed a cluster with it set to the default idle time of 30 seconds in Azure and the workers were continuously thrashing leading up to 49 instances being created over the course of a PR. With an idle time of 120 seconds, there was no thrashing and 28 instances were created over the course of the PR (16 standard + job private etc.). The cluster nicely scaled down at the end of the PR. It looked like a couple of times the `standard-np` pool scaled up and then scaled down so I assume the `standard` pool wasn't at full capacity while that was happening. It might be worth configuring the `standard-np` pool to be 4 or 5 standing instances with 16 cores and see what happens -- that might help as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13314:364,down,down,364,https://hail.is,https://github.com/hail-is/hail/pull/13314,2,['down'],['down']
Availability,"I observed this in my namespace while testing fixes for a different transient error.; https://internal.hail.is/dking/batch/batches/5/jobs/7034 (this URL will obviously not last long). The worker actually successfully wrote the correct output to GCS, but it received this error anyway which caused the job to fail. Since QoB checks for the results files first, it didn't even notice that the worker job had failed. That seems not great. We should probably fail if an worker job fails, even if we find output because that output could be corrupt. ```; (base) dking@wm28c-761 hail % hexdump -C foo ; 00000000 01 82 00 00 00 7e 00 00 00 67 73 3a 2f 2f 31 2d |.....~...gs://1-|; 00000010 64 61 79 2f 74 6d 70 2f 68 61 69 6c 2f 54 53 4f |day/tmp/hail/TSO|; 00000020 66 4f 72 67 5a 55 6d 62 56 69 78 6e 52 69 4b 57 |fOrgZUmbVixnRiKW|; 00000030 51 46 42 2f 61 67 67 72 65 67 61 74 65 5f 69 6e |QFB/aggregate_in|; 00000040 74 65 72 6d 65 64 69 61 74 65 73 2f 2d 50 74 33 |termediates/-Pt3|; 00000050 67 4e 74 51 57 35 57 6f 42 64 43 54 44 50 51 69 |gNtQW5WoBdCTDPQi|; 00000060 77 48 64 61 39 63 32 36 35 66 32 2d 66 62 64 38 |wHda9c265f2-fbd8|; 00000070 2d 34 66 31 62 2d 62 63 64 65 2d 66 62 66 32 39 |-4f1b-bcde-fbf29|; 00000080 31 38 30 63 33 34 37 00 00 00 00 |180c347....|; 0000008b; ```. code:; ```ipython3; In [1]: import hail as hl; ...: import gnomad.utils.sparse_mt; ...: ; ...: ; ...: tmp_dir = 'gs://danking/tmp/'; ...: vds_file = 'gs://neale-bge/bge-wave-1.vds'; ...: out = 'gs://danking/foo.vcf.bgz'; ...: ; ...: vds = hl.vds.read_vds(vds_file); ...: mt = hl.vds.to_dense_mt(vds); ...: t = gnomad.utils.sparse_mt.default_compute_info(mt); ...: t = t.annotate(info=t.info.drop('AS_SB_TABLE')); ...: t = t.annotate(info = t.info.drop(; ...: 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; ...: )); ...: t = t.drop('AS_lowqual'); ...: ; ...: hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```; worker failure:; ```; 2023-09-27 16:43:10.389 JVMEntryway: INFO: is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,2,['error'],['error']
Availability,I often observe the JVM getting stuck in a high memory state unable to recover from after an OOM. Best to just kill the JVM.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12805:71,recover,recover,71,https://hail.is,https://github.com/hail-is/hail/pull/12805,1,['recover'],['recover']
Availability,I particularly like that a `PropertySuite` would reduce the possibility of human error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241139977:81,error,error,81,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241139977,1,['error'],['error']
Availability,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8229:56,Down,Downside,56,https://hail.is,https://github.com/hail-is/hail/pull/8229,2,['Down'],['Downside']
Availability,"I pushed a commit that will error on resource warning. Hopefully we can figure out where we're leaking, fix the leaks, and stop the hangs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731#issuecomment-1518368590:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1518368590,1,['error'],['error']
Availability,"I pushed the logic #2861 down so I can remove this copy. I'll add that change to that PR once this is merged (good catch) since I may need to rebase anyway. See here:; https://github.com/hail-is/hail/pull/2861/files#diff-912e03c9c34a874ecdc0e520a13cb572R133. This avoids the copy if the BDM is compact, which blocks always are. If the BDM is not compact, then we could add logic to stream out the bytes without an intermediate compactification but I don't want to add that complexity now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946,1,['down'],['down']
Availability,I put the WIP tag on this. I don't have the energy to debug any failures today. Will merge it on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071:64,failure,failures,64,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071,1,['failure'],['failures']
Availability,I ran into an error on another branch where ArrayFor doesn't work if the object being aggregated is a Set. I'm assuming here the ToArray is free for all container objects.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3796:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/3796,1,['error'],['error']
Availability,"I ran into issues in the development of the query service wherein, if the; aiohttp `ClientSession` is not created in the same event pool as the one; from which the request is made, then unusual errors occur. This change makes it harder to accidentally create a BatchClient in the; wrong event loop because the factory method, `BatchClient.create` is; itself an async function. It is still possible to create the BatchClient; in one event loop and use it in another (do not do this!!), but that seems; to be an unlikely mistake. cc: @daniel-goldstein",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10923:194,error,errors,194,https://hail.is,https://github.com/hail-is/hail/pull/10923,1,['error'],['errors']
Availability,"I read somewhere that kube-system pods tolerate any taint because, e.g. the kube-proxy, is necessary to even participate in k8s. These all exist:; ```; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-0msn 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-fqkt 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-g3z7 1/1 Running 0 60m; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-pdlv 1/1 Running 0 14h; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-td40 1/1 Running 0 3d20h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-4fkb 1/1 Running 0 12m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-51jf 1/1 Running 0 100m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-cr11 1/1 Running 0 2d22h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-x6cl 1/1 Running 0 18h; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784#issuecomment-571218268:39,toler,tolerate,39,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-571218268,1,['toler'],['tolerate']
Availability,"I really borked our SQL linting. This PR is short but it catches a few critical problems. 1. The point of `check-sql.sh` is to detect modifications or deletions of SQL files in PRs and fail if such a change occurs. Currently on `main` it does not detect modifications. In #13456, I removed the `delete-<service>-tables.sql` files (intentionally), so added the `^D` to the `grep` regex to indicate that it is OK to have a deletion. What I inadvertently did though is change the rule from ""It's ok to have Additions of any file OR Modifications of estimated-current.sql / delete-<service>-tables.sql"" to ""It's ok to have Additions OR Modifications OR Deletions of estimated-current.sql / delete-<service>-tables.sql"". Really this should have been ""It's ok to have Additions OR Modifications of estimated-current.sql OR Deletions of delete-<service>-tables.sql"". I've changed it to reflect that rule. 2. Rules currently silently *pass* in CI with an error message that git is not installed. In #13437 I changed the image used to run the linters and inadvertently didn't include `git` which `check-sql.sh` needs to run. Here's how it failed in a sneaky way:; - Since `git` is not installed, all calls to `git` fail, but the script is not run with `set -e` so every line of the script is executed; - Since `git` lines fail, `modified_sql_file_list` remains empty; - Since `modified_sql_file_list` remains empty, it appears to the check at the end that everything checked out; - The if statement runs successfully and the script returns with error code 0. To fix this I do a few things:; - installed `git` in the linting image; - `set -e` by default and only enable `set +e` later on when necessary (because we don't want a failed `git diff` to immediately exit); - Do away with the file checking and instead check the error code of the grep. If nothing survives the grep filter, which means there were no illegal changes made, grep will return with exit code 1. So we treat that exit code as a success.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13745:947,error,error,947,https://hail.is,https://github.com/hail-is/hail/pull/13745,3,['error'],['error']
Availability,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:102,down,down,102,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365,2,"['down', 'error']","['down', 'error']"
Availability,I remember you said this was hitting some error/segfault -- anything you need from me to unblock?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8952#issuecomment-666412422:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-666412422,1,['error'],['error']
Availability,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:580,error,error,580,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308,1,['error'],['error']
Availability,"I retried Azure. It ran into the exact same problem with Azure Blob Storage flakiness https://ci.azure.hail.is/batches/4126780/jobs/136 . We gotta fix our interaction with Azure Blob Storage, it's the only way to get CI to be stable again. It's possible https://github.com/hail-is/hail/pull/13325 will help, but it doesn't address the root cause of the SDK giving us these mysterious errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13306#issuecomment-1656089547:384,error,errors,384,https://hail.is,https://github.com/hail-is/hail/pull/13306#issuecomment-1656089547,1,['error'],['errors']
Availability,"I run into this ~1/20,000 QoB just when writing result files. I'm going to make a separate issue with the ABS client repo but I suspect this might be difficult to track down and fix. Example stack trace:. ```; RuntimeException: java.lang.IllegalStateException: Faulted stream due to underlying sink write failure	Gjava.lang.RuntimeException: java.lang.IllegalStateException: Faulted stream due to underlying sink write failure; 	at is.hail.shadedazure.com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:92); 	at is.hail.shadedazure.com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:102); 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:308); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241); 	at is.hail.utils.package$.using(package.scala:669); 	at is.hail.io.index.IndexWriterUtils.writeMetadata(IndexWriter.scala:253); 	at __C511collect_distributed_array_table_native_writer.apply_region3_328(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:198); 	at is.hail.services.package$.retryTransientErrors(package.scala:187); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:197); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.service.Worker$.main(Worker.scala:195); 	at is.hail.backend.service.Main$.main(Main.scala:9); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAcces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14430:169,down,down,169,https://hail.is,https://github.com/hail-is/hail/pull/14430,5,"['Fault', 'down', 'failure']","['Faulted', 'down', 'failure']"
Availability,"I saw an error in a CI job that was copying in some files. To be clear,; this change would not restart a copy of the entire directory. It just; retries the directory listing. We currently use O(N_FILES) memory to; store the list of files in a directory *before* we start copying. ```; Traceback (most recent call last):; File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 110, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1501, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 104, in main; files=files; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 76, in copy_from_dict; transfers=transfers; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 50, in copy; bytes_listener=make_tqdm_listener(byte_pbar)); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 439, in copy; await copier._copy(sema, copy_report, transfer, return_exceptions); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 532, in _copy; raise e; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/fs/copier.py"", line 527, in _copy; ], return_exceptions=return_exceptions, cancel_on_error=True); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 519, in bounded_gather2; return await bounded_gather2_raise_exceptions(sema, *pfs, cancel_on_error=cancel_on_error); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 504, in bounded_gather2_raise_exceptions; return await asyncio.gather(*tasks); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", lin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11511:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/11511,1,['error'],['error']
Availability,I saw different errors if a `show()` vs `_force_count_rows()` was the last line.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603568619:16,error,errors,16,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603568619,1,['error'],['errors']
Availability,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:125,error,error,125,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['error'],['error']
Availability,"I search the google logs for the pull request in question. For example,. ```; (resource.labels.namespace_name=""pr-11471-default-fn1xhr9ahy9v"" AND; labels.""k8s-pod/app"":""batch""; ) OR (; labels.""compute.googleapis.com/resource_name"":""batch-worker-pr-11471-default-fn1xhr9ahy9v"" AND; logName:""worker.log""); ```. That PR seems to have a worker stuck alive: `batch-worker-pr-11471-default-fn1xhr9ahy9v-standard-ux0sp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059448548:346,alive,alive,346,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059448548,1,['alive'],['alive']
Availability,"I searched [here](https://portal.azure.com#@d6c9f2ea-d3bb-4ca9-8b14-231bac999aa6/blade/Microsoft_OperationsManagementSuite_Workspace/Logs.ReactView/resourceId/%2Fsubscriptions%2F22cd45fe-f996-4c51-af67-ef329d977519%2Fresourcegroups%2Fhaildev%2Fproviders%2Fmicrosoft.operationalinsights%2Fworkspaces%2Fhaildev-logs/source/LogsBlade.AnalyticsShareLinkToQuery/q/H4sIAAAAAAAAA21Sy27CMBC88xVbLiRSqNprUSpVgFpUVCraOzLxJhgcO7LNI2r7710nKRBoDokzmp2dnbVEB6%252FbJb5rPtWZhRiCQnPFciwMpuLwYJ0RKovAQ7ZgCR4RqTOJO5QNEMJXB%252BiRpDjUyjGh0Ez4VFhHqk2PidqhctqUFfUb9is0CG%252BkDdYx4%252BxeuBUdTcJc20gE3X43bJURTaIKTs1G4ePdlXBlGuL4NEFD4eRMqOTc7SgCHwRbooxOxRU4KWpkUFUfSyi0Rg4PDhUHGs%252BUccGMxcXaahUQY%252ByxsM0jbcsyjCv%252BbfPXMn9mC4SC4CLUdha1jKV9GOFKP%252B7fev6h4SFZCJVquElqTQvdj5enbkOVWm%252B2BWyE4rHE1OmtQ3PdH7Q699gUF0avMXF9tmclTHIaKqo%252Fn4zuzBwLbYW%252FAHWaEXyKHGfpUOc5U%252FxCw6DfmU%252Ffc2P%252Fuu%252F8DDpnFzboJaIXQY9jyrbS%252BeN4Pp%252FNeyHpaMPJ%252BLKsmjwj2WQOOXC0yS9DxlTe%252BAIAAA%253D%253D/timespan/2023-04-16T20%3A37%3A28.000Z%2F2023-04-28T20%3A37%3A28.000Z) and couldn't really find anything insightful. It does feel odd that we went that long with so *few* error messages, so maybe some silent error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142:1150,error,error,1150,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142,2,['error'],['error']
Availability,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218:71,avail,available,71,https://hail.is,https://github.com/hail-is/hail/issues/1218,2,['avail'],['available']
Availability,"I see this happening quite a bit. ```; =================================== FAILURES ===================================; ______________________________ Test.test_callback ______________________________. self = <test.test_batch.Test testMethod=test_callback>. def test_callback(self):; app = Flask('test-client'); ; d = {}; ; @app.route('/test', methods=['POST']); def test():; d['status'] = request.get_json(); return Response(status=200); ; server = ServerThread(app); try:; server.start(); ; j = self.batch.create_job(; 'alpine',; ['echo', 'test'],; attributes={'foo': 'bar'},; callback=server.url_for('/test')); j.wait(); ; > status = d['status']; E KeyError: 'status'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5817:75,FAILURE,FAILURES,75,https://hail.is,https://github.com/hail-is/hail/issues/5817,2,"['FAILURE', 'echo']","['FAILURES', 'echo']"
Availability,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:635,error,error,635,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395,1,['error'],['error']
Availability,"I see, it wasn't doing redundant work, just generating redundant IR by regenerating the IR to load covariates per set of phenotypes. This would only affect chained linear regression, since that's the only time there's more than one `one_y_field_name_set in y_field_names`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871:23,redundant,redundant,23,https://hail.is,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871,2,['redundant'],['redundant']
Availability,I see. It's a bug in that the debug information if the test had failed would have been wrong / thrown an error. But the actual test right now was testing the right thing. Is this correct?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11203#issuecomment-1006756945:105,error,error,105,https://hail.is,https://github.com/hail-is/hail/pull/11203#issuecomment-1006756945,1,['error'],['error']
Availability,"I see.. if there's a place where it's easy to check for this, I could add an error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639,1,['error'],['error']
Availability,"I set a `.curlrc` file to ensure we use the right options by default. In a few places that are; intended to be run on a developer's laptop, I had to explicitly specify the necessary; options. I replaced all the short flags with long flags, the important conversions to know:; - `-L` `--location`, follow redirects; - `-O` `--remote-name`, use the basename of the URL as the name of a local file in which to save the data; - `-s` `--silent`, do not print anything to stdout/stderr; - `-S` `--show-error`, when used with `-s`, shows errors on stderr; - `-f` `--fail`, return non-zero exit code if the response is 4xx or 5xx (normally the response would be printed / saved and curl will exit successfully)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11183:496,error,error,496,https://hail.is,https://github.com/hail-is/hail/pull/11183,2,['error'],"['error', 'errors']"
Availability,"I set it to Running at the end of `_create_pod`. Maybe that's not correct. It should probably be this then:. Pending -> Ready -> (Error, Created -> Running -> (Failed, Success))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499331021:130,Error,Error,130,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499331021,1,['Error'],['Error']
Availability,"I started looking into the test failures last week, but I can't reproduce them locally; I'm very confused. Anyways, I'm working on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084:32,failure,failures,32,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084,1,['failure'],['failures']
Availability,I still don't understand how the error you posted above relates to the changes in this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592590862:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592590862,1,['error'],['error']
Availability,"I still have the index error, so it must be inherent to my branch. Let's close and go with your implementation instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766743037:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766743037,1,['error'],['error']
Availability,"I still haven't looked at the code carefully, but one thing I'd say with database migrations is to make sure that your incremental change won't make the next change you want to make in the future more difficult or impossible. I don't think that's the case here, but keep in mind that this migration is O(n_batches). Anything that is O(n_jobs) right now is not great on hail-vdc as it could take hours to complete. How big of a lift is it to the next change you wanted to try out? Where is the new deadlock error coming from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036214712:506,error,error,506,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036214712,1,['error'],['error']
Availability,I still haven't managed to set it up to work from that. I hit errors while installing the Genesis stuff,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312:62,error,errors,62,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312,1,['error'],['errors']
Availability,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8878:400,down,down,400,https://hail.is,https://github.com/hail-is/hail/pull/8878,3,"['down', 'failure']","['down', 'failures']"
Availability,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:587,Error,Error,587,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751,1,['Error'],['Error']
Availability,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635,1,['error'],['error']
Availability,I tested this by adding the check_aggregated_resources loop which didn't throw any errors. I tested the cost by looking at the UI and submitting one job that cost $0.0004 and then two of that same job in a batch and it cost $0.0008. The database looked like this:. mysql> select * FROM aggregated_batch_resources where batch_id > 46;; +----------+--------------------------+-------------+-------+; | batch_id | resource | usage | token |; +----------+--------------------------+-------------+-------+; | 47 | compute/n1-preemptible/1 | 25867000 | 11 |; | 47 | compute/n1-preemptible/1 | 0 | 115 |; | 47 | compute/n1-preemptible/1 | 0 | 132 |; | 47 | disk/local-ssd/1 | 9932928000 | 11 |; | 47 | disk/local-ssd/1 | 0 | 54 |; | 47 | disk/local-ssd/1 | 0 | 132 |; | 47 | disk/pd-ssd/1 | 529756160 | 11 |; | 47 | disk/pd-ssd/1 | 0 | 132 |; | 47 | disk/pd-ssd/1 | 0 | 186 |; | 47 | ip-fee/1024/1 | 26487808 | 11 |; | 47 | ip-fee/1024/1 | 0 | 132 |; | 47 | ip-fee/1024/1 | 0 | 188 |; | 47 | memory/n1-preemptible/1 | 99329280 | 11 |; | 47 | memory/n1-preemptible/1 | 0 | 26 |; | 47 | memory/n1-preemptible/1 | 0 | 132 |; | 47 | service-fee/1 | 25867000 | 11 |; | 47 | service-fee/1 | 0 | 110 |; | 47 | service-fee/1 | 0 | 132 |; | 48 | compute/n1-preemptible/1 | 0 | 31 |; | 48 | compute/n1-preemptible/1 | 0 | 76 |; | 48 | compute/n1-preemptible/1 | 27659000 | 94 |; | 48 | compute/n1-preemptible/1 | 26520000 | 122 |; | 48 | compute/n1-preemptible/1 | 0 | 156 |; | 48 | compute/n1-preemptible/1 | 0 | 168 |; | 48 | disk/local-ssd/1 | 10621056000 | 94 |; | 48 | disk/local-ssd/1 | 10183680000 | 122 |; | 48 | disk/local-ssd/1 | 0 | 125 |; | 48 | disk/local-ssd/1 | 0 | 154 |; | 48 | disk/local-ssd/1 | 0 | 156 |; | 48 | disk/local-ssd/1 | 0 | 168 |; | 48 | disk/pd-ssd/1 | 0 | 69 |; | 48 | disk/pd-ssd/1 | 566456320 | 94 |; | 48 | disk/pd-ssd/1 | 0 | 102 |; | 48 | disk/pd-ssd/1 | 543129600 | 122 |; | 48 | disk/pd-ssd/1 | 0 | 156 |; | 48 | disk/pd-ssd/1 | 0 | 168 |; | 48 | ip-fee/1024/1 | 0 | 57 |; | 48 ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9346:83,error,errors,83,https://hail.is,https://github.com/hail-is/hail/pull/9346,1,['error'],['errors']
Availability,"I tested this by submitting 64 true, 10 Gi 0.25 core jobs on one node with a local SSD. It took 7 minutes for the whole batch to finish. This is a bit disappointing, but at least there were no errors. We can potentially revisit using get instead of wait for the polling loop, but the default sleep backoff method we have quickly blew our quota.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-871705329:193,error,errors,193,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-871705329,1,['error'],['errors']
Availability,I tested this locally by downloading the HTML for the UI page and tinkering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795:25,down,downloading,25,https://hail.is,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795,1,['down'],['downloading']
Availability,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:56,error,errors,56,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288,1,['error'],['errors']
Availability,"I tested this with a hard-hitting batch that used a bunch of storage, looked through the UI and didn't get any 500s, and checked the logs on both the k8s pods and the instances for errors. I also commented out each part of the garbage collection loops and made sure everything got cleaned up. For example, commenting out the activity logs loop or the monitor instances loop with the deactivate API point not doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466:181,error,errors,181,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466,1,['error'],['errors']
Availability,I think GitHub doesn't show that far up from the diff but there is a comment describing the error code already on line 27. I tried to keep them close together.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14251#issuecomment-1927987394:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/pull/14251#issuecomment-1927987394,1,['error'],['error']
Availability,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230288836:109,fault,fault,109,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836,1,['fault'],['fault']
Availability,"I think I squashed all the bugs, failure is from the unrelated test_ci issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335#issuecomment-602304418:33,failure,failure,33,https://hail.is,https://github.com/hail-is/hail/pull/8335#issuecomment-602304418,1,['failure'],['failure']
Availability,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:362,down,down,362,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,1,['down'],['down']
Availability,"I think fixing this will mask an issue where a 1 CPU job blocks the scheduling of smaller jobs. I'm gonna debug that first, then remove WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067:25,mask,mask,25,https://hail.is,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067,1,['mask'],['mask']
Availability,"I think in an ideal world, every project including ours, our dependencies, and our end users, maintain public wide-open requirements files and private fully-pinned lockfiles. Our users know we need X and Y and they pin whatever versions are compatible with the union of our code and theirs. If we don't want our end users to have to do that, we need to compromise by narrowing the window and accepting the faulted but practical middle-ground. @jmarshall, out of curiosity does your group fully pin your dependencies / do you have any thoughts? It would be interesting to hear from the perspectives of end users but also third-party collaborators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773:406,fault,faulted,406,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773,1,['fault'],['faulted']
Availability,"I think it was wrong -- the buffered thing probably already implements it in terms of write. I didn't even define flush on the java side, so it wasn't getting called in my tests (or it would have errored)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949:196,error,errored,196,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949,1,['error'],['errored']
Availability,I think it's passing fine now. Just a transient failure in the latest one: https://ci.hail.is/batches/8118694,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14224#issuecomment-1922387328:48,failure,failure,48,https://hail.is,https://github.com/hail-is/hail/pull/14224#issuecomment-1922387328,1,['failure'],['failure']
Availability,"I think it's useful if you want to write something like:. ```; hl.if_else(x < y, ...., hl.die(""Error: x must be less than y"")); ```. Otherwise you can't do input validation on expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584:95,Error,Error,95,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584,1,['Error'],['Error']
Availability,"I think maybe `tolerations` is not currently indented to the same level as `volumes`, causing the error you're getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380:15,toler,tolerations,15,https://hail.is,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380,2,"['error', 'toler']","['error', 'tolerations']"
Availability,I think no. The frequency of failures is a bit lower than it was.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11265#issuecomment-1027174857:29,failure,failures,29,https://hail.is,https://github.com/hail-is/hail/pull/11265#issuecomment-1027174857,1,['failure'],['failures']
Availability,"I think old repos still work, which is why CI is mostly fine, but I was seeing errors when testing locally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5798:79,error,errors,79,https://hail.is,https://github.com/hail-is/hail/pull/5798,1,['error'],['errors']
Availability,I think one answer to this would be to just cut out the Java parts and show just the summary. But I also wrote up some further thoughts for how to improve here: https://dev.hail.is/t/better-python-error-messages-idea/201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952:197,error,error-messages-idea,197,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952,1,['error'],['error-messages-idea']
Availability,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6011:153,down,down,153,https://hail.is,https://github.com/hail-is/hail/pull/6011,3,"['down', 'downtime', 'failure']","['down', 'downtime', 'failure']"
Availability,"I think that would probably be fine---I just got worried since `_error_from_cdf` had some non-working code and wasn't being called anywhere explicitly (it's imported in `plots`, but not used in the error calculation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039#issuecomment-490256479:198,error,error,198,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490256479,1,['error'],['error']
Availability,I think the best thing to do is to throw an error here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1587#issuecomment-288400774:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/issues/1587#issuecomment-288400774,1,['error'],['error']
Availability,I think the primary question is:. > Is the difference between `0.84` and `1.31` a sign of an error or is this a reasonable value for the search to find for `delta`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281827437:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281827437,1,['error'],['error']
Availability,I think the syntax change is needed because numpy versions older than 1.12 don't let you filter a numpy array on a boolean mask in this way.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2940:123,mask,mask,123,https://hail.is,https://github.com/hail-is/hail/pull/2940,1,['mask'],['mask']
Availability,I think there is some unresolved issue with asyncgen shutdown that is keeping; workers alive. This is not an issue in worker because worker calls sys.exit; which forcibly stops execution. cc: @daniel-goldstein @jigold.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10136:87,alive,alive,87,https://hail.is,https://github.com/hail-is/hail/pull/10136,1,['alive'],['alive']
Availability,"I think this PR is just about as good as it's going to get for now. From looking at the Grafana API metrics, I think I was hitting the maximum scheduler throughput. The get running cancellable jobs is around 40ms each call for 5000 jobs while the getting the job head queue is 123ms. If the 40ms becomes a problem, then we can pull less records (see explanation below) or we can not do a json array agg and figure out the regions using bit shifting. When we did the load tests yesterday getting the job head queue was around 1-2 seconds with us each having 20k records. I think we just have to keep an eye on it. I did some further optimization of the scheduler by allowing it to pull up to 10000 jobs from the database to try and schedule before it hits its fair share of jobs scheduled. This helps a lot with efficiency to use the existing capacity if there are jobs further down the queue that are schedulable. I know it's a bit of a departure from what we've done in the past, but I think since we're going in order of fair share now and pulling more jobs from the database isn't that expensive, then this is fine. Happy to make this number 1000 even. 300 was too small though. Jobs at the front of the queue will eventually be able to run because the next iteration of the autoscaler will create the correct instances for those jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928:877,down,down,877,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928,1,['down'],['down']
Availability,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8759:276,error,errors,276,https://hail.is,https://github.com/hail-is/hail/pull/8759,1,['error'],['errors']
Availability,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377,2,['error'],['error']
Availability,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248592567:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567,1,['error'],['error']
Availability,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210903100:293,error,errors,293,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100,1,['error'],['errors']
Availability,I think this is a very similar bug: https://github.com/aio-libs/aiomysql/issues/539. Turning off uvloop seemed to get rid of these errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13863#issuecomment-1788047416:131,error,errors,131,https://hail.is,https://github.com/hail-is/hail/issues/13863#issuecomment-1788047416,1,['error'],['errors']
Availability,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:285,error,error,285,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974,2,['error'],['error']
Availability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:546,alive,alive,546,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,4,"['alive', 'avail']","['alive', 'available']"
Availability,"I think this is just pip not honoring retries everywhere. The crash appears to occur while downloading a dependency. Sure, I can add a retry to apt as well, though apt seems to be more careful about retrying on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755:91,down,downloading,91,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755,1,['down'],['downloading']
Availability,"I think this is probably a bug, since we write out the reference genome and seem to support it just fine in Scala. I want to be able to do e.g.:. ```; >>> import hail as hl; >>> rg = hl.ReferenceGenome(""foo"", ['a', 'b'], {'a': 4, 'b': 6}); >>> t = hl.utils.range_table(10); >>> t = t.annotate(locus=hl.locus_from_global_position(t.idx, reference_genome='foo')); >>> t.write('test.t'); ```. and then, in a separate instance of hail, do:. ```; >>> import hail as hl; >>> t = hl.read_table('test.t'); ```. Currently, I get the following error:; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/parsimonious/nodes.py"", line 217, in visit; return method(node, [self.visit(n) for n in node]); File ""/Users/wang/code/hail/hail/python/hail/expr/type_parsing.py"", line 80, in visit_locus; return hl.tlocus(gr); File ""<decorator-gen-56>"", line 2, in __init__; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 512, in check_all; args_.append(checker.check(arg, name, arg_name)); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 56, in check; return tc.check(x, caller, param); File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 303, in check; return f(tc.check(x, caller, param)); File ""/Users/wang/code/hail/hail/python/hail/genetics/reference_genome.py"", line 10, in <lambda>; reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type); File ""/Users/wang/code/hail/hail/python/hail/context.py"", line 362, in get_reference; return ReferenceGenome._references[name]; KeyError: 'foo'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in read_table; File ""/Users/wang/code/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6907:534,error,error,534,https://hail.is,https://github.com/hail-is/hail/issues/6907,1,['error'],['error']
Availability,I think this is the last one. The tests passed 30 times without a failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10044:66,failure,failure,66,https://hail.is,https://github.com/hail-is/hail/pull/10044,1,['failure'],['failure']
Availability,"I think this issue is going to arise often with type imputation. As Tim wrote, you can also use --impute and say str(contig) or give it type string which will overwrite the imputation...but it's going to confuse folks. We could expand on the error message, or maybe we should implicitly convert Int to String for contig in the Variant constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520#issuecomment-236401286:242,error,error,242,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236401286,1,['error'],['error']
Availability,"I think this just isn't something you should do. You should use a newer version of hail than 0.2.60. We admittedly had a slightly rough transition into supporting multiple spark versions. It does seem like that message is a warning not an error, so you may be able to remove ` ""-Xfatal-warnings"", ` option from build.gradle and get things to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862:239,error,error,239,https://hail.is,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862,1,['error'],['error']
Availability,"I think this may help users discover `group_by` and also help users; who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:722,failure,failure,722,https://hail.is,https://github.com/hail-is/hail/pull/6856,1,['failure'],['failure']
Availability,"I think this needs to be rewritten to be robust to non-primitive elements. We can't just use copyFrom -- We need to loop and use constructAtAddressFromValue, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-663025712:41,robust,robust,41,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663025712,1,['robust'],['robust']
Availability,I think those failures are due to legacy tables not being keyed. See the workaround in the interpreted path here:; https://github.com/hail-is/hail/blob/bb7f847d9b3af888d4a59376fdd70900265bdeda/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L877-L881,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10379#issuecomment-829606450:14,failure,failures,14,https://hail.is,https://github.com/hail-is/hail/pull/10379#issuecomment-829606450,1,['failure'],['failures']
Availability,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:57,toler,tolerate,57,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,2,"['down', 'toler']","['down', 'tolerate']"
Availability,"I think we need to figure out how to get cblas on the broad cluster. ```; # use UGER; # ish -l os=RedHat7; # use Anaconda3; # use Java-1.8; # use OpenBLAS; # source activate hail; # ipython; In [1]: import hail as hl . In [2]: mt = hl.balding_nichols_model(3, 100, 100) ; Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:693,avail,available,693,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['avail'],['available']
Availability,"I think we should continue with another review and then a load test. I'm still a bit hesitant about the query change, but we can keep an eye on it. I'm still get errors with the typing:. ```; (venv) jigold@wm349-8c4 hail % make -C hail/python check; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; python3 -m pylint --rcfile ../../pylintrc hailtop --score=n; python3 -m mypy --config-file ../../setup.cfg hailtop; hailtop/batch/backend.py:481: error: Incompatible types in assignment (expression has type ""Union[str, List[str], None]"", variable has type ""Optional[List[str]]""); Found 1 error in 1 file (checked 146 source files); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464:162,error,errors,162,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464,3,['error'],"['error', 'errors']"
Availability,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1013,Down,Download,1013,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,1,['Down'],['Download']
Availability,"I think we're okay in terms of not having errors currently. This was a backwards compatibility code path. I will double check again, but I think we first create the config on the driver `GCPSlimInstanceConfig.create()`. This config is sent to the worker which deserializes it, but it's on the last part of that if/else and runs `resources = [gcp_resource_from_dict(data) for data in resources]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825:42,error,errors,42,https://hail.is,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825,1,['error'],['errors']
Availability,I think what we want is an error code on apply nodes! There are lots of functions that can throw errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10119#issuecomment-786836520:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/10119#issuecomment-786836520,2,['error'],"['error', 'errors']"
Availability,"I think you'll still have the doctest failure. It just returns list of lists now ,right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8265#issuecomment-601433785:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601433785,1,['failure'],['failure']
Availability,I think you're getting an error because you need to add this to `hl.experimental.__init__.__all__`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4052#issuecomment-409700497:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700497,1,['error'],['error']
Availability,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,2,"['Error', 'error']","['Error', 'error']"
Availability,I timed creating the new indices. Confirmed the operations are not blocking and they took 50 minutes on a 4 core database. We'd want to change the database size to 6 or 8 cores before doing this as it used about 70-80% of the available CPU on a 4 core machine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748#issuecomment-1468431358:226,avail,available,226,https://hail.is,https://github.com/hail-is/hail/pull/12748#issuecomment-1468431358,1,['avail'],['available']
Availability,"I tossed this up in my namespace and this is what seems to be the issue:; ```. Job Step	Image Pulling Time (s)	Running Time (s)	Error Type	State; main	0.135	30.011	timed out	error; Logs; Main; Log; executable file `sleep 5` not found in $PATH: No such file or directory; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 680, in _run; raise ContainerTimeoutError(f'timed out after {self.timeout}s'); ContainerTimeoutError: timed out after Nones",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811:128,Error,Error,128,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811,3,"['Error', 'error']","['Error', 'error']"
Availability,I tracked down the failure. sc.textFile with a glob is non-deterministic (unlike the sc.union it replaced). So we need to either go back to the old code or reorder the partitions a la partitioned parquet reader. I'll deal with it tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248515508:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248515508,2,"['down', 'failure']","['down', 'failure']"
Availability,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['down'],['down']
Availability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,2,['error'],['error']
Availability,"I tried out the tiebreaking_expr with some dummyish data:. ```from hail import *; from hail.representation import *; import subprocess; import os; hc = HailContext(). vds = hc.read('gs://daly_atgu_finnish_swedish_exomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178:983,error,error,983,https://hail.is,https://github.com/hail-is/hail/issues/2178,1,['error'],['error']
Availability,"I tried to achieve this within the CSS flex system, but I had trouble making the buttons equal-sized. The CSS grid system seemed more suited to our needs. The `grid-gap` property tells the browser to leave at least that much space between elements (the current code achieved that with this funky `a + a` rule). The key new rules are:; ```css; grid-template-columns: repeat(auto-fit, minmax(130px, 1fr));; width: 100%;; ```; The first rule tells the browser to put as many columns in the grid as possible, except that:; 1. no column may be less than 130px (approximately the length of the phrase ""Hail Query""); 2. no column may occupy more than one ""fraction"" of the horizontal space (which is to say: each column gets an equal portion of the horizontal space). The second rule tells the browser how much space is available for the grid. Without that rule, the browser tries to make the grid as small as possible, i.e. one column wide. <img width=""1020"" alt=""Screen Shot 2021-12-13 at 4 09 29 PM"" src=""https://user-images.githubusercontent.com/106194/145889522-af0c2367-0f37-43e2-8451-720a25981460.png"">; <img width=""331"" alt=""Screen Shot 2021-12-13 at 4 09 17 PM"" src=""https://user-images.githubusercontent.com/106194/145889525-b6a12ecc-0e7c-4af1-9b0f-5a6fdda322ab.png"">; <img width=""379"" alt=""Screen Shot 2021-12-13 at 4 09 11 PM"" src=""https://user-images.githubusercontent.com/106194/145889526-05773377-1ada-4a5d-81de-749ddac412e1.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11152:813,avail,available,813,https://hail.is,https://github.com/hail-is/hail/pull/11152,1,['avail'],['available']
Availability,"I tried to benchmark 100M rows against Spark:. ```; $ spark-shell; scala> val df = spark.range(100000000); df: org.apache.spark.sql.Dataset[Long] = [id: bigint]. scala> val df2 = df.select(df.col(""id""), functions.rand().as(""x"")); df2: org.apache.spark.sql.DataFrame = [id: bigint, x: double]. scala> df2.write.parquet(""df2.parquet""); 18/07/29 13:47:09 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2); Caused by: java.lang.OutOfMemoryError: Java heap space; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4030#issuecomment-408697268:352,ERROR,ERROR,352,https://hail.is,https://github.com/hail-is/hail/pull/4030#issuecomment-408697268,1,['ERROR'],['ERROR']
Availability,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:77,error,errors,77,https://hail.is,https://github.com/hail-is/hail/issues/4733,3,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:186,down,download,186,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579,1,['down'],['download']
Availability,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009:143,error,error,143,https://hail.is,https://github.com/hail-is/hail/issues/8009,2,['error'],['error']
Availability,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:96,error,error,96,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520,1,['error'],['error']
Availability,"I use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:325,ERROR,ERROR,325,https://hail.is,https://github.com/hail-is/hail/issues/9742,2,"['ERROR', 'error']","['ERROR', 'errored']"
Availability,I use these functions to monitor the k8s cluster. These are useful in the interim while we; move towards more robust monitoring solutions. To make these accessible modify your ~/.bashrc; or ~/.zshrc to have this line:. source /path/to/hail-repository/devbin/functions.sh. cc: services-team: @jigold @CDiaz96 @catoverdrive @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10049:110,robust,robust,110,https://hail.is,https://github.com/hail-is/hail/pull/10049,1,['robust'],['robust']
Availability,I use this:; ```; dking@wmb16-359 # echo $SPARK_HOME; /Users/dking/borg/spark-2.0.2-bin-hadoop2.7/; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319704034:36,echo,echo,36,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319704034,1,['echo'],['echo']
Availability,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:620,echo,echo,620,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,1,['echo'],['echo']
Availability,I used pip install - currently struggling to install the pyspark 2.0.2 version after downgrading to spark 2.0.2 . ``$SPARK_HOME is /Users/ih/languages/spark-2.0.2-bin-hadoop2.7/bin``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678:85,down,downgrading,85,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678,1,['down'],['downgrading']
Availability,I used the gsutil storage bandwidth tool and confirmed we get 1.2 Gibit / second upload and download speeds from within a 1 core job and 10 Gi storage. Adding more cores didn't change anything. I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. I'm wondering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write thr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:92,down,download,92,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['down'],['download']
Availability,I verified the test failed with the same [error as KC's](https://discuss.hail.is/t/repartition-on-read/2148/2) before my change.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10697:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/10697,1,['error'],['error']
Availability,"I verified this now works and also verified it fails on current main:; ```; In [2]: from hailtop.hail_logging import *; ...: import logging; ...: configure_logging(); ...: logging.getLogger('foo').info(""hello!""); ...: ; ...: try:; ...: raise ValueError('boom!'); ...: except:; ...: logging.getLogger('foo').exception(""hello!""); {""severity"":""INFO"",""levelname"":""INFO"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:4"",""message"":""hello!"",""hail_log"":1}; {""severity"":""ERROR"",""levelname"":""ERROR"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:9"",""message"":""hello!"",""exc_info"":""Traceback (most recent call last):\n File \""<ipython-input-2-740eb5422cd6>\"", line 7, in <module>\n raise ValueError('boom!')\nValueError: boom!"",""hail_log"":1}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535:524,ERROR,ERROR,524,https://hail.is,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535,2,['ERROR'],['ERROR']
Availability,"I want the function to exit in error if any step fails. I achieve this by; starting a sub-shell and setting -e inside that sub shell. That causes the; sub-shell to exit if any individual command fails. Previously, the `git clone` could fail but `clone` would still return exit code; zero.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8190:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/pull/8190,1,['error'],['error']
Availability,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209:200,down,down,200,https://hail.is,https://github.com/hail-is/hail/pull/2209,1,['down'],['down']
Availability,"I want to get it to not only omit the Java, but also show the desirable python stack trace that points to the source of the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226#issuecomment-672876115:124,error,error,124,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672876115,1,['error'],['error']
Availability,"I want to have a functionality when import VCF, disable the filter based on `sum(AD) != DP`. Although this is a good sanity check, but since we are not clear if this error will lead to inaccurate genotype calls, there will be an option for analyst to keep those calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/427:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/issues/427,1,['error'],['error']
Availability,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964:368,down,down,368,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964,1,['down'],['down']
Availability,"I was able to figure out how to remove all the ""network shuffle""s and ""coerced sorted dataset""s and that improved the time down to 73 seconds, so a big improvement! I would still hope to improve performance a bit more, being reliably under a minute would be helpful. Here are the logs from that search, let me know what else I can do to help improve the performance or to help you figure it out: ; [hail-search.log](https://github.com/hail-is/hail/files/13310449/hail-search.log). PR is here if you are interested: https://github.com/broadinstitute/seqr/pull/3717",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779:123,down,down,123,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779,2,"['down', 'reliab']","['down', 'reliably']"
Availability,"I was able to narrow this down a bit further. The issue appears due to this statement: https://github.com/broadinstitute/gnomad-browser/blob/80430090645ce087aa54d67688a4f0920ad1c8fd/data-pipeline/src/data_pipeline/datasets/gnomad_v3/gnomad_v3_variants.py#L127-L143. `subsets` contains 8 elements: `{'non_cancer', 'tgp', 'controls_and_biobanks', 'non_neuro', None, 'non_topmed', 'hgdp', 'non_v2'}`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609:26,down,down,26,https://hail.is,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609,1,['down'],['down']
Availability,"I was confused at how these could work because `self.conn.commit` and `self.conn.rollback` are async functions, but then I couldn't find any invocations of these methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11367:81,rollback,rollback,81,https://hail.is,https://github.com/hail-is/hail/pull/11367,1,['rollback'],['rollback']
Availability,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:442,avail,available,442,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,3,"['avail', 'echo', 'error']","['available', 'echo', 'error']"
Availability,"I was surprised to see this didn't fail by fixing some fails_local_backend tests. It turns out that we're writing out invalid files with the lowered matrix writer:; ```; def test_indexed_read(self):; mt = hl.utils.range_matrix_table(2000, 100, 10); f = new_temp_file(extension='mt'); mt.write(f); mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ]); self.assertEqual(mt2.n_partitions(), 2); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ], _filter_intervals=True); self.assertEqual(mt2.n_partitions(), 3); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). E Java stack trace:; E is.hail.utils.HailException: `intervals` specified on an unindexed matrix table.; E This matrix table was written using an older version of hail; E rewrite the matrix in order to create an index to proceed; E 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); E 	at is.hail.utils.package$.fatal(package.scala:77); E 	at is.hail.expr.ir.MatrixNativeReader$.apply(MatrixIR.scala:166); E 	at is.hail.expr.ir.MatrixNativeReader$.fromJValue(MatrixIR.scala:184); ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282:1221,Error,ErrorHandling,1221,https://hail.is,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282,2,['Error'],['ErrorHandling']
Availability,"I was trying to filtervariants with an interval_list, and I wrote:. filtervariants list --keep -i /user/satterst/exome_evaluation_regions.v1.interval_list count; (when clearly I should have done filtervariants intervals... instead of filtervariants list...). and it kept getting most of the way done and then failing, with the error message:; hail: count: fatal: invalid variant. Invalid variant? I double-checked that my dataset was OK by running count on the whole thing, telling me that the filtering was the problem, so I created different versions of the interval_list file, trying to figure out if something was specified incorrectly in the file... I was literally troubleshooting the interval_list file for over half an hour before I realized what the problem was. And it's not the first time I've made this mistake. . I humbly submit that a better error message here would be helpful, something like:; filtervariants: fatal: list expected but intervals encountered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/383:327,error,error,327,https://hail.is,https://github.com/hail-is/hail/issues/383,2,['error'],['error']
Availability,I went down that route once before and the main issue is how to trigger the change in batch state to completed. I couldn't figure out how to do that correctly and efficiently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594:7,down,down,7,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594,1,['down'],['down']
Availability,"I will submit a larger batch PR soon, not sure this is worth addressing until then, because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to addres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5118:955,error,error,955,https://hail.is,https://github.com/hail-is/hail/issues/5118,1,['error'],['error']
Availability,"I wonder if related to #4754?; ```; ht = hl.experimental.import_gtf('gs://konradk/gencode.v19.annotation.gtf.bgz', 'GRCh37', True, min_partitions=12); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0],; transcript_id=ht.transcript_id.split('\\.')[0],; length=ht.interval.end.position - ht.interval.start.position + 1); coding_regions = ht.filter(ht.feature == 'CDS').select('gene_id', 'transcript_id', 'transcript_type', 'length', 'level'); transcripts = coding_regions.group_by('transcript_id', 'transcript_type', 'gene_id',; transcript_level=coding_regions.level).aggregate(; cds_length=hl.agg.sum(coding_regions.length),; num_coding_exons=hl.agg.count(); ).key_by('transcript_id'); ```; Afterwards:; ```; transcripts.count() # fails with error below; transcripts.persist().count() # succeeds; ```; on current master (d33e2d1c19b2); ```; Py4JJavaError: An error occurred while calling z:is.hail.expr.ir.Interpret.interpretPyIR.; : java.util.NoSuchElementException: key not found: interval; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.rvd.RVDType.<init>(RVDType.scala:23); 	at is.hail.expr.types.TableType.<init>(TableType.scala:16); 	at is.hail.expr.types.TableType.copy(TableType.scala:15); 	at is.hail.expr.ir.TableMapRows.<init>(TableIR.scala:592); 	at is.hail.expr.ir.Simplify$$anonfun$tableRules$1.applyOrElse(Simp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4799:743,error,error,743,https://hail.is,https://github.com/hail-is/hail/issues/4799,2,['error'],['error']
Availability,"I would also suggest the following as necessary for an upcoming release:. * #13728. Google's gcsfuse APT repository currently produces 502 Bad Gateway errors when accessed via http, which shows no sign of being resolved any time soon. I've commented on #13728 noting how this PR can work around the problem. At present (since early October), the `batch_worker_image` job always fails with 502 during a hail deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602:151,error,errors,151,https://hail.is,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602,1,['error'],['errors']
Availability,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:184,error,error,184,https://hail.is,https://github.com/hail-is/hail/issues/2070,3,"['Error', 'error']","['Error', 'error']"
Availability,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195:175,error,erroring,175,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195,1,['error'],['erroring']
Availability,"I'd be ok with taking it off the nav bar if there was some other link to it that was reasonable and findable. Python is a good example. They always show newest thing, but they have a drop down at the top that lets you switch: https://docs.python.org/3/library/re.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8210#issuecomment-593504954:188,down,down,188,https://hail.is,https://github.com/hail-is/hail/pull/8210#issuecomment-593504954,1,['down'],['down']
Availability,"I'd probably suggest a better error message if possible, but if you don't want to, go ahead and close",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161,1,['error'],['error']
Availability,"I'd think this: `ht.transmute(ref=ht.alleles[0], alt=ht.alleles[1])` should error out since it's modifying a key field, but it doesn't seem to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/3673,1,['error'],['error']
Availability,"I'll admit I didn't test this since I'm currently debugging the deadlock PR, but this should quiet down the error logs that are being emitted whenever a test runs that intentionally throws an exception in Query.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11376:99,down,down,99,https://hail.is,https://github.com/hail-is/hail/pull/11376,2,"['down', 'error']","['down', 'error']"
Availability,"I'll digest over the next day or so, then let's sit down and go through some of the design choices + trajectory together.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074#issuecomment-320641116:52,down,down,52,https://hail.is,https://github.com/hail-is/hail/pull/2074#issuecomment-320641116,1,['down'],['down']
Availability,"I'll do a performance test, but there's still foreign key constraints on these rows. They're just redundant. We don't need a check on both `batches` and `attempts`. The rows in `attempts` wouldn't have been inserted without the check in `batches`. All of these proposed changes don't change anything about data integrity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811:98,redundant,redundant,98,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811,1,['redundant'],['redundant']
Availability,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/747#issuecomment-256325883:147,echo,echo,147,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883,1,['echo'],['echo']
Availability,"I'll rerun CI now. In case it's helpful, here are the failures from the last run, all in the doctests:; ```; FAILED usr/local/lib/python3.9/dist-packages/hail/matrixtable.py::hail.matrixtable.MatrixTable.from_parts; FAILED usr/local/lib/python3.9/dist-packages/hail/matrixtable.py::hail.matrixtable.MatrixTable.localize_entries; FAILED usr/local/lib/python3.9/dist-packages/hail/table.py::hail.table.Table.to_matrix_table; FAILED usr/local/lib/python3.9/dist-packages/hail/table.py::hail.table.Table.to_matrix_table_row_major; FAILED usr/local/lib/python3.9/dist-packages/hail/experimental/full_outer_join_mt.py::hail.experimental.full_outer_join_mt.full_outer_join_mt; FAILED usr/local/lib/python3.9/dist-packages/hail/methods/impex.py::hail.methods.impex.import_vcf; FAILED usr/local/lib/python3.9/dist-packages/hail/methods/statgen.py::hail.methods.statgen.balding_nichols_model; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14250#issuecomment-1991409946:54,failure,failures,54,https://hail.is,https://github.com/hail-is/hail/pull/14250#issuecomment-1991409946,1,['failure'],['failures']
Availability,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:867,redundant,redundant,867,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,1,['redundant'],['redundant']
Availability,I'll take another look. Looks like you're hitting some failures in Python now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3414#issuecomment-385520903:55,failure,failures,55,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-385520903,1,['failure'],['failures']
Availability,I'm a little puzzled by this solution. I feel that we shouldn't be modifying class composition in method calls; the structure of our code seems somewhat upside down. If anything the StagedArrayBuilder should only take a class builder.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13939#issuecomment-1785433230:160,down,down,160,https://hail.is,https://github.com/hail-is/hail/pull/13939#issuecomment-1785433230,1,['down'],['down']
Availability,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:176,down,downloaded,176,https://hail.is,https://github.com/hail-is/hail/issues/1520,2,"['down', 'error']","['downloaded', 'error']"
Availability,I'm confused by this failure. It looks the CI didn't merge with the latest master. Maybe rebase?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1127#issuecomment-263737204:21,failure,failure,21,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263737204,1,['failure'],['failure']
Availability,"I'm curious how you would test this. I've manually tested and confirmed it works with the currently commented out test. That test is kind of slow and heavy though, and it's hard to write a good version if I don't know how much memory someone has available. Do you think it's worth adding a new CI job to run certain tests in constrained memory environment to verify they stay below a prescribed limit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10233#issuecomment-808504566:246,avail,available,246,https://hail.is,https://github.com/hail-is/hail/pull/10233#issuecomment-808504566,1,['avail'],['available']
Availability,"I'm currently running this branch of CI on a pull request of itself on my own fork of hail, and it nearly passes all tests except for hailtop_batch_* because of requester pays permissions issues and monitoring, because I don't have a service account in my project with all the permissions for broad-ctsa. So unfortunately haven't fully validated that it will _not_ merge a passing PR, but this seemed good enough that we can push it through for azure (since both of these errors are gcp-dependent). If this goes through I can put in a follow-up PR that mirrors the infra resources that CI needs in azure (blob storage, acr permissions, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539:472,error,errors,472,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539,1,['error'],['errors']
Availability,I'm down to about 350 issues. The next one will probably be the last one.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8804:4,down,down,4,https://hail.is,https://github.com/hail-is/hail/pull/8804,1,['down'],['down']
Availability,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:840,down,down,840,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['down'],['down']
Availability,"I'm fine removing copy-paste-tokens. They were for a prototype with Terra. We obviously are pursuing a different approach now. Hmm. I suppose old versions of hailctl have no way to know that the fix is to upgrade to a newer version of hailctl? Like, the server can't send a message in the auth failure? We can just ask our local users to upgrade. As long as there's a stable & robust version of query that they can rely on, I think they're happy to upgrade. Which version of hailctl is compatible with new auth?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024:294,failure,failure,294,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024,2,"['failure', 'robust']","['failure', 'robust']"
Availability,"I'm fine with calling it whatever. `die` could be mistaken for something that will shut down hail, at the same time, that's what the IR node is called.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-634292347:88,down,down,88,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-634292347,1,['down'],['down']
Availability,"I'm fine with this, though I think frequency of failures is still relatively high. I have been hitting retry on PRs when i notice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11265#issuecomment-1027179803:48,failure,failures,48,https://hail.is,https://github.com/hail-is/hail/pull/11265#issuecomment-1027179803,1,['failure'],['failures']
Availability,"I'm going to close for now. If we realize this solves some of the deadlock errors, then we can reopen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7853#issuecomment-581597915:75,error,errors,75,https://hail.is,https://github.com/hail-is/hail/pull/7853#issuecomment-581597915,1,['error'],['errors']
Availability,I'm going to close this issue because I feel like we've moved past the sticking point that this issue is referring to. @konradjk if downstream operations are still having problems please feel free to open issues for them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5320#issuecomment-479696405:132,down,downstream,132,https://hail.is,https://github.com/hail-is/hail/issues/5320#issuecomment-479696405,1,['down'],['downstream']
Availability,I'm going to close this. The problem is more systemic -- got other errors with the types not being correct for `Die` and `In`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4784#issuecomment-439439892:67,error,errors,67,https://hail.is,https://github.com/hail-is/hail/pull/4784#issuecomment-439439892,1,['error'],['errors']
Availability,I'm going to work on making the pvc failure not throw an exception and have a polling loop that tries to recreate the jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6075#issuecomment-490967834:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-490967834,1,['failure'],['failure']
Availability,"I'm gonna push a change that puts a hard 2 minute limit on all tests, we'll see which ones timeout, then I'll mark the ones that are legitimately slow with a per-test timeout. Hopefully this will isolate us down to both the test and particular portion of code that's getting stuck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144:207,down,down,207,https://hail.is,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144,1,['down'],['down']
Availability,"I'm good with this, but I want my service PR to merge first so that we can adjust the test failure annotations accordingly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10421#issuecomment-831299029:91,failure,failure,91,https://hail.is,https://github.com/hail-is/hail/pull/10421#issuecomment-831299029,1,['failure'],['failure']
Availability,"I'm good with waiting, ping here when ready",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-526623710:23,ping,ping,23,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-526623710,1,['ping'],['ping']
Availability,I'm having second thoughts on echo. let's let @danking tie break.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1573#issuecomment-287745455:30,echo,echo,30,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287745455,1,['echo'],['echo']
Availability,"I'm looking at `worker.py` now and it looks like you worked around this with the addition of `ignore_job_deletion`, which maybe Dan wasn't aware of but is still a workaround since Timings just shouldn't care about deletion in the first place. Without that flag you'd get this:. 1. Running step would start; 2. Job is cancelled, so `Job.deleted` would be set to `True`.; 3. Job would set the Container's `deleted_event`, which would abort the run function inside `run_until_done_or_deleted`; 4. Container would jump to the uploading logs step, which would raise a job deleted error before running `upload_log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946:575,error,error,575,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946,1,['error'],['error']
Availability,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1913:385,down,download,385,https://hail.is,https://github.com/hail-is/hail/issues/1913,1,['down'],['download']
Availability,"I'm not 100% sure this will fix Ben's out of memory error when copying input files. It's possible there's a memory leak somewhere else. My guess on why this change could solve the issue is that because we open the files in ""r+b"" mode when writing to a local file in parallel, the kernel was caching the data as much as possible based on the available amount of memory to the container because it was anticipating sequential reads. I'd think the kernel is smart enough to evict data when needed, but maybe when using an attached disk with a network drive, the contention is high if the disk starts to lag and it couldn't write all of the data in the write buffer quickly enough and evict unneeded data to meet demand??? At the very least, hopefully this change will allow us to see if there is a true memory leak beyond the kernel using all of the available memory for caching potential reads. <img width=""652"" alt=""Screen Shot 2023-01-26 at 3 52 38 PM"" src=""https://user-images.githubusercontent.com/1693348/214948120-e6bd9671-8cda-48e9-994c-6bab93ddb11a.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12625:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/pull/12625,3,"['avail', 'error']","['available', 'error']"
Availability,"I'm not 100% sure, but I did this locally. I'd like to test it with aiodocker as well to make sure and confirm on the worker before merging. I couldn't find anything online that 100% confirmed, but my understanding of associated volumes was those created by the container create command based on this behavior. ```; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; (base) wmecc-475:ci jigold$ docker volume create foo; foo; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; (base) wmecc-475:ci jigold$ docker create -v foo:/foo google/cloud-sdk:237.0.0-alpine echo hello; 1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local b4b0706c4dfd3ed1907c1fd3325303578f4805a626b88ecbc4935852440577aa; local foo; (base) wmecc-475:ci jigold$ curl --unix-socket /var/run/docker.sock -H ""Content-Type: application/json"" -X DELETE http:/v1.40/containers/1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7?v=true; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658:604,echo,echo,604,https://hail.is,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658,1,['echo'],['echo']
Availability,I'm not aware of an existing problem. We produce an error if sample IDs are not unique and user can rename as needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/78#issuecomment-316223517:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/78#issuecomment-316223517,1,['error'],['error']
Availability,I'm not sure how I feel about the warning / error suggestion when always run jobs have inputs that aren't always copied out. Cleanup jobs shouldn't care whether the outputs don't get copied out on failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11884#issuecomment-1165813547:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1165813547,2,"['error', 'failure']","['error', 'failure']"
Availability,"I'm not sure if this is the right change, but I'm pretty sure the Azure deployment was stuck because `_heal` kept aborting early on a GitHub post error. See #13050 for context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13115:146,error,error,146,https://hail.is,https://github.com/hail-is/hail/pull/13115,1,['error'],['error']
Availability,I'm not sure if this is the right fix here or if we can get rid of the deprecated FS but this is the lint error that's breaking main.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11806:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/11806,1,['error'],['error']
Availability,"I'm not sure quite what we want to do about the DLLs under prebuilt. In the world of dynamic-generated C++, you're going to be linking C++ code compiled on; the master node against libhail. And that isn't going to work reliably if the libhail is; prebuilt, with no guarantees about which compiler/version is used either for the libhail,; or for the dynamic-generated code. The current kludge is that src/main/c/Makefile copies newly-built libraries into prebuilt -; so that *if* you've built from source, then those will be compiled with your compiler.; But it's going to be a crapshoot if you haven't built from source, because the prebuilt; libraries may not work a) against whatever libstdc++.so you have, and b) against your; fresh dynamic-compiled C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-407205066:219,reliab,reliably,219,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-407205066,1,['reliab'],['reliably']
Availability,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8038:58,error,errors,58,https://hail.is,https://github.com/hail-is/hail/pull/8038,1,['error'],['errors']
Availability,"I'm not sure this is the only reason why we're getting worker log errors when a user deletes jobs, but this code is definitely wrong in the case a container hasn't been started. I'm conflicted on whether we should do nothing or write empty files though.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13727:66,error,errors,66,https://hail.is,https://github.com/hail-is/hail/pull/13727,1,['error'],['errors']
Availability,I'm not sure this will make such a performance difference in the common case -- the genotype-level downcode/subset operation will dominate runtime,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484:99,down,downcode,99,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484,1,['down'],['downcode']
Availability,I'm not sure what to do about the invalid block id transient error...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784929:61,error,error,61,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784929,1,['error'],['error']
Availability,I'm not sure why my exception didn't get picked up by the aiohttp.ClientOSError block. I added a plain OSError just in case. Feel free to push back on that. It's possible I didn't have updated is_transient_error code when I got the original error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7340:241,error,error,241,https://hail.is,https://github.com/hail-is/hail/pull/7340,1,['error'],['error']
Availability,I'm preparing a change to a `-Wall` and `-Werror` to the `Makefile` as well so we catch these as build failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1342#issuecomment-277010916:103,failure,failures,103,https://hail.is,https://github.com/hail-is/hail/pull/1342#issuecomment-277010916,1,['failure'],['failures']
Availability,"I'm pretty sure all the failures are from a bug I fixed in #8564. Was waiting for that to merge, but I can rebase to double check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8565#issuecomment-615229346:24,failure,failures,24,https://hail.is,https://github.com/hail-is/hail/pull/8565#issuecomment-615229346,1,['failure'],['failures']
Availability,"I'm repeatedly getting this error on when attempting to run vep GRCh38 on a dataproc cluster started using hailctl with 25 preemptible nodes, and otherwise default params.; ```; 2019-07-14 20:54:55 TaskSetManager: INFO: Finished task 1611.1 in stage 8.0 (TID 21696) in 49183 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1601/10000); 2019-07-14 20:54:57 TaskSetManager: INFO: Starting task 1559.1 in stage 8.0 (TID 21702, bw2-sw-dp3j.c.seqr-project.internal, executor 1, partition 1559, PROCESS_LOCAL, 8800 bytes); 2019-07-14 20:54:57 TaskSetManager: INFO: Finished task 1570.1 in stage 8.0 (TID 21697) in 45412 ms on bw2-sw-dp3j.c.seqr-project.internal (executor 1) (1602/10000); 2019-07-14 20:55:04 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 1.; 2019-07-14 20:55:04 DAGScheduler: INFO: Executor lost: 1 (epoch 0); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-07-14 20:55:04 TransportClient: ERROR: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['error'],['error']
Availability,"I'm running the following script: . /psych/genetics_data/working/cseed/bin/hail read -i ${input_vds} \; annotatevariants tsv file:///medpop/esp2/mzekavat/Estonia/UPDATED_TOOLS/dbNSFPv3.2/dbNSFP3.2a.ALLChr.bgz \; -r va.dbNSFP \; -t 'SIFT_pred: String, PROVEAN_pred: String, Polyphen2_HDIV_pred: String, Polyphen2_HVAR_pred: String, LRT_pred: String, MutationTaster_pred: String, MutationAssessor_pred: String, FATHMM_pred: String, MetaSVM_pred: String, MetaLR_pred: String, CADD_phred: Double, `Eigen-raw`: Double, `Eigen-phred`: Double, `Eigen-raw_rankscore`: Double' \; -v ""#chr,pos(1-based),ref,alt"" \; -m ""."" \; annotatevariants expr -c 'va.of8 = (if (""D"" ~ va.dbNSFP.SIFT_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.PROVEAN_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HDIV_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.Polyphen2_HVAR_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.LRT_pred) 1 else 0) + (if (""H"" ~ va.dbNSFP.MutationAssessor_pred || ""M"" ~ va.dbNSFP.MutationAssessor_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.MutationTaster_pred) 1 else 0) + (if (""D"" ~ va.dbNSFP.FATHMM_pred) 1 else 0)' \; exportvariants -c 'v.contig,v.start,v.ref,v.alt,va.of8,va.dbNSFP.MetaSVM_pred,va.dbNSFP.MetaLR_pred,va.dbNSFP.CADD_phred,va.dbNSFP.`Eigen-raw`,va.dbNSFP.`Eigen-phred`,va.dbNSFP.`Eigen-raw_rankscore`' -o /user/mzekavat/MiGen/dbNSFP.MiGen.tsv. and I'm getting an error here: /medpop/esp2/mzekavat/MiGen/Annotation/hail.log; Would greatly appreciate thoughts on this as soon as possible!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317:1352,error,error,1352,https://hail.is,https://github.com/hail-is/hail/issues/317,1,['error'],['error']
Availability,I'm seeing a PR failure that I can't debug further without the status information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9942:16,failure,failure,16,https://hail.is,https://github.com/hail-is/hail/pull/9942,1,['failure'],['failure']
Availability,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6093:18,failure,failures,18,https://hail.is,https://github.com/hail-is/hail/pull/6093,1,['failure'],['failures']
Availability,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8975:42,ERROR,ERROR,42,https://hail.is,https://github.com/hail-is/hail/pull/8975,1,['ERROR'],['ERROR']
Availability,"I'm still at a loss as to the source of this specific error, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319677878:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319677878,1,['error'],['error']
Availability,"I'm still looking, but I could only find the logs for PR 13509 as PR 13458 is too old. There are no batch worker logs at all for these two instances, but there are a bunch of sys logs. I didn't see an obvious error message, but there's 1000s of sys log messages in there. https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-pr-13509-default-p2aogbaogrsp-highmem-np-zx6w4%22;summaryFields=:false:32:beginning;cursorTimestamp=2023-08-29T20:39:28Z;aroundTime=2023-08-29T20:16:33.950Z;duration=PT24H?project=hail-vdc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516:209,error,error,209,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516,1,['error'],['error']
Availability,"I'm sure this is fine, but I would like to track down all places where .init() (or its side effects) are called, since otherwise I would be approving something I didn't fully understand; we could add these to the note. Currently having some spark version mismatch that I'm working through.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4987#issuecomment-447929316:49,down,down,49,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-447929316,1,['down'],['down']
Availability,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321:628,error,error,628,https://hail.is,https://github.com/hail-is/hail/issues/321,2,"['Avail', 'error']","['Available', 'error']"
Availability,"I'm trying to address three separate error messages:. ```; /usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py:458: Warning: This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'; ```. Add more debug info to warning message with the query executed. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 775, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 812, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 181, in monitor_instances; await asyncio.gather(*[check(instance) for instance in instances]); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 179, in check; await self.check_on_instance(instance); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_collection.py"", line 157, in check_on_instance; assert last_start_timestamp is not None, f'lastStartTimestamp does not exist {spec}'; ```. Handle case where last_start_timestamp is None. ```; Failed to collect and upload profile: [Errno 32] Broken pipe; ```. This is from the google cloud profiler. I reduced the logging level from error to warning for messages from this module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10702:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/10702,2,['error'],['error']
Availability,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:270,down,downloaded,270,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246,2,"['down', 'error']","['downloaded', 'error']"
Availability,"I've added a second commit that fixes the remainder of #13191, marking the individual JobResourceFiles within the ResourceGroup as `_mentioned` and hence preventing the “undefined resource” BatchException previously observed. (Some of the tests in _hail/python/test/hailtop/batch/test_batch.py_ would also need adjusting to account for the re-imagined `_mentioned`.). Having now studied f6fe19c085a9d9ebee23866961cb582a713cc1ad, which introduced `_mentioned` and this error message hint, IMHO this is a reasonable fix. Apart from the code in _backend.py_ to do with `symlink_input_resource_group`, which I haven't looked at, `_mentioned` is maintained solely to decide whether to emit this BatchException hinting to the user that the resource ought to be defined if you're going to use it in `write_output`. In this case, because the filenames are related, `foo.gz.tbi` may well have been created even though only `foo.gz` appears explicitly in the command text, so it may be a false positive (as in #13191's case) to raise the exception. So the conservative thing to do is to suppress the message in these `declare_resource_group` cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633:468,error,error,468,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633,1,['error'],['error']
Availability,"I've addressed the comments, but I can't run the benchmark anymore, I get weird errors about partitions being empty that I expected to be non-empty. I will continue to investigate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2253#issuecomment-332664586:80,error,errors,80,https://hail.is,https://github.com/hail-is/hail/pull/2253#issuecomment-332664586,1,['error'],['errors']
Availability,"I've addressed the two comments: now using an `entry_fields` parameter and throwing an error if 'dosage' is requested and any variant is multi-allelic. Docs updated accordingly. I considered setting dosage on multi-allelics to missing rather than throwing an error, but I think error is safest since I could imagine the missingness leading to QC confusion, and if users want dosage in the presence of multi-allelics than they should either use a custom expression or split and then use `hl.gp_dosage`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913:87,error,error,87,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913,3,['error'],['error']
Availability,"I've been working on an R interface to Hail through the sparklyr package, with some minor success. However, a recent commit (e7552fd55a9d) is somehow causing Spark to stop prematurely when R calls the `is.hail.table.Table.count()` method. Any clues as to why this might be happening?. <details>; <summary>Stack trace</summary>. 	Error: org.apache.spark.SparkException: Job 3 cancelled because SparkContext was shut down; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732); 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651); 	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1920); 	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581); 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); 	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:329,Error,Error,329,https://hail.is,https://github.com/hail-is/hail/issues/4513,2,"['Error', 'down']","['Error', 'down']"
Availability,"I've done some extensive remodeling of Pedigree and MendelErrors, shorter and conceptually cleaner now, got to delete a bunch of code. But I'm having a serialization issue, which may be related to changing MendelError to include the CompleteTrio rather than the sample. For example, if I replace ""implicatedSample"" by pasting the body in the closure instead, then the serialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:382,error,error,382,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['error'],['error']
Availability,"I've eventually reproduced this. I was a bit thrown that the error message in this issue is different to the one in the op. Here's my work so far:; ```python; # create `variants` heterogeneous array as described in the op; variants = [[""10"", 123, ""G"", ""C""], [""10"", 456, ""T"", ""A""]]. # not sure how the `mt` was created, but not sure it's important for the; # purposes of reproducing the failure; mt = hl.struct(; locus=hl.locus(contig='10', pos=60515, reference_genome='GRCh37'),; alleles=['C', 'T']; ). expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2:])),; variants; ). hl.eval(expr); ```; This fails in the call to `any` with the following: ; ```; Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 78, in check; return self.coerce(to_expr(x)); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 275, in to_expr; return cast_expr(e, dtype, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 281, in cast_expr; dtype = impute_type(e, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 129, in impute_type; t = _impute_type(x, partial_type=partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in _impute_type; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in <setcomp>; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 182, in _impute_type; raise ExpressionException(""Hail does not support heterogeneous arrays: ""; hail.expr.expressions.base_e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:61,error,error,61,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,2,"['error', 'failure']","['error', 'failure']"
Availability,"I've merged a stack of changes to this branch including:; - Cleaned up tests, including refactoring, making Balding-Nichols covariates deterministic and removing lots of extra test code; - Reorder args in Scala to match Python, related bug fixes; - Improved large N performance by using single array D rather than A and B; - Moved dense versus sparse matching outside of loop; - Improved Python docs and Scala remarks; - Debugged test failure only occurring in Spark 2.1.0, which turned out to be related to accuracy of Davies. I've increased accuracy to 1e-8 which is enough to make current tests pass. Once this goes in, I'll make PRs to:; - Allow users to set accuracy and iterations on Davies, will use same defaults as R: 1e-6 and 10k.; - Add number of variants per key as column.; - Fix behavior to finish running even if some groups are too big upper bound, or if Cholesky fails. Document this behavior. Less urgently, but to keep in mind:; - If bottleneck, improve performance of Gramian computation in large N case using blocking; - Improve Davies C code",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707:435,failure,failure,435,https://hail.is,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707,1,['failure'],['failure']
Availability,"I've merged fixes to two old bugs and one new bug:; - `typecheck_method` => `typecheck`; - an extra `rvb.startStruct()`; - match error for `alleles: Array[String]` to `IndexedSeq[_]` in `RegionValueBuilder.addAnnotation`. (apologies, I meant to PR rather rather than merge directly but forgot to change cseed to origin). Adding; ```; --init gs://hail-common/nirvana/nirvana-init-GRCh37.sh; ```; at cluster startup and running; ```; import hail as hl; mt = hl.import_vcf(path='gs://jbloom/profile225.vcf.bgz'); mt = mt.filter_rows(hl.len(mt.alleles) == 2); ht = hl.nirvana(mt, config='/nirvana/nirvana-cloud-GRCh37.properties', block_size=10000).rows(); (ht.filter((ht.locus.position > 24430000) & (ht.locus.position < 24580000)); .export(output='gs://jbloom/nirvana_cabin1_3.tsv')); ```; yields the same output as before modulo superficial changes to our representation of variant and flattening of `va` to `rsid	qual	filters	info	nirvana`. Here's an examplar now:; ```; locus	alleles	rsid	qual	filters	info	nirvana; 22:24468386	[""G"",""A""]	NA	3.8351e+04	NA	{""AC"":[306],""AF"":[0.061],""AN"":5018,""BaseQRankSum"":26.807,""ClippingRankSum"":-0.538,""DP"":22432,""DS"":null,""FS"":1.203,""HaplotypeScore"":null,""InbreedingCoeff"":0.0335,""MLEAC"":[309],""MLEAF"":[0.062],""MQ"":59.13,""MQ0"":0,""MQRankSum"":16.406,""QD"":14.9,""ReadPosRankSum"":-0.637,""set"":null}	{""chromosome"":""22"",""refAllele"":""G"",""position"":24468386,""altAlleles"":[""A""],""cytogeneticBand"":""22q11.23"",""quality"":null,""filters"":null,""jointSomaticNormalQuality"":null,""copyNumber"":null,""strandBias"":null,""recalibratedQuality"":null,""variants"":[{""altAllele"":""A"",""refAllele"":""G"",""chromosome"":""22"",""begin"":24468386,""end"":24468386,""phylopScore"":3.457,""isReferenceMinor"":null,""variantType"":""SNV"",""vid"":""22:24468386:A"",""isRecomposed"":null,""regulatoryRegions"":null,""clinvar"":null,""cosmic"":[{""id"":""COSM3759087"",""isAlleleSpecific"":true,""refAllele"":""G"",""altAllele"":""A"",""gene"":""CABIN1"",""sampleCount"":2,""studies"":[{""id"":376,""histology"":""carcinoma"",""primarySite"":""large intestine""},{""id",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231:129,error,error,129,https://hail.is,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231,1,['error'],['error']
Availability,"I've narrowed down where the problem is. `agg.explode` is not treating missing values correctly. This is true if both PL is missing or the entry is missing (filtered out). `agg.explode` is equivalent to `flatMap` aggregator in Scala. This will pass:; `agg.counter(agg.explode(hl.empty_array(hl.tint32)))`. This will fail:; `agg.counter(agg.explode(hl.null(tarray(tint32))))`. This will pass:; ```; e = mt.entries().select('locus', 'alleles', 's', 'PL'); e = e.filter(hl.is_defined(e.PL)); e.aggregate(hl.agg.counter(hl.agg.explode(e.PL))); ```. I tried looking at the `flatMap` function registry function. It looks like the non-aggregator version has `flattenOrNull`, but not the aggregator version. @danking @catoverdrive can you look at the `flatMap` code both in the function registry and the IR to make sure missing values are handled correctly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276#issuecomment-380222316:14,down,down,14,https://hail.is,https://github.com/hail-is/hail/issues/3276#issuecomment-380222316,1,['down'],['down']
Availability,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:269,failure,failure,269,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"I've reviewed most of this and it looks good, just waiting for Chris to finish squashing this tail of FS errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1072729431:105,error,errors,105,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1072729431,1,['error'],['errors']
Availability,"I've reworked the docs, breaking the description into three stages and describing strategies for dealing with the Hadoop write error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-394003822:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-394003822,1,['error'],['error']
Availability,I've simplified / improved the test to show both modes of failure that indeed occur on master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583:58,failure,failure,58,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583,1,['failure'],['failure']
Availability,"I've taking a similar approach to try to remove unwanted elements before exploding. Sadly, I haven't seen any noticable improvement. I'm also not sure about correctness as I couldn't get your changes in [2e45403](https://github.com/broadinstitute/seqr/commit/2e45403efc159b58cec723f86e6de7653d64cf5f) to work (got errors about missing `gene_ids`). I saw ~20 fewer results in the second query with the code below. Here's what I wrote based off master. ```python; def _filter_compound_hets(self):; ch_ht = self._ht; if self._is_recessive_search:; ch_ht = ch_ht.filter(ch_ht.comp_het_family_entries.any(hl.is_defined)). # Get possible pairs of variants within the same gene; ch_ht = ch_ht.annotate(gene_ids=self._gene_ids_expr(ch_ht, comp_het=True)); ch_ht = ch_ht.explode(ch_ht.gene_ids). # Filter allowed transcripts to the grouped gene; transcript_annotations = {; k: ch_ht[k].filter(lambda t: t.gene_id == ch_ht.gene_ids); for k in [ALLOWED_TRANSCRIPTS, ALLOWED_SECONDARY_TRANSCRIPTS] if k in ch_ht.row; }; if transcript_annotations:; ch_ht = ch_ht.annotate(**transcript_annotations); primary_filters = self._get_annotation_filters(ch_ht); secondary_filters = self._get_annotation_filters(ch_ht, is_secondary=True). self.unfiltered_comp_het_ht = ch_ht.filter(hl.any(primary_filters + secondary_filters)); if self._has_secondary_annotations and not (primary_filters and secondary_filters):; # In cases where comp het pairs must have different data types, there are no single data type results; return None. primary_variants = hl.agg.filter(hl.any(primary_filters), hl.agg.collect(ch_ht.row)); if secondary_filters:; row_agg = ch_ht.row; if ALLOWED_TRANSCRIPTS in row_agg and ALLOWED_SECONDARY_TRANSCRIPTS in row_agg:; # Ensure main transcripts are properly selected for primary/secondary annotations in variant pairs; row_agg = row_agg.annotate(**{ALLOWED_TRANSCRIPTS: row_agg[ALLOWED_SECONDARY_TRANSCRIPTS]}); secondary_variants = hl.agg.filter(hl.any(secondary_filters), hl.agg.collect(row_agg)); el",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1828689808:314,error,errors,314,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1828689808,1,['error'],['errors']
Availability,ICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/pr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10041,echo,echo,10041,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"IJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.81 yields a different error (`Class too large`):. ```; Traceback (most recent call last):; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1123>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:14101,error,error,14101,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['error'],['error']
Availability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/gi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3799,echo,echo,3799,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c H,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9275,echo,echo,9275,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + ec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11730,echo,echo,11730,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,IP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'REMOTE is unset or empty'; REMOTE is unset or empty; + exit 1; make: *** [release] Error 1. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:16084,echo,echo,16084,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,3,"['Error', 'echo']","['Error', 'echo']"
Availability,IR Dict lookups should include key/dict in error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6158:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/6158,1,['error'],['error']
Availability,IR types are always available,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3332:20,avail,available,20,https://hail.is,https://github.com/hail-is/hail/pull/3332,1,['avail'],['available']
Availability,"IRSuite passes, but all tests do not. From test_docs:. ```. Java stack trace:; is.hail.utils.HailException: not a streamable IR: (ToArray; (ArrayMap __iruid_226; (ToStream; (ToArray; (GetTupleElement 0; (Ref __iruid_225)))); (MakeTuple (0 1); (GetField key; (Ref __iruid_226)); (GetTupleElement 0; (GetField value; (Ref __iruid_226)))))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:851); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191:357,Error,ErrorHandling,357,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191,2,['Error'],['ErrorHandling']
Availability,"Ideally, a stream would be able to recover from a transient error by; seeking, but until we have that functionality, this avoids having; one failure out of 5000 (which I have now seen twice). Example: https://batch.hail.is/batches/1531518/jobs/2094.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11716:35,recover,recover,35,https://hail.is,https://github.com/hail-is/hail/pull/11716,3,"['error', 'failure', 'recover']","['error', 'failure', 'recover']"
Availability,"If I can GET the URL I'm about to redirect to, then the only point of failure; remaining is the gateway nginx, and I trust that to work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4692:70,failure,failure,70,https://hail.is,https://github.com/hail-is/hail/pull/4692,1,['failure'],['failure']
Availability,"If a KeyTable has no keys, then the KeyedRDD should have an empty row as the key rather than throwing a fatal error. This was causing problems when using `same` for KeyTables with no keys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2115:110,error,error,110,https://hail.is,https://github.com/hail-is/hail/pull/2115,1,['error'],['error']
Availability,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8830:92,error,errors,92,https://hail.is,https://github.com/hail-is/hail/pull/8830,4,['error'],"['error', 'errors']"
Availability,"If a compatible annotation dataset can't be found in `index_compatible_version`, show the user the available versions and reference genome builds of the requested annotation dataset in the raised `ValueError`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10515:99,avail,available,99,https://hail.is,https://github.com/hail-is/hail/pull/10515,1,['avail'],['available']
Availability,"If a job is ""Running"", but the pod will always enter ""CrashLoopBackOff"" (due to a bad image), it will never finish. If a batch is deleted, the job is not marked cancelled so it is returned at the top of the refresh loop, but when the updated job is looked up using `undeleted` records, it is missing. This causes an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6737:316,error,error,316,https://hail.is,https://github.com/hail-is/hail/issues/6737,1,['error'],['error']
Availability,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6995:318,checkpoint,checkpoint,318,https://hail.is,https://github.com/hail-is/hail/issues/6995,1,['checkpoint'],['checkpoint']
Availability,"If a pod is unreachable for any reason, we previously retried forever. However,; a pods are ephemeral; we cannot assume they will return. Instead, if we fail to; contact a pod, we remove it from the pods list and log the error. If the pod; really does exist, the monitor_pods loop will attempt to initialize it again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9858:221,error,error,221,https://hail.is,https://github.com/hail-is/hail/pull/9858,1,['error'],['error']
Availability,"If instance name isn't active, look to see whether it existed in the database before printing an error message about an unknown instance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10325:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/pull/10325,1,['error'],['error']
Availability,"If it's an important VCF, it shouldn't be corrupted... My solution to this error message will be to add something like `requirement failed: ref was equal to alt` or something like that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-216535838:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535838,1,['error'],['error']
Availability,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613:63,error,errors,63,https://hail.is,https://github.com/hail-is/hail/pull/1613,1,['error'],['errors']
Availability,"If there is a true issue, we raise the exception which is caught and printed by; docker_call_retry, or, if that re-raises, it is stored as an error and serialized; back to the driver in Container.run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9980:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/pull/9980,1,['error'],['error']
Availability,"If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518:131,down,downloaded,131,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518,1,['down'],['downloaded']
Availability,"If we cannot authenticate the user, we should send them to a publicly accessible page where the error message can be presented.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12401:96,error,error,96,https://hail.is,https://github.com/hail-is/hail/pull/12401,1,['error'],['error']
Availability,"If we close the db pool before shutting down the task manager and worker pool, then we get coroutines trying to `acquire` a connection after the connection pool is already closed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11433:40,down,down,40,https://hail.is,https://github.com/hail-is/hail/pull/11433,1,['down'],['down']
Availability,"If we feel confident the APIs make sense, then that's a great idea!. I'm a bit worried that localize_entries was an API mistake (my fault :/) that we use because we lack better tools. I'm especially curious to see how its used and whether there's a better API for that kind of work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9121#issuecomment-662711391:132,fault,fault,132,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662711391,1,['fault'],['fault']
Availability,"If we hit an exception and exit the iterator early, then we are no longer iterating. We need to record that fact so that we can retry transient errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12492:144,error,errors,144,https://hail.is,https://github.com/hail-is/hail/pull/12492,1,['error'],['errors']
Availability,"If we miss an event, the refresh loop won't find it because it errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6703:63,error,errors,63,https://hail.is,https://github.com/hail-is/hail/pull/6703,1,['error'],['errors']
Availability,"If with my changes it is still slower, then let's abandon. My bad for heading us down this route. Seems natural to use binary, but given that pandas probably has native code for parsing TSVs, maybe we're fighting a losing battle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484,1,['down'],['down']
Availability,"If you dev deploy right now you'll likely see warnings like this:. ```; (hail) dgoldste@wmce3-cb7 hail % hailctl dev deploy -b hail-is/hail:main -s merge_code; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fb64d58f1f0>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7fb64d5a1520>, 1.908669784)]']; connector: <aiohttp.connector.TCPConnector object at 0x7fb64d579040>; Created deploy batch, see https://ci.hail.is/batches/7992015; (hail) dgoldste@wmce3-cb7 hail %; ```. `HailCredentials` recently changed such that now they contain resources (GCP or Azure credentials) that require closing, so `hail_credentials()` needs to be used as a context manager or you get those unclosed errors on exit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13602:757,error,errors,757,https://hail.is,https://github.com/hail-is/hail/pull/13602,1,['error'],['errors']
Availability,"If you invoke 'hail read -i vds write -o file.tsv', hail will delete that tsv and throw a requirement error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/322:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/issues/322,1,['error'],['error']
Availability,"If you look at the driver logs, there should be a bunch of errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956513975:59,error,errors,59,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956513975,1,['error'],['errors']
Availability,"If you start a HailContext with no arguments:; ```python; HailContext(); ```; then Hail does not require you to set any Spark variables. This error:; ```; hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; ```; is caused by a failure in your Spark cluster. I suggested investigating `spark.cleaner.ttl` due to [Spark bug 5594](https://issues.apache.org/jira/browse/SPARK-5594). This also seems to happen when [you're running more than one spark context at once](https://github.com/spark-jobserver/spark-jobserver/issues/147). You might also be encountering [Spark bug 116599](https://issues.apache.org/jira/browse/SPARK-16599). I think the most productive use of your time is to:; 1. restart your spark cluster; 2. ensure there are no pending jobs and no one will submit any jobs while you run the next steps; 3. start a fresh `pyspark` session; 4. execute your hail commands. If this _still_ fails, then I suspect your Spark cluster is misconfigured in some way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796,2,"['error', 'failure']","['error', 'failure']"
Availability,"If you're encountering this issue the quick fix is to use `array_elements_required=False`. ```; hl.import_vcf(..., array_elements_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:257,checkpoint,checkpoint,257,https://hail.is,https://github.com/hail-is/hail/issues/13346,6,"['Error', 'checkpoint', 'error']","['ErrorHandling', 'checkpoint', 'error']"
Availability,"Implements existing query service endpoints using websockets instead of long-running http requests. I open a new socket for each request instead of attempting to hold one open for each client to send all its requests; not sure if one is preferable to the other, but this one seemed easier to handle and more in line with what we were doing before. ~~I haven't put any sort of heartbeat on either end for now for simplicity; the `blocking_to_async` wrapper around the jvm execution interferes with the server's ability to send/receive pings and pongs, and I'm not currently handling retries for timeouts anyways; would love suggestions on how to make this more robust.~~. Since nginx's default behavior is to close the websocket after 60s of non-activity (which seems pretty reasonable), I have a 30s heartbeat on the server-side websocket connection. This meant rewriting the flow to split a task off to execute the blocking JVM function and keeping the websocket task open to handle the heartbeat. Currently, we rely on the client to close the connection once the jvm task is completed and the result response is received; if the connection is closed/something errors for some other reason, we check to see if the task is completed and cancel it if it's not. The client side still doesn't poll the server for existence, but if the socket is unexpectedly closed we'll retry the request.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636:376,heartbeat,heartbeat,376,https://hail.is,https://github.com/hail-is/hail/pull/9636,6,"['error', 'heartbeat', 'ping', 'robust']","['errors', 'heartbeat', 'pings', 'robust']"
Availability,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:141,Error,Error,141,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083,1,['Error'],['Error']
Availability,Importing several single-sample VCFs with different IDs doesn't generate an error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1989:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/1989,1,['error'],['error']
Availability,Improve assert error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1878:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/1878,1,['error'],['error']
Availability,Improve error message for incorrect type for dict.get default argument,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7377:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/7377,1,['error'],['error']
Availability,Improve error message for starting a cluster without a region,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8791:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/8791,1,['error'],['error']
Availability,Improve error message when bgen.idx does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2412:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/2412,1,['error'],['error']
Availability,Improve robustness of export_plink and export_gen tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4528:8,robust,robustness,8,https://hail.is,https://github.com/hail-is/hail/pull/4528,1,['robust'],['robustness']
Availability,Improved Variant.parse error message.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2026:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/2026,1,['error'],['error']
Availability,"In 0.2, there is Table.to_spark but Table.from_spark is missing. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2938:225,error,error,225,https://hail.is,https://github.com/hail-is/hail/issues/2938,1,['error'],['error']
Availability,"In InferPType, first 2 I looked at: ArrayRef, Coalesce (in getNestedElementPTypesOfSameType). ##### Experiment with ToStream removed in the case _ => condition in streamify:. ```scala; // as above, but catch all condition in streamify:; private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren); x; }; }; ```. results in many errors in IRSuite, one of which is:. > is.hail.utils.HailException: not a streamable IR: (Literal Array[Int32] ""[3,null,7]""); > ...; > at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ##### With `ToStream(x)` as the return of `case _ =>` and the rest same as above. 100 errors in IRSuite, first one I looked at:. > Caused by: java.lang.ClassCastException: is.hail.expr.types.virtual.TInterval cannot be cast to is.hail.expr.types.virtual.TIterable; > at is.hail.expr.ir.InferType$.apply(InferType.scala:95)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051:521,error,errors,521,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051,2,['error'],['errors']
Availability,"In Order Theory, this operation is called ""the upper/downward closure of x"" https://en.wikipedia.org/wiki/Upper_set . `groups_affected_by` maybe? `group_self_and_ancestors`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1760381169:53,down,downward,53,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760381169,1,['down'],['downward']
Availability,"In `SparkBackend`, TableIRs get lowered into SparkStages and value IRs get lowered into SparkPipelines. `SparkStage` represents the necessary computation on a partition of a table, as well as the partitioning information. This can either directly represent a TableIR, in which case the partition IR (`body`) is an array of all the rows of that given partition, or whatever the downstream ValueIR needs---e.g. for `TableCount`, the length of that array; for `TableWrite`, the filename that the partition was written out to, etc. `SparkPipeline` represents a local value that can use the results from the referenced stages. One assumption that I've made in this PR is that none of the bindings across all `SparkStage.globals` will have the same name, and none of them will be named ""context"". (I think this is a fairly reasonably assumption, since we'll just use genUID() to generate unique IDs for all of them and then use unique symbols once #5080 goes in.). In this PR, I've lowered:; - TableCount; - TableCollect; - TableGetGlobals; - TableRange; - TableMapGlobals; - TableMapRows. in order to write some tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5127:377,down,downstream,377,https://hail.is,https://github.com/hail-is/hail/pull/5127,1,['down'],['downstream']
Availability,"In a TableValue, the RVD key may be longer than the TableType key, so it's wrong for the row type of the result of a TableIR execute to depend on the RVD key. I tried to find all cases of this error in TableIR execute methods, and only found these two.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8890:193,error,error,193,https://hail.is,https://github.com/hail-is/hail/pull/8890,1,['error'],['error']
Availability,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7942:91,ERROR,ERROR,91,https://hail.is,https://github.com/hail-is/hail/pull/7942,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"In brief: . Computing an LD (X @ X.T) matrix using hail's BlockMatrices that have been normalized using the default function seems to produce values >> 1 (~1.00000001, but much larger than floating point error). This can cause problems in downstream applications. Example:. # Normalize genotypes; BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(), ; out_dir + out_name + ""_norm"" + ""_bm"", ; mean_impute = True, center = True, normalize = True); bm_norm = BlockMatrix.read(out_dir + out_name + ""_norm"" + ""_bm""). # LD (unadjusted); starts_and_stops = hl.linalg.utils.locus_windows(mt.locus, radius = 2.1e6, _localize = False); bm_ld = (bm_norm @ bm_norm.T); bm_ld = BlockMatrix._from_java(bm_ld._jbm.filterRowIntervalsIR(Env.backend()._to_java_ir(starts_and_stops._ir), False)); bm_ld.write(out_dir + out_name + ""_LD"" + ""_bm"", overwrite = True); bm_ld = BlockMatrix.read(out_dir + out_name + ""_LD"" + ""_bm""). # Export LD matrices; list_range = [list(range(x.start_idx, x.end_idx + 1)) for x in list_meta[0:5]]; bms = [bm_ld.filter(x,x) for x in list_range]; hl.experimental.export_block_matrices(bms, out_dir + out_name + ""_tissue"" + ""_ld""). # Example image of problem:; <img width=""594"" alt=""Screen Shot 2019-06-13 at 5 36 58 PM"" src=""https://user-images.githubusercontent.com/24594616/59470325-52676800-8e05-11e9-93fe-e48c0e06e70b.png"">. If genotypes are normalized to N(0,1), then X @ X.T should never have values larger than 1 except for floating point precision. This is anecdotal, but I never had this problem when using > 100k samples, but here I'm using ~700 samples. I'm not sure what's causing this, but I had a conversation with @liameabbott a while ago about how one should normalize these matrices. His understanding was that hail normalizes by dividing by `sqrt(sum(x^2))` whereas one may prefer to divide `sd(x)`. The example he sent me to do this is below:. # Liam's example; g = BlockMatrix.read('gs://ukbb-ldsc-dev/1000_genomes.phase_3.europeans.GT.autosomes.bm'). n = g.shape[1]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6351:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/issues/6351,2,"['down', 'error']","['downstream', 'error']"
Availability,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239686959:887,error,error,887,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959,1,['error'],['error']
Availability,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9259:301,error,error,301,https://hail.is,https://github.com/hail-is/hail/pull/9259,3,['error'],['error']
Availability,"In main, we assumed that if `self._batch is not None` then there is a Batch that needs cancelling, but that is no longer true. Now, we must track whether we have called `submit` or not. I made a few other minor cleanups of `ServiceBackend` while I was in there. There is no longer any need to have a `None` batch b/c there is no distinction between a builder and a batch now. Example error:; ```; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:92 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for testns-test-418@hail-vdc.iam.gserviceaccount.com; _________ ERROR at teardown of Tests.test_loop_with_struct_of_strings __________. init_hail = None; request = <SubRequest 'set_query_name' for <TestCaseFunction test_loop_with_struct_of_strings>>. @pytest.fixture(autouse=True); def set_query_name(init_hail, request):; backend = current_backend(); if isinstance(backend, ServiceBackend):; backend.batch_attributes = dict(name=request.node.name); yield; backend.batch_attributes = dict(); references = list(backend._references.keys()); for rg in references:; backend.remove_reference(rg); backend.initialize_references(); if backend._batch:; report: Dict[str, CollectReport] = request.node.stash[test_results_key]; if any(r.failed for r in report.values()):; > log.info(f'cancelling failed test batch {backend._batch.id}'). test/hail/conftest.py:81: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /usr/local/lib/python3.9/dist-packages/hailtop/batch_client/aioclient.py:347: in id; self._raise_if_not_created(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <hailtop.batch_client.aioclient.Batch object at 0x7ffae6f11bb0>. def _raise_if_not_created(self):; if not self.is_created:; > raise BatchNotCreatedError; E hailtop.batch_client.aioclient.BatchNotCreatedError. ```. https://batch.hail.is/batches/7950601/jobs/156",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13564:384,error,error,384,https://hail.is,https://github.com/hail-is/hail/pull/13564,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:200,down,down,200,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['down'],['down']
Availability,"In the following hail call, the `sa` binding is not available in the filter's lambda argument. In almost all modern programming languages, bindings are lexical, descending all the way into nested code. We would like to support the same intuitive notion of binding in hail. ```; vds.annotate_samples_expr(; ""sa.mendel = gs.filter(g => va.mendel.filter(x => x.fam == sa.fam).length).count()""); ```. ### Design Suggestion. As we move towards the compiler, this should become more natural because these filters will always be inlined. We need only not reset the environment when descending into a lambda.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1351:52,avail,available,52,https://hail.is,https://github.com/hail-is/hail/issues/1351,1,['avail'],['available']
Availability,"In the latter two cases, the error does not come from zstd decompression. It comes later during region allocation and using isHet on a Call with ploidy 3. When zstd does notice a decompression issue, it's always immediately after a read. In this case, immediately after a read of the entries data, but in the past we've seen reads of other MTs/HTs. Note that the entries are the bulk of the bytes, so if there's something that's rare in terms of bytes processed, we're just much more likely to see it in the entries.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843765049:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843765049,1,['error'],['error']
Availability,"In the short term, a fix which makes the UI usable again for these kinds of jobs is to check blob size, if it's over some threshold, show no log and instruct the user to download it. Then fix the download to use aiohttp's StreamResponse. We should maybe split this issue into a frontend-side and worker-side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936:170,down,download,170,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936,2,['down'],['download']
Availability,"In this PR, I rewrite `linear_regression_rows_nd` to use `_map_partitions` instead of `_group_within_partitions`. By doing this, I've eliminated the need to do a `key_by` at the end of `linear_regression_rows_nd`. I also think this makes the code clearer. . This PR also makes a few seemingly random changes that are actually bug fixes:. 1. When emitting `Apply` nodes, we were grabbing the `Code[Region]` from the first argument to the `MethodBuilder`. However, the assumption that the first argument will always be a `Region` seems to no longer be true. As such, we just construct a `CodeParam` from the `StagedRegion` we have available. . 2. In the NDArrayEmitter, I want to make sure I call the local `emit` method that passes off to `emitWithRegion`, for the same reason as 1: (Can't trust first argument to be a `Region`). 3. In `EmitStream`, I need to use `memoizeField` instead of `memoize`, because regular `memoize` saves to a `LocalRef`, and that will get reset to 0 when `next` is called on a stream. Lesson: don't trust locals for things that must live between elements of a stream. I feel like you have a better idea of how the Stream stuff gets emitted than I do Patrick. I'm curious if what I wrote in `process_block` could be written in a way that would lead to better code getting emitted, as I still need to figure out how to squeeze more performance out of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9469:629,avail,available,629,https://hail.is,https://github.com/hail-is/hail/pull/9469,1,['avail'],['available']
Availability,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1652,Error,Error,1652,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733,1,['Error'],['Error']
Availability,Inbreeding errors out on multiallelic GTs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4373:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/4373,1,['error'],['errors']
Availability,"Includes a general version of Ward's algorithm, a common hierarchical clustering technique important for implementing the UNICORN model. . Specifically pinging @alexb-3 for code review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/593:152,ping,pinging,152,https://hail.is,https://github.com/hail-is/hail/pull/593,1,['ping'],['pinging']
Availability,Incorrect array indexing into a genotype should have a better error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3713:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/issues/3713,1,['error'],['error']
Availability,"Increase memory and cpu for test_hail_services_java to match java query tests. This contains tests of the shuffler IR, which runs the hail compiler, so it seems it should have the same resource limits as the other java query tests. #9401 is getting an out of memory error in `testShuffleIR`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9423:266,error,error,266,https://hail.is,https://github.com/hail-is/hail/pull/9423,1,['error'],['error']
Availability,"Increasing the executor memory per core to 20G/core seemed to help get by this memory error. . It would be useful to have some rule of thumbs for estimating memory requirements based on number of samples and variants. spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 12\; --executor-cores 4\; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 80G\; --driver-memory 60g\; --driver-cores 1\; --name ""$1"" \; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303,2,"['error', 'heartbeat']","['error', 'heartbeatInterval']"
Availability,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:340,redundant,redundant,340,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511,1,['redundant'],['redundant']
Availability,"Indeed, if i do `hl.literal()`, I get the right error message:; ```; hail.expr.expressions.base_expression.ExpressionException: Hail does not support heterogeneous dicts: found dict with values of types [dtype('str'), dtype('int32')]; ```; But fixing the value types upstream removes the requirement for `hl.literal()` and works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653,1,['error'],['error']
Availability,"Installing certbot is hard so I stopped doing that. Instead, I use the cerbot docker image. I also eliminated `sed` use in the letsencrypt directory. We now maintain the options-ssl-nginx.conf file ourselves. I copied the settings from certbot; GitHub. They're straightforward, as a part of regular package versioning maintenance we should also; reconsider our cipher suites and TLS versions. We now have a `make run DRY_RUN=true` option which can be run repeatedly without affecting the default certs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9971:318,mainten,maintenance,318,https://hail.is,https://github.com/hail-is/hail/pull/9971,1,['mainten'],['maintenance']
Availability,Instead of throwing match errors when trying to parse format fields.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['error'],['errors']
Availability,"Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the input, decide what to do, and then perform a second action. I approached testing two ways: First, hand test common operations and errors (copy file, copy dir, overwrite, overwrite dir with file and vice versa, the various treat_dest_as settings, large files, detecting copy-and-files on input on Google Storage, etc.) Second, I enumerated essentially all single input ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:1518,error,error,1518,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['error'],['error']
Availability,"Interesting, this directly contradicts the [k8s documentation on pod lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/):; > Running | The Pod has been bound to a node, and all of the Containers have been created. At least one Container is still running, or is in the process of starting or restarting. However, given the available statuses, Running seems like the most reasonable one to describe a pod in the process of shutting down. Shall we close the issue now that we've understood the semantics or is there further action for us to take?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860:348,avail,available,348,https://hail.is,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860,2,"['avail', 'down']","['available', 'down']"
Availability,Interestingly I was have a consistent test failure here but never got the chance to diagnose it. Would like to come back to it once the glut of PRs is through. Would you rather keep it open or closed?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11931#issuecomment-1266120559:43,failure,failure,43,https://hail.is,https://github.com/hail-is/hail/pull/11931#issuecomment-1266120559,1,['failure'],['failure']
Availability,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/624:593,down,downstream,593,https://hail.is,https://github.com/hail-is/hail/issues/624,1,['down'],['downstream']
Availability,"Introduce FASTAReaderConfig to act as a kind of factory for FASTAReader,; while all FASTAReaders themselves are confined to ThreadLocals (except; in tests). Furthermore, add a lock around the fasta file map to prevent more than; one fasta from being copied per jvm. The can be lock contention on the; map, but if there is large amounts of waiting for said lock, then it; usually means that a fasta is downloading and we definitely should be; waiting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9435:401,down,downloading,401,https://hail.is,https://github.com/hail-is/hail/pull/9435,1,['down'],['downloading']
Availability,Is there a way for me to test this further? My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. This should seed up the feedback substantially for anyone working on an image that transitively depends on other images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7534:100,down,download,100,https://hail.is,https://github.com/hail-is/hail/pull/7534,1,['down'],['download']
Availability,Is there currently a debugging version of Region (one that checks bounds and throws an exception)? Dan and I have both uses that profitably in the past to isolate bugs like this quickly. That might be the first step to tracking this down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522#issuecomment-429179210:233,down,down,233,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429179210,1,['down'],['down']
Availability,Is this as easy as d38896d5b3e5160d50070103f7948158af5a5ea1? The commit gives this error instead:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: ; Index Expressions: int32; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663#issuecomment-514769232:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514769232,1,['error'],['error']
Availability,"Issue came up on doctest branch. Reproducible example:; ```; assoc_vds = hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .variant_qc(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = rnorm(1,1) * sa.culprit'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']).annotate_variants_expr('va.useInKinship = va.qc.AF > 0.05'). kinship_vds = assoc_vds.filter_variants_expr('va.useInKinship'); lmm_vds = assoc_vds.lmmreg(kinship_vds, 'sa.pheno', ['sa.cov1', 'sa.cov2']). lmm_vds.globals; ```. Error message:; ```; Failed example:; lmm_vds.globals; Exception raised:; Traceback (most recent call last):; File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run; compileflags, 1) in test.globs; File ""<doctest default[1]>"", line 1, in <module>; lmm_vds.globals; File ""/Users/jigold/hail/python/hail/dataset.py"", line 1958, in globals; self._globals = self.global_schema._convert_to_py(self._jvds.globalAnnotation()); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 423, in _convert_to_py; d[f.name] = f.typ._convert_to_py(annotation.get(i)); File ""/Users/jigold/hail/python/hail/type.py"", line 243, in _convert_to_py; lst = env.jutils.iterableToArrayList(annotation); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/Users/jigold/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; format(target_id, ""."", name, value)); Py4JError: An error occurr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1368:702,Error,Error,702,https://hail.is,https://github.com/hail-is/hail/issues/1368,1,['Error'],['Error']
Availability,"It also fixes numeric promotion of TInt to TLong, which threw an; assertion error before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/748:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/pull/748,2,['error'],['error']
Availability,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1341:444,error,error-no-such-instruction-while-assembling-project-on-mac-os-x,444,https://hail.is,https://github.com/hail-is/hail/issues/1341,1,['error'],['error-no-such-instruction-while-assembling-project-on-mac-os-x']
Availability,"It causes errors on GRCh38. I propose that the default argument is None, and we if None we use a default dict for GRCh37 or GRCh38 and an empty dictionary for other references.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4925:10,error,errors,10,https://hail.is,https://github.com/hail-is/hail/issues/4925,1,['error'],['errors']
Availability,"It definitely looks like ""ZONE_RESOURCE_POOL_EXHAUSTED"" is the cause of these GPU test failures. In this case it looks like it took ~4 minutes to successfully get a VM (after two exhaustion errors) & schedule the job. By then, our uniform 6 minute timeout per test left us with just two minutes. It looks like the job actually did succeed in the worker (seems to have taken ~2 minutes, seems long, does testing for CUDA do some kind of initialization work?). Looks like backing that off to 10 minutes might be just enough to eventually get us a GPU. Might be worth pulling that into its own build.yaml test job so that it does not block the queue of other tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739:87,failure,failures,87,https://hail.is,https://github.com/hail-is/hail/pull/13739,2,"['error', 'failure']","['errors', 'failures']"
Availability,"It doesn't seem like a bad change. I suspect it's rare for this range to be more than 2 long, and I'd hope that the InsertFields avoids copying the entire row, so I'd be surprised if this ever made a significant difference. But I also doubt adding memory management would slow it down much, so better to be safe I guess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315:280,down,down,280,https://hail.is,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315,1,['down'],['down']
Availability,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:903,down,down,903,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,1,['down'],['down']
Availability,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8468:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/8468,1,['down'],['down']
Availability,It is currently possible to write a blocked index where the virtual file; offset is exactly ((REAL_FILE_OFFSET << 16) | BLOCK_SIZE). This is a bug; and leads to assertion errors when trying to seek to the appropriate row; because `off == end` for that index.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6633:171,error,errors,171,https://hail.is,https://github.com/hail-is/hail/pull/6633,1,['error'],['errors']
Availability,"It is impossible to submit large batches without this. What happens? The timeout per request is 60s. We have 50 x 8MB = 400MB worth of requests in flight. That means the client needs a reliable sustained MINIMUM bandwidth of ~7MB/s to not time out. This doesn't seem reasonable. Without this change, Konrad wasn't able to submit a large batch (although it probably would have gone through eventually with enough retry/backoff). With this, 136K jobs took 2-3m to submit. FYI @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7971:185,reliab,reliable,185,https://hail.is,https://github.com/hail-is/hail/pull/7971,1,['reliab'],['reliable']
Availability,It is my intention to eventually document it when when local mode is feature complete and reliable. I'm not sure what the standard process is for this. I'd be OK to rename it when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9596#issuecomment-707933032:90,reliab,reliable,90,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707933032,1,['reliab'],['reliable']
Availability,It is no longer the case that VCF does not support phased haploid calls. Make a note of this in the code. ## Security Assessment; - This change has no security impact. ### Impact Description; Change error messages only.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14742:199,error,error,199,https://hail.is,https://github.com/hail-is/hail/pull/14742,1,['error'],['error']
Availability,"It is not free and we get emails about having too many page views pretty frequently. I suspect this is due to the jobs page having a download icon. The font provided by Google with its material design icons seems to be free to access at any scale. I got the GitHub octocat from GitHub's website. It's a bit bigger. <img width=""1130"" alt=""this PR"" src=""https://github.com/hail-is/hail/assets/106194/31e1cc67-ce9f-4e1f-a6b2-64258a8596c0"">; <img width=""1130"" alt=""main"" src=""https://github.com/hail-is/hail/assets/106194/ce9cc44d-3332-4b88-b733-4ac46a9f8e16"">. I didn't actually dev deploy batch to check the other assets but I suspect they're fine enough. This is what the question mark in a circle looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Ahelp%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024 And this is what the download icon looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Adownload%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14302:133,down,download,133,https://hail.is,https://github.com/hail-is/hail/pull/14302,2,['down'],['download']
Availability,"It is possible for socket connect to fail if the shuffle service is down (e.g. https://ci.hail.is/batches/91027/jobs/105).; This change ensure we retry forever, periodically logging that we are retrying",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9378:68,down,down,68,https://hail.is,https://github.com/hail-is/hail/pull/9378,1,['down'],['down']
Availability,"It is possible there is a race condition, though I have not witnessed this before. In fact, it seems rather reasonable that GitHub had some intermittent slow down that delayed repository creation or ability to find said repository temporarily.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153:158,down,down,158,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153,1,['down'],['down']
Availability,"It looks like batch test was an infinite loop for: https://github.com/hail-is/hail/pull/4536. So I bumped off the pod running the test (this was maybe bad behavior on my part, but I was also curious how ci/batch would respond):. ```; $ kubectl delete pod job-17-lz6m5; ```. I thought CI would re-run the test, but it got merged!. Output did get uploaded, here it is: https://storage.googleapis.com/hail-ci-0-1/ci/ee92f64477f68737987fd8f21411b0348a3d3420/e4ae86ea520fbc5d98b84811b2cdb83640163910/index.html. In particular the job log consists of:. ```; failed to get container status {"""" """"}: rpc error: code = OutOfRange desc = EOF; ```. I had a `logs -f` running when I did this, so here is the log up to the failure:. [job-17-lz6m5.log](https://github.com/hail-is/hail/files/2474367/job-17-lz6m5.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4541:596,error,error,596,https://hail.is,https://github.com/hail-is/hail/issues/4541,2,"['error', 'failure']","['error', 'failure']"
Availability,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:169,down,downloading,169,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082,1,['down'],['downloading']
Availability,"It looks like permissions for deleting disks and VMs are broken for the `delete_batch_instances` CI step. This job also hung for a long time and then got restarted. There's some other wonky things about this PR, but it just seems like the main issue was the Batch deployment was cancelled mid-run and the driver didn't have time to cleanup those 2 VMs that weren't responding before being shut off. Then the cleanup step isn't actually working so they didn't get cleaned up. The only remaining question I have is why these VMs weren't starting up correctly. There were at least 5 in this one PR that didn't start up in time before the driver was shut down. https://batch.hail.is/batches/7908998/jobs/207",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569:651,down,down,651,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569,1,['down'],['down']
Availability,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948:132,error,errors,132,https://hail.is,https://github.com/hail-is/hail/issues/4948,1,['error'],['errors']
Availability,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240306249:308,Down,Download,308,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249,3,"['Down', 'down']","['Download', 'download']"
Availability,It may be good to have a better error message when users forget to add the file:/// to a unix file path when using hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/381:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/381,1,['error'],['error']
Availability,"It seems the test failures are due to:; 1. TemporaryDirectory (and TemporaryFilename); 2. `hailtop.batch.backend.ServiceBackend` absolutely should not use sync `BatchClient`, the async one is right there!; 3. `hailctl batch submit` is broken because of (2); 4. `test_callback` should use async `BatchClient` b/c it is async. TemporaryDirectory & TemporaryFilename use `hailtop.fs`, which is sync. This is nearly the async FS API except:; 1. `isfile` vs `is_file`; 2. `isdir` vs `is_dir`; 3. `stat` returns a `FileListEntry` instead of a `FileStatus`.; 4. `listfiles` vs `ls`. `hailtop.fs.router_fs.RouterFS` is a sync shim between these APIs. So there's basically sync-vs-async and Python-vs-Hail FS APIs. We have:; 1. sync, Python: `hailtop.fs.FS`.; 2. async, Python: does not exist.; 3. async, Hail: `hailtop.aiotools.fs.FS`.; 4. sync, Hail: `hailtop.fs.router_fs.RouterFS`. If we had (2), we could write an async version of TemporaryDirectory and TemporaryFilename and use those in async methods (in particular, in `hail.backend.ServiceBackend`). The high-level need is that we gotta be careful about not interleaving async-sync-async. Your PR reveals that we were inadvertently violating that rule. It seems best to follow the rule and only use `nest_asyncio` when we're in a Jupyter Notebook.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933:18,failure,failures,18,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933,1,['failure'],['failures']
Availability,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:1784,down,down,1784,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['down'],['down']
Availability,"It seems to only occur when I use bp.read_input(..) to localize many files per job. ; Above a certain number of input files (many thousands), I started getting this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940#issuecomment-1786375579:165,error,error,165,https://hail.is,https://github.com/hail-is/hail/issues/13940#issuecomment-1786375579,1,['error'],['error']
Availability,"It should never not be in the database. That's why it should be an error if we get an event for an instance we've never created before. The reason we keep getting these events is because when we run this query:. ```; filter = f'''; logName=""projects/{PROJECT}/logs/cloudaudit.googleapis.com%2Factivity"" AND; resource.type=gce_instance AND; protoPayload.resourceName:""{self.machine_name_prefix}"" AND; timestamp >= ""{mark}""; '''; ```. We have `timestamp >= {mark}`. This means if `mark` doesn't change each time the event polling loop reruns, then we'll always keep getting the same event as the last one processed. We need that `>=` though because different events can have the same timestamp. So the driver could have been shut down in the middle of processing multiple events with the same timestamp. So we do the conservative thing and try to reprocess the event again until the mark changes to a different timestamp. Does that make sense?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10325#issuecomment-819729595:67,error,error,67,https://hail.is,https://github.com/hail-is/hail/pull/10325#issuecomment-819729595,2,"['down', 'error']","['down', 'error']"
Availability,"It still thought it had `s`, an error message to print as a python traceback, rather than an error id like it has now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10720:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/10720,2,['error'],['error']
Availability,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3288:260,error,errors,260,https://hail.is,https://github.com/hail-is/hail/pull/3288,1,['error'],['errors']
Availability,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922,1,['error'],['error']
Availability,It was failing with 'Error file not found:' for me. Putting all the; arguments on one line fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6956:21,Error,Error,21,https://hail.is,https://github.com/hail-is/hail/pull/6956,1,['Error'],['Error']
Availability,It was previously hard to retry transient errors from synchronous libraries like; requests because `hailtop` lacked a synchronous retry wrapper. This PR; implements such a function and uses it in every place that hail imports; `requests`. I also finally addressed the 1kg download issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8391:42,error,errors,42,https://hail.is,https://github.com/hail-is/hail/pull/8391,2,"['down', 'error']","['download', 'errors']"
Availability,It would be a new class to help express what you're trying to do that doesn't exist yet. To `localize` means to download the file to a VM and `delocalize` is to upload a file back to the cloud.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599635933:112,down,download,112,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599635933,1,['down'],['download']
Availability,"It would be easy to implement TDT, de novo, and mendel errors in expr using this, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2374#issuecomment-340212371:55,error,errors,55,https://hail.is,https://github.com/hail-is/hail/pull/2374#issuecomment-340212371,1,['error'],['errors']
Availability,"It would be great if it was possible to have Hail skip rows that don't have the correct number of fields, and just report them via error messages (without crashing), so that the annotation files (such as EIGEN) can still be used.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/402:131,error,error,131,https://hail.is,https://github.com/hail-is/hail/issues/402,1,['error'],['error']
Availability,"It's a bit confusing, but on the CI page you can navigate to the Artifacts tab from which you can open the [build report's index.html](https://ci.hail.is/repository/download/HailSourceCode_HailCi/33165:id/build/reports/tests/index.html). The error looks to me like a Hail problem, not a you problem. I'll investigate further.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320333635:165,down,download,165,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320333635,2,"['down', 'error']","['download', 'error']"
Availability,It's as if it was just spinning on `exit $BUILD_EXIT`. It had to have finished the last for loop because all the logs open in my browser instead of downloading.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864:148,down,downloading,148,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864,1,['down'],['downloading']
Availability,"It's getting a forbidden error when trying to download the secret. I think I know why. The service account being used is the `batch2` service account from the default namespace. I think we need to do what I did with CI where the service account lives in the default namespace, but it can read secrets, service accounts in the batch pods namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837,2,"['down', 'error']","['download', 'error']"
Availability,"It's hard to go through every line to look for bugs, but the structure looks great. You've got a couple test failures (code cast errors)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9926#issuecomment-767796221:109,failure,failures,109,https://hail.is,https://github.com/hail-is/hail/pull/9926#issuecomment-767796221,2,"['error', 'failure']","['errors', 'failures']"
Availability,"It's not so much so that **we** can check out a tagged release, as we have already worked around the problem. But I would expect that you and any other installations will also run into the same `batch_worker_image` failure. We have been running our production instance using gcsfuse 1.2.0 for about a week now, and I think @illusional will agree with me that we haven't seen any problems from it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622:215,failure,failure,215,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622,1,['failure'],['failure']
Availability,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286,2,['error'],['error']
Availability,"Iterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:4493,error,error,4493,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['error'],['error']
Availability,Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:52); 	at is.hail.expr.ir.ExtractAggregators$.apply(ExtractAggregators.scala:25); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:138); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:217); 	at is.hail.expr.MatrixMapRows.execute(Relational.scala:1272); 	at is.hail.expr.MatrixMapGlobals.execute(Relational.scala:1575); 	at is.hail.expr.MatrixRowsTable.execute(Relational.scala:2275); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:504); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); 	at is.hail.table.Table.write(Table.scala:905); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-258b35d26fb3; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3731:11483,Error,Error,11483,https://hail.is,https://github.com/hail-is/hail/issues/3731,1,['Error'],['Error']
Availability,"Iterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:2234,alive,alive,2234,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['alive'],['alive']
Availability,"It’s a random test, and it seems the current tolerance still allows rare sporadic failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14053:45,toler,tolerance,45,https://hail.is,https://github.com/hail-is/hail/pull/14053,2,"['failure', 'toler']","['failures', 'tolerance']"
Availability,"I’ll look for closely once I get to the retreat, but first impression is that centering and normalizing are redundant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054:108,redundant,redundant,108,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054,1,['redundant'],['redundant']
Availability,"I’m out today. I’m good with the PR if Tim is. On Friday, November 9, 2018, Christopher Vittal <notifications@github.com>; wrote:. > @tpoterba <https://github.com/tpoterba> @jigold; > <https://github.com/jigold> ping; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/4720#issuecomment-437412681>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABnWpLgmNZwI4lVd3I0vKAkwVhicd-4_ks5utawIgaJpZM4YLr0h>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4720#issuecomment-437417490:212,ping,ping,212,https://hail.is,https://github.com/hail-is/hail/pull/4720#issuecomment-437417490,1,['ping'],['ping']
Availability,Java core dump error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/4418,1,['error'],['error']
Availability,Java heapspace error in to_pandas,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/12035,1,['error'],['error']
Availability,"Jobs with large logs (>2GiB-ish) can break workers because the current worker code attempts to load the whole log as `bytes` before uploading it to blob storage. This loading into `bytes` also plagues the batch front end when loading logs from blob storage to present to the user.; ; This updates the worker and front end to always stream through logs, never load them into memory. Additionally, in order to make page loads in the UI reasonable, we limit the length of the log that is shown in the UI, with some advice to download the file if it's too large to render on the page. Fixes #13329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076:522,down,download,522,https://hail.is,https://github.com/hail-is/hail/pull/14076,1,['down'],['download']
Availability,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8177:199,error,errors,199,https://hail.is,https://github.com/hail-is/hail/pull/8177,2,"['down', 'error']","['down', 'errors']"
Availability,"Joins were not being tested and fail with source mismatch if joins are present in both key and agg expressions. This fix is analogous to that on Table.group_by.aggregate in #3730, processing all joins at once, and not reprocessing later. I don't address here a deeper bug that throws a source error when processing more than one entry-indexed. I've made an issue #3763",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3762:293,error,error,293,https://hail.is,https://github.com/hail-is/hail/pull/3762,1,['error'],['error']
Availability,"Julia has regenerated the file with the correct extension, and it is available at `gs://gnomad-public/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.021520.ht`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9955#issuecomment-791633393:69,avail,available,69,https://hail.is,https://github.com/hail-is/hail/pull/9955#issuecomment-791633393,1,['avail'],['available']
Availability,"Just a few comments. Looking good! Looking forward to tests. Also, need to abstract out the code you share with Mendel errors as we discussed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/109#issuecomment-169135201:119,error,errors,119,https://hail.is,https://github.com/hail-is/hail/pull/109#issuecomment-169135201,1,['error'],['errors']
Availability,Just fix the error message in AST and I'm happy with it!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242153148:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242153148,1,['error'],['error']
Availability,Just linting failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13163#issuecomment-1585078013:13,failure,failures,13,https://hail.is,https://github.com/hail-is/hail/pull/13163#issuecomment-1585078013,1,['failure'],['failures']
Availability,Just make all the FS streams double close resilient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9001:42,resilien,resilient,42,https://hail.is,https://github.com/hail-is/hail/pull/9001,1,['resilien'],['resilient']
Availability,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:463,down,down,463,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868,1,['down'],['down']
Availability,"Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756:248,down,download,248,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756,1,['down'],['download']
Availability,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906:61,Error,Error,61,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906,2,['Error'],['Error']
Availability,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:134,error,error,134,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150,1,['error'],['error']
Availability,"Just to be sure, I checked the logs for errors and found none. I manually logged into the database and watched the compaction happen for both tables. I don't think we can do any more due diligence with this. Let's plan on merging on Tuesday when you're back from vacation. ```; mysql> select * from aggregated_billing_project_user_resources_v3 where resource_id = 6 limit 100;; +----------------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:40,error,errors,40,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['error'],['errors']
Availability,"K-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:6391,avail,available,6391,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"K-PYTHON-CRYPTOGRAPHY-3316211) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5663682](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5663682) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **691/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.4 | Improper Certificate Validation <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5777683](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5777683) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Insufficient Verification of Data Authenticity <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813745](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813745) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813746](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813746) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-5813750](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-5813750) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:6383,avail,available,6383,https://hail.is,https://github.com/hail-is/hail/pull/14327,2,['avail'],['available']
Availability,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:463,avail,available,463,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,1,['avail'],['available']
Availability,Key Error `va` in `aggregate_rows`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8316:4,Error,Error,4,https://hail.is,https://github.com/hail-is/hail/issues/8316,1,['Error'],['Error']
Availability,"Keys mismatch by type, not number of fields, but error suggests it's the wrong number.; ```; constraint_ht.key; Out[29]: <StructExpression of type struct{gene: str, expressed: str}>; caf_ht.key; Out[30]: <StructExpression of type struct{gene: str, expressed: bool}>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-31-6560bd06e877>"", line 1, in <module>; constraint_ht.annotate(**caf_ht[constraint_ht.key]); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 349, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1299, in index; raise ExpressionException(f'Key mismatch: table has {len(self.key)} key fields, '; hail.expr.expressions.base_expression.ExpressionException: Key mismatch: table has 2 key fields, found 1 index expressions.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4951:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/issues/4951,1,['error'],['error']
Availability,"Known facts:; - the last CI k8s deployment that started the deploy=1 batch on April 27th was active since at least April 25th.; - For a small window of time that I looked at on April 25th, it kept getting errors when trying to get the Github status for possible merge candidates: 12848, 12849, 12547. There might be other PRs at later dates. I saw at least the same errors for 12848 on April 27th. I'm going to throw out a hypothesis. I merged the dedup attempt resources PR on April 19th. The PRs that were stacked on previous commits of that PR now have merge conflicts with the set of commits that actually got merged. This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. CI didn't get restarted at least from the 25th to the 27th so the merge candidate never would have been refreshed. We know that there's less GKE node turnover in Azure, so not unexpected that the ci pod wouldn't get redeployed on its own. I'm thinking it's possible that I merged the database trigger fix on April 27th in response to the excessive deadlocks we noticed and then rebased the subsequent stacked PRs that had merge conflicts, thus unblocking CI, but I'm not sure (it's really hard to get what I want from the Azure log analytics system). I think the ""bug fix"" here is to reassess the code in CI and possibly harden it where we select the merge candidate and try to get the status so it doesn't block deployments. I have a screenshot from April 25th below in case it's helpful. The log analytics query that is helpful is:. ```; ContainerLog; | where ContainerID == ""273584134970cdae08cf0d412461862e2a0e558888a52c91870ca46a146cbb8a""; | order by TimeGenerated; ```. <img width=""1085"" alt=""Screen Shot 2023-05-24 at 12 58 18 PM"" src=""https://github.com/hail-is/hail/assets/1693348/e2da08b6-5982-46cb-9e2c-2178a19f2f86"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011:205,error,errors,205,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011,2,['error'],['errors']
Availability,Konrad and Beryl have seen this error before trying to use Spark 2.2:; http://discuss.hail.is/t/typeerror-javapackage-object-is-not-callable/250,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319708208:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319708208,1,['error'],['error']
Availability,"Konrad ran into a problem where we started shutting down the worker because it was idle, but in between checking if there were any jobs still running and shutting down the site, a create job request came in. MJS was sent to the driver, but MJC was never sent because the worker shut down. The driver thought the job failed to schedule because the deactivate request was sent in before create job could return. The end result was Konrad's job still ran, but the database was left with an attempt that has a start time but no end time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10594:52,down,down,52,https://hail.is,https://github.com/hail-is/hail/pull/10594,3,['down'],['down']
Availability,"Kubernetes interprets a pod terminating quickly (e.g. `echo hi`) as pod start up failure. If you submit a batch job for `echo hi`, the batch job will take a very long time to complete. My understanding is that it eventually we get lucky and `sh` takes long enough to `echo hi` that k8s is satisfied. ```; # k describe pod job-5-8hbt5 -n batch-pods; Name: job-5-8hbt5; Namespace: batch-pods; Node: minikube/10.0.2.15; Start Time: Wed, 21 Nov 2018 15:12:01 -0500; Labels: app=batch-job; hail.is/batch-instance=91332f5563704be7a54c56dd334de2ba; uuid=fd1810a5a3cd4fa1b60caeb182eff5e5; Annotations: <none>; Status: Running; IP: 172.17.0.49; Containers:; default:; Container ID: docker://b627d8df102687e50c95272980fbfe0fb634caecd3ace6217e3a6ce92cde1b21; Image: alpine:3.8; Image ID: docker-pullable://alpine@sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528; Port: <none>; Host Port: <none>; Command:; echo; left; State: Waiting; Reason: CrashLoopBackOff; Last State: Terminated; Reason: Completed; Exit Code: 0; Started: Wed, 21 Nov 2018 15:13:30 -0500; Finished: Wed, 21 Nov 2018 15:13:30 -0500; Ready: False; Restart Count: 4; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-5-8hbt5 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-lfdr4 (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-lfdr4:; Type: Secret (a volume populated by a Secret); SecretName: default-token-lfdr4; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 2m default-scheduler Successfully assigned job-5-8hbt5 to minikube; Normal SuccessfulMountVolume 2m kubelet, minikube MountVolume.SetUp succeeded for volume ""default-token-lfdr4""; Normal Pulled 53s (x5 over 2m) kubelet, minikube Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4822:55,echo,echo,55,https://hail.is,https://github.com/hail-is/hail/issues/4822,5,"['echo', 'failure']","['echo', 'failure']"
Availability,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-226535225:212,down,down,212,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225,1,['down'],['down']
Availability,LD prune throws a requirement error on non-diploid calls,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12971:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/12971,1,['error'],['error']
Availability,"LOL yeah right u l33t. My fault, should have caught that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468828579:26,fault,fault,26,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468828579,1,['fault'],['fault']
Availability,"LTRjZjJhNTdhZDkzOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""prPublicId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:5791,avail,available,5791,https://hail.is,https://github.com/hail-is/hail/pull/13718,1,['avail'],['available']
Availability,"LWJmMWY5Mzc1NTVhYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""prPublicId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:5859,avail,available,5859,https://hail.is,https://github.com/hail-is/hail/pull/13836,1,['avail'],['available']
Availability,"LWY3ZGM4YjIwOTVhNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""prPublicId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:5859,avail,available,5859,https://hail.is,https://github.com/hail-is/hail/pull/13933,1,['avail'],['available']
Availability,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:410,error,error,410,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635,3,"['Error', 'FAILURE', 'error']","['Error', 'FAILURE', 'error']"
Availability,Left to do:; - error checking on bounds of slices. This could either be done in the emitted code for slice or in the IR with Die and what not. Open to advice on which is better.; - bind shape in the python iR generation so it doesn't get repeated a bunch of times. This would go along with the bounds checking in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6225:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/6225,1,['error'],['error']
Availability,Let me do some experiments and see whether there is a performance cost. The third post down seemed to think the way I wrote it there shouldn't be a performance penalty and it should still use the index. I'm not sure whether I believe that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914:87,down,down,87,https://hail.is,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914,1,['down'],['down']
Availability,Let me know if you need the context. I don't think it's necessary and will break my interface. I didn't want the job id to be available until the whole batch is submitted and the batch id is available.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6332:126,avail,available,126,https://hail.is,https://github.com/hail-is/hail/pull/6332,2,['avail'],['available']
Availability,Let me know if you think the error message is clear.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/9164,1,['error'],['error']
Availability,Let's close until we see another reproduction. Transient error handling has improved a bit since this ticket was opened.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980#issuecomment-1551538732:57,error,error,57,https://hail.is,https://github.com/hail-is/hail/issues/12980#issuecomment-1551538732,1,['error'],['error']
Availability,"Let's compare 8093951-8854 to 8093977-8854. The latter is a failed task (partition 6914) the former is successful. We'll download the logs and make toss away some debug info that changed between the experiments. ```; cat log | rg StreamBlockInputBuffer: | sed 's/bytes.*//' > newlog; ```. Since the latter failed, the log obviously ends earlier, but there are *no differences* (besides timestamps) in the size of the blocks read from GCS. Since these block sizes are read from the input stream, this is pretty good evidence that the bytes aren't corrupted up until now. ```; # git diff --no-index --word-diff good bad ; ...; 2023-12-06 [-19:47:11.500-]{+21:39:00.885+} StreamBlockInputBuffer: INFO: reading 2081[-2023-12-06 19:47:11.531 StreamBlockInputBuffer: INFO: reading 2499-]; ```. The decompressed data size is the same: 65536. It's worth noting this is a relatively small compressed buffer after a series of much larger compressed buffers. This one is 2081 and the immediately previous one is 14675. Most of the ones before this are also in the 14k range. ---. Same experiment on job 7157 again shows no differences in bytes read before the exception occurs. ```; 2023-12-06 [-19:45:18.693-]{+21:36:52.116+} StreamBlockInputBuffer: INFO: reading 17923 ; 2023-12-06 [-19:45:18.809-]{+21:36:52.388+} StreamBlockInputBuffer: INFO: reading 17843[-2023-12-06 19:45:18.810 StreamBlockInputBuffer: INFO: reading 17657-]; [-2023-12-06 19:45:18.811 StreamBlockInputBuffer: INFO: reading 17646-]; ```. The network reads are identical other than the size of the first read. That first read is the serialized function. I'm not that surprised it differs in size between different commits of Hail. The byte counting is done in our code. If we're counting bytes correctly, then it seems like we're reading the same series of chunks from GCS. . ```; GoogleStorageFS$: INFO: read 1755052 (0 of 1755052) oldbb(0, 8388608) newbb(0, 1755052); GoogleStorageFS$: INFO: read 8388608 (62604 of 58870664) oldbb(0, 8388",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843799744:121,down,download,121,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843799744,1,['down'],['download']
Availability,Let's merge this so people don't get horrible error messages for now. I'll make an issue to make typecheck more powerful.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652:46,error,error,46,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652,1,['error'],['error']
Availability,"Let's see if #13969 fixes this. If we don't see these errors again over the next week, let's close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13863#issuecomment-1792532420:54,error,errors,54,https://hail.is,https://github.com/hail-is/hail/issues/13863#issuecomment-1792532420,1,['error'],['errors']
Availability,Liftover from UK Biobank bgen(37) to vcf(38) error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['error'],['error']
Availability,Lindo reports an error with a side-length of 177860. Cal reports a side length of 544768. The square of both is larger than 2^32.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734193173:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734193173,1,['error'],['error']
Availability,Linear Regression in Hail on Broad Cluster Errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:43,Error,Errors,43,https://hail.is,https://github.com/hail-is/hail/issues/5559,1,['Error'],['Errors']
Availability,Load vcf file error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['error'],['error']
Availability,LoadVCF should error on call fields that are not Number=1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['error'],['error']
Availability,"Loader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1710,error,error,1710,https://hail.is,https://github.com/hail-is/hail/issues/11705,8,['error'],['error']
Availability,LocalBackend Better Errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9569:20,Error,Errors,20,https://hail.is,https://github.com/hail-is/hail/pull/9569,1,['Error'],['Errors']
Availability,"Long awaited, this change prompts the batch driver to only schedule jobs on workers with the most recent instance version, i.e. matches the `INSTANCE_VERSION` global variable. This way we can make backwards incompatible changes between the worker and driver without having to manually kill the whole fleet. This will allow pre-existing workers to finish gracefully, as they will just stop receiving work when the new batch driver is deployed and eventually die off. ### Scheduler changes; Just skips instances where the instance version doesn't match `INSTANCE_VERSION`. ### Autoscaler changes; Cluster stats like free mcpu and live instances are tracked per instance version. The autoscaler now only looks at instances of the latest version when deciding whether it needs more workers. This way we don't get stuck unable to schedule new jobs until the old workers die off because there technically are enough cores available to meet demand but they are from old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055:916,avail,available,916,https://hail.is,https://github.com/hail-is/hail/pull/13055,1,['avail'],['available']
Availability,Look into the test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359,1,['failure'],['failure']
Availability,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:1729,redundant,redundant,1729,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['redundant'],['redundant']
Availability,"Looking at the `…/Packages` URL in the previous comment, 1.2.0 is now available (and 1.1.0 does not appear to be there). In our recent local hail update deployment, the `batch_worker_image` job failed repeatedly due to GoogleCloudPlatform/gcsfuse#1424. We worked around this as initially suggested on that issue with populationgenomics/hail@607408bee752dabca48d9a2732b14d32813ace9f, but later comments on the issue suggest that the better approach would be this PR with an additional change to access the apt repo via https:. ```diff; - echo ""deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; + echo ""deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502:70,avail,available,70,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502,3,"['avail', 'echo']","['available', 'echo']"
Availability,"Looking at the auth image, it is down from 2.76GB in main to 674MB. The hail-ubuntu image underneath it has stayed basically the same at about half the new auth image. Nearly all of the 674MB is split evenly between the layer that installs python in hail-ubuntu and the layer that installs the pip dependencies in the auth image. I've not yet inspected the hail-ubuntu layer, but for the pip dependencies the main offenders are:. ```; 77M	/usr/local/lib/python3.7/dist-packages/googleapiclient; 76M	/usr/local/lib/python3.7/dist-packages/botocore; 33M	/usr/local/lib/python3.7/dist-packages/_sass.abi3.so; 29M	/usr/local/lib/python3.7/dist-packages/kubernetes_asyncio; 20M	/usr/local/lib/python3.7/dist-packages/uvloop; 14M	/usr/local/lib/python3.7/dist-packages/pip; 14M	/usr/local/lib/python3.7/dist-packages/cryptography; 8.9M	/usr/local/lib/python3.7/dist-packages/google; 7.9M	/usr/local/lib/python3.7/dist-packages/pygments; 7.0M	/usr/local/lib/python3.7/dist-packages/azure; 5.0M	/usr/local/lib/python3.7/dist-packages/setuptools; 4.2M	/usr/local/lib/python3.7/dist-packages/aiohttp; 2.5M	/usr/local/lib/python3.7/dist-packages/googlecloudprofiler; 2.2M	/usr/local/lib/python3.7/dist-packages/yaml; 2.2M	/usr/local/lib/python3.7/dist-packages/hailtop; 2.0M	/usr/local/lib/python3.7/dist-packages/rich; 1.6M	/usr/local/lib/python3.7/dist-packages/pyasn1_modules; 1.5M	/usr/local/lib/python3.7/dist-packages/boto3; 1.4M	/usr/local/lib/python3.7/dist-packages/pkg_resources; 1.4M	/usr/local/lib/python3.7/dist-packages/oauthlib; 1.1M	/usr/local/lib/python3.7/dist-packages/pycparser; ```. Most of a gigabyte still feels annoyingly bloated but might just have to do for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308:33,down,down,33,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308,1,['down'],['down']
Availability,"Looking at the logs, I think these two new states are because we added the log analytics agent based on when the PR merged and the absence of these errors before December 10th. ```; Unknown azure statuses [{'code': 'ProvisioningState/updating', 'level': 'Info', 'displayStatus': 'Updating'}, {'code': 'PowerState/running', 'level': 'Info', 'displayStatus': 'VM running'}] for instance batch-worker-default-standard-166xu; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11160:148,error,errors,148,https://hail.is,https://github.com/hail-is/hail/pull/11160,1,['error'],['errors']
Availability,Looks good other than a couple nits on the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820,1,['error'],['error']
Availability,"Looks great, aside from Value.fromLIR stuff which I think is causing test failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10791#issuecomment-901427655:74,failure,failures,74,https://hail.is,https://github.com/hail-is/hail/pull/10791#issuecomment-901427655,1,['failure'],['failures']
Availability,Looks like a compilation error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8282#issuecomment-611255794:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-611255794,1,['error'],['error']
Availability,"Looks like it was opened and closed here #12381. I haven't taken a look at this yet, but there are two footguns that should be avoided as much as possible:. - A job waiting on a batch that it is a part of. We add the batch id into the container so we should be able to throw an error here; - Waiting on a batch in general is really wonky when there is more than 1 entity controlling it. I don't know of a good way to control this, so it might just be a ""be sure you know what you're doing thing"", but worth thinking about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229:278,error,error,278,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229,1,['error'],['error']
Availability,"Looks like some Python failures. It will take me a little while to track them down, but they all look minor and of two forms I understand:; - RVDType being constructed with the row type being optional,; - and incorrect type signatures when building emit methods due to mismatched missingness.; Go ahead and review while I fix them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840:23,failure,failures,23,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840,2,"['down', 'failure']","['down', 'failures']"
Availability,Looks like test failure is due to needing to wrap phenotype in array in this line; `top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption')`; of; https://github.com/hail-is/hail/blob/master/python/hail/docs/tutorials/expression-language-part-2.ipynb,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001:16,failure,failure,16,https://hail.is,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001,1,['failure'],['failure']
Availability,Looks like there's quite a few rebase errors to touch up as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1304186719:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1304186719,1,['error'],['errors']
Availability,Looks like they changed their error message. See [here](https://ci.hail.is/batches/3181369/jobs/80) for an example.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11942:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/11942,1,['error'],['error']
Availability,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:20,failure,failures,20,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172,1,['failure'],['failures']
Availability,"Looks like we're getting some intermittent failures, monitoring.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063:43,failure,failures,43,https://hail.is,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063,1,['failure'],['failures']
Availability,Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175:190,error,error,190,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175,1,['error'],['error']
Availability,Lots of aggregators breaks down,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:27,down,down,27,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['down'],['down']
Availability,"MENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **711/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.5 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570772](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.clou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:6229,avail,available,6229,https://hail.is,https://github.com/hail-is/hail/pull/13717,3,['avail'],['available']
Availability,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:12,robust,robust,12,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662,1,['robust'],['robust']
Availability,"Made changes to clean up/add to the documentation for the datasets API and annotation DB. Changes to documentation:. - Moved raw html in `annotation_database_ui.rst` into `hail/python/hail/docs/_static/annotationdb/annotationdb.html`.; - Added html table to bottom of datasets doc page to show all available datasets, similar to what is currently on annotation DB doc page. Added relevant files to `hail/python/hail/docs/_static/datasets`.; - Moved schemas currently on datasets doc page to their own page. Datasets doc page links to this new page.; - Moved some minor styling from html files to `annotationdb.css`, and cleaned up formatting of html.; - Added doc page for the `DB` class, referenced on the Python API doc page for the experimental module. Only exposes the `available_datasets` attribute and `annotate_rows_db` method, as I didn't think most users need to see any internal methods or the `Dataset` and `DatasetVersion` classes/methods. Most of diff is just formatting changes to `db.py` and `datasets.py` for consistency/readability that did not change functionality. Also added docstrings to methods in `db.py` that were missing them. Added a check in `DB` constructor to make sure the cloud and region combination is valid. To prevent an empty annotation DB instance from being created if user specifies `db = hl.experimental.DB(region='eu', cloud='aws')`, since we don't have an EU bucket on AWS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9663:298,avail,available,298,https://hail.is,https://github.com/hail-is/hail/pull/9663,1,['avail'],['available']
Availability,"Made read back-compatible, with error checking.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/610:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/610,1,['error'],['error']
Availability,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2093:566,error,error,566,https://hail.is,https://github.com/hail-is/hail/pull/2093,1,['error'],['error']
Availability,"Main$.runCommand(Main.scala:91); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:115); at org.broadinstitute.hail.utils.package$.time(package.scala:119); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:114); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:108); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:108); at org.broadinstitute.hail.driver.Main$.main(Main.scala:233); at org.broadinstitute.hail.driver.Main.main(Main.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4, localhost): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:6275,failure,failure,6275,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['failure'],['failure']
Availability,"Mainly bug fixes and criu support. I don't like that we're still building from source and hope to replace that soon, but Vedant will need this to start playing with checkpoint/restore.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11695:165,checkpoint,checkpoint,165,https://hail.is,https://github.com/hail-is/hail/pull/11695,1,['checkpoint'],['checkpoint']
Availability,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5054:84,Failure,Failure,84,https://hail.is,https://github.com/hail-is/hail/pull/5054,2,"['Failure', 'alive']","['Failure', 'alive']"
Availability,Make `SemanticHash` Resilient to `FileNotFoundExceptions`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13919:20,Resilien,Resilient,20,https://hail.is,https://github.com/hail-is/hail/pull/13919,1,['Resilien'],['Resilient']
Availability,"Make available [pan-ukb datasets](https://pan.ukbb.broadinstitute.org/docs/hail-format) via datasets API. Includes:; - Summary statistics MatrixTable, meta-analysis MatrixTable (both on GCS and S3); - LD score Table, variant index table, and LD BlockMatrix for each population (AFR, AMR, CSA, EAS, EUR, MID) (S3 only)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186:5,avail,available,5,https://hail.is,https://github.com/hail-is/hail/pull/10186,1,['avail'],['available']
Availability,Make evaluates the entire recipe before executing it. Consider this:; ```; (base) # cat Makefile; rm -rf file; make foo; foo:; 	echo hello > file; 	echo $(shell cat file); cat: file: No such file or directory; echo hello > file; echo . ```; The file does not exist because Make runs cat before the recipe is executed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9338:128,echo,echo,128,https://hail.is,https://github.com/hail-is/hail/pull/9338,4,['echo'],['echo']
Availability,Make sure that all local alleles are less than the n_total_alleles parameter passed to the function. This error will often occur if a vds is split with regular split_multi rather than vds.split_multi. Closes #13479,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13485:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/13485,1,['error'],['error']
Availability,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6201:14,resilien,resiliency,14,https://hail.is,https://github.com/hail-is/hail/pull/6201,3,"['resilien', 'toler']","['resiliency', 'tolerate', 'tolerations']"
Availability,Make the version length coming from error messages consistent,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3593:36,error,error,36,https://hail.is,https://github.com/hail-is/hail/pull/3593,1,['error'],['error']
Availability,"MakeNDArray should return a missing NDArray if shape or data are missing, but should raise an error if any elements of the shape or data are missing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6957:94,error,error,94,https://hail.is,https://github.com/hail-is/hail/issues/6957,1,['error'],['error']
Availability,Makes it possible to proceed past the duplicate-multiallelic-loci error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7071:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/7071,1,['error'],['error']
Availability,"Makes some further progress on simplifying the `PruneDeadFields` pass, with the primary goal of decoupling it from the details of the binding structure. The primary change is to `memoizeValueIR`. Before, it passed in only the requested type of the node, and returned and environment containing all free variables and their requested types. Any bound variables would then need to be removed, and the environments of all children then merged. This low-level manipulation of environments made it closely tied to the binding structure, essentially redundantly encoding everything in `Binds.scala`. Now we pass an environment down into the children, which maps variables to a mutable state tracking the requested type. Each `Ref` node unions the requested type at the reference with the state in the environment. This lets us use the general environment infrastructure. I didn't do an assertion directly comparing the old and new implementations, as I've done with some other pass rewrites. But `PruneDeadFields` has pretty good test coverage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14509:544,redundant,redundantly,544,https://hail.is,https://github.com/hail-is/hail/pull/14509,2,"['down', 'redundant']","['down', 'redundantly']"
Availability,"Man, you got some *weird* errors in this PR. Why is asyncio giving us a `None` for a loop?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11098#issuecomment-983797608:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/11098#issuecomment-983797608,1,['error'],['errors']
Availability,"Many of our tests look like:. ```scala; // ExprSuite.scala; assert(eval[Int](""a[0]"").contains(1)); assert(eval[Int](""a[1]"").contains(2)); assert(eval[Int](""a[2]"").isEmpty); assert(eval[Int](""a[3]"").contains(6)); assert(eval[Int](""a[-1]"").contains(8)); assert(eval[Int](""a[-2]"").contains(-1)); ```. Test failures from these expressions simply state that the result was not as expected. If these tests were instead written as below,. ```scala; assert(eval[Int](""a[0]"") == Some(1)); assert(eval[Int](""a[1]"") == Some(2)); assert(eval[Int](""a[2]"") == None); assert(eval[Int](""a[3]"") == Some(6)); assert(eval[Int](""a[-1]"") == Some(8)); assert(eval[Int](""a[-2]"") == Some(-1)); ```; then test failures would print both the expected value and the actual value:; ```; org.scalatest.exceptions.TestFailedException: Some(7) did not equal Some(1); ```. Furthermore, there are a tools in the [scalatest library](http://www.scalatest.org/at_a_glance/FlatSpec) which enable richer specifications. Suppose that Hail included a `randInt : (Int, Int) => Int` function, we might like to verify that this is true:. ```scala; eval[Int](""randInt(0, 10) * 2 + 1"") should (be > 0 and be (even)); ```. When this expression fails, the messages look like:. ```; org.scalatest.exceptions.TestFailedException: 7 was greater than 0, but 7 was odd; ```. These natural language matchers are a bit finicky. I'm not sure if I like them, but I do like having nice error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1286:303,failure,failures,303,https://hail.is,https://github.com/hail-is/hail/issues/1286,3,"['error', 'failure']","['error', 'failures']"
Availability,Map values serialization error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1179:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/1179,1,['error'],['error']
Availability,MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(Exec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:6179,error,error,6179,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['error'],['error']
Availability,"Maryam,; I ran this command in Unix:. ```; gunzip -c <file> | cut -f4 | sort | uniq -c; 20709505 A; 20934670 C; 20968049 G; 20693812 T; 25 alt; ```. I think the problem is that the headers from all the files were included in the one file. I'm running another grep now to be sure. I'll fix the error message though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317#issuecomment-212477402:293,error,error,293,https://hail.is,https://github.com/hail-is/hail/issues/317#issuecomment-212477402,1,['error'],['error']
Availability,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8161:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/8161,1,['error'],['error']
Availability,MatrixEntriesTable didn't define `uid_field_name` in `_handle_randomness`. Downstream operations failed to fetch the field and inserted a NA into `RNGSplit`. Assert that TableIRs define `uid_field_name` when provided to `handle_randomness`. Fixes: #14303,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14371:75,Down,Downstream,75,https://hail.is,https://github.com/hail-is/hail/pull/14371,1,['Down'],['Downstream']
Availability,MatrixReader errors are wrapped in Serialization MappingExceptions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5030:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/issues/5030,1,['error'],['errors']
Availability,MatrixTable.show errors if it tries to show more columns than in the MT,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5832:17,error,errors,17,https://hail.is,https://github.com/hail-is/hail/issues/5832,1,['error'],['errors']
Availability,"May not be implemented in aiohttp. Flag prevents cookie from being carried with cross-origin request. Lax allows cookie to be carried with GET requests. Only available in relatively recent browsers. With CSRF protection in place not strictly necessary, but an added layer of protection, for low cost",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7132:158,avail,available,158,https://hail.is,https://github.com/hail-is/hail/issues/7132,1,['avail'],['available']
Availability,"Maybe more damning is the scaling of the error with n:; ```; In [35]: t = hl.utils.range_table(10**4). In [36]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [37]: t.aggregate(hl.agg.stats(t.x)); Out[37]: Struct(mean=10000000.012717757, stdev=1.043072384832424, min=9999996.160907382, max=10000003.396893706, n=10000, sum=100000000127.17758). In [38]: t = hl.utils.range_table(10**5). In [39]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [40]: t.aggregate(hl.agg.stats(t.x)); Out[40]: Struct(mean=10000000.000944939, stdev=1.0017584539199058, min=9999994.870601522, max=10000004.903980266, n=100000, sum=1000000000094.4939). In [41]: t = hl.utils.range_table(10**6). In [42]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [43]: t.aggregate(hl.agg.stats(t.x)); Out[43]: Struct(mean=10000000.000245763, stdev=0.9050966799187808, min=9999995.23730167, max=10000004.54308704, n=1000000, sum=10000000000245.764). In [44]: t = hl.utils.range_table(10**7). In [45]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [46]: t.aggregate(hl.agg.stats(t.x)); Out[46]: Struct(mean=9999999.999657989, stdev=nan, min=9999994.645387085, max=10000005.969974454, n=10000000, sum=99999999996579.89). In [47]: t = hl.utils.range_table(10**7). In [48]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [49]: t.aggregate(hl.agg.stats(t.x)); Out[49]: Struct(mean=9999999.999756364, stdev=1.5274428303540528, min=9999995.005864669, max=10000005.419492438, n=10000000, sum=99999999997563.64); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10132#issuecomment-788973494:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/10132#issuecomment-788973494,1,['error'],['error']
Availability,"Maybe no longer relevant, but zeroing missings *after* centering is; equivalent to using non-missing terms only rather than mean imputing,; provided you then use N_nonmissing for the final normalization. On Tue, Dec 10, 2019 at 8:50 AM Jon Bloom <notifications@github.com> wrote:. > I’ll look for closely once I get to the retreat, but first impression is; > that centering and normalizing are redundant.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/7653?email_source=notifications&email_token=ACC577VJUORGGYMDZUE72IDQX6NDNA5CNFSM4JVAFXT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGPJKXQ#issuecomment-564041054>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC577UWWGRAMQHAOV7TGADQX6NDNANCNFSM4JVAFXTQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326:394,redundant,redundant,394,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326,1,['redundant'],['redundant']
Availability,Maybe we make the name change for success and put the failure conditions in a separate PR? I wonder if there are any sneaky bits with those additions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1289351886:54,failure,failure,54,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1289351886,1,['failure'],['failure']
Availability,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877,1,['error'],['error']
Availability,"Maybe? We have an assertion in `TextMatrixReader.parseOptionalValue` that the missing values are not empty strings. I didn't trace through why, I just moved the error up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11078#issuecomment-975610252:161,error,error,161,https://hail.is,https://github.com/hail-is/hail/pull/11078#issuecomment-975610252,1,['error'],['error']
Availability,Mendel error code fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3303:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/pull/3303,1,['error'],['error']
Availability,Mendel error computation need to be adapted to multi-allelic sites,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/45:7,error,error,7,https://hail.is,https://github.com/hail-is/hail/issues/45,1,['error'],['error']
Availability,"Mendel errors should stay a separate pass for various reasons. Perhaps this can be revisited when we start working on a higher-level, plink-like tool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/148#issuecomment-240001350:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/issues/148#issuecomment-240001350,1,['error'],['errors']
Availability,Mendel errors shouldn't report variants on the MT chromosome,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2005:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/issues/2005,1,['error'],['errors']
Availability,Mention 'unify' in the union error message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5888:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/5888,1,['error'],['error']
Availability,"Merge of https://github.com/hail-is/hail/pull/12868 and https://github.com/hail-is/hail/pull/12867 because I expect each is likely to fail due to the other's error. ---. [qob] retry `storage.writer` and `storage.reader`; ; I do not think we frequently get errors in `storage.reader`, but I think `storage.writer` was; always flaky and we were protected by the `retryTransientErrors` on `createNoCompression`. My; change to fix requester pays delayed the error until either the first `write` or the `close`; which do not have a `retryTransientErrors` (and it is not obvious to me that it is safe to retry; a `flush`). ---. [qob] retry transient errors reading the results file. ---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12869:158,error,error,158,https://hail.is,https://github.com/hail-is/hail/pull/12869,4,['error'],"['error', 'errors']"
Availability,Merging in https://github.com/hail-is/hail/pull/14233 causes the failure in `test_union_rows1`. Some strangeness with these new dependencies - running without this commit and everything works fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341:65,failure,failure,65,https://hail.is,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341,1,['failure'],['failure']
Availability,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3480:181,failure,failure,181,https://hail.is,https://github.com/hail-is/hail/issues/3480,4,"['Error', 'failure']","['ErrorHandling', 'failure']"
Availability,Missing key error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4799:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/issues/4799,1,['error'],['error']
Availability,Mmm. Yes. I need a more robust IR testing plan. I think testing these individually will be more painful than testing them in the context of IR expressions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2514#issuecomment-349092532:24,robust,robust,24,https://hail.is,https://github.com/hail-is/hail/pull/2514#issuecomment-349092532,1,['robust'],['robust']
Availability,Modify compiler arguments to emit warnings required for scalafix.; Fix failures that arise from the new build configuration.; Run scalafix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14103:71,failure,failures,71,https://hail.is,https://github.com/hail-is/hail/pull/14103,1,['failure'],['failures']
Availability,More info here: https://discuss.hail.is/t/export-elasticsearch-error/1755/4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9686:63,error,error,63,https://hail.is,https://github.com/hail-is/hail/pull/9686,1,['error'],['error']
Availability,More specific error message when trying to read non-existent vds,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/327:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/327,1,['error'],['error']
Availability,More tolerance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6776:5,toler,tolerance,5,https://hail.is,https://github.com/hail-is/hail/pull/6776,1,['toler'],['tolerance']
Availability,"Most of the failures were coming from `SUnreachableValue`s inheriting from both `SValue` and `SCode`, which has caused me problems before too. Since `SCode` will be going away, I just gave in and duplicated them all for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076:12,failure,failures,12,https://hail.is,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076,1,['failure'],['failures']
Availability,"Most of the functionality was already available in EmitFunctionBuilder, but Compile() didn't make it available. This PR creates a `PrintWriter` during assertEvalsTo if you set the `jvm_bytecode_dump` flag to a file path you want the bytecode to be written to. Example:; ```scala; HailContext.setFlag(""jvm_bytecode_dump"", ""arr_filter_bytecode.java""); assertEvalsTo(ArrayFilter(a, ""x"",; ApplyComparisonOp(LT(TInt32()), Ref(""x"", TInt32()), I32(6))), FastIndexedSeq(3)); HailContext.setFlag(""jvm_bytecode_dump"", null); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7199:38,avail,available,38,https://hail.is,https://github.com/hail-is/hail/pull/7199,2,['avail'],['available']
Availability,"Mostly me not knowing how this code works, so, my apologies for slowing you down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13385#issuecomment-1668220060:76,down,down,76,https://hail.is,https://github.com/hail-is/hail/pull/13385#issuecomment-1668220060,1,['down'],['down']
Availability,"Mostly small, straightforward stuff. /auth must only return 2xx, 401 or 403, or nginx returns 500. Redirect auth failures connecting to instance to /error, too. Changed ""Create/Open Notebook"" to ""Launch/Open Jupyter"" and associated language throughout. I'll run through the whole test playbook again after these go in. Note to self, some improvements to consider:; - Validate image, memory, cpu values in workshop-admin. Right now, if you enter invalid values, you get a 500 on launch Jupyter with invalid pod spec.; - Could change notebook.hail.is/notebook URL to notebook.hail.is/jupyter now.; - A background loop to kill any notebook workers associated to inactive workshops. Then if you just inactivate the workshop at the end, everything gets cleaned up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7162:113,failure,failures,113,https://hail.is,https://github.com/hail-is/hail/pull/7162,2,"['error', 'failure']","['error', 'failures']"
Availability,Move Mendel errors to Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3240:12,error,errors,12,https://hail.is,https://github.com/hail-is/hail/pull/3240,1,['error'],['errors']
Availability,"Move installed location of hail-all-spark.jar to from hail/ to; hail/backend/. We were not finding the jar properly with pkg_resources, and so were not; setting the paths appropriately for pip installs, causing 'JavaPackage; not callable' errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8405:239,error,errors,239,https://hail.is,https://github.com/hail-is/hail/pull/8405,1,['error'],['errors']
Availability,"Moves multi-pod deployments over to using Headless Services, which enables client-side load-balancing to the underlying pods. See #12095 for more context. The reason I put this in its own PR is that Kubernetes won't let me apply the `clusterIP: None` changes to existing `Services`, and I must delete the `Service` resources first. I can manually delete and apply new headless services in a way that is compatible with what is currently on main and with just a few seconds of downtime, but I should do this manually just before this PR merges.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094:476,downtime,downtime,476,https://hail.is,https://github.com/hail-is/hail/pull/12094,1,['downtime'],['downtime']
Availability,Movie lens hosting is down. We can re-enable when it is back up.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5632:22,down,down,22,https://hail.is,https://github.com/hail-is/hail/pull/5632,1,['down'],['down']
Availability,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2136:419,repair,repairStandardHeaderLines,419,https://hail.is,https://github.com/hail-is/hail/issues/2136,1,['repair'],['repairStandardHeaderLines']
Availability,"My change https://github.com/hail-is/hail/pull/8581 introduced a bug, count of native reads with pushed down intervals could give the wrong answer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8631:104,down,down,104,https://hail.is,https://github.com/hail-is/hail/pull/8631,1,['down'],['down']
Availability,"My first approach was to populate an error.html template and return that. I could get the title to be ""Error"", but none of the content was showing up. I couldn't figure out why, so I switched to raising HTTPErrorFound with the traceback message. This works fine, but it won't work if we need the decorator above the authentication decorators. This decorator has to be the furthest down the call stack.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10503:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/10503,3,"['Error', 'down', 'error']","['Error', 'down', 'error']"
Availability,NDArrayRef Better Error Message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9444:18,Error,Error,18,https://hail.is,https://github.com/hail-is/hail/pull/9444,1,['Error'],['Error']
Availability,"NFO: Finished task 0.0 in stage 5.0 (TID 5). 1119 bytes result sent to driver; ```; </details>. <details>; <summary>Broken hail.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31772,AVAIL,AVAILABLE,31772,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"NOTE: This issue is **only** about the error message. We can definitely produce a more insightful error message (perhaps suggesting the use of `.rows()[key_field1, key_field2]`) without also addressing the confusing syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399,2,['error'],['error']
Availability,"NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-5750273](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-5750273) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium seve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:5816,avail,available,5816,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"NYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **489/1000** <br/> **Why?** Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxM2UyYzQ2MC1mZTA2LTQwOTktYWRhYi1lMWY4ZmE5MzFkZTAiLCJldmVudCI6IlBSIH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:9783,avail,available,9783,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['avail'],['available']
Availability,"Naturally, every possible Spark version uses a different elasticsearch library. . This also uses curl instead of gsutil to download the jar, so we don't require people to have gsutil to make.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10255:123,down,download,123,https://hail.is,https://github.com/hail-is/hail/pull/10255,1,['down'],['download']
Availability,Need more information. Please re-open when more information is available!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993#issuecomment-1969996616:63,avail,available,63,https://hail.is,https://github.com/hail-is/hail/issues/13993#issuecomment-1969996616,1,['avail'],['available']
Availability,Needed to guard against errors in the k8s pod stream.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6511:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/6511,1,['error'],['errors']
Availability,"Needs a `python3 -m black batch --line-length=120 --skip-string-normalization`; ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/driver/main.py	2023-04-05 14:40:12.638902 +0000; +++ batch/driver/main.py	2023-04-05 14:44:25.172615 +0000; @@ -1226,11 +1226,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, `user`, resource_id, token, `usage`); SELECT billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; async def compact_agg_billing_project_users_by_date_table(app):; @@ -1254,11 +1255,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_by_date_v3 (billing_date, billing_project, `user`, resource_id, token, `usage`); SELECT billing_date, billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; USER_CORES = pc.Gauge('batch_user_cores', 'Batch user cores (i.e. total in-use cores)', ['state', 'user', 'inst_coll']); would reformat batch/driver/main.py. Oh no! 💥 💔 💥; 1 file would be reformatted, 93 files would be left unchanged.; make[1]: *** [Makefile:18: check] Error 1; make[1]: Leaving directory '/io/repo/batch'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925:1442,Error,Error,1442,https://hail.is,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925,1,['Error'],['Error']
Availability,Nested array element aggregations weren't working. This fixes it and adds a test for nested ArrayAggs. Caught by test failures from #6698.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6743:118,failure,failures,118,https://hail.is,https://github.com/hail-is/hail/pull/6743,1,['failure'],['failures']
Availability,New entry filtering semantics was removing children with no errors; leading to incorrect counts. fixes #5786,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5855:60,error,errors,60,https://hail.is,https://github.com/hail-is/hail/pull/5855,1,['error'],['errors']
Availability,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114,1,['failure'],['failure']
Availability,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963,3,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417,2,['failure'],"['failure', 'failures']"
Availability,"Nice! I think we should add some error checking code that verifies the name is non-empty when we, for example, `create`. It's unlikely to happen but would create some confusing situations if a user creates a file with an empty name. I'm not even sure if it's allowed in the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13435#issuecomment-1679764031:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/pull/13435#issuecomment-1679764031,1,['error'],['error']
Availability,"Nice, `test_paired_elementwise_ops` down to under a minute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12347#issuecomment-1282578133:36,down,down,36,https://hail.is,https://github.com/hail-is/hail/pull/12347#issuecomment-1282578133,1,['down'],['down']
Availability,"Nice. This is looking better. I don’t care so much about fluctuations in the handful of seconds but do we believe these big changes to pc relate, block matrix, and range MT sum? Can you re-run just those five or so slow ones and confirm they really slowed down?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1564333638:256,down,down,256,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1564333638,1,['down'],['down']
Availability,Nicer error for case/switch statements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3830:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/3830,1,['error'],['error']
Availability,No error checking on LoadVCF if user-provided header file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2437:3,error,error,3,https://hail.is,https://github.com/hail-is/hail/pull/2437,1,['error'],['error']
Availability,"No one has complained about this yet, but I suspect there's a lurking issue in `linreg`. On line 75 of LinearRegression.scala, we create a writable region value using a context managed region. This region will be kept alive (in the garbage collection sense) by the context until the end of the Task (which I believe is the end of processing one partition). As such, `linreg` will generate a bunch of garbage. We close the child as soon as we know it is no longer used, thus saving memory use, at the cost of copying the results. In a future where we can reference-count regions then we could avoid the copy and simply return the writable region value. This future is not here yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3590:218,alive,alive,218,https://hail.is,https://github.com/hail-is/hail/pull/3590,1,['alive'],['alive']
Availability,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017,1,['error'],['error']
Availability,"No. There's one test that only fails on Azure and I haven't figured out why yet. ```. =================================== FAILURES ===================================; ___________________________ test_dir_outside_curdir ____________________________. runner = <typer.testing.CliRunner object at 0x7f95b3d14b50>. def test_dir_outside_curdir(runner: CliRunner):; with tempfile.TemporaryDirectory() as dir:; os.mkdir(f'{dir}/working_dir'); os.chdir(f'{dir}/working_dir'); write_hello(f'{dir}/hello1.txt'); write_hello(f'{dir}/hello2.txt'); write_script(dir, '/hello1.txt'); res = runner.invoke(cli.app, ['submit', '--files', f'{dir}/:/', '../test_job.py']); > assert res.exit_code == 0; E AssertionError: assert 1 == 0; E + where 1 = <Result HttpResponseError('The specified block list is invalid.\nRequestId:86424c6a-d01e-004a-272b-0b6b10000000\nTime:2023-10-30T12:21:01.7415144Z\nErrorCode:InvalidBlockList')>.exit_code; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13785#issuecomment-1785325401:122,FAILURE,FAILURES,122,https://hail.is,https://github.com/hail-is/hail/issues/13785#issuecomment-1785325401,1,['FAILURE'],['FAILURES']
Availability,"Non-daemon threads [keep a JVM alive](https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html):. > When a Java Virtual Machine starts up, there is usually a single non-daemon thread (which typically calls the method named main of some designated class). The Java Virtual Machine continues to execute threads until either of the following occurs:; >; > The exit method of class Runtime has been called and the security manager has permitted the exit operation to take place.; >; > All threads that are not daemon threads have died, either by returning from the call to the run method or by throwing an exception that propagates beyond the run method. Spark appears to wait for the JVM to terminate before it considers a job complete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13916:31,alive,alive,31,https://hail.is,https://github.com/hail-is/hail/pull/13916,1,['alive'],['alive']
Availability,Non-specific OOB error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3041:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/3041,1,['error'],['error']
Availability,"Nope, seeing the same error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301759591:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301759591,1,['error'],['error']
Availability,Not a correctness bug because we raise an assertion error in the partition function.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13550:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/pull/13550,1,['error'],['error']
Availability,"Not all of the Hail Tables and MatrixTables that are publicly available on the gnomAD [downloads](https://gnomad.broadinstitute.org/downloads) page are currently available in the Datasets API/Annotation DB. . This PR makes the following changes to the datasets available via the Hail Datasets API/Annotation DB:. - Add `gnomad_genome_sites` Table, versions: 3.1.1, 3.1.2; - Add `gnomad_hgdp_1kg_subset_dense` MatrixTable, version: 3.1.2; - Rename `gnomad_hgdp_1kg_callset` MatrixTable to `gnomad_hgdp_1kg_subset_dense`, version: 3.1; - Add `gnomad_hgdp_1kg_subset_sparse` MatrixTable, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_sample_metadata` Table, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_variant_annotations` Table, version: 3.1.2; - Add `gnomad_variant_co-occurrence` Table, version: 2.1.1; - Add `gnomad_pca_variant_loadings` Table, versions: 2.1, 3.1. Other general changes:. - Add/update the schema `.rst` files, for the datasets listed above, for the [docs](https://hail.is/docs/0.2/datasets/schemas.html); - Set the example dataset loaded in `hl.experimental.load_dataset` to be the most recent `gnomad_hgdp_1kg_subset_dense` MatrixTable (version 3.1.2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11608:62,avail,available,62,https://hail.is,https://github.com/hail-is/hail/pull/11608,5,"['avail', 'down']","['available', 'downloads']"
Availability,Not great that CI hasn't been erroring because of this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13671:30,error,erroring,30,https://hail.is,https://github.com/hail-is/hail/pull/13671,1,['error'],['erroring']
Availability,"Not helpful for debugging, we still get logs on errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11977:48,error,errors,48,https://hail.is,https://github.com/hail-is/hail/pull/11977,1,['error'],['errors']
Availability,Not including a test since Patrick has an open PR that obviates this kind of error completely.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11181:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/pull/11181,1,['error'],['error']
Availability,"Not really sure why this is coming up now (I don't see anything that changed in this PR, but the scala isn't compiling because of a redundant function definition in is.hail.expr.types and is.hail.expr.ir (coerce)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089:132,redundant,redundant,132,https://hail.is,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089,1,['redundant'],['redundant']
Availability,Not sure if this is unidiomatic but I always write little scripts that don't close the service backend and end with errors about unclosed sessions. This auto-closes a `ServiceBackend`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10776:116,error,errors,116,https://hail.is,https://github.com/hail-is/hail/pull/10776,1,['error'],['errors']
Availability,Not sure if this will help make GKE move the pod if the node needs to be repaired rather than waiting for the node to repair.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9653:73,repair,repaired,73,https://hail.is,https://github.com/hail-is/hail/pull/9653,2,['repair'],"['repair', 'repaired']"
Availability,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820,3,"['Error', 'error']","['Error', 'ErrorHandling', 'error']"
Availability,"Not sure who to assign this to since it spans everything. I targeted the slowest test jobs. Currently CI's PR page timings are wrong. If you scroll down to ""Build History"" and click on a batch, that page has the right timings. (The CI PR page timings will be fixed by #6746",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6863:148,down,down,148,https://hail.is,https://github.com/hail-is/hail/pull/6863,1,['down'],['down']
Availability,"Not sure why the extra line is required here... riddles of Sphinx. https://hail.is/docs/0.2/hail.Table.html#hail.Table.checkpoint; https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint. Before; <img width=""1029"" alt=""Screen Shot 2020-06-08 at 9 30 12 PM"" src=""https://user-images.githubusercontent.com/1156625/84096487-0cb98d00-a9d0-11ea-8623-12288df6eace.png"">. After; <img width=""1042"" alt=""Screen Shot 2020-06-08 at 9 35 06 PM"" src=""https://user-images.githubusercontent.com/1156625/84096490-10e5aa80-a9d0-11ea-9be9-c3dfd8b049a9.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8938:119,checkpoint,checkpoint,119,https://hail.is,https://github.com/hail-is/hail/pull/8938,2,['checkpoint'],['checkpoint']
Availability,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999,1,['error'],['error']
Availability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:267,ERROR,ERRORS,267,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,1,['ERROR'],['ERRORS']
Availability,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9121:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/issues/9121,2,['error'],['error']
Availability,"Note that pandas 2.0.0 [removes the deprecated `DataFrame.iteritems()`](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes), which is used by bokeh-1.4.0. That particular old version of bokeh is listed in _hail/python/requirements.txt_ but it is thus incompatible with pandas 2; so one or the other of these pinnings probably needs to be revisited. (This incompatibility has caused the [large_cohort unit test failure](https://github.com/populationgenomics/production-pipelines/actions/runs/4782280056/jobs/8501466504?pr=354#step:5:134) in populationgenomics/production-pipelines#354.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581:457,failure,failure,457,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581,1,['failure'],['failure']
Availability,"Note that we never see trisomy 22, but do see trisomy 21 (down)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9085#issuecomment-658826248:58,down,down,58,https://hail.is,https://github.com/hail-is/hail/pull/9085#issuecomment-658826248,1,['down'],['down']
Availability,"Note this PR replaces the previous [Feature/sas token merge](https://github.com/hail-is/hail/pull/12877) because the original PR branch got jacked up beyond repair. All the comments on the earlier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS toke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:157,repair,repair,157,https://hail.is,https://github.com/hail-is/hail/pull/13140,1,['repair'],['repair']
Availability,Note to self to check the logs for error messages before merging!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870,1,['error'],['error']
Availability,"Note, this means migrations for batch will shut batch down, but batch won't be running to restart itself (!) so this will have to be done manually. The UI will also be down, so we'll have to watch the database to verify the migration has been applied to know when to restart batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765:54,down,down,54,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765,2,['down'],['down']
Availability,"Note, when I made this change, I ran into a bug where the label in whileLoop got laid down twice. I found the two cuprits:; - one was && and ||, which duplicated the code on different code paths so was conceptually correct but could lead to code explosion. Either way, I rewrote them.; - and the other was in checkedConvertFrom which actually executed its input twice. I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8207#issuecomment-593374823:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593374823,1,['down'],['down']
Availability,"Noted. I understand, I will switch to a newer version of Hail.; I had some other trouble with Hail 0.2.74, but that is for another ticket. There is an error as you can see in the last line ; ```; one error found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831#issuecomment-918748397:151,error,error,151,https://hail.is,https://github.com/hail-is/hail/issues/10831#issuecomment-918748397,2,['error'],['error']
Availability,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,2,['error'],"['errorNum', 'errors']"
Availability,"Notice that this script is working with spark 1.6, the error appears with spark 2. ```; exac_vds_split = hc.read(root + 'andrea_subset_splitmulti_hard.vds'); dbNSFP_vds = hc.read(root + 'dbNSFP_3.2a_variant.filtered.allhg19_nodup.vds'); discovEHR_vds = hc.read(root + 'discoverEHR.vds'); exacV2_vds = hc.read(root + 'exacV2_split_variants_non_in_andrea_subset.vds'); fin_vds = hc.read(root + 'finnish_noexac_subset.vds'). (exac_vds_split; # .filter_variants_expr('v.contig==""7"" && v.start > 75013221 && v.start < 76253221', keep=True); # .filter_variants_expr('v.contig==""20""', keep=True); .annotate_variants_expr('va = drop(va, vep)'); .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); .write(stroot + '/andrea_subset_splitmulti_hard_vep.vds', overwrite=True)). exac_vds_split_vep = hc.read(stroot + 'andrea_subset_splitmulti_hard_vep.vds'). (exac_vds_split_vep; .annotate_global_list(root + 'all_scores_reduced.scores', root='global.allgenes'); .annotate_global_expr_by_sample('global.allgenes = global.allgenes.map(x => x.split(""\\t""))'); .variant_qc(); .annotate_variants_vds(discovEHR_vds,root='va.EHR'); .annotate_variants_vds(exacV2_vds,root='va.EXACV2'); .annotate_variants_vds(fin_vds,root='va.FIN'); .annotate_variants_table(root + 'clinvar_clean.txt','Variant(Variant)', root='va.clinvar'); .annotate_variants_table(root + 'lethal_clean.txt','Variant(Variant)', root='va.lethal'); .annotate_variants_expr(; 	""""""; 	va.clinvar.yes=if(isMissing(va.clinvar.Variant)) 0 else 1,; 	va.lethal.yes=if(isMissing(va.lethal.Variant)) 0 else 1,; 	va.nNonRef = gs.filter(g => g.isCalledNonRef).count(); 	""""""); .annotate_variants_expr(; 	""""""; 	va.freq.AF01 = (va.qc.AF < 0.01),; 	va.freq.AF001 = (va.qc.AF < 0.001),; 	va.freq.DOUBLE = (va.qc.AC == 2),; 	va.freq.SING = (va.nNonRef == 1),; 	va.freq.URVEXACV2 = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC)),; 	va.freq.URVEXACV2EHR = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC) && isMissing(va.EHR.info.AF) && isMissing(va.FIN.qc.AC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541:55,error,error,55,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541,1,['error'],['error']
Availability,Noticed that ArrayExpression.head was not documented as deprecated when it was deprecated in #9482. This also fixes a rendering error with one of its examples. https://hail.is/docs/0.2/hail.expr.ArrayExpression.html#hail.expr.ArrayExpression.head,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10032:128,error,error,128,https://hail.is,https://github.com/hail-is/hail/pull/10032,1,['error'],['error']
Availability,Now `hc.import_vcf('/Users/jbloom/data/bgz_error/sample_plain.vcf.bgz')` on mislabeled plaintext file gives:. ```; FatalError: ZipException: File does not conform to block gzip format. Java stack trace:; java.util.zip.ZipException: File does not conform to block gzip format.; 	at is.hail.io.compress.BGzipInputStream$BGzipHeader.<init>(BGzipInputStream.java:35); ```. This error message is closer to that thrown in the .gz case when reading a plain text file:. ```; FatalError: IOException: not a gzip file. Java stack trace:; java.io.IOException: not a gzip file; 	at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362:374,error,error,374,https://hail.is,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362,1,['error'],['error']
Availability,Now available in Dataproc 1.1: https://cloud.google.com/dataproc/docs/concepts/dataproc-versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/695:4,avail,available,4,https://hail.is,https://github.com/hail-is/hail/issues/695,1,['avail'],['available']
Availability,"Now image fetcher asks the running notebook image what worker image its using and pulls that. Also, add a five second sleep after the service's endpoints are configured. Hopefully that prevents these intermittent gateway errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4690:221,error,errors,221,https://hail.is,https://github.com/hail-is/hail/pull/4690,1,['error'],['errors']
Availability,"Now it seems you should use string.isMissing, while this returns an error.; I suggest isMissing(a), which makes clear the proper use of the syntax.; In Brief: update help here: https://github.com/broadinstitute/hail/blob/master/docs/HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/374:68,error,error,68,https://hail.is,https://github.com/hail-is/hail/issues/374,1,['error'],['error']
Availability,Now raises an error instead of asserting. resolves #4770 by clarifying problem with old syntax introduced by [breaking change](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5110:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/5110,1,['error'],['error']
Availability,"Now that we are running with very small nodes, image-fetcher doesn't seem to provide all that much benefit. The intention is that caching through the memory service will ultimately prove better without having to run a daemonset. This also was running a whole daemon set for every PR namespace which meant a lot of our pods were redundant image-fetchers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10272:328,redundant,redundant,328,https://hail.is,https://github.com/hail-is/hail/pull/10272,1,['redundant'],['redundant']
Availability,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520:271,down,down,271,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520,1,['down'],['down']
Availability,"Now the command:; ```; pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(); ```; gives the right error; ```; FatalError Traceback (most recent call last); <ipython-input-28-2fb5c41b2314> in <module>(); ----> 1 pca_vds.logreg('wald', 'sa.isCase', ['sa.scores.PC1, sa.scores.PC2']).count(). <decorator-gen-218> in logreg(self, test, y, covariates, root). /Users/jbloom/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs); 105 except Py4JJavaError as e:; 106 msg = env.jutils.getMinimalMessage(e.java_exception); --> 107 raise FatalError(msg); 108 except Py4JError as e:; 109 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: `|' expected but `,' found; <input>:1:sa.scores.PC1, sa.scores.PC2; ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1502:123,error,error,123,https://hail.is,https://github.com/hail-is/hail/pull/1502,2,['error'],['error']
Availability,"OGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **489/1000** <br/> **Why?** Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14329:9047,avail,available,9047,https://hail.is,https://github.com/hail-is/hail/pull/14329,1,['avail'],['available']
Availability,"OGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Denial of Service (DoS) <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6050294](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6050294) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **561/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:9039,avail,available,9039,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['avail'],['available']
Availability,"OK, I can see why you didn't enable PLW2901 but it did discover one bug in a CLI command. It's kind of annoying and invalidates reasonable programs but it seems really valuable for catching errors that are common amongst new programmers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12967#issuecomment-1535590575:190,error,errors,190,https://hail.is,https://github.com/hail-is/hail/pull/12967#issuecomment-1535590575,1,['error'],['errors']
Availability,"OK, I eliminated a bunch of `log.exception` that are either retried or re-raised. I completely eliminated some of them and in other cases I downgraded them to warning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9980#issuecomment-775247626:140,down,downgraded,140,https://hail.is,https://github.com/hail-is/hail/pull/9980#issuecomment-775247626,1,['down'],['downgraded']
Availability,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5891:880,error,error,880,https://hail.is,https://github.com/hail-is/hail/pull/5891,1,['error'],['error']
Availability,"OK, I improved the tests two ways:. 1. I allocate a random amount of memory in the region to start so things don't always start at offset 0. 2. I test addRegionValue adding a value at the top level and and a nested level (by allocating a non-unsafe Row when t == TStruct) so it calls through to RVB.addRow. I verified it would have caught the previous errors, and it caught another error (toOff was wrong in addRegionValue because we called currentOffset before allocateRoot). Hopefully good to go now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521:352,error,errors,352,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521,2,['error'],"['error', 'errors']"
Availability,"OK, I moved the file format test changes to https://github.com/hail-is/hail/pull/11906. This change can go in independently, but #11906 will even out the test job times and make developer experience better. Service backend tests on #11904 which should be representative of a normal PR:. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Success | Success 🎉 | 24 minutes; 119 | test_hail_python_service_backend_1 | Success | Success 🎉 | 27 minutes; 120 | test_hail_python_service_backend_2 | Success | Success 🎉 | 24 minutes; 121 | test_hail_python_service_backend_3 | Success | Success 🎉 | 41 minutes; 122 | test_hail_python_service_backend_4 | Success | Success 🎉 | 21 minutes. Service backend tests on this PR (albeit with #11906 which evens out test times):. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Failed | Failure 🤷‍♀️ (1) | 31 minutes; 119 | test_hail_python_service_backend_1 | Success | Success 🎉 | 31 minutes; 120 | test_hail_python_service_backend_2 | Success | Success 🎉 | 28 minutes; 121 | test_hail_python_service_backend_3 | Success | Success 🎉 | 33 minutes; 122 | test_hail_python_service_backend_4 | Success | Success 🎉 | 26 minutes. I think there is almost no effect on service backend test times! We should really see if there's a way to improve the autoscaler & schedule to achieve this on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508:939,Failure,Failure,939,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508,1,['Failure'],['Failure']
Availability,"OK, I overcame my irrational fear of taits, and changed everything to use tolerations. If I can't get things to work like this, I will try node selectors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-562079366:74,toler,tolerations,74,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-562079366,1,['toler'],['tolerations']
Availability,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:139,down,down,139,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089,2,['down'],['down']
Availability,"OK, I seem to have resolved this error, but now another transient error has dramatically increased; its frequency. I included my test code which was reliably reproducing this error approximately once per run. I ran; this three times using a commit very similar to `main` [1]. All three runs failed:. 1. In run 1, three partitions had this error.; 2. In run 2, one partition had a different error (#13721 to be exact).; 3. In run 3, two partitions had this error. After my fix [2] for this issues bug, the #13721 bug became super common! I saw it 50 times in my first run:; ```; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; ```. Luckily, that one is actually trivial to fix, we just need to [update to the latest GCS client; library](https://github.com/hail-is/hail/issues/13721#issuecomment-1737924344). # Test Code. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://danking/tmp/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://danking/foo.vcf.bgz'. vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184,7,"['error', 'reliab']","['error', 'reliably']"
Availability,"OK, I tested my branch twice with stress. No error logs. Many warnings due to known deadlock errors. Otherwise it looks clean. I've pushed those changes onto this branch. Let's merge tomorrow first thing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956662015:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956662015,2,['error'],"['error', 'errors']"
Availability,"OK, I think I addressed all the comments. I'm going to make the change to not stat the destination if we set treat_dest_as=Transfer.TARGET_FILE. Then we just need to decide if we want to throw an error on file:// for a non-existent Transfer.TARGET_DIR. I say no.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822#issuecomment-764847097:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764847097,1,['error'],['error']
Availability,"OK, I think I've addressed all the comments. I made some additional changes:. - pipe input file directly into tar instead of writing to disk (writing to SSDs, I get ~100MB/s/core download saturating gs://),; - report records read,; - directly create OrderedRVD instead of coercing,; - updated GenomicsDB to latest: 0.9.2-proto-3.0.0-beta-1+ed318f7e815 which involved revising GenomicsDBFeatureReader ctor call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861:179,down,download,179,https://hail.is,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861,1,['down'],['download']
Availability,"OK, I think this is actually ready for a real review. Almost everything was spurious (I marked as such, so hopefully we won't have to do that on every PR). There were a few real things:; 1. use integrity checks for CDN javascript libraries; 2. don't let edits to the search textbox modify the URL arbitrarily; 3. don't let the target pages of anchor tags mutate the source page's DOM (wtf, how is this the default behavior???); 4. don't send the IntegrityError from mysql back to the users. I think this is basically safe because of how restrictive we are with which error is printed, but it's also not necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860:567,error,error,567,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860,1,['error'],['error']
Availability,"OK, I will PR improved error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470,1,['error'],['error']
Availability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1005,error,errors,1005,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['error'],['errors']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777:93,avail,available,93,https://hail.is,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777,8,['avail'],['available']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:93,avail,available,93,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,255,['avail'],['available']
Availability,"OK, I'm not sure how to fix this but the work is to explain to the GCS Hadoop Connector which credentials we want it to use. See the failure here: https://batch.hail.is/batches/8136069/jobs/49 . It uses CI's credentials instead of the test credentials. We use core-site.xml to do this in Spark <3.5, but the GCS connector is different in Spark 3.5 and it uses different configuration parameters. My most recent change did not successfully configure it. Daniel G can help you a bit with credentials in Batch if that's necessary but the real work is to figure out how to tell the GCS Hadoop Connector to use the /gsa-key/key.json file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844:133,failure,failure,133,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844,1,['failure'],['failure']
Availability,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:632,error,error,632,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['error'],['error']
Availability,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:192,down,downsampling,192,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696,2,['down'],['downsampling']
Availability,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240309173:114,down,download,114,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173,1,['down'],['download']
Availability,"OK, alright. I think you're right. I'm just reliving how frustrated I am by this situation. Can you modify aggregate_cols to include the same warning from `cols()`?; ```; In [6]: import hail as hl; ...: mt = hl.utils.range_matrix_table(3, 3); ...: mt = mt.choose_cols([2, 1, 0]); ...: mt = mt.checkpoint('/tmp/foo.mt', overwrite=True); ...: mt.col_idx.show(); ...: print(mt.aggregate_cols(hl.agg.collect(mt.col_idx))); ...: mt = mt.key_cols_by(); ...: print(mt.aggregate_cols(hl.agg.collect(mt.col_idx))); 2023-08-10 14:43:14.286 Hail: INFO: wrote matrix table with 3 rows and 3 columns in 3 partitions to /tmp/foo.mt; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 2 |; | 1 |; | 0 |; +---------+; 2023-08-10 14:43:16.338 Hail: INFO: Coerced sorted dataset; [0, 1, 2]; [2, 1, 0]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405#issuecomment-1673731948:293,checkpoint,checkpoint,293,https://hail.is,https://github.com/hail-is/hail/pull/13405#issuecomment-1673731948,1,['checkpoint'],['checkpoint']
Availability,"OK, but I'm not sure it's the right change to make. Now some jobs will fail silently.; I think the right thing to do would be to change how benchmark jobs are run and always collect results, regardless of job outcome (making it resiliant to some benchmark files not being in their expected locations, which we'll have to do anyway). That way, you'd still see the failures in the batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527578232:363,failure,failures,363,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527578232,1,['failure'],['failures']
Availability,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:27,failure,failure,27,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,5,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"OK, let's always mount it. People download and run software on their laptops which can read their gcloud and hail credentials awnyway!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-768637760:34,down,download,34,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-768637760,1,['down'],['download']
Availability,"OK, should be resolved now. I had a lint error and needed to mount the global config into hello and website.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402,1,['error'],['error']
Availability,"OK, so the big insight is that ""InstanceConfig"" is really just ""ResourcesForAParticularInstance"" (well, and, sometimes, ""ResourcesOfARepresentativeInstance""). I trimmed the InstanceConfig down *significantly* removing the ""vm_config"". Now the InstanceConfig is cheap and easy to create and there's no circularity between vm_config and instance config. I pushed that through everywhere and then abstracted the common create_instance logic for pool and job-private into InstanceCollection. With both of those changes, I was able to modify the ResourceManager's API to expose methods for constructing instance configs. However, the instance config isn't critical to the operation of the ResourceManager. It's just an interface for communicating an instance's resources to the rest of the code base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279:188,down,down,188,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279,1,['down'],['down']
Availability,"OK, so, I took the GRCh38 file that we test against and named it `bar`. I downloaded the gist and named it `foo`. | header | footer | success? |; |---|---|---|; |foo|bar|success|; |bar|bar|success|; |bar|foo|failure|; |foo|foo|failure|. So clearly the issue is the variants. Here's an example of running on just the first handful of variants: https://batch.hail.is/batches/8089052. ```; chr1	1339585	.	G	A	.	.	.; chr1	24907372	.	C	T	.	.	.; chr1	36859143	.	G	T	.	.	.; chr1	37969436	.	T	C	.	.	.; chr1	40416828	.	G	A	.	.	.; chr1	41581842	.	G	A	.	.	.; chr1	43920822	.	T	C	.	.	.; chr1	45327881	.	G	A	.	.	.; chr1	46817055	.	CT	C	.	.	.; chr1	54999203	.	C	T	.	.	.; chr1	65218884	.	C	T	.	.	.; chr1	102962250	.	G	T	.	.	.; chr1	111756087	.	G	C	.	.	.; chr1	113881802	.	G	A	.	.	.; chr1	117920205	.	G	A	.	.	.; chr1	151408784	.	G	C	.	.	.; chr1	151428261	.	C	T	.	.	.; chr1	152305539	.	G	C	.	.	.; chr1	152884596	.	C	A	.	.	.; chr1	153933240	.	C	T	.	.	.; chr1	156624012	.	G	A	.	.	.; chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344:74,down,downloaded,74,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344,3,"['down', 'failure']","['downloaded', 'failure']"
Availability,"OK, so, this really feels like bad data. We just merged https://github.com/hail-is/hail/commit/98adcce1d07001995b0819fd6afe161bf34ba840 which fixed https://github.com/hail-is/hail/issues/13979 . Google Cloud Storage's Java library was very rarely returning just flat-out bad data. The frequency of occurrence on one particularly large pipeline appears to be 1/30000 tasks (0.003% or 3 in 100,000). The tasks were reading two files, the larger of which was 131MiB. The Java library reads in 8MiB chunks so that's at least 17 network requests per partition. That puts the frequency of this closer to 1 in 1,000,000 requests or 1 in 10TiB of data read. Before we had Zstandard, it seems that this data corruption either (a) was unnoticed (b) caused a rare decoding error or (c) caused segfaults. After we added Zstandard (0.2.119), decompression often failed due to corrupt data. It seems to me that Zstandard more aggressively verifies integrity than LZ4 does. OK, so, when was this bug introduced in Hail? As far as I can tell, this new code path was added in google-cloud-storage 2.17.0 almost one year ago: https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77 . We upgraded to 2.17.1 (😭 ) in Hail 0.2.109 https://github.com/hail-is/hail/commit/fec0cc2263c04c00e02cef5dda8ec46916717152 . All of the attempts above could have been plagued by this rare transient data corruption error. OK, action items:. - [ ] Ask Cal and Lindo to try their pipelines again with the next release of Hail 0.2.127.; - [x] Hail must introduce large-scale testing before releases. We, sadly, cannot assume our underlying storage libraries are reliable. https://github.com/hail-is/hail/issues/14082. Once the first action item is successfully completed, I will close this issue. For the second action item, I have created a separate ticket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114:762,error,error,762,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114,3,"['error', 'reliab']","['error', 'reliable']"
Availability,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:258,down,down,258,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146,3,"['down', 'error']","['down', 'error']"
Availability,"OK, so. The issue seems to be that docker shuts down a connection when an invalid packet is received. https://github.com/moby/libnetwork/issues/1090. Why `git clone` isn't exiting with a non-zero code and thus bailing out of the function, I don't know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8190#issuecomment-600700900:48,down,down,48,https://hail.is,https://github.com/hail-is/hail/pull/8190#issuecomment-600700900,1,['down'],['down']
Availability,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205:239,reliab,reliable,239,https://hail.is,https://github.com/hail-is/hail/pull/8205,1,['reliab'],['reliable']
Availability,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,1,['error'],['error']
Availability,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:96,avail,available,96,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826,2,['avail'],['available']
Availability,"OK, this PR basically works except we're encountering OOMs somewhat often. I'm trying to track down which tests are triggering the OOMs, but its a bit tricky because the OOMKiller doesn't necessarily kill the test which is using a ton of memory.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1034272076:95,down,down,95,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1034272076,1,['down'],['down']
Availability,"OK, this is now higher priority for me. The Query-on-Batch tests are absolutely hammering the database with huge spikes in deadlock errors during working hours (when deploys trigger tests).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265:132,error,errors,132,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265,1,['error'],['errors']
Availability,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:617,toler,tolerate,617,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['toler'],['tolerate']
Availability,"OK, this is working and ready for review. I tested manually that on a variety of node types, we both (1) get the expected number of containers (all the cores in the cluster are used) and (2) we get the right OOM error instead of container crashing. Your comment is addressed -- we always call increment before actually allocating, so we won't exceed the threshold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11016#issuecomment-964540617:212,error,error,212,https://hail.is,https://github.com/hail-is/hail/pull/11016#issuecomment-964540617,1,['error'],['error']
Availability,"ON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:2471,avail,available,2471,https://hail.is,https://github.com/hail-is/hail/pull/13718,3,['avail'],['available']
Availability,"ON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:2539,avail,available,2539,https://hail.is,https://github.com/hail-is/hail/pull/13873,1,['avail'],['available']
Availability,"ON-JUPYTERSERVER-5862881](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862881) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **/1000** <br/> **Why?** | Open Redirect <br/>[SNYK-PYTHON-JUPYTERSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:2580,avail,available,2580,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['avail'],['available']
Availability,"ON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:4028,avail,available,4028,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"ON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **726/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 8.1 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-NBCONVERT-2979829](https://snyk.io/vuln/SNYK-PYTHON-NBCONVERT-2979829) | `nbconvert:` <br> `5.6.1 -> 6.3.0b0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **434/1000** <br/> **Why?** Has a fix available, CVSS 4.4 | Open Redirect <br/>[SNYK-PYTHON-NOTEBOOK-1041707](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:4720,avail,available,4720,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"ON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyMjlkNGUyNC0xMDE4LTQ5ZDItYTQ3NC04MmViZDVhNzZlMDEiLCJldmVudCI6IlBSIHZpZ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:8459,avail,available,8459,https://hail.is,https://github.com/hail-is/hail/pull/13717,1,['avail'],['available']
Availability,"ON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-WHEEL-3180413](https://snyk.io/vuln/SNYK-PYTHON-WHEEL-3180413) | `wheel:` <br> `0.30.0 -> 0.38.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:8494,avail,available,8494,https://hail.is,https://github.com/hail-is/hail/pull/14108,2,['avail'],['available']
Availability,"Oh I see, I actually didn't realize / forgot you could have functions down below that were not present in the summary at the top. Agree we should fix this. I'm going to keep a running list of missing functions in the issue description",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7292#issuecomment-558284093:70,down,down,70,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558284093,1,['down'],['down']
Availability,"Oh yeah, to get my code to work you need to comment out line 778 `gene_id=ch_ht.gene_ids,` in `_annotated_comp_het_variant`. It doesn't break search to be missing that annotation, it just has some downstream display affects that I would need to fix if I actually wanted to use the code, but given the performance hit I wasn't sure it was worth figuring that out as this code may be too slow to use. I was not able to get the code you provided here to run either, but one concern I have with it is that the unique combinations are computed per gene, but if you have a pair of variants that are each in the same 2 genes, you would get the pair twice in the results, one for each gene. The error I get when I run the code you provide is; ```; ""Key type mismatch: cannot index table with given expressions:; Table key: <<<empty key>>>; Index Expressions: locus<GRCh38>, array<str>, set<str>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, struct{z_score: float32}, struct{region_type_ids: array<int32>}, locus<GRCh37>, str, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, str, int64, struct{PHRED: float32}, struct{alleleId: int32, conflictingPathogenicities: array<struct{pathogenicity_id: int32, count: int32}>, goldStars: int32, pathogenicity_id: int32, assertion_ids: array<int32>}, struct{REVEL_score: float32, VEST4_score: float32, MutPred_score: float32, SIFT_pred_id: int32, Polyphen2_HVAR_pred_id: int32, MutationTaster_pred_id: int32, fathmm_MKL_coding_pred_id: int32}, struct{Eigen_phred: float32}, struct{AF_POPMAX: float3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465:197,down,downstream,197,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465,1,['down'],['downstream']
Availability,"Oh, interesting idea Jackie. It has always bothered me that the batches state is duplicated as both the state column and the four job state count columns. @daniel-goldstein , responding specifically to your point about the new deadlock. I agree, that seems strange. I'd bump up the number of tokens and hope that contention for batch_inst_coll_cancellable_resources goes down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039672959:371,down,down,371,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039672959,1,['down'],['down']
Availability,"Oh, it looks like this error isn't from a subprocess call. The thing you added would help debug the first error you posted (No such file or directory) but not this one, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319718587:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319718587,2,['error'],['error']
Availability,"Oh, woah, that test does look wrong. It's concerning that its suddenly failing. I'm not sure I care too much about tracking down exactly which dependency change caused this. We should fix the test obviously. We should add a test that verifies both `?a` and `?` have the expected data (in particular, that we didn't overwrite one with the other!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186:124,down,down,124,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186,1,['down'],['down']
Availability,"Ok I just looked at the scala code, and this must have happened around the sex chromosomes when my dataset shifted to haploid (or more specifically, a mix of haploid and diploid calls for male and female). I'll write in the workaround for my own pipeline, but you might want to have a more explicit error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263:299,error,error,299,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263,1,['error'],['error']
Availability,"Ok, I think I sorted out the make->mill dependency propagation. Any real target which invokes mill to build it now depends on a target `FORCE` which is always out-of-date, so mill is always invoked. But mill will not change the modification time if it doesn't need to, so downstream targets aren't forced to be run. For example, we have targets; ```; FORCE:. SHADOW_JAR := out/assembly.dest/out.jar; $(SHADOW_JAR): FORCE; 	$(mill) assembly. PYTHON_JAR := python/hail/backend/hail-all-spark.jar; $(PYTHON_JAR): $(SHADOW_JAR); 	cp -f $(SHADOW_JAR) $@. .PHONY: python-jar; python-jar: $(PYTHON_JAR); ```. If I remove the python jar and invoke make, it runs mill then copies:; ```; ❯ rm python/hail/backend/hail-all-spark.jar. ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If run again, mill is invoked to check for changes, but as the jar doesn't change it isn't copied again:; ```; ❯ make python-jar; bash millw assembly; [105/110] memory.resources; ```. If I change some scala sources, the jar is updated and copied:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If I change some scala sources in a way that doesn't actually affect the jar, such as modifying comments, mill is smart enough to not change the jar, so it won't be copied again:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 1 Scala source to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [105/110] memory.resources; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793:272,down,downstream,272,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793,1,['down'],['downstream']
Availability,"Ok, I've addressed your comment, corrected requiredness inference for `PartitionNativeWriter` and `SplitPartitionWriter` and revamped things so that the key is not determined based on whether or not there is an index. I've also bumped file version and regenerated files. . I'd be interested to see what you think about testing / whether there's more testing you would do. This is harmless change right now if all the normal tests pass since it's just adding a new metadata field that's unused save for a few tests, but we don't want to start marking files ""distinctlyKeyed"" that aren't and then run into problems down the road when we implement the new join behavior.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11151#issuecomment-1015626695:613,down,down,613,https://hail.is,https://github.com/hail-is/hail/pull/11151#issuecomment-1015626695,1,['down'],['down']
Availability,"Ok, I've downloaded JSON for the batch, default, and CI dashboards (I don't think any of the other ones are really used?), and recorded the configurations for the GCP and prometheus datasources, so I think I should be able to quickly reconfigure grafana if everything is lost.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923:9,down,downloaded,9,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923,1,['down'],['downloaded']
Availability,"Ok, merged to resolve conflicts. Going to now add a few commits to fix errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9228#issuecomment-683803596:71,error,errors,71,https://hail.is,https://github.com/hail-is/hail/pull/9228#issuecomment-683803596,1,['error'],['errors']
Availability,"Ok, so I thought I left a comment on here but I guess I didn't: when I tested this with dev deploy, I didn't see any plots show up, got JS console errors. So I'm not sure this was quite ready to be merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394:147,error,errors,147,https://hail.is,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394,1,['error'],['errors']
Availability,"Ok, so now there's a generic catch all error on `Expression`, and I override it on `StructExpression` to make sure that one works. `CollectionExpression` and `StringExpression` also get overrides to give appropriate examples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164#issuecomment-665678053:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665678053,1,['error'],['error']
Availability,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066,2,['error'],['error']
Availability,"Ok. This exact scenario is what I was worried about when we merge PRs without checking the logs by hand in a full testing scenario. I want a way to check the PR driver, front-end, and worker logs automatically that they don't have ERROR messages. Like test_invariants. For example, I'm still looking at your change for time_since_last_state_change. When I had the code you wanted, there were errors because time_since_last_state_change was None. The current tests would not have caught that. I think we need either a white list of acceptable front-end/driver errors or some kind of threshold for error types. I'll think about it some more once the batch porting is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959:231,ERROR,ERROR,231,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Ok. What's going on here is like a proper pipe, we're consuming vep's standard error on a background thread and outputting it to System.err , then in the failure path, we're attempting to consume it again. This goes poorly. Fix incoming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561,2,"['error', 'failure']","['error', 'failure']"
Availability,"Okay, python tests in local mode now have the same number of failures as on main. I just needed to be more careful in preserving the information that determines the subregion relation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921:61,failure,failures,61,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921,1,['failure'],['failures']
Availability,"Old State Diagram:; Created -> Ready -> Complete; Cancelled. New State Diagram:; Pending -> Ready -> (Error, Running -> (Failed, Success)); Cancelled",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268:102,Error,Error,102,https://hail.is,https://github.com/hail-is/hail/pull/6268,1,['Error'],['Error']
Availability,Old versions of pip could only download some packages in source form which; requires compiling them. New version of pip can download these packages in; binary form which requires no compilation. This *substantially* improves docker; build times when you have to run `pip` in any layer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9127:31,down,download,31,https://hail.is,https://github.com/hail-is/hail/pull/9127,2,['down'],['download']
Availability,"On Oct 21, 2017, at 1:27 PM, Sun-shan <notifications@github.com> wrote:. > hail: info: SparkUI: http://10.131.101.159:4040; > Welcome to; > __ __ <>__; > / // /__ __/ /; > / __ / _ `/ / /; > // //_,/// version 0.1-f69b497; > ; > print sc; > ; > >>> print hc >>> hc.import_vcf() Traceback (most recent call last): File """", line 1, in TypeError: import_vcf() takes at least 2 arguments (1 given) >>> hc.import_vcf('/hail/sample.vcf') [Stage 0:> (0 + 1) / 2]Traceback (most recent call last): File """", line 1, in File """", line 2, in import_vcf File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)) hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. This type of Spark exception seems to be related to the configuration option spark.cleaner.ttl. Have you set that to a value other than the default?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661:614,Error,Error,614,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661,1,['Error'],['Error']
Availability,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6967:103,down,down,103,https://hail.is,https://github.com/hail-is/hail/pull/6967,1,['down'],['down']
Availability,"On an n1-highmem-8, this is the RAM in use after startup. About 200MiB lower overhead than [the last time I did this](https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790).; ```; Dec 12 15:20:34 dk-m post-hdfs-startup-script[10260]: + echo 'All done'; Dec 12 15:20:34 dk-m post-hdfs-startup-script[10260]: All done; Dec 12 15:20:42 dk-m earlyoom[6529]: mem avail: 42959 of 52223 MiB (82.26%), swap free: 0 of 0 MiB ( 0.00%); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14093#issuecomment-1852287669:251,echo,echo,251,https://hail.is,https://github.com/hail-is/hail/pull/14093#issuecomment-1852287669,2,"['avail', 'echo']","['avail', 'echo']"
Availability,On the web site:; https://hail.is/docs/stable/getting_started.html. The links to the current distributions are broken:; Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. The error is:; NoSuchKeyThe specified key does not exist.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2217:200,error,error,200,https://hail.is,https://github.com/hail-is/hail/issues/2217,1,['error'],['error']
Availability,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:655,error,error,655,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842,1,['error'],['error']
Availability,"One more if/when you want to take a look. Changes:; - Keep track of failed requests to workers, failed_request_count. Reset on a successful communication in Instance.mark_healthy.; - Don't retry the /job/create request. The scheduler loop will just retry.; - Don't schedule on nodes with failed count > 1. This basically ignores 1-off hiccups.; - The /jobs/delete (unscheduled) call is an interested situation. I decided to retry with back off and stop if the instance gets deactivated. Retry abstractions seem hard, I'm not quite sure how to share this code with request_retry_transient_error, for example, given different exit conditions. Sometime to think about as we see more examples.; - Ignore errors for each schedule attempt, so if there is a failure, count the request as failed and keep scheduling the current block of jobs before getting another block.; - Don't kill unhealthy instances. You might object to this, but I'm worried about when a job has been running for 3hrs (or 5 weeks, once we support scheduling on non-premptibles) and we delete an instance after a 5m network disconnect. I appreciate the need to clean up resources, I have more thoughts about that that I'll write elsewhere.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7441:700,error,errors,700,https://hail.is,https://github.com/hail-is/hail/pull/7441,2,"['error', 'failure']","['errors', 'failure']"
Availability,One more step of stripping down router. ukbb lives in its own namespace and we never run dev versions of it so it's pretty much a drop-in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10179:27,down,down,27,https://hail.is,https://github.com/hail-is/hail/pull/10179,1,['down'],['down']
Availability,"One more time, with feeling! (was: #10072). - [x] (@tpoterba) a1f3b2a5c9 add fails_service_backend; - [ ] (@tpoterba, @cseed) dc0bee7ce1 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) 4b663be367 [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) d3c1f0987c [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) aab6ba98be [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) a1619cff36 [query-service] make user cache thread-safe; - [ ] (@tpoterba) c315fcb0b1 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) 912c21f709 [shuffler] log ShuffleCodecSpec anytime it is created; - [x] (@daniel-goldstein) c2495837e7 [scala-lsm] bugfix: least key may equal greatest key; - [x] (@daniel-goldstein) 5fb3db703e [services] discovered new transient error; - [x] (@daniel-goldstein) 9cd0999938 [shuffler] more assertions in ShuffleClient; - [x] (@daniel-goldstein) a71a3c9b8c [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [x] (@daniel-goldstein) 41b06aeaa8 [query-service] move hail.jar earlier in Dockerfile; - [x] (@daniel-goldstein) 8df4029698 [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 0354e1f557 [query-service] simplify socket handling; - [x] (@jigold) 6690a4decc [batch] teach JVMJob where to find the hail configuration files; - [x] (@daniel-goldstein) ae2e3d2996 [query-service] switch to services team approved logging; - [ ] (@tpoterba) b18f86e647 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 6d5d0b68af [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) 0d42df8b08 [query-service] run tests against query service; - [x] (@jigold) f9d361e686 [query-service] aiohttp.ClientSession must be created in async code; - [ ] (@cseed) c35f2e10e3 [query-service][hail][build.yaml] address miscellaneous comments from cotton; - [x]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:858,error,error,858,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['error'],['error']
Availability,One other question: Is it a breaking change to change the type of error we throw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398#issuecomment-685798692:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/9398#issuecomment-685798692,1,['error'],['error']
Availability,"One other snarl I've hit -- I'll need to reboot the jupyter service in the init script in order to use `jgscm` as the content manager, and need to find time to test that and make sure everything continues to work afterwards.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12788#issuecomment-1479689103:41,reboot,reboot,41,https://hail.is,https://github.com/hail-is/hail/pull/12788#issuecomment-1479689103,1,['reboot'],['reboot']
Availability,Only one coro waits on receive now. We still error if a message is sent before; we make our first response.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10159:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/10159,1,['error'],['error']
Availability,"Oof, method verification error on one of the lowering tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923,1,['error'],['error']
Availability,"Oops, sorry, I misread where the failure was happening. You need `cseed/hail:batch-web`, not `cseed:batch-web`. I created an issue to fix this: https://github.com/hail-is/hail/issues/7074",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532274736:33,failure,failure,33,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532274736,1,['failure'],['failure']
Availability,"Oops, sorry. Although I really blame PruneSuite. It does a bunch of serious work on construction, and basically makes the tests unusable if there are any bugs and testng silently bails with a fatal error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768:198,error,error,198,https://hail.is,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768,1,['error'],['error']
Availability,"Open question: we're using ~20GiB on /prometheus for 15d. We request 150GiB (and get closer to 146GiB). Should we increase the storage to give ourselves more slack? Assuming linear scaling, 90d would use 120GiB (26GiB of slack). https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/Grafana.20retention.20period; ```; /prometheus $ df -h; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 28.9G 65.3G 31% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 3.6G 0 3.6G 0% /sys/fs/cgroup; /dev/sdf 146.6G 18.9G 127.6G 13% /prometheus; /dev/sda1 94.3G 28.9G 65.3G 31% /etc/prometheus; /dev/sda1 94.3G 28.9G 65.3G 31% /etc/hosts; /dev/sda1 94.3G 28.9G 65.3G 31% /dev/termination-log; /dev/sda1 94.3G 28.9G 65.3G 31% /etc/hostname; /dev/sda1 94.3G 28.9G 65.3G 31% /etc/resolv.conf; shm 64.0M 4.0K 64.0M 0% /dev/shm; tmpfs 5.5G 12.0K 5.5G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 3.6G 0 3.6G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 3.6G 0 3.6G 0% /proc/scsi; tmpfs 3.6G 0 3.6G 0% /sys/firmware; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14194:376,Avail,Available,376,https://hail.is,https://github.com/hail-is/hail/pull/14194,1,['Avail'],['Available']
Availability,"Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `fileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes. fix test failures. passes tests. fixes. fix tests to not use fileStatus for folders. only file vs directory status matters. fix azure. azure dislikes %. finally get azure right. nix empty line. fix merge cruft. azure bug. lots of changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13883:1827,failure,failures,1827,https://hail.is,https://github.com/hail-is/hail/pull/13883,1,['failure'],['failures']
Availability,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93:53,down,downsamplevariants,53,https://hail.is,https://github.com/hail-is/hail/pull/93,1,['down'],['downsamplevariants']
Availability,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-244517801:974,failure,failure,974,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801,1,['failure'],['failure']
Availability,OrderedJoinDistinctRDD2 fails if the right side has no partitions (assertion failure in BinarySearch),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2876:77,failure,failure,77,https://hail.is,https://github.com/hail-is/hail/issues/2876,1,['failure'],['failure']
Availability,OrderedRVD assertion error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3998:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/3998,1,['error'],['error']
Availability,OrderedRVD error!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/4096,1,['error'],['error']
Availability,Otherwise you get an error from the compiler on IR construction/typechecking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12080:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/12080,1,['error'],['error']
Availability,Our CI had a sporadic failure here -- you can retest again by pushing an empty commit to the branch:. git commit --allow-empty -m bump && git push,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6264#issuecomment-499467470:22,failure,failure,22,https://hail.is,https://github.com/hail-is/hail/pull/6264#issuecomment-499467470,1,['failure'],['failure']
Availability,"Our GKE nodes come with 2 CPU and 7.5 GB each, but not all of that is allocatable to our pods. In reality, somewhere between 5.7-5.9GB ([GCP Docs](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture) say 5.7, GKE console says 5.9) of memory and 1.9CPU are available for us to use. Some of our Big services request 1 CPU and 3.75GB, but then we can never fit two big pods on one node. This is an attempt to standardize our requests so their easier to reason about while hopefully getting better packing. . | resource | Big | Medium | Small |; | --- | --- | --- | --- |; | CPU | 600m | 100m | 20m |; | Memory | 2G | 200M | 20M |. The intentions here are:; - always be able to comfortably get 2 Big pods on a node; - medium pods shouldn't have to force new nodes to spin up just because there's a Big pod there already; - small pods take up minimal resources; - there's ample room for small pods (which are mostly on HPA) to scale up considering most nodes shouldn't be at their medium pod capacity. The ratios don't match exactly, because I didn't want to assign CPU lower than 20m to prevent HPA thrashing we saw with auth and see a bit now with router. Setting it to 20m should hopefully convince k8s that idle small apps don't need to be scaled up under normal fluctuation. ## Big; - query; - batch-driver; - shuffler; - memory. ## Medium; - grafana; - ukbb-browser; - ukbb-static; - blog; - ci; - internal-gateway. ## Small; - amundsen; - router; - gateway; - site; - batch; - address; - atgu; - router-resolver; - ci/test statefulset & deployment; - auth-driver; - echo; - benchmark; - image-fetcher. ### Fun surprises I found along the way; - CI test statefulsets and deployment are getting .5GB and .5 CPU each; - We run a lot of image fetchers because a daemon set gets added per PR namespace. EDIT: It seems that discrepancy between GCP docs and GKE console is the console counts kube-system pods in ""Allocatable Memory"", and it really does take 2GB to run the Kuberne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10117:282,avail,available,282,https://hail.is,https://github.com/hail-is/hail/pull/10117,1,['avail'],['available']
Availability,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:342,error,error,342,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754,2,"['downtime', 'error']","['downtime', 'error']"
Availability,"Our current HTML display of tables flattens the table and concatenates the field; names to produce table headers. This leads to long, unreadable headers. This change reproduces the nesting structure of the types with several table; header layers. The essential activity is to convert:. ```; bing; foo:; bar:; baz; quux; fizzle:; fazz:; fazz1; fazz2; fezz; quork; bang; ```. into. ```; foo; -------------------------------; fizzle; ----------------; bar fazz; -------- -----------; bing baz quux fazz1 fazz2 fezz quork bang; ```. The bottom layer are the names of the leaves of this tree. Working from the; bottom, a name appears when the row corresponds to that name's tree height. For; this reason `bar` appears lower than `fizzle`. This frustrates finding peer; fields. However, I prefer it. I think I have some sense of visual gravity that; wants bar to fall down. Anyway, I implement this with some html grunginess in `Table._Show` and a new; class named `PlacementTree`. We construct a `PlacementTree` from a type. It is a; tree whose internal and leaf nodes contain a name, width, and height. It has a; method `to_grid` which converts it to an HTML-table-like structure with ""spacer""; elements. Our above example looks like:. ```python3; [[(None, 1), ('foo', 6), (None, 1)],; [(None, 1), (None, 2), ('fizzle', 3), (None, 1), (None, 1)],; [(None, 1), ('bar', 2), ('fazz', 2), (None, 1) (None, 1), (None, 1)],; [('bing', 1), ('baz', 1), ('quux', 1), ('fazz1', 1), ('fazz2', 1), ('fezz', 1) ('quork', 1), ('bang', 1)]]; ```. The code in `Table._Show` converts this to HTML table rows that looks like:; ```html; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; ```. Which looks like:. <table>; <tr><t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8811:862,down,down,862,https://hail.is,https://github.com/hail-is/hail/pull/8811,1,['down'],['down']
Availability,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/216#issuecomment-279518768:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768,1,['error'],['error']
Availability,"Our team is currently trying to run kinship analysis with [king()](https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.king) on just under 110k samples. We have run this successfully in the past on 10k samples using a google cloud cluster with the following configuration. ```; hailctl dataproc start cluster --vep GRCh38 \; 	--requester-pays-allow-annotation-db \; 	--packages gnomad --requester-pays-allow-buckets gnomad-public-requester-pays \; 	--master-machine-type=n1-highmem-8 --worker-machine-type=n1-highmem-8 \; 	--num-workers=300	--num-secondary-workers=0 \; 	--worker-boot-disk-size=1000 \; 	--properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:772,error,error,772,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['error'],['error']
Availability,"Output for a requirement failed exception now looks like:. ```; Traceback (most recent call last):; File ""hail_extract_cohorts.py"", line 37, in <module>; vds = vds.annotate_global('global.samples_to_exclude', set(samples_to_exclude_list), hail.TSet(hail.TString())); File ""<decorator-gen-390>"", line 2, in annotate_global; File ""/home/cotton/hail/python/hail/java.py"", line 167, in handle_py4j; 'Error summary: %s' % (e.desc, e.stackTrace, Env.hc().version, e.desc)); hail.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); 	 at is.hail.variant.VariantSampleMatrix.annotateGlobal(VariantSampleMatrix.scala:564); 	 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	 at java.lang.reflect.Method.invoke(Method.java:498); 	 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	 at py4j.Gateway.invoke(Gateway.java:280); 	 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	 at py4j.commands.CallCommand.execute(CallCommand.java:79); 	 at py4j.GatewayConnection.run(GatewayConnection.java:214); 	 at java.lang.Thread.run(Thread.java:748); Hail version: devel-75de081; Error summary: requirement failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2554:396,Error,Error,396,https://hail.is,https://github.com/hail-is/hail/pull/2554,2,['Error'],['Error']
Availability,"P; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T03:09:04Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradj",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466:6490,toler,tolerationSeconds,6490,https://hail.is,https://github.com/hail-is/hail/issues/6466,1,['toler'],['tolerationSeconds']
Availability,"P; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradj",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14765,toler,tolerationSeconds,14765,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['toler'],['tolerationSeconds']
Availability,PCA throws error using Breeze 0.12 and natives (BLAS),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/209:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/209,1,['error'],['error']
Availability,"PHY-6092044) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | Information Exposure <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6126975](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6126975) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **561/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.5 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6210214](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6210214) | `cryptography:` <br> `3.3.2 -> 42.0.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5ZjJhMGZlMy1kYmVkLTQ2YzAtYmQyMC0yMjM3NzFiYzE0OTciLCJldmVudCI6IlBSIH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14327:9795,avail,available,9795,https://hail.is,https://github.com/hail-is/hail/pull/14327,1,['avail'],['available']
Availability,PR #1437 resolves the `is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM` failure under Spark 2.1.0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-282776933:80,failure,failure,80,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282776933,1,['failure'],['failure']
Availability,"PTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **711/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.5 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570772](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570772) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **701/1000** <br/> **Why?** Mature exploit, Has a fix available, CVSS 6.3 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-SPHINX-570773](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:7354,avail,available,7354,https://hail.is,https://github.com/hail-is/hail/pull/13717,3,['avail'],['available']
Availability,"PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:8116,avail,available,8116,https://hail.is,https://github.com/hail-is/hail/pull/13717,1,['avail'],['available']
Availability,"PYTHON-SPHINX-570773) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Mature ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5811865](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5811865) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **586/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.3 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SPHINX-5812109](https://snyk.io/vuln/SNYK-PYTHON-SPHINX-5812109) | `sphinx:` <br> `1.8.6 -> 3.3.0` <br> | No | Proof of Concept ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `5.1.1 -> 6.3.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14108:8151,avail,available,8151,https://hail.is,https://github.com/hail-is/hail/pull/14108,2,['avail'],['available']
Availability,"PartitionedParquetRelation currently does not compile on spark 1.6.0. If compiled with spark 1.5.0 and run on 1.6.0, cryptic errors appear from changed method signatures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/667#issuecomment-242106481:125,error,errors,125,https://hail.is,https://github.com/hail-is/hail/issues/667#issuecomment-242106481,1,['error'],['errors']
Availability,"Passing CI tests for Spark 2.4. Do you have a stack trace for a failure?. Sriram saw issues related to Breeze in #9199, but I think it was the bug you noted above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022:64,failure,failure,64,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022,1,['failure'],['failure']
Availability,"Pausing for a few hours. Error in testEmitLeftJoinDistinct is a bit strange. IR contains only constants, and appears correctly inferred as having required elements, but a missing element is found at emit time. <img width=""2270"" alt=""Screenshot 2020-02-21 20 38 52"" src=""https://user-images.githubusercontent.com/5543229/75083876-2f3f9d00-54ea-11ea-9bb8-45f9e708a50b.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885:25,Error,Error,25,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885,1,['Error'],['Error']
Availability,Pending confirmation from Ben W. that this is indeed a user error...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10645:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/10645,1,['error'],['error']
Availability,Ping @chrisvittal. Could you pass this off or review?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-500844860:0,Ping,Ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500844860,1,['Ping'],['Ping']
Availability,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414,1,['failure'],['failure']
Availability,Place mendel error output in state / annotations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/597:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/597,1,['error'],['error']
Availability,"Plus better error checking!. - Some bioinformatic tools expect a secondary implied file to be present. For example, sample.vcf and it's index file sample.vcf.tbi. This PR adds file localization such that if any file in a resource group is used, the entire resource group will be copied and not just the mentioned file. - Added a mentioned set that tracks whether a resource was defined in the command or declare resource group functions. Otherwise, you could do something like this which would throw an error upon execution:. ```python; p = Pipeline(); t = p.new_task(); p.write_output(t.undefined_variable, 'gs://foo/foo'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5455:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/pull/5455,2,['error'],['error']
Availability,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8537:332,error,error,332,https://hail.is,https://github.com/hail-is/hail/pull/8537,1,['error'],['error']
Availability,"Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef202da1 Pod spec.containers{main} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef7b3cef Pod spec.containers{cleanup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 7s 7s 1 batch-12728-job-287-742170.15c2403cf8c02f7f Pod spec.containers{cleanup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a5a03fb Pod spec.containers{cleanup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a914c20 Pod spec.containers{keep-alive} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 4s 4s 1 batch-12728-job-287-742170.15c2403d8c542f61 Pod spec.containers{keep-alive} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 4s 4s 1 batch-12728-job-287-742170.15c2403da37c4943 Pod spec.containers{keep-alive} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:2853,alive,alive,2853,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368,3,['alive'],['alive']
Availability,"Pool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.107-2387bb00ceee; Error summary: SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 10) (all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_e01_1690206305672_0001_01_000007 on host: all-of-us-1774-w-0.c.terra-vpc-sc-23dfb1a3.internal. Exit status: 137. Diagnostics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; ```; [hail-20230724-1347-0.2.107-2387bb00ceee.log](https://github.com/hail-is/hail/files/12146671/hail-20230724-1347-0.2.107-2387bb00ceee.log)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:12193,Error,Error,12193,https://hail.is,https://github.com/hail-is/hail/issues/13287,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"Popping in here with some context/suggestion but please take it or leave it! Very very few jobs in practice are AlwaysRun, and this ticket originated because of confusion that a cancelled batch still had `Ready` jobs. In a way you can consider AlwaysRun a modifier that affects the job's state machine. Maybe for the page with the jobs table, instead of just exposing the data we could tackle this particular confusion by making the state column for AlwaysRun jobs that are not in completed states (success / failed / error) like `Ready (always run)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14425#issuecomment-2030160823:518,error,error,518,https://hail.is,https://github.com/hail-is/hail/pull/14425#issuecomment-2030160823,1,['error'],['error']
Availability,"Posting with recommendation from @konradjk ; ### Hail version:; devel-2c596b7. ### What you did:; Creating a ClinVar matrixtable from a tsv and vep'ing. ```; import hail as hl; import hail.expr.aggregators as agg; from gnomad_hail import *; hl.init(). clinvar_ht= hl.import_table(""gs://gnomad-resources/clinvar/source/clinvar_alleles.single.b37.tsv.gz"", impute=True, missing='NA'); clinvar_ht = clinvar_ht.annotate(locus = hl.locus(clinvar_ht.chrom, clinvar_ht.pos),; alleles = hl.array({clinvar_ht.ref, clinvar_ht.alt})). clinvar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3099:911,error,error,911,https://hail.is,https://github.com/hail-is/hail/issues/3099,1,['error'],['error']
Availability,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8846:241,error,error,241,https://hail.is,https://github.com/hail-is/hail/pull/8846,1,['error'],['error']
Availability,"Previously, accidentally passing an empty string to the list of missing values throws an inscrutable assertion error. This checks in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11078:111,error,error,111,https://hail.is,https://github.com/hail-is/hail/pull/11078,1,['error'],['error']
Availability,"Previously, if you did:. ```; x = hl.bool(True); if x:; ....; ```. You'd get a message like: ""Expressions do not have a static length"", because in Python, truthiness is resolved by first checking if `__bool__` is defined, then checking if `__len__` is nonzero. This PR gives a better message suggesting that the user has in some way tried to coerce an `Expression` into a bool, and only shows the other error if someone does something that specifically needs the length.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10741:403,error,error,403,https://hail.is,https://github.com/hail-is/hail/pull/10741,1,['error'],['error']
Availability,"Previously, when we did `hl.agg.group_by(group_expr, aggregation_expr)`, we were only tracking the indices picked up from the `aggregation_expr`. This led to us throwing bad error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9570:174,error,error,174,https://hail.is,https://github.com/hail-is/hail/pull/9570,1,['error'],['error']
Availability,Print VEP stderr in error message if VEP fails,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4767:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/4767,1,['error'],['error']
Availability,Print a whole traceback on build error page,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6778:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/pull/6778,1,['error'],['error']
Availability,Print lhs on out of bounds error on [] in expr language. Also throwing a FatalException will distinguish bugs on our end vs bugs in user expressions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2921:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/issues/2921,1,['error'],['error']
Availability,"Prior to Kubernetes 1.24, when a `ServiceAccount` called `sa-foo` is created, a corresponding `Secret` containing a token for the service account is created call `sa-foo-token-xxxx`. The `ServiceAccount` resource contains the name of the corresponding secret, and to use the service account in Batch the batch-driver discovers the secret name from the service account resource. In [Kubernetes >=1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes), creating a `ServiceAccount` no longer automatically creates a corresponding token secret. Our cluster auto-upgraded and PRs are failing because batch cannot find the secret field in the service account resource for SAs in test namespaces. From here on out, we need to make those token secrets ourselves. I explicitly added a token secret for the service accounts that need it and changed the batch-driver to handle both old and new service accounts. I tested this in my own project since I already had an instance of Hail Batch / CI up and running. It was running on 1.23 so did not encounter this issue, but I:; 1. Upgraded the cluster to 1.24; 2. Dev deployed and received the same error that we're now seeing in [PRs](https://ci.hail.is/batches/7103889/jobs/14); 3. Manually redeployed batch and CI (from this branch); 4. Dev deployed successfully",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12745:1189,error,error,1189,https://hail.is,https://github.com/hail-is/hail/pull/12745,1,['error'],['error']
Availability,"Problem: Currently in the LD Matrix case I compute V, multiply it through the genotype matrix to get U, then subset columns of U. U is probably bigger than V though, so this could limit the number of eigenvectors that can be used. Probably should just subset columns of V to the desired number of eigenvectors for performance and to increase maximum number of eigenvectors available.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605:373,avail,available,373,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605,1,['avail'],['available']
Availability,"Produces better error messages, and allows for deeply nested; conversion if we want to do that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3158:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/3158,1,['error'],['error']
Availability,Properly print full stacktrace on error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/612:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/pull/612,1,['error'],['error']
Availability,Proposal: subset behavior should default to downcoding behavior if PLs are missing. Downcode should be fixed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004:44,down,downcoding,44,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004,2,"['Down', 'down']","['Downcode', 'downcoding']"
Availability,"Proposed approach: annotate variants with Map[Int, Set[String]], which maps the error code to the set of trios (child string) with error of that code at that variant. We should still annotate samples with counts that are harder to pull out of this, like count per nuclear family and for the individual overall.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/597#issuecomment-240548537:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/issues/597#issuecomment-240548537,2,['error'],['error']
Availability,"Proposed fix for https://github.com/hail-is/hail/issues/9833. Summary: fix run time error when a matrix table row has an empty struct. Example: `info` field below is an empty struct; ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38> ; 'alleles': array<str> ; 'rsid': str ; 'qual': float64 ; 'filters': set<str> ; 'info': struct {} ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------; ```. The function `max` will throw this error; ```; ValueError: max() arg is an empty sequence; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9834:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/pull/9834,2,['error'],['error']
Availability,"Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failovers (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108486"">kubernetes/kubernetes#108486</a>, <a href=""https://github.com/sonasingh46""><code>@​sonasingh46</code></a>)</li>; <li>Omit enum declarations from the static openapi file captured at <a href=""https://git.k8s.io/kubernetes/api/openapi-spec"">https://git.k8s.io/kubernetes/api/openapi-spec</a>. This file is used to generate API clients, and use of enums in those generated clients (rather than strings) can break forward compatibility with additional future values in those fields. See <a href=""https://issue.k8s.io/109177"">https://issue.k8s.io/109177</a> for details. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109178"">kubernetes/kubernetes#109178</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>OpenAPI V3 is turned on by default (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:10212,failover,failovers,10212,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['failover'],['failovers']
Availability,Providing users with more helpful errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10655:34,error,errors,34,https://hail.is,https://github.com/hail-is/hail/pull/10655,1,['error'],['errors']
Availability,Push down `filtervariants interval --keep` queries into DAG construction,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041:5,down,down,5,https://hail.is,https://github.com/hail-is/hail/pull/1041,1,['down'],['down']
Availability,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:109,toler,tolerations,109,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109,3,['toler'],"['tolerate', 'toleration', 'tolerations']"
Availability,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:139,error,error,139,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491,1,['error'],['error']
Availability,"Pushed two more changes:; - Fixed bug in construction of volumes/volumeMounts in pod spec; - Increased resource limits on some build steps. Note, my change from yesterday set requests = limits (since that's what batch2 currently does), and some steps were requesting less memory than they required. If they landed on low-memory nodes, they would have failed. I will continue to increase as needed based on trial and error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7324#issuecomment-543208801:416,error,error,416,https://hail.is,https://github.com/hail-is/hail/pull/7324#issuecomment-543208801,1,['error'],['error']
Availability,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:169,down,downstream,169,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085,1,['down'],['downstream']
Availability,Put an example of downloading logs from google cloud in `Hail on the Cloud`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8005:18,down,downloading,18,https://hail.is,https://github.com/hail-is/hail/issues/8005,1,['down'],['downloading']
Availability,"Putting you down John, but feel free to reassign.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10820#issuecomment-1074478128:12,down,down,12,https://hail.is,https://github.com/hail-is/hail/pull/10820#issuecomment-1074478128,1,['down'],['down']
Availability,"Python API doesn't support lists like it claims. Should be on VariantDatasetFunctions, not VSM[T] (will cause class cast errors)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1511:121,error,errors,121,https://hail.is,https://github.com/hail-is/hail/issues/1511,1,['error'],['errors']
Availability,"Qin He reported that listing a folder containing around 50k files took 1h15. This new code takes ~16 seconds which is about how long it takes `gcloud storage ls`. There are two improvements:. 1. Use `bounded_gather2`. The use of a semaphore in `bounded_gather2`, which is missing from `bounded_gather`, allows it to be used recursively. In particular, suppose we had a semaphore of; 50. The outer `bounded_gather2` might need 20 slots to run its 20 paths in parallel. That leaves 30 slots of parallelism left over for its children. By passing the semaphore down, we let our children optimistically use some of that excess parallelism. 2. If we happen to have the `StatResult` for a particular object, we should never again look it up. In particular, getting the `StatResult` for every file in a directory can be done in O(1) requests. Getting the `StatResult` for each of those files individually (using their full paths) is necessarily O(N). If there was at least one glob and also there are no `suffix_components`, then we can use the `StatResult`s that we learned when checking the glog pattern. The latter point is perhaps a bit more clear with examples:. 1. `gs://foo/bar/baz`. Since there are no globs, we can make exactly one API request to list `gs://foo/bar/baz`. 2. `gs://foo/b*r/baz`. In this case, we must make one API request to list `gs://foo/`. This gives us a list of paths under that prefix. We check each path for conformance to the glob pattern `gs://foo/b*r`. For any path that matches, we must then list `<the matching path>/baz` which may itself be a directory containing files. Overall we make O(1) API requests to do the glob and then O(K) API requests to get the final `StatResult`s, where K is the number of paths matching the glob pattern. 3. `gs://foo/bar/b*z`. In this case, we must make one API request to list `gs://foo/bar/`. In `main`, we then throw away the `StatResult`s we got from that API request! Now we have to make O(K) requests to recover those `StatResult`s ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13253:557,down,down,557,https://hail.is,https://github.com/hail-is/hail/pull/13253,1,['down'],['down']
Availability,"R. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - web_common/pinned-requirements.txt. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091621](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091621) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **663/1000** <br/> **Why?** Proof of Concept exploit, Recently disclosed, Has a fix available, CVSS 5.4 | Improper Input Validation <br/>[SNYK-PYTHON-AIOHTTP-6091622](https://snyk.io/vuln/SNYK-PYTHON-AIOHTTP-6091622) | `aiohttp:` <br> `3.8.6 -> 3.9.0` <br> | No | Proof of Concept . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZTZiMDk2ZC0xYzc5LTQ2ZjctYjY5Ni0yNjFlM2QzYzU",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14036:1286,avail,available,1286,https://hail.is,https://github.com/hail-is/hail/pull/14036,1,['avail'],['available']
Availability,"RANSACTION:; TRANSACTION 1215034156, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 22 lock struct(s), heap size 1136, 13 row lock(s), undo log entries 7; MySQL thread id 962349, OS thread handle 139741180090112, query id 6809294284 10.32.5.50 jigold update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll, token, n_running_cancellable_jobs, running_cancellable_cores_mcpu); VALUES (OLD.batch_id, OLD.inst_coll, rand_token, -1, -OLD.cores_mcpu); ON DUPLICATE KEY UPDATE; n_running_cancellable_jobs = n_running_cancellable_jobs - 1,; running_cancellable_cores_mcpu = running_cancellable_cores_mcpu - OLD.cores_mcpu; *** (2) HOLDS THE LOCK(S):; RECORD LOCKS space id 1578686 page no 3 n bits 72 index PRIMARY of table `jigold`.`instances_free_cores_mcpu` trx id 1215034156 lock_mode X locks rec but not gap; Record lock, heap no 3 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d6a69676f6c642d7374616e646172642d62; asc batch-worker-jigold-standard-b; (total 34 bytes);; 1: len 6; hex 0000486bf32c; asc Hk ,;;; 2: len 7; hex 600001287513cb; asc ` (u ;;; 3: len 4; hex 80002de6; asc - ;;. *** (2) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1578672 page no 3 n bits 272 index PRIMARY of table `jigold`.`batch_inst_coll_cancellable_resources` trx id 1215034156 lock_mode X locks rec but not gap waiting; Record lock, heap no 162 PHYSICAL RECORD: n_fields 10; compact format; info bits 0; 0: len 8; hex 8000000000000001; asc ;;; 1: len 8; hex 7374616e64617264; asc standard;;; 2: len 4; hex 80000020; asc ;;; 3: len 6; hex 0000486bf329; asc Hk );;; 4: len 7; hex 5d0000e7531a42; asc ] S B;;; 5: len 4; hex 7ffffff6; asc ;;; 6: len 8; hex 7ffffffffffff63c; asc <;;; 7: len 4; hex 7fffffff; asc ;;; 8: len 8; hex 7fffffffffffff06; asc ;;; 9: len 4; hex 80000000; asc ;;. *** WE ROLL BACK TRANSACTION (1); ```. We cannot separate these two changes out as the number of deadlock errors will actually increase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11959:3810,error,errors,3810,https://hail.is,https://github.com/hail-is/hail/pull/11959,1,['error'],['errors']
Availability,"RCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2190,ERROR,ERROR,2190,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['ERROR'],['ERROR']
Availability,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:18498,error,error,18498,https://hail.is,https://github.com/hail-is/hail/issues/8944,7,['error'],['error']
Availability,RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:971); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1507); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1495); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1495); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1495); E at is.hail.utils.richUtils.RichRDD$.writeTable$extension1(RichRDD.scala:77); E at is.hail.utils.richUtils.RichRDD$.writeTable$extension0(RichRDD.scala:38); E at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:453); E at is.hail.variant.VariantDatasetFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748); E; E; E; E Hail version: 0.1-74bf1eb; E Error summary: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:15416,Error,Error,15416,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Error'],['Error']
Availability,"ROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/backend/backend.py"", line 108, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/BROAD.MIT.EDU/cvittal/.local/opt/spark/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/utils/java.py"", line 221, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus). Java stack trace:; scala.MatchError: locus<GRCh37> (of class is.hail.expr.types.virtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:2389,Error,Error,2389,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Error'],['Error']
Availability,RR: https://github.com/hail-is/hail/issues/13045; RR: https://github.com/hail-is/hail/issues/13046 ; Support symmetric comparison of structs and struct expressions.; Provide better error messages when attempting to construct literals from expressions with free variables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13226:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/pull/13226,1,['error'],['error']
Availability,RR: https://github.com/hail-is/hail/issues/13261. Grouping asserts of distributed `BlockMatrix` queries via `BatchAssert` lead to repeated timeout failures during tests that used the batch-service backend.; This change removes all `BatchAssert`s from `test_linalg.py`. It uses `pytest.mark.parameterize` to gain parallelism in test execution from the test driver.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13348:147,failure,failures,147,https://hail.is,https://github.com/hail-is/hail/pull/13348,1,['failure'],['failures']
Availability,"RROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.908 [ERROR] [system.err] Ran 7 tests in 83.523s. I noticed that a previous issue #209 from early last year had the exact same issue in a different context where the function `breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;` didn'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:1332,ERROR,ERROR,1332,https://hail.is,https://github.com/hail-is/hail/issues/1419,1,['ERROR'],['ERROR']
Availability,"RSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2ZTAyYTQ3Zi02MzNlLTQ2MDUtYjM1OS1mN2RjOGIyMDk1YTYiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:2880,avail,available,2880,https://hail.is,https://github.com/hail-is/hail/pull/13933,1,['avail'],['available']
Availability,"RSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3NTkyMDJiMS1hZTUwLTQxMjUtYjNhNS1iZjFmOTM3NTU1YWMiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:2880,avail,available,2880,https://hail.is,https://github.com/hail-is/hail/pull/13836,1,['avail'],['available']
Availability,"RSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkMzg0ZDAwZS1iMThiLTQxYmMtODcxZi00Y2YyYTU3YWQ5MzgiLCJldmVudCI6IlBSIHZpZXd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:2812,avail,available,2812,https://hail.is,https://github.com/hail-is/hail/pull/13718,1,['avail'],['available']
Availability,"RSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13873:2880,avail,available,2880,https://hail.is,https://github.com/hail-is/hail/pull/13873,1,['avail'],['available']
Availability,"RSERVER-5862882](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-5862882) | `jupyter-server:` <br> `1.24.0 -> 2.7.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:**",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14026:2921,avail,available,2921,https://hail.is,https://github.com/hail-is/hail/pull/14026,1,['avail'],['available']
Availability,RVD physical key bug: assertion error if physical key field is not part of requested type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4874:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/4874,1,['error'],['error']
Availability,RVD$$anonfun$orderedJoin$1.apply(KeyedOrderedRVD.scala:56); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-eb5d13fe97fc; Error summary: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:13131,Error,Error,13131,https://hail.is,https://github.com/hail-is/hail/issues/4055,2,"['Error', 'error']","['Error', 'error']"
Availability,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:21953,Error,Error,21953,https://hail.is,https://github.com/hail-is/hail/issues/3063,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"Rahul reported this failing on the following gnomad pipeline:. ```; import hail as hl; from gnomad.utils.vep import process_consequences; from gnomad.resources.grch37 import gnomad. gnomad_v2_exomes = gnomad.public_release(""exomes""); ht_exomes = gnomad_v2_exomes.ht(); ht_exomes_proc = process_consequences(ht_exomes); ht_exomes_proc._force_count(); ```. No test case included, but this kind of error will be impossible soon; (when requiredness exists on EmitType, not SType/PType).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10286:395,error,error,395,https://hail.is,https://github.com/hail-is/hail/pull/10286,1,['error'],['error']
Availability,Raise expression error for misuse of filter/explode without aggregation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5110:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/5110,1,['error'],['error']
Availability,"Ran hit this with a typo in --num-workers. We just exited, instead of calling google or returning an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7717:101,error,error,101,https://hail.is,https://github.com/hail-is/hail/pull/7717,1,['error'],['error']
Availability,"Rate limiting is a bit too restrictive methinks. That said, there are a lot of deadlocks when the rate limits kick in. Perhaps worth diving more deeply into the deadlocks at some point. The critical path service backend test is #35. Here are its slowest ones. I think we should probably checkpoint the import VCF, but, regardless, pc_relate just needs to be made faster. I know how to double the speed of PCA. It's in a dead branch of mine. Patrick will incorporate those ideas into his rewrite using the new math. If #35 was the same speed as the next slowest, we'd save 3 minutes. I think we can save ~7 minutes by cutting all these slow tests down so that the distribution of runtimes is more uniform. ```; 256.68s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 178.28s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 102.60s call hail/vds/test_vds.py::test_truncate_reference_blocks; 82.86s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; ```. f1ac37dbeb3625cbf91f1f9df5399f3723843029 (40 minutes) (https://ci.hail.is/batches/7484187):. <img width=""2032"" alt=""Screen Shot 2023-05-25 at 12 01 55"" src=""https://github.com/hail-is/hail/assets/106194/902a0624-46a0-4beb-ae03-6c419350ca41"">; <img width=""542"" alt=""Screen Shot 2023-05-25 at 12 01 28"" src=""https://github.com/hail-is/hail/assets/106194/3cfa366d-5719-428a-9f4f-5bd07caaf6ca"">; <img width=""521"" alt=""Screen Shot 2023-05-25 at 12 01 07"" src=""https://github.com/hail-is/hail/assets/106194/da778828-9f9e-46cc-b574-68b9835e6589"">; <img width=""522"" alt=""Screen Shot 2023-05-25 at 12 01 03"" src=""https://github.com/hail-is/hail/assets/106194/3074a6f9-06d5-487e-941c-995b47177181"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790:287,checkpoint,checkpoint,287,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790,2,"['checkpoint', 'down']","['checkpoint', 'down']"
Availability,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1156:115,error,error,115,https://hail.is,https://github.com/hail-is/hail/issues/1156,1,['error'],['error']
Availability,"Re testing, I think the primary error here isn't that the TypeInfo of i2b disagreed with the jvm, it's that `CodeInt.toB` returns a `Code[Byte]`, using an lir instruction with (wrong) TypeInfo Boolean. That should be much easier to catch with some basic typechecking. (Though agreed the fix should go in first.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328#issuecomment-1032604404:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/11328#issuecomment-1032604404,1,['error'],['error']
Availability,"Re. the questions about the PCA step, I think you'll be beter off modifying `_hwe_normalized_blanczos`. For one thing, this ensures that PC-AiR always returns results in the same form as normal PCA. More importantly, `_hwe_normalized_blanczos` performs the SVD using a ""tall-skinny matrix"" representation, which is just a table of matrices (2d ndarrays). This is more efficient than using block matrices for several reasons that aren't directly relevant here. The result of the SVD is computed as local numpy ndarrays. Given these forms of the data, projecting the related sampled onto the computed PCs should be straightforward and efficient. But once everything is converted to tables and matrixtables, it's much harder and does a lot of redundant work. Let me know if you want to schedule a time to walk through the PCA internals and where you can plug in to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230:740,redundant,redundant,740,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230,1,['redundant'],['redundant']
Availability,"Re: test failure, I'll rebase once the cloud script is in 0.1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2160#issuecomment-325781229:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/2160#issuecomment-325781229,1,['failure'],['failure']
Availability,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:55,avail,available,55,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579,1,['avail'],['available']
Availability,Readable error messages for command-line filtering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/84:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/84,1,['error'],['error']
Availability,Reading Matrix table error: Parsed JSON values do not match with class constructor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/5744,1,['error'],['error']
Availability,Reading a table as a matrixtable produces a bad error:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3089:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/3089,1,['error'],['error']
Availability,Reading datasets with custom reference genomes throws an error if reference wasn't already predefined in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6907:57,error,error,57,https://hail.is,https://github.com/hail-is/hail/issues/6907,1,['error'],['error']
Availability,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10315:686,down,down,686,https://hail.is,https://github.com/hail-is/hail/pull/10315,1,['down'],['down']
Availability,"Realized that the notebook python app should in fact speak https because it is exposed on the pod even though it does not have a service in front of it. For example, prometheus scrapes all visible ports on a pod and it anticipates https. This was triggering the deluge of errors from notebook and deploying this into default seems to have stopped them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10250:272,error,errors,272,https://hail.is,https://github.com/hail-is/hail/pull/10250,1,['error'],['errors']
Availability,"Recent log demonstrating the failure: https://cloudlogging.app.goo.gl/ayiTFRnkLdrSzY2j7. In retrospect this seems kind of obvious. Consider `JVMEntryway`. The first `log` statement occurs on line 98 (the line after we set the filename). I think, in my head, the Appender would be created when we initialized the Logger on line 17. That's apparently incorrect. The Appender is lazily created when some internal buffer fills and the logger flushes that buffer. That internal buffer is most likely to fill on the `log.error` lines because they dump a (large) stack trace to the log. That's why we always see the error there. The fix is simple: we track the currently desired output filename and, if we happen to create an appender *after* someone has called `changeFileInAllAppenders`, we initialize that new appender with the filename. This change ensures that, except for a short period during start up, there is always a valid filename. That short period is just the time between the JVM starting, allocating a `JVMEntryway`, calling `main` and getting to line 97. During that time, we carefully use `System.err.println` (not a logger) if something goes wrong.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13664:29,failure,failure,29,https://hail.is,https://github.com/hail-is/hail/pull/13664,3,"['error', 'failure']","['error', 'failure']"
Availability,"Ref:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: 100m; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /test-gsa-key; name: test-gsa-key; - mountPath: /gsa-key; name: gsa-key; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-1f89; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: test-gsa-key; secret:; defaultMode: 420; optional: false; secretName: test-gsa-key; - name: gsa-key; secret:; defaultMode: 420; secretName: ci-gsa-key; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-07-12T17:17:15Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: gcr.io/hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:4441,toler,tolerationSeconds,4441,https://hail.is,https://github.com/hail-is/hail/issues/6625,1,['toler'],['tolerationSeconds']
Availability,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1687:85,checkpoint,checkpoint,85,https://hail.is,https://github.com/hail-is/hail/pull/1687,8,"['Checkpoint', 'checkpoint', 'error']","['Checkpoint', 'checkpoint', 'errors']"
Availability,"Refactors `Bindings` to return an object encoding the change to the environment (any new bindings, whether the agg/scan env is promoted, etc). This allows the deletion of `SegregatedBindingEnv`. Follow up work will use this to replace the other specializations of `GenericBindingEnv`, and to greatly simplify compiler passes, such as `NormalizeNames` and `PruneDeadFields`, which currently need to redundantly encode the binding structure of every node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14496:398,redundant,redundantly,398,https://hail.is,https://github.com/hail-is/hail/pull/14496,1,['redundant'],['redundantly']
Availability,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:164,error,error,164,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,2,"['avail', 'error']","['available', 'error']"
Availability,Regenie defaults to requesting 1 core. I also fixed some syntax errors in test_batch.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9470:64,error,errors,64,https://hail.is,https://github.com/hail-is/hail/pull/9470,1,['error'],['errors']
Availability,"Related: if you host the pdf within docs, we could keep users within the docs url, while also giving them the use of their native PDF viewer (which will allow download). I think that's neater. Actually, this should be possible with the raw link, not sure why my browser prompts me to immediately download. edit: Ah, this is why: https://webapps.stackexchange.com/questions/48061/can-i-trick-github-into-displaying-the-pdf-in-the-browser-instead-of-downloading. Github places a header to prevent this (content-disposition: attachment). We should just host this file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309:159,down,download,159,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309,3,['down'],"['download', 'downloading']"
Availability,"Remarkable. `hailtop` doesn't even import pandas and yet it triggers this error. I can run pylint on `hail` and it produces many errors, but does not crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907:74,error,error,74,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907,2,['error'],"['error', 'errors']"
Availability,"Remarkably, this is just a warning, not an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1342:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/1342,1,['error'],['error']
Availability,Remove extra checkpoint from blanczos_pca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9434:13,checkpoint,checkpoint,13,https://hail.is,https://github.com/hail-is/hail/pull/9434,1,['checkpoint'],['checkpoint']
Availability,"Remove the `Begin` node, as its behavior can now be represented by the `Let` node. Besides removing redundant nodes, this will also make the new ssa-style text representation simpler. The `Begin` node emmitter performed method splitting, emitting groups of 16 children in seperate methods. This preserves that behavior by doing a similar optimization in the `Let` emitter. This is a significant change in how we split generated code into methods, so we should watch out for how this affects things.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14068:100,redundant,redundant,100,https://hail.is,https://github.com/hail-is/hail/pull/14068,1,['redundant'],['redundant']
Availability,"Removes any occurences of async / sync / async nesting in the code, i.e. a coroutine should not involve somewhere deep down a synchronous call that blocks on the completion of an async task.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677:119,down,down,119,https://hail.is,https://github.com/hail-is/hail/pull/13677,1,['down'],['down']
Availability,"Removes this error from the logs:. ```; Traceback (most recent call last):; --; File ""/usr/local/lib/python3.7/dist-packages/batch/resource_usage.py"", line 220, in periodically_measure; await self.measure(); File ""/usr/local/lib/python3.7/dist-packages/batch/resource_usage.py"", line 187, in measure; memory_usage_bytes = self.memory_usage_bytes(); File ""/usr/local/lib/python3.7/dist-packages/batch/resource_usage.py"", line 126, in memory_usage_bytes; return int(f.read().rstrip()); OSError: [Errno 19] No such device; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12752:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/12752,1,['error'],['error']
Availability,"Removing a plural, for example:. movies = movies.explode('genres', name='genre'). I feel like this is natural and will be pretty common so it should be easy. Yes, I agree it is redundant. The alternative is:. movies = movies.explode('genres').rename({'genres': 'genre'}). which duplicates the column being exploded. If we're happy with the latter, I'm happy to close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981:177,redundant,redundant,177,https://hail.is,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981,1,['redundant'],['redundant']
Availability,"Reopening, every error will produce a stack trace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190590:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190590,1,['error'],['error']
Availability,"Replicable bug:. ```; hail -b 0 importvcf src/test/resources/multipleChromosomes.vcf -n 10 exportvcf -o /tmp/out.vcf.bgz importvcf /tmp/out.vcf.bgz -n 10 count --genotypes. hail: count: caught exception: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4, localhost): htsjdk.samtools.SAMFormatException: Invalid GZIP header; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:72); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:410); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:392); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:127); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.readWithinBlock(BGZFSplitCompressionInputStream.java:81); at org.seqdoop.hadoop_bam.util.BGZFSplitCompressionInputStream.read(BGZFSplitCompressionInputStream.java:48); at java.io.InputStream.read(InputStream.java:101); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130); at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216); at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174); at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/566:229,failure,failure,229,https://hail.is,https://github.com/hail-is/hail/issues/566,3,"['avail', 'failure']","['available', 'failure']"
Availability,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:250,failure,failure,250,https://hail.is,https://github.com/hail-is/hail/issues/2803,2,['failure'],['failure']
Availability,Reported by Theodore Wang [on Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/failure.20running.20recent.20builds.20of.20Hail.200.2E2).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4105:115,failure,failure,115,https://hail.is,https://github.com/hail-is/hail/pull/4105,1,['failure'],['failure']
Availability,Requiredness mismatch throws error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4127:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/4127,1,['error'],['error']
Availability,"Requiredness stuff was assuming that tuples were well-ordered and contiguously indexed, which is wrong because the pruner can prune tuple elements. See https://discuss.hail.is/t/arrayindexoutofboundsexception-error-in-hail-0-2-40/1413/5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8786:209,error,error-in-hail-,209,https://hail.is,https://github.com/hail-is/hail/pull/8786,1,['error'],['error-in-hail-']
Availability,Requiredness suite test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10763#issuecomment-895262768:24,failure,failures,24,https://hail.is,https://github.com/hail-is/hail/pull/10763#issuecomment-895262768,1,['failure'],['failures']
Availability,Requirements.txt has spaces that causes errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12002:40,error,errors,40,https://hail.is,https://github.com/hail-is/hail/pull/12002,1,['error'],['errors']
Availability,Requires network requests -- I had to comment this out to build during a power outage :). Totally open to better ways to do this than converting it to a shell script -- this is not my expertise.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12616:79,outage,outage,79,https://hail.is,https://github.com/hail-is/hail/pull/12616,1,['outage'],['outage']
Availability,"Rereading the tutorial example reminds me that `{j.counts}` gives me the basename of the resource group. So the following would work:. ```python; j = b.new_job(…). j.declare_resource_group(counts={; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand … --output {j.counts}.counts.tsv; bgzip {j.counts}.counts.tsv; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_base_path); ```. This is perhaps a better use of the `ResourceGroup` as reflecting that I want to delocalise this entire group (of two files). This version is worse in that the command string now contains duplicated hardcoded appearances of the `.counts.tsv` extension in the various commands. This is an invitation to typos that will only be detected as file-not-found errors when the job runs, rather than earlier as invalid-key-for-`j.counts` errors at batch submission time. I may convert the code to use this two-file group. However there is still a benefit in the three-file group version as it avoids this repetition of filenames, so it would still be good in general to address the bug mentioned in the last paragraph of the initial comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192:836,error,errors,836,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192,2,['error'],['errors']
Availability,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10756:315,error,error,315,https://hail.is,https://github.com/hail-is/hail/pull/10756,1,['error'],['error']
Availability,Resolves #10843 . Now you'll see:. ```; Hail version: 0.2.74-467a12fcbef9; Error summary: HailException: No file or directory found at gs://hail-datasets-us/annotations/THIS_PATH_DOES_NOT_EXIST; ```. instead of the error about it not being a directory.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10847:75,Error,Error,75,https://hail.is,https://github.com/hail-is/hail/pull/10847,2,"['Error', 'error']","['Error', 'error']"
Availability,"Resolves error where exportVariants exported an Interval as `Interval(14:968765858,22:1565768082)` and import parser expects `14:968765858-22:1565768082`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1538:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/pull/1538,1,['error'],['error']
Availability,"Resource groups are permitted to use dashes, underscores, uppercase letters,; and probably other characters not permitted in storage account names. This; PR cahanges `bootstrap.sh` to:. 1. Ignore invalid characters in the resouce group. 2. Ensure (via randomness) that the generated name is unique. 3. Do not try to create a new storage account if `backend-config.tfvars` exists. I lightly tested this. Here is an example of how it sanitizes a resource group name:. ```; RESOURCE_GROUP=bu__ild-batch-worker-i32mage; possibly_invalid_storage_account_name=""$(cat /dev/urandom | LC_ALL=C tr -dc 0-9 | head -c 4)${RESOURCE_GROUP}""; STORAGE_ACCOUNT_NAME=$(LC_ALL=C tr -dc a-z0-9 <<< ""${possibly_invalid_storage_account_name}"" | head -c 24); echo $STORAGE_ACCOUNT_NAME; 7241buildbatchworkeri32m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11313:736,echo,echo,736,https://hail.is,https://github.com/hail-is/hail/pull/11313,1,['echo'],['echo']
Availability,"Retry ALL requests with temporary failures. We might want to add more exceptions/status codes, but these seem like a good first cut. Also, don't cancel a batch that failed to submit. It shouldn't be cancellable unless it was closed (in case the submit block in question would have succeeded). At some point we should have something that cleans up unclosed batches after a while (e.g. 24hrs).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7215:34,failure,failures,34,https://hail.is,https://github.com/hail-is/hail/pull/7215,1,['failure'],['failures']
Availability,Returning null from the JSON parsing was throwing an error a few lines below when we try to parse a variant. This will throw the real error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995,2,['error'],['error']
Availability,"Revert ""[batch] ensure batches are cancelled on error (#10762)""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10976:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/pull/10976,1,['error'],['error']
Availability,Reverts hail-is/hail#10693 while we inspect errors pulling large numbers of images at once.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10884:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/pull/10884,1,['error'],['errors']
Availability,Reverts hail-is/hail#11750. The code is still causing out of disk errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11968:66,error,errors,66,https://hail.is,https://github.com/hail-is/hail/pull/11968,1,['error'],['errors']
Availability,"Rewrite invocations of `hl.cond()` to `hl.if_else()`, `hl.null()` to `hl.missing()`, and `hl.zip_with_index()` to `hl.enumerate()`. Very minor, but a few of these appear in our test logs (and probably yours as well), which makes for noise when you're tracking down other problems in the logs:. ```; hail/methods/misc.py:437: DeprecationWarning: Call to deprecated function (or staticmethod) cond. (Replaced by hl.if_else) -- Deprecated since version 0.2.59.; hail/vds/methods.py:79: DeprecationWarning: Call to deprecated function (or staticmethod) zip_with_index. (Replaced by hl.enumerate) -- Deprecated since version 0.2.56.; hail/vds/methods.py:75: DeprecationWarning: Call to deprecated function (or staticmethod) null. (Replaced by hl.missing) -- Deprecated since version 0.2.62.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13349:260,down,down,260,https://hail.is,https://github.com/hail-is/hail/pull/13349,1,['down'],['down']
Availability,Right now dependencies to include the in shadowJar are managed by hand and it is incredibly error prone. There has to be a better way to do this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/577:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/577,1,['error'],['error']
Availability,Right now it will return either a double if it finds one or NA if it doesn't. But it kind of seems like if I'm going to impose a restriction like the one above maybe it should throw an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-305807984:185,error,error,185,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-305807984,1,['error'],['error']
Availability,"Right now, IR function registration fails in the following case.; - Multiple Python HailContexts are running on the same cluster; - User on HailContext A calls a function (eg. create histogram) that triggers `hl.experimental.define_function`; * An anonymous function is used to create a new `Set` in `IRFunctionRegistry.irRegistry`; - User on HailContext B calls the same function; * The anonymous function is added to create a `Set` of size `2` in `IRFunctionRegistry.irRegistry`, as anonymous functions are never considered equivalent; * This triggers a fatal error `Multiple functions found that satisfy ...`. By changing the definition of `IRFunctionRegistry.irRegistry` from a `MultiMap(functionName -> Set[argumentTypes, returnType, alwaysInline, anonymousFunction])` to a `Map(functionName -> Map((argumentTypes, returnType, alwaysInline) -> anonymousFunction))` , we ensure function registration is idempotent as we do not compare on the `anonymousFunction`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8123:562,error,error,562,https://hail.is,https://github.com/hail-is/hail/pull/8123,1,['error'],['error']
Availability,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:78,error,errors,78,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690,1,['error'],['errors']
Availability,"Right, we had to manually re-deploy CI for our setup as well. Pinging @daniel-goldstein just in case :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318:62,Ping,Pinging,62,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318,1,['Ping'],['Pinging']
Availability,"Rori encountered some confusing output. _same is private, but this is good both for GVS/AoU and also for us. Things are indented properly, it uses as much terminal width as is available, the names are slightly less confusing, and we see both globals & row failures if both fail. ```python3; Table._same: rows differ:; Row mismatch at key=Struct(locus=Locus(contig=1, position=1, reference_genome=GRCh37), alleles=['A', 'C']):; Left:; [Struct(ancestral_af=0.381520365258488,; af=[0.6482459117152142],; __uid_entries_85=[Struct(GT=Call(alleles=[1, 1], phased=False)), Struct(GT=Call(alleles=[1, 1], phased=False))])]; Right:; [Struct(ancestral_af=0.381520365258488,; af=[0.2510276144176496],; __uid_entries_85=[Struct(GT=Call(alleles=[0, 0], phased=False)), Struct(GT=Call(alleles=[0, 1], phased=False))])]; Row mismatch at key=Struct(locus=Locus(contig=1, position=2, reference_genome=GRCh37), alleles=['A', 'C']):; Left:; [Struct(ancestral_af=0.7058845354840656,; af=[0.5224710728099119],; __uid_entries_85=[Struct(GT=Call(alleles=[0, 0], phased=False)), Struct(GT=Call(alleles=[0, 0], phased=False))])]; Right:; [Struct(ancestral_af=0.7058845354840656,; af=[0.5042641171983404],; __uid_entries_85=[Struct(GT=Call(alleles=[1, 1], phased=False)), Struct(GT=Call(alleles=[1, 1], phased=False))])]; ```. versus. ```python3; Table._same: rows differ:; Row mismatch at key=Struct(_key=Struct(locus=Locus(contig=1, position=1, reference_genome=GRCh37), alleles=['A', 'C'])):; Left:; [Struct(ancestral_af=0.381520365258488,; af=[0.08835032612615329],; __uid_39=[Struct(GT=Call(alleles=[0, 0], phased=False)),; Struct(GT=Call(alleles=[0, 0], phased=False))])]; Right:; [Struct(ancestral_af=0.381520365258488,; af=[0.6631710694002383],; __uid_39=[Struct(GT=Call(alleles=[0, 1], phased=False)),; Struct(GT=Call(alleles=[1, 1], phased=False))])]; Row mismatch at key=Struct(_key=Struct(locus=Locus(contig=1, position=2, reference_genome=GRCh37), alleles=['A', 'C'])):; Left:; [Struct(ancestral_af=0.7058845354840",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13825:176,avail,available,176,https://hail.is,https://github.com/hail-is/hail/pull/13825,2,"['avail', 'failure']","['available', 'failures']"
Availability,"Ruff linting the tests is not too hard, so I did it. I also looked at pyright/pylint, but the Hail package has way too many errors to make that feasible on a Friday night.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14159:124,error,errors,124,https://hail.is,https://github.com/hail-is/hail/pull/14159,1,['error'],['errors']
Availability,"Running a function like:; ```; def annotate_tx_expression_data(ht, tx_ht, location):; key = ht.key if isinstance(ht, hl.Table) else ht.row_key; return hl.find(lambda csq: (csq.ensg == location.gene_id) &; (csq.csq == location.most_severe_consequence),; tx_ht[key].tx_annotation); mt = mt.annotate_rows(expressed=annotate_tx_expression_data(mt, tx_ht, mt.lof_csqs).mean_expression > 0.1); mt.describe(); mt.group_rows_by(*list(annotation_expr.keys())).aggregate_rows(; classic_caf=hl.agg.sum(mt.freq[0].AF),; max_af=hl.agg.max(mt.freq[0].AF),; classic_caf_array=hl.agg.array_sum(mt.freq.map(lambda x: x.AF)); ).aggregate_entries(; num_homs=hl.agg.count_where(mt.GT.is_hom_var()),; num_hets=hl.agg.count_where(mt.GT.is_het()),; defined_sites=hl.agg.count_where(hl.is_defined(mt.GT)); ).result(); ```; errors out with `Error summary: AssertionError: assertion failed: ensg not in struct{mean_expression: float64}`. the describe shows that it's doing the right thing (`expressed` is a `bool`), but i'm guessing that since ensg is not strictly referred to except in a lambda, that it's getting pruned out?. Full log posted on zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4754:799,error,errors,799,https://hail.is,https://github.com/hail-is/hail/issues/4754,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/issues/5744,1,['error'],['error']
Availability,"Running:; ```; def generate_downsamplings_cumulative(mt: hl.MatrixTable) -> Tuple[hl.MatrixTable, List[int]]:; pop_data = [x[0] for x in get_sample_data(mt, [mt.meta.pop])]; pops = Counter(pop_data); downsamplings = DOWNSAMPLINGS + list(pops.values()); downsamplings = sorted([x for x in downsamplings if x <= sum(pops.values())]); kt = mt.cols(); kt = kt.annotate(r=hl.rand_unif(0, 1)); kt = kt.order_by(kt.r).add_index('global_idx'). for i, pop in enumerate(pops):; pop_kt = kt.filter(kt.meta.pop == pop).add_index('pop_idx'); if not i:; global_kt = pop_kt; else:; global_kt = global_kt.union(pop_kt). return mt.annotate_cols(downsampling=global_kt[mt.s]), downsamplings. ```; Getting this. Guessing it's something with the ordering?; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-8-0255aa65130d> in <module>(); 48 ; 49 if calculate_downsampling:; ---> 50 mt, downsamplings = generate_downsamplings_cumulative(mt); 51 print(f'Got {len(downsamplings)} downsamplings: {downsamplings}'); 52 cut_dict = {'pop': hl.agg.counter(hl.agg.filter(hl.is_defined(mt.meta.pop), mt.meta.pop)),. <ipython-input-8-0255aa65130d> in generate_downsamplings_cumulative(mt); 18 global_kt = pop_kt; 19 else:; ---> 20 global_kt = global_kt.union(pop_kt); 21 ; 22 return mt.annotate_cols(downsampling=global_kt[mt.s]), downsamplings. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in union(self, *tables); 1718 f"" Expected: {self.row.dtype}\n""; 1719 f"" Table {i}: {ht.row.dtype}""); -> 1720 elif list(ht.key) != list(self.key):; 1721 raise TypeError(f""'union': table {i} has a different key.""; 1722 f"" Expected: {list(self.key)}\n"". TypeError: 'NoneType' object is not iterable; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4080:200,down,downsamplings,200,https://hail.is,https://github.com/hail-is/hail/issues/4080,10,"['DOWN', 'down']","['DOWNSAMPLINGS', 'downsampling', 'downsamplings']"
Availability,"Ryan Koesterer is reporting a related issue: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/export.20table.20failure. The former causes inexplicable container failures, the latter succeeds.; ```; mt = mt.annotate_cols(phenoFemale =; hl.cond(hl.literal({""female"",""Female"",""f"",""F"",""2""}).contains(mt.pheno[args.sex_col]), True, False)); ```; ```; mt = mt.annotate_cols(; phenoFemale = hl.cond(~ hl.is_missing(mt.pheno[args.sex_col]),; (mt.pheno[args.sex_col] == 'female') |; (mt.pheno[args.sex_col] == 'Female') |; (mt.pheno[args.sex_col] == 'f') |; (mt.pheno[args.sex_col] == 'F') |; (mt.pheno[args.sex_col] == '2'), False)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522#issuecomment-429349677:190,failure,failures,190,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429349677,1,['failure'],['failures']
Availability,"SB is a reserved INFO field in VCFs, and so downstream tools may; overwrite the header. SB_TABLE is more what we want and the header will; be correct for the datatype (array of 4 ints)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5400:44,down,downstream,44,https://hail.is,https://github.com/hail-is/hail/pull/5400,1,['down'],['downstream']
Availability,"SE (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12330"">#12330</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/8650f5c2eedc26f11b6f5c35cf0c0d752aaf51fb""><code>8650f5c</code></a> stubgen: fix non default keyword-only argument positioning (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12303"">#12303</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/226661f62f365102f5fd913b39b32ed3f12e208b""><code>226661f</code></a> Exhaustiveness checking for match statements (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12267"">#12267</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/fce1b548be74f7c65f8e3645f2a2b46aeff0c5a8""><code>fce1b54</code></a> CI: Do not run mypy_primer on stubtest/stubgen PRs (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12295"">#12295</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/c7a81620bef7585cca6905861bb7ef34ec12da2f""><code>c7a8162</code></a> stubtest: ignore more dunder positional-only errors (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12294"">#12294</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/python/mypy/compare/v0.780...v0.941"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=mypy&package-manager=pip&previous-version=0.780&new-version=0.941)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11573:2194,error,errors,2194,https://hail.is,https://github.com/hail-is/hail/pull/11573,3,['error'],['errors']
Availability,"SERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `39.0.1 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:**",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14109:4246,avail,available,4246,https://hail.is,https://github.com/hail-is/hail/pull/14109,2,['avail'],['available']
Availability,"SERVER-6099119](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERSERVER-6099119) | `jupyter-server:` <br> `1.24.0 -> 2.11.2` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **509/1000** <br/> **Why?** Has a fix available, CVSS 5.9 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-SETUPTOOLS-3180412](https://snyk.io/vuln/SNYK-PYTHON-SETUPTOOLS-3180412) | `setuptools:` <br> `40.5.0 -> 65.5.1` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **384/1000** <br/> **Why?** Has a fix available, CVSS 3.4 | Open Redirect <br/>[SNYK-PYTHON-TORNADO-5537286](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5537286) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **494/1000** <br/> **Why?** Has a fix available, CVSS 5.6 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-5840803](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-5840803) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **539/1000** <br/> **Why?** Has a fix available, CVSS 6.5 | HTTP Request Smuggling <br/>[SNYK-PYTHON-TORNADO-6041512](https://snyk.io/vuln/SNYK-PYTHON-TORNADO-6041512) | `tornado:` <br> `6.2 -> 6.3.3` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:**",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14365:3580,avail,available,3580,https://hail.is,https://github.com/hail-is/hail/pull/14365,1,['avail'],['available']
Availability,"SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-MISTUNE-2940625](https://snyk.io/vuln/SNYK-PYTHON-MISTUNE-2940625) | `mistune:` <br> `0.8.4 -> 2.0.3` <br> | No | No Known Exploit ; ![high severity](https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13717:2200,avail,available,2200,https://hail.is,https://github.com/hail-is/hail/pull/13717,2,['avail'],['available']
Availability,"SNYK-PYTHON-CERTIFI-3164749) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![critical severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/c.png ""critical severity"") | **704/1000** <br/> **Why?** Has a fix available, CVSS 9.8 | Improper Following of a Certificate&#x27;s Chain of Trust <br/>[SNYK-PYTHON-CERTIFI-5805047](https://snyk.io/vuln/SNYK-PYTHON-CERTIFI-5805047) | `certifi:` <br> `2021.10.8 -> 2023.7.22` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **624/1000** <br/> **Why?** Has a fix available, CVSS 8.2 | Arbitrary Code Execution <br/>[SNYK-PYTHON-IPYTHON-2348630](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-2348630) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **531/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 4.2 | Remote Code Execution (RCE) <br/>[SNYK-PYTHON-IPYTHON-3318382](https://snyk.io/vuln/SNYK-PYTHON-IPYTHON-3318382) | `ipython:` <br> `5.10.0 -> 8.10.0` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **556/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.4 | Cross-site Scripting (XSS) <br/>[SNYK-PYTHON-JINJA2-6150717](https://snyk.io/vuln/SNYK-PYTHON-JINJA2-6150717) | `jinja2:` <br> `2.11.3 -> 3.1.3` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **604/1000** <br/> **Why?** Has a fix available, CVSS 7.8 | Improper Privilege Management <br/>[SNYK-PYTHON-JUPYTERCORE-3063766](https://snyk.io/vuln/SNYK-PYTHON-JUPYTERCORE-3063766) | `jupyter-core:` <br> `4.6.3 -> 4.11.2` <br> | No | No Known Exploit ; ![high severity](ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:2518,avail,available,2518,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['avail'],['available']
Availability,"SQL migrations are not permitted to be modified. Unfortunately, our tests; previously did not verify this at all. Indeed, a PR merged which modified a SQL; file. This PR caused main to fail a deploy. This change verifies that no SQL migration is mutated in the source SHA relative; to the target SHA. One can also use it locally by running `make; check-services` from the root. Unfortunately, it does not work properly when run; on the main branch because there is no obvious point of comparison. I considered comparing against the previous commit, but that might cause; failures if we have to manually fix something in batch. As such, I prefer a; non-deploy only test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9544:571,failure,failures,571,https://hail.is,https://github.com/hail-is/hail/pull/9544,1,['failure'],['failures']
Availability,"SS	AC=2,4,6,1;AF=1.23e-03,5.550e-05,4.44e-05,2.00e-04;AN=265;AS_AltDP=10,0,3,10;AS_BaseQRankSum=0.000,.,0.100,0.500;AS_FS=7.777,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7348,Error,ErrorHandling,7348,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Error'],['ErrorHandling']
Availability,SVD tests should be robust to sign ambiguity,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9727:20,robust,robust,20,https://hail.is,https://github.com/hail-is/hail/pull/9727,1,['robust'],['robust']
Availability,"S_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53938) with ID 21; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21, partition 39, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21, partition 9, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21, partition 19, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q03.scc.bu.edu:36955 with 21.2 GB RAM, BlockManagerId(12, scc-q03.scc.bu.edu, 36955, None); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:169355,ERROR,ERROR,169355,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,Same failure on #7742,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7742#issuecomment-566308517:5,failure,failure,5,https://hail.is,https://github.com/hail-is/hail/pull/7742#issuecomment-566308517,1,['failure'],['failure']
Availability,Saw this in https://ci.azure.hail.is/batches/38760/jobs/99. I think the log of a container is actually never None any more. It can; be empty if bash has started but the echo command has not yet run.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11552:169,echo,echo,169,https://hail.is,https://github.com/hail-is/hail/pull/11552,1,['echo'],['echo']
Availability,"Screenshots should give the main overview of the changes; Questions for reviewers. Technical:; - [X] Are there any CSS conventions within Hail? I assume I need to migrate the ad-hoc ""style"" tags into CSS?; - [X] There still seems to be a bunch of unused space after truncated batch names. I'm not sure why. UX:; - [x] I've moved the status indicator to the front of the line. Is that ok?; - to help with layout within the batch-name box; - to put it in a reliable place (ie not moving around based on how long the name is); - [x] I'm not really sure I like the change to Pending. Curious for others' thoughts. #### Example: Batches page; (layout and columns). ##### Before:; <img width=""1735"" alt=""image"" src=""https://github.com/user-attachments/assets/c2966f9a-1802-479f-8fb4-3882a4552fad"">. ##### After:; <img width=""1748"" alt=""image"" src=""https://github.com/user-attachments/assets/4a6a5c5a-23a5-42a4-bc8e-6624f83880fa"">. #### Example: Batch Details page; (Renaming confusing 'Pending' field). ##### Before:; <img width=""1044"" alt=""image"" src=""https://github.com/user-attachments/assets/ebb3eb52-69d7-44ba-a2c5-f0f219a0b5bb"">. ##### After:; <img width=""1059"" alt=""image"" src=""https://github.com/user-attachments/assets/6fa01eae-567d-49e5-a59e-768bf936a1b1"">. Fixes #14628. Adds and shuffles content on the new Batches table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14640:455,reliab,reliable,455,https://hail.is,https://github.com/hail-is/hail/pull/14640,1,['reliab'],['reliable']
Availability,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:789,avail,available,789,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['avail'],['available']
Availability,"Search `.zipWithIndex()` and you'll see five places we used Spark's zipWithIndex, which triggers a job. When partitionCounts are available I think we could avoid that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408:129,avail,available,129,https://hail.is,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408,1,['avail'],['available']
Availability,See attached log. Error not clear:. `[Stage 0:==========> (596 + 168) / 2836]hail: write: caught exception: Job aborted.`. [hail.log.txt](https://github.com/broadinstitute/hail/files/269500/hail.log.txt),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/391:18,Error,Error,18,https://hail.is,https://github.com/hail-is/hail/issues/391,1,['Error'],['Error']
Availability,See discuss post: https://discuss.hail.is/t/redirect-or-find-vep-or-other-error-output-from-a-hail-pipeline/1308/9?u=danking. It looks like Hail isn’t capturing all the VEP output. Can someone look into this? Probably the way we’re executing external commands needs to also capture stderr and print it. Assigning Tim for delegation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146:74,error,error-output-from-a-hail-pipeline,74,https://hail.is,https://github.com/hail-is/hail/issues/8146,1,['error'],['error-output-from-a-hail-pipeline']
Availability,"See discussion on Zulip https://hail.zulipchat.com/#narrow/stream/127527-team/topic/batch. Our worst case monthly cost moves from 40 USD to 4000 USD. However, PVCs seem to be rather reliably cleaned up now, so I am not overly concerned about this. We also have monitoring on PVC storage capacity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6366:182,reliab,reliably,182,https://hail.is,https://github.com/hail-is/hail/pull/6366,1,['reliab'],['reliably']
Availability,"See each message below. ---. [[query/vds] Fix local_to_global with missing fill](https://github.com/hail-is/hail/pull/13325/commits/7d84189ca1a1b9460f4e0c96821cd43b8b0068fa) ; ; There was a logic error in constructFromIndicesUnsafe, if a missing; value was pushed, pushing a present value with the same index would not; clear the missing bit. ---. [[batch/test] Wait for job to be running in list_jobs_v2 test](https://github.com/hail-is/hail/pull/13325/commits/724da249255c06ea4ed1816704e4de51bd8f9b89). ---. [[qob] halve the number of active tests](https://github.com/hail-is/hail/pull/13325/commits/c2638702325526b29bebd416fceeedea52d42245). ---. [[batch] Turn off oms_agent in test and dev](https://github.com/hail-is/hail/pull/13325/commits/bbd65e4f66d41ef69c130091b0506087975c4851). ---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13325:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/13325,1,['error'],['error']
Availability,"See for example https://cloudlogging.app.goo.gl/ziaRD9HKxxca8Nd3A. in which ~15 MJCs have to retry because of `ServerDisconnectedError` or `TimeoutError`. With this PR, I think we would have seen just the three ""two errors observed"" warning messages. Here's a possible extension to this PR that fuses the thinking of both PRs (this one and #12505): use the total delay instead of `errors = 2`. We retry really quickly, so two errors could occur in ~500ms which really isn't enough time for batch driver to fix itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106:216,error,errors,216,https://hail.is,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106,3,['error'],['errors']
Availability,"See here:; https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20when.20writing.20HailTable.2E. ```; logger.info('Read MatrixTable.'); mt = hl.read_matrix_table('path'). logger.info('Calculate median x in each group2.'); mt = mt.group_cols_by('group1', 'group2').aggregate(x = hl.median(hl.agg.collect(mt.x))). logger.info('Calculate mean x in group1.'); mt = mt.group_cols_by('group1').aggregate(x_stats = hl.agg.stats(mt.x)). logger.info('Calculate relative x.'); mt = mt.annotate_entries(x = mt.x_stats.mean); mt = mt.annotate_rows(row_sum = hl.agg.sum(mt.x)); mt = mt.select_entries(rx = mt.x/mt.row_sum). #----; logger.info('Export as HailTable.'); ht = mt.entries(); ht = ht.drop('row_sum', 'gene_id'). ht.export('path'); ```. Can write the MatrixTable before entries, but not the HailTable after",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6295:87,Error,Error,87,https://hail.is,https://github.com/hail-is/hail/issues/6295,1,['Error'],['Error']
Availability,See https://ci.hail.is/batches/532603/jobs/112. We encounter a `is.hail.relocated.com.google.cloud.storage.StorageException` which is caused by a; `com.google.api.client.http.HttpResponseException`. The latter exception is not currently; considered a transient error. This PR changes isTransientError to recognize `HttpResponseException`; as a transient error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11402:261,error,error,261,https://hail.is,https://github.com/hail-is/hail/pull/11402,2,['error'],['error']
Availability,"See https://github.com/broadinstitute/gnomad-browser/issues/914. In [the line in question](https://github.com/broadinstitute/gnomad-browser/blob/b497106d97773affd81b48eadfa5586259e011e5/data-pipeline/src/data_pipeline/data_types/gtex_tissue_expression.py#L14), we attempt to export a `Table` with ~13,000 columns, and get the following error: `is.hail.relocated.org.objectweb.asm.MethodTooLargeException: Method too large: __C19580collect_distributed_array.__m19633split_InsertFields ()V` (see above-referenced issue for full stacktrace). Hail version was 0.2.96-39909e0a396f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11972:336,error,error,336,https://hail.is,https://github.com/hail-is/hail/issues/11972,1,['error'],['error']
Availability,"See https://github.com/erdewit/nest_asyncio/issues/11. Original error was this:. ```; _________________ ServiceTests.test_single_task_resource_group _________________. self = <test.hailtop.batch.test_batch.ServiceTests testMethod=test_single_task_resource_group>. def test_single_task_resource_group(self):; b = self.batch(); j = b.new_job(); j.declare_resource_group(output={'foo': '{root}.foo'}); j.command(f'echo ""hello"" > {j.output.foo}'); > res = b.run(). ../test/hailtop/batch/test_batch.py:484: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; batch/batch.py:565: in run; run_result = self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs) # pylint: disable=assignment-from-no-return; batch/backend.py:475: in _run; self._async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait, open, disable_progress_bar, callback, token, **backend_kwargs)); utils/utils.py:127: in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); /usr/local/lib/python3.7/dist-packages/nest_asyncio.py:63: in run_until_complete; return self._run_until_complete_orig(future); /usr/lib/python3.7/asyncio/base_events.py:574: in run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:64,error,error,64,https://hail.is,https://github.com/hail-is/hail/pull/10705,2,"['echo', 'error']","['echo', 'error']"
Availability,See https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20down/near/297868370 for details. I'll file a ticket with Azure to see if this is a breaking change that they're fine making and if so if there is a more robust way for us to filter to disk costs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12169:222,robust,robust,222,https://hail.is,https://github.com/hail-is/hail/pull/12169,1,['robust'],['robust']
Availability,"See the discuss post here: https://discuss.hail.is/t/error-indexing-bgen-files/833. @catoverdrive At check-in, we discussed this error that occurs in `PackCodecSpec.buildEncoder` and thought you'd be the best person to fix this bug since you added this feature. If not, I'll try and come up with a fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5144:53,error,error-indexing-bgen-files,53,https://hail.is,https://github.com/hail-is/hail/issues/5144,2,['error'],"['error', 'error-indexing-bgen-files']"
Availability,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649,1,['error'],['errors']
Availability,"Seeing sporadic failures, random each run of ci, looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9675#issuecomment-730488453:16,failure,failures,16,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-730488453,1,['failure'],['failures']
Availability,Seems like you have one syntax error still.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11444#issuecomment-1067201429:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/pull/11444#issuecomment-1067201429,1,['error'],['error']
Availability,Seems the `testImplementation` doesn't configure the classpath for tests correctly - javatests are currently failing with `Error: Could not find or load main class org.testng.TestNG`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249:123,Error,Error,123,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249,1,['Error'],['Error']
Availability,"Seems to have been introduced sometime between `0.2-29fbaeaf265e` (works) and `0.2-60a06028e9db` (see error below). The code is pretty involved, so whoever gets assigned, if you need it, let me know and I can send.; ```; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.MatrixNativeReader.apply(MatrixIR.scala:242); 	at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixAnnotateColsTable.execute(MatrixIR.scala:1725); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); 	at is.hail.expr.ir.CastTableToMatrix.execute(MatrixIR.scala:2283); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1413); 	at is.hail.expr.ir.CastMatrixToTable.execute(TableIR.scala:1167); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:656); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:514); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4746:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/issues/4746,1,['error'],['error']
Availability,"Self-explanatory :). -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3047:177,error,error,177,https://hail.is,https://github.com/hail-is/hail/issues/3047,1,['error'],['error']
Availability,Set `contains` - segmentation fault,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522:30,fault,fault,30,https://hail.is,https://github.com/hail-is/hail/issues/4522,1,['fault'],['fault']
Availability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 14:46:40 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 14:46:41 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:32158,AVAIL,AVAILABLE,32158,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:04:36 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 15:04:36 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14643,AVAIL,AVAILABLE,14643,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['AVAIL'],['AVAILABLE']
Availability,"Setup failure is distinguished from creation failure because we at least have a pod status. Jobs now have three ways to finish:. - creation failure; never scheduled due to PVC failure or image pull back off; - setup failure; setup container failed, we probably won't get the logs; - normal termination; keep alive container survived, we'll get the logs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6961:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/pull/6961,6,"['alive', 'failure']","['alive', 'failure']"
Availability,"Shoot, this was branched from the resource-link branch, which hasn't merged yet due to batch failures. Will reissue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7289#issuecomment-541355452:93,failure,failures,93,https://hail.is,https://github.com/hail-is/hail/pull/7289#issuecomment-541355452,1,['failure'],['failures']
Availability,"Shortly after this merged, we encountered issues where some workers shut down soon after starting up, but while they were running jobs. I think the `startup_tasks` idled out even though we activated, which caused us to return without shutting down the site. That meant we shut down while we were processing jobs and those workers spewed a bunch of errors before they finally killed themselves off. I think it's wrong here to tie in the timing out trying to activate with other startup tasks that could potentially take a longer amount of time but will certainly finish and we should let them finish. We also don't need all of the network namespaces created in order to start accepting jobs. For context, I think it takes just about MAX_IDLE_TIMEOUT_SECONDS to create all the network namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11728:73,down,down,73,https://hail.is,https://github.com/hail-is/hail/pull/11728,4,"['down', 'error']","['down', 'errors']"
Availability,Should fix this error:. ```; azure.core.exceptions.HttpResponseError: The specified block list is invalid.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11110:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/11110,1,['error'],['error']
Availability,Should throw an error on gt>2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2634:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/issues/2634,1,['error'],['error']
Availability,"Should unify the two schemas, setting fields to missing as needed. If a similarly-named field in the two tables can be coerced (int32 / int64), do that. Throw an error if coercion is not posible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5854:162,error,error,162,https://hail.is,https://github.com/hail-is/hail/issues/5854,1,['error'],['error']
Availability,Should we not just crash when someone does this? It feels like this ought to be a python enforced type error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11168#issuecomment-996141311:103,error,error,103,https://hail.is,https://github.com/hail-is/hail/pull/11168#issuecomment-996141311,1,['error'],['error']
Availability,Significantly improve error messages for expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2659:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/2659,1,['error'],['error']
Availability,"Simple replication:; ```. In [6]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834356993:176,echo,echo,176,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834356993,1,['echo'],['echo']
Availability,"Since Amanda is out, fixed test failure, replacing: https://github.com/hail-is/hail/pull/3817. @catoverdrive wrote it and I already reviewed it. Last commit is my fix, mostly unrelated to this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3832:32,failure,failure,32,https://hail.is,https://github.com/hail-is/hail/pull/3832,1,['failure'],['failure']
Availability,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:136,error,errors,136,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446,1,['error'],['errors']
Availability,"Since `ServiceBackend` only uses cloud storage, the user should receive an error when trying to pass in a local (`file://` or unprefixed) path, which is implemented in this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12186:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/pull/12186,1,['error'],['error']
Availability,"Since netcdf broke my R installation, I upgraded R. Now to revert to 3.3.1, I'm trying to install from the downloadable tarball and running into a bunch of errors. Is this worth it? Why don't we just test against a static results file?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235:107,down,downloadable,107,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235,2,"['down', 'error']","['downloadable', 'errors']"
Availability,Slightly improves error message for Expression.__iter__,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4088:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/4088,1,['error'],['error']
Availability,"Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **496/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.2 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-6002459](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-6002459) | `urllib3:` <br> `1.26.17 -> 1.26.18` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0MmRjMDIwMC02MDI1LTQ1M2QtYWUxNC00NDRlZjM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13854:1129,avail,available,1129,https://hail.is,https://github.com/hail-is/hail/pull/13854,1,['avail'],['available']
Availability,"Snyk has created this PR to fix one or more vulnerable packages in the `pip` dependencies of this project.</h3>. #### Changes included in this PR. - Changes to the following files to upgrade the vulnerable dependencies to a fixed version:; - hail/python/hailtop/pinned-requirements.txt. <details>; <summary>⚠️ <b>Warning</b></summary>. ```; msal-extensions 1.0.0 requires portalocker, which is not installed.; aiosignal 1.3.1 requires frozenlist, which is not installed.; aiohttp 3.8.5 requires frozenlist, which is not installed. ```; </details>. #### Vulnerabilities that will be fixed. ##### By pinning:; Severity | Priority Score (*) | Issue | Upgrade | Breaking Change | Exploit Maturity; :-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **581/1000** <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.9 | Information Exposure Through Sent Data <br/>[SNYK-PYTHON-URLLIB3-5926907](https://snyk.io/vuln/SNYK-PYTHON-URLLIB3-5926907) | `urllib3:` <br> `1.26.16 -> 1.26.17` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIxOGU4YzI4Yi1kYWQ0LTQ5ZDUtOTExNi04NjFkYTd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13773:1129,avail,available,1129,https://hail.is,https://github.com/hail-is/hail/pull/13773,1,['avail'],['available']
Availability,"So I just had the following situation: saved an Excel doc from a collaborator as CSV to the import it into hail. Running `hl.import_table` didn't return any error... but somehow the header was both correctly assigned as column names but also added as the first line. After some poking around, it turns out that the file encoding was UTF8 with BOM and that it somehow tripped `hl.import_table`. Same file works after re-encoding it as plain UTF8. . Please include the full Hail version and as much detail as possible.; version 0.2.34-914bd8a10ca2; -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8395:157,error,error,157,https://hail.is,https://github.com/hail-is/hail/issues/8395,1,['error'],['error']
Availability,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674,1,['down'],['down']
Availability,"So I'm moderately unhappy with this for various reasons, and almost didn't PR it. We don't generally respect IR identity (e.g. optimize copies everything), so I'm not sure how useful memoizing partition counts on the IR actually is. We could try to carry this information forward inside copy. In that cases that (MatrixIR child) has the same partition counts as child, we can actually push the computed partition counts down into child. But all of this is getting pretty complicated ... just to optimize count? I wonder if it is worth it. Yes, Count and PartitionCounts have different requirements. I think we either need both, or we need to recognize in (Sum (PartitionCounts child)) that child doesn't need to preserve order. (An analysis pass that determines which IR need to preserve order in general will be better than determining this syntactically from context with a rule like (Count (Unkey child)). How do you optimize (Count (Filter (Unkey ...))?). Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809:420,down,down,420,https://hail.is,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809,1,['down'],['down']
Availability,"So I'm of two minds here, and I don't quite know which to choose. Imagine we make an internal (service to service) request. It fails with a transient error like a timeout. What do we do?. Option 1. Retry. Option 2. Return a transient error 503 service unavailable to our client, and let them to decide what to do. I've implemented both options here: request_{retry, raise}_transient_errors. Which should I use in, for example, the auth decorators which hit the auth/userinfo endpoint?. I've chosen option 1 after bouncing back and forth a few times. I feel like retrying will give a better experience in the common case (a real transient error) and both will recover eventually in the case of a real outage. It appears that browsers don't retry 503 even with Retry-After header set. I'd want it to retry immediately or after a very short delay (1s). In the end, this is what convinced me we should retry. Currently CI uses option 1 when calling batch because it is hardcoded into the batch client. The signature of these functions match aiohttp.ClientSession.request. @danking I'm compelled by your concern that we have a potentially infinite loop of failures nobody will be notified about. I will follow up with another PR to add some logging to the request_retry function. Finally, I'm not quite sure why we're getting so many transient errors. I suspect some of it is gaps in k8s service handoffs, but I'm not sure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7284:150,error,error,150,https://hail.is,https://github.com/hail-is/hail/pull/7284,7,"['error', 'failure', 'outage', 'recover']","['error', 'errors', 'failures', 'outage', 'recover']"
Availability,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:3,error,error,3,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584,3,['error'],['error']
Availability,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:710,error,error,710,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982,1,['error'],['error']
Availability,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828,1,['error'],['error']
Availability,"So there appear to be to distinct issues:. - failure to push the initial data to a repository due to a 404 https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. - 404s due to someone else already bound to port 5000 (maybe `batch`?) https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html; ```; Traceback (most recent call last):; File ""ci/ci.py"", line 372, in <module>; app.run(host='0.0.0.0', threaded=False); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/flask/app.py"", line 943, in run; run_simple(host, port, self, **options); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 814, in run_simple; inner(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 774, in inner; fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 666, in make_server; passthrough_errors, ssl_context, fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 577, in __init__; self.address_family), handler); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 449, in __init__; self.server_bind(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/http/server.py"", line 137, in server_bind; socketserver.TCPServer.server_bind(self); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 463, in server_bind; self.socket.bind(self.server_address); OSError: [Errno 98] Address already in use; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941:45,failure,failure,45,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941,1,['failure'],['failure']
Availability,"So there's a double regex substitution now in this version. I couldn't figure out how to avoid this without having nice error checking at the exact line there's a problem. For example, `j.command(f'{b}')` right now immediately errors with a nice error message. But if the error checking doesn't come until the massive parallel `_compile` in `Backend.run`, then it will be harder to tell where the error is. I thought about having a `debug_mode` which is on by default that does the double check while the `debug_mode` being off is more efficient.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694,5,['error'],"['error', 'errors']"
Availability,"So this evening I noticed that one of my vds files (written four days ago) makes Hail crash when I try to read it. When I do:; hail read -i /user/satterst/DBS_v3/DBS_v3_split_vep.vds. I get the following error message: ; hail: read: caught exception: java.lang.IllegalArgumentException: requirement failed; and then a big stack trace, captured here:. /mnt/lustre/satterst/hail.crash.log. I'd be interested to know what's up. One line in the log says:; 2016-08-27 20:16:41 WARN AbstractLifeCycle:204 - FAILED SelectChannelConnector@0.0.0.0:54054: java.net.BindException: Address already in use; but I don't know what this means or if it's relevant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/700:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/issues/700,1,['error'],['error']
Availability,"So this is partially my fault. I'm using parse_known_args, so we don't know where unknown args are placed, and therefore can't give the right usage (even if I can intervene on the usage being printed by argparse). I really only want to allow unknown args in the relevant subcommands, but don't know how to write that argument given the strange behavior I'm seeing with `nargs='*'` and `nargs=argparse.REMAINDER`. . Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9836#issuecomment-747653966:24,fault,fault,24,https://hail.is,https://github.com/hail-is/hail/pull/9836#issuecomment-747653966,1,['fault'],['fault']
Availability,"So this works (`cuts` is an array of 50 entries, so this is 2500 `counters`):; ```; joint_sfs = ht.aggregate(hl.struct(; joint_freq_bin_counters=[[hl.agg.counter((ht.freq_bins[i], ht.freq_bins[j], ht.consequence)); for i, _ in enumerate(cuts)] for j, _ in enumerate(cuts)])); ```; but this:; ```; counters = ht.aggregate(hl.struct(; enrichment_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments[i]); for i, _ in enumerate(cuts)],; enrichment_pseudo_counters=[hl.agg.array_agg(lambda x: hl.agg.counter(x), ht.enrichments_pseudo[i]); for i, _ in enumerate(cuts)])); ```; immediately results in OOMs. Each of `enrichments[i]` is also 50 elements, so this should be the same amount of work (well double since I have 2). But a few tasks finish but they generally struggle and eventually die with:; ```; [Stage 3:> (4 + 13) / 9997]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f94d8700000, 5428477952, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 5428477952 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/04eb6abfd9594f99ad2fac1a8e4cd0d1/hs_err_pid25110.log; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6074:946,error,error,946,https://hail.is,https://github.com/hail-is/hail/issues/6074,2,['error'],['error']
Availability,"So what I did to address the comments:; 1. I encoded the task names and the offsets of the data into the response from the worker to the front end and got rid of the MultiPart Reader/Writer.; 2. I reorganized the front end code so it's hopefully clearer what's going on.; 3. I got rid of the periodically_call changes and just did what I wanted directly in the `measure` function in the Resource Manager. Now it should be guaranteed that we do not call the measure function with the write more than once every 5 seconds. I do not want to retry calling this function at all on any error including transient errors. Otherwise, I can't figure out how the set of changes in this PR would use up all the disk space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785:580,error,error,580,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785,2,['error'],"['error', 'errors']"
Availability,"So, there was a test run that got cancelled (probably main branch changed) but which passed the service backend tests. It confirms that this most recent run ran all the tests, but it has some fishy looking error outputs:; ```; [gw2] PASSED [2023-04-21 18:31:07] test/hail/table/test_table.py::Tests::test_from_pandas_missing_and_nans Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; ```; Are we not cleaning up files somewhere and that's somehow hanging the system?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731#issuecomment-1518364066:206,error,error,206,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1518364066,1,['error'],['error']
Availability,Some Spark tests timed out (e.g. in https://ci.hail.is/batches/7644244/jobs/74 it was `test_spectral_moments_4`) which often crashes the JVM leading to the other errors. Retry and post links into Query Dev to alert them that QoS can timeout on spectral moments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229:162,error,errors,162,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229,1,['error'],['errors']
Availability,"Some delimited text processors use quotes to escape quotes so that the string `a""b` is rendered as `a""""b`. Moreover an individual entry of the delimited text is itself wrapped in double quotes, so, for example, a delimited text file representing one row containing the strings: `hello`, `a""b`, `goodbye` would contain the following bytes:; ```; ""hello"",""a""""b"",""goodbye""; ```. ---. Attempting to import and show the attached TSV file with `hl.import_table(""test.txt"", quote='""').show()` throws an exception:; ```; is.hail.utils.HailException: terminating quote character '""' not at end of field; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5796:613,Error,ErrorHandling,613,https://hail.is,https://github.com/hail-is/hail/issues/5796,2,['Error'],['ErrorHandling']
Availability,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:27,failure,failures,27,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Some fixes to my recent resiliency changes. These weren't caught because gateway and router-resolver are part of infrastructure that isn't automated by ci yet. I needed to make these changes to deploy them by hand (which I did successfully).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6211:24,resilien,resiliency,24,https://hail.is,https://github.com/hail-is/hail/pull/6211,1,['resilien'],['resiliency']
Availability,Some kind of batch service account error on this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7729#issuecomment-565757263:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/7729#issuecomment-565757263,1,['error'],['error']
Availability,"Some of the array tests (min, max, etc.) are failing because they rely on the `If` to protect from calling `ArrayRef` on a zero-length array. :-| I'm not sure if you can mitigate this by just not folding along nodes that might reasonably error, like `ArrayRef`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223:238,error,error,238,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223,1,['error'],['error']
Availability,"Some of the examples create errors in the new version (ex: need to use ""importannotations table"", and the -r flag doesn't exist anymore). (also -- would be great to add a link to this page from the main page!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/519:28,error,errors,28,https://hail.is,https://github.com/hail-is/hail/issues/519,1,['error'],['errors']
Availability,"Some parser failures:. for example:; `Rule 'type' didn't match at 'PCStruct{id:PCString' (line 1, column 1).`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7868#issuecomment-574469050:12,failure,failures,12,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574469050,1,['failure'],['failures']
Availability,"Some progress and new blocker on this topic. I moved to emr-6.11.1 that come with spark 3.3.2 & scala 2.12.15.; I upgraded the environment to get python 3.9 and java 11. * `emr-6.11.1`; * Java: java -version `11.0.20` (/usr/bin/java); * Python: python --version `3.9.18` (/usr/bin/python3); * Hadoop: hadoop version `3.3.3` (/usr/bin/hadoop); * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.15` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. But then once I build hail on this environment, the spark version is downgraded to 2.12.13 and the Java error above come back. ```sh; cd /tmp; git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```. * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.13` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. If I purposly build Hail for scala 2.12.13, the Java error above come back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000:742,down,downgraded,742,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000,3,"['down', 'error']","['downgraded', 'error']"
Availability,"Some strangeness going on with build... ; ```. ------------------------------------------------------------; Gradle 6.8.3; ------------------------------------------------------------. Build time: 2021-02-22 16:13:28 UTC; Revision: 9e26b4a9ebb910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:730,echo,echo,730,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,4,['echo'],['echo']
Availability,"Somehow, this didn't trigger an error in master but did in 0.1. Nonetheless, removing unnecessary data seems good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2349:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/2349,1,['error'],['error']
Availability,Something must actually be wrong... Shouldn't get the same error when just adding test files to a directory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808,1,['error'],['error']
Availability,"Sorry @lgruen these errors were transient issues mostly from our click/dependencies breaking style, but nothing to do with the PR. For CI builds, unfortunately your hail account must be a developer account to see the page. Dan would know whether or not we can do that for your account.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898:20,error,errors,20,https://hail.is,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898,1,['error'],['errors']
Availability,"Sorry I missed your message! The code as written now is plainly wrong: we access a mutable map from two threads without synchronization. We need this change regardless of how it affects error messages. If the tests pass, I'm confident this is fine. Are there components of the system you don't think are well tested by our tests?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471:186,error,error,186,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471,1,['error'],['error']
Availability,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:293,avail,available,293,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,1,['avail'],['available']
Availability,"Sorry for cutoff review line, I remarked on the test failure in a comment. You also need to rebase.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2546#issuecomment-350494085:53,failure,failure,53,https://hail.is,https://github.com/hail-is/hail/pull/2546#issuecomment-350494085,1,['failure'],['failure']
Availability,"Sorry for taking so long with this. There are some changes I think will be needed for this to be production ready, but at this point it's probably best to get it merged and iterate from there. @pwc2 It looks like the tolerance might need to be relaxed in test_pc_relate. And there are some python linter errors. Once those are fixed, I'm ready to approve. Thanks again for doing the work on this, and thanks for your patience!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10873#issuecomment-1146259813:217,toler,tolerance,217,https://hail.is,https://github.com/hail-is/hail/pull/10873#issuecomment-1146259813,2,"['error', 'toler']","['errors', 'tolerance']"
Availability,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:129,error,error-prone,129,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619,1,['error'],['error-prone']
Availability,Sorry this was a lot more broken than I thought. I didn't remember everything I stripped down for the previous PRs and didn't add back in. The commit update and update-fast endpoints need to return the `start_job_id`. Hopefully that's the last of these issues,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276:89,down,down,89,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276,1,['down'],['down']
Availability,"Sorry to bring those troubles, is there anything I should do to locate the error?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321265505:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321265505,1,['error'],['error']
Availability,"Sorry! I didn't see this because of the review. The root problem is that `raise_for_status` ignores the response body. This is a [known issue in aiohttp](https://github.com/aio-libs/aiohttp/issues/4600). It will be fixed in 4.0.0, but development on that seems slow. There's a variety of solutions to this problem. Every solution avoids aiohttp's raise_for_status and replaces it with something that includes the response body in the error message. A thorough fix to this is to finish the work I started in `httpx.py`. Instead of returning an `aiohttp.ClientSession` we could return a shim class that wraps `aiohttp.ClientSession` and checks the status code itself and raises an error *with the response body*. A smaller fix that only addresses aiogoogle would be to modify `aiogoogle.auth.Session` to:; 1. Not pass `raise_for_status` on to `aiohttp.ClientSession`.; 2. Store raise_for_status as a field on `aiogoogle.auth.Session`.; 2. In `aiogoogle.auth.Session.request`, if `self.raise_for_status` is true and the response status is greater than or equal to 400, retrieve the response body and raise an exception (maybe `HailHTTPError`) that includes the body.; 3. Ensure `is_transient_error` properly handles whatever exception we raise.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289:434,error,error,434,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289,2,['error'],['error']
Availability,"Sorry, didn't see this earlier -- . this is intentional. We do this so that we don't require users to recompile the C libraries when the install the Python library. Our native library distribution story is definitely a work in progress and this will change (hopefully improve) in the future. Feel free to ping us if this answer isn't sufficient!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152:305,ping,ping,305,https://hail.is,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152,1,['ping'],['ping']
Availability,"Sorry, was actually a docs failure. I'll see if I can figure out what it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287901000:27,failure,failure,27,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901000,1,['failure'],['failure']
Availability,Sorry. I need to get my head back into this again and I need to do the billing fixes from the redundant resource prices first. Unassigning you for now until it's ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316:94,redundant,redundant,94,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316,1,['redundant'],['redundant']
Availability,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:161,failure,failure,161,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,2,['failure'],['failure']
Availability,"Spaces in sample names are a nightmare for downstream analysis tools, so when I get my callsets from Picard, I immediately remove spaces in sample names and re-write my VCF. It usually takes a day or two to re-write the entire VCF, depending on the size of the callset. I would be an extremely happy camper if I could immediately import my VCF from Picard and either fix sample names on the fly during the import, or fix sample names in the .vds after the import (as a separate step). Either way, this should save a substantial chunk of time right off the top in the QC process.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/157:43,down,downstream,43,https://hail.is,https://github.com/hail-is/hail/issues/157,1,['down'],['downstream']
Availability,"Spaces were screwing things up because the `location` directive matches the decoded string (e.g. ""%20"" is converted back to "" ""). We don't need to reconstruct the URL explicitly with the regex pieces and the `$args` (which refers to HTTP query parameters), `$request_uri` is all that, but still encoded. The notebook was receiving requests without encoded spaces which appear as a bunch of weird hex characters. Obviously the notebook errors when it sees such demonic lettering.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4978:435,error,errors,435,https://hail.is,https://github.com/hail-is/hail/pull/4978,1,['error'],['errors']
Availability,"Spark 3.1 still relies on Breeze 1.0, which is very broken: https://github.com/scalanlp/breeze/issues/772. We can never allow use of Breeze 1.0. . To fix, I have hard coded the insistence that we use Breeze 1.1, relocated it into our hail jar. In the process, I also made it so that we don't support building with Scala 2.11 anymore, but that doesn't preclude us from still building with Spark 2.4.8 for now. . Our old ""fix"" in the build.gradle that said to change Spark 1.0 to 1.1 was actually making things more confusing. It was making it so that when we pulled down Spark and Breeze from Maven ourselves we'd switch out Breeze 1.0 for Breeze 1.1. However, it had no effect on what happened in dataproc, when breeze and Spark are provided on the classpath and we just use what's available. . I also added a dataproc test to catch this behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11555:565,down,down,565,https://hail.is,https://github.com/hail-is/hail/pull/11555,2,"['avail', 'down']","['available', 'down']"
Availability,"Spark 3.3.0 uses log4j2. Note the ""2"". If you use the log4j1 programmatic reconfiguration system, you will break log4j2 for you and everyone else. The only way to recover from such a breakage is to use the log4j2 programmatic reconfiguration system. Changes in this PR:. 1. Include JVM output in error logs when the JVM crashes. This should help debugging of JVM crashing in production until the JVM logs are shown on a per-worker page. 2. JVMEntryway is now a real gradle project. I need to compile against log4j, and I didn't want to do that by hand with `javac`. Ignore gradlew, gradlew.bat, and gradle/wrapper, they're programmatically generated by gradle. 3. Add logging to JVMEntryway. JVMEntryway now logs its arguments into the QoB job log. I also log exceptions from the main thread or the cancel thread into the job log. We also flush the logs after the main thread completes, the cancel thread completes, and when the try-catch exits. This should ensure that regardless of what goes wrong (even if both threads fail to start) we at least see the arguments that the JVMEntryway received. 4. Use log4j2 programmatic reconfiguration after every job. This restores log4j2 to well enough working order that, *if you do not try to reconfigure it using log4j1 programmatic configuration*, logs will work. All old versions of Hail use log4j1 programmatic configuration. As a result, **all old versions of Hail will still have no logs**. However, new versions of Hail will log correctly even if an old version of Hail used the JVM before it. 5. `QoBAppender`. This is how we always should have done logging. A custom appender which we can flush and then redirect to a new file at our whim. I followed the log4j2 best practices for creating a new appender. All these annotations, factory methods, and managers are The Right Way, for better or worse. If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941:163,recover,recover,163,https://hail.is,https://github.com/hail-is/hail/pull/12941,2,"['error', 'recover']","['error', 'recover']"
Availability,"Spark breaks down when a job has too many partitions. We should modify the implementation of CollectDistributedArray on the Spark backend to automatically break up jobs that are above some threshold of number of partitions into a few sequential smaller jobs. This would have a large impact on groups like AoU who are using Hail on the biggest datasets, who currently have to hack around this issue with trial and error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14584:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/issues/14584,2,"['down', 'error']","['down', 'error']"
Availability,"Spark depends on a very old verison of SLF4J. We cannot upgrade. This removes this message:; ```; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; ```. Which, IMO, really should be a stop-the-world error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14054:634,error,error,634,https://hail.is,https://github.com/hail-is/hail/pull/14054,1,['error'],['error']
Availability,"Spark depends on a very old verison of SLF4J. We cannot upgrade. We added this dependency ages ago to fix some undocumented issue with logging and SLF4J. It seems reasonable to me that we should just accept whatever version of SLF4J that Spark provides. This removes this message:; ```; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; ```. Which, IMO, really should be a stop-the-world error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14055:823,error,error,823,https://hail.is,https://github.com/hail-is/hail/pull/14055,1,['error'],['error']
Availability,Spark executor heartbeat timeout during hl.king(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:15,heartbeat,heartbeat,15,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['heartbeat'],['heartbeat']
Availability,"SparkBackend.scala:75); 	at is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:18); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:483); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.14-8dcb6722c72a; Error summary: ScalaSigParserError: Unexpected failure; ```; If that does execute, which it does sometimes (unclear why, I don't make any code changes), I get an error from mt = hl.read_matrix_table('data/1kg.mt'):. ```; [Stage 1:> (0 + 2) / 2]2019-06-10 14:40:22 Hail: INFO: Coerced sorted dataset; [Stage 2:> (0 + 2) / 2]2019-06-10 14:40:25 Hail: INFO: wrote matrix table with 10961 rows and 284 columns in 2 partitions to data/1kg.mt; Traceback (most recent call last):; File ""gwas_tutorial.py"", line 13, in <module>; mt = hl.read_matrix_table('data/1kg.mt'); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-1136>"", line 2, in read_matrix_table; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/methods/impex.py"", line 1708, in read_matrix_table; return MatrixTable(MatrixRead(MatrixNativeReader(path), _drop_cols, _drop_rows)); File ""/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:6556,Error,Error,6556,https://hail.is,https://github.com/hail-is/hail/issues/6299,3,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,"Speaking of not understanding what keys mean, I found what looks to me like a bug, but I'm not sure. `OrderedRVD.downcastToPK` creates an `OrderedRVD` for which `typ.kType` is different from `partitioner.kType`. It's triggering the assert I made in `RepartitionedOrderedRDD2` that says the new key must be a prefix of the old, to ensure that no sorting needs to be done. I want to make join keys parameters of `OrderedRVD.join`, allowing them to be different from the partitioner keys. I was putting that off for a later PR, but now I think I might need to do that to fix this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3159#issuecomment-373773742:113,down,downcastToPK,113,https://hail.is,https://github.com/hail-is/hail/pull/3159#issuecomment-373773742,1,['down'],['downcastToPK']
Availability,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6878:43,Down,Downloading,43,https://hail.is,https://github.com/hail-is/hail/pull/6878,1,['Down'],['Downloading']
Availability,Split from https://github.com/hail-is/hail/pull/14103; Co-Authored by @patrick-schultz . This change includes some edits to prevent build failures caused by using the scala formatter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14126:138,failure,failures,138,https://hail.is,https://github.com/hail-is/hail/pull/14126,1,['failure'],['failures']
Availability,Split multi error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3469:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/issues/3469,1,['error'],['error']
Availability,"Spyder IPython Import Error | TypeError: An asyncio.Future, a coroutine or an awaitable is required",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:22,Error,Error,22,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['Error'],['Error']
Availability,Stack overflow error inside of NormalizeNames.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7638#issuecomment-560197965:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-560197965,1,['error'],['error']
Availability,Stack trace from @lfrancioli ([full trace](https://nealelab.slack.com/files/laurent/F3P268282/error.txt)). ```; Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at org.broadinstitute.hail.utils.richUtils.RichIterable$$anon$4$$anon$10.next(RichIterable.scala:71); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:54); at org.broadinstitute.hail.methods.Aggregators$$anonfun$buildVariantAggregations$1.apply(Aggregators.scala:45); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$4$$anonfun$apply$1.apply(AnnotateVariantsExpr.scala:51); ⋮; ```. The iterator returned by the genotype stream has an additional constraint (over the `Iterator[T]` interface) that `hasNext` must be called before every call to `next`. The failing assertion verifies that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1227:94,error,error,94,https://hail.is,https://github.com/hail-is/hail/issues/1227,1,['error'],['error']
Availability,StackOverflow Kryo error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1528:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/issues/1528,1,['error'],['error']
Availability,"Stacked on #11905 . Before this change, we had one long running test which verified all; the old files still parsed properly. In the service backend in particular,; one test split job was substantially longer than the rest. This is bad; for PR merge time. This change has one significant downside, ""collecting"" the tests in pytest; requires evaluating two glob patterns which takes several seconds against; GCS. It is relatively fast against the local filesystem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11906:288,down,downside,288,https://hail.is,https://github.com/hail-is/hail/pull/11906,1,['down'],['downside']
Availability,"Stacked on #12757. - This PR gets the ranges of existing rows from the attempt_resources, aggregated_*_resources_v2 tables in bunches of 100 and then migrates each bunch by triggering an after update trigger for those rows that haven't been migrated. The triggers were added in #12757. ; - There's an audit at the end to make sure the new v3 tables give the same answer as the old v2 tables with duplicate resources.; - We use the same trick with a burn-in period to avoid the birthday problem with deadlocks.; - I added a function that generates the where statements programmatically based on looking at the where statement from previous migrations where we wrote out the where statement by hand. I think this way is less error-prone than writing out the where statement for each table, but it might be harder to reason about. Let me know if this way is too confusing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12761:723,error,error-prone,723,https://hail.is,https://github.com/hail-is/hail/pull/12761,1,['error'],['error-prone']
Availability,"Stacked on #7260 . Also set wire and memory spec to LZ4Fast. It was a bit annoying to make serialization work with a shared super class, so if anything looks funny down there that's probably why.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7261:164,down,down,164,https://hail.is,https://github.com/hail-is/hail/pull/7261,1,['down'],['down']
Availability,"Stacked on #9346 . Changes:; - infrastructure needed for kill switch; - UI page; - Default value for the limit is None. Testing:; - In the database migration, there's two updates that populate the initial state of the aggregated_billing_resources_table. I tested this by hand using a database that hadn't been migrated previously, but this might be good to double check.; - I ran the `check_resource_aggregation` loop while running `test_batch` and made sure there were no errors.; - I tested the UI page editing the limits with negative values and gibberish by hand to make sure those failed. I also refreshed the page to make sure the values were in the database and the update worked. So here's a PR where I convinced myself it was correct a couple of days ago, but the longer this sits, the less confident I'm going to be that there's not a mistake somewhere, especially if there are a lot of changes that need to be made to the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9354:473,error,errors,473,https://hail.is,https://github.com/hail-is/hail/pull/9354,1,['error'],['errors']
Availability,"Stacked on: https://github.com/hail-is/hail/pull/5507. Drops one broadcast from my test dataset from 1.4MB => 300KB (5x). I think that corresponds to the parallelize for writeSplitSpecs, which is now constant (won't scale according to the number of inputs). The RDD actually doing the writing, the OriginUnionRDD, still scales linearly. I think that's inevitable unless we do the LightweightContextRVDDistributedArray thing I mentioned on Zulip since we necessarily allocate at least one RDD per input. It might still be possible to push the constants down. The point of this change is to avoid capturing the OriginUnionRDD partitions inside the map step. I did this essentially by turning OriginUnionRDD into a union with ""mapPartitionsWithOriginIndex"". I think it might be wroth trying to re-run it after this goes in. Between this one and the last one, there are some pretty big memory/broadcast savings here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5509:552,down,down,552,https://hail.is,https://github.com/hail-is/hail/pull/5509,1,['down'],['down']
Availability,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5907:144,error,error,144,https://hail.is,https://github.com/hail-is/hail/pull/5907,1,['error'],['error']
Availability,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7035:151,error,error,151,https://hail.is,https://github.com/hail-is/hail/pull/7035,1,['error'],['error']
Availability,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7447:358,avail,available,358,https://hail.is,https://github.com/hail-is/hail/pull/7447,3,"['avail', 'down']","['available', 'down']"
Availability,"Stacks on #5874. Commit specific to this pr are: https://github.com/hail-is/hail/pull/5878/commits/e959eaf270c5dd9966e3c9c96f21d4f914097012, https://github.com/hail-is/hail/pull/5878/commits/fcbb0dc6ec6c678a54655afda996ee6b1148f2a1. Minor oddity: the 'updated' property isn't always available for folders. I can get around this if needed. I return the bucket for the ""owner"" property because the google sa isn't returned in the response. Will change this to the sa email (read at GoogleStorageFS instantiation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878:283,avail,available,283,https://hail.is,https://github.com/hail-is/hail/pull/5878,1,['avail'],['available']
Availability,Stage failure error when aggregating,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/issues/12083,2,"['error', 'failure']","['error', 'failure']"
Availability,Starving auth of the cycles to complete authentication requests causes cascading failures in the system.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12658:81,failure,failures,81,https://hail.is,https://github.com/hail-is/hail/pull/12658,1,['failure'],['failures']
Availability,"Stepping back a little bit, there might be a reasonable (if unsatisfying) middle ground. Presumably the operations most at risk are long streams that we always do in chunks anyway, and in that case we can create new downloaders on `AzureReadableStream.read` if the SAS token expires. That would probably solve most of these problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732:216,down,downloaders,216,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732,1,['down'],['downloaders']
Availability,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:314,error,error,314,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630,1,['error'],['error']
Availability,"Still don't know what's happening with this. I met with Zan a couple weeks ago and gave them some ideas to try to narrow down to a smaller example that reproduces the issue, but I haven't heard from them since.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13689#issuecomment-1759998425:121,down,down,121,https://hail.is,https://github.com/hail-is/hail/issues/13689#issuecomment-1759998425,1,['down'],['down']
Availability,"Still need to add row, col, and table tests. Have pretty robust entry tests, so those should go quick tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993:57,robust,robust,57,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993,1,['robust'],['robust']
Availability,"Still needs a black reformat. You can run `make -C batch/ check` ahead of time to catch these errors or add pre-commit hooks. ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:28:49.199357 +0000; +++ batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:31:14.836500 +0000; @@ -78,15 +78,17 @@; 'automaticRestart': False,; 'onHostMaintenance': 'TERMINATE',; }; ; if preemptible:; - result.update({; - 'provisioningModel': 'SPOT',; - 'instanceTerminationAction': 'DELETE',; - 'preemptible': True,; - }); + result.update(; + {; + 'provisioningModel': 'SPOT',; + 'instanceTerminationAction': 'DELETE',; + 'preemptible': True,; + }; + ); ; return result; ; return {; 'name': machine_name,; would reformat batch/cloud/gcp/driver/create_instance.py; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521:94,error,errors,94,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521,1,['error'],['errors']
Availability,"Still seeing this error in the deploy_batch job:; ```python; utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1054, ""Unknown",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340,1,['error'],['error']
Availability,Still some build errors coming from tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11306#issuecomment-1034147074:17,error,errors,17,https://hail.is,https://github.com/hail-is/hail/pull/11306#issuecomment-1034147074,1,['error'],['errors']
Availability,"Store: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1618,Error,ErrorHandling,1618,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Error'],['ErrorHandling']
Availability,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:782,error,error,782,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662,2,['error'],['error']
Availability,"Stray ""n"" snuck in, everything failing syntax errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1065413540:46,error,errors,46,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1065413540,1,['error'],['errors']
Availability,Struct annotate gives wrong error for mixed type dictionaries,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3886:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/3886,1,['error'],['error']
Availability,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7672:171,failure,failure,171,https://hail.is,https://github.com/hail-is/hail/pull/7672,1,['failure'],['failure']
Availability,Suddenly getting really surprising error here about low level codegen stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9146#issuecomment-663720122:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/9146#issuecomment-663720122,1,['error'],['error']
Availability,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149:967,down,down,967,https://hail.is,https://github.com/hail-is/hail/pull/8149,1,['down'],['down']
Availability,"Summary of changes:; - add a / index page to the workshop service with some chipper content. FYI @tpoterba, feel free to change if you don't like.; - make csrf token session-based; - add common render_template function to web_common that handles csrf and jinja2 rendering. This is necessary because the header has a logout button (potentially) so every page needs make sure the csrf is set.; - added a toplevel make check target; - fixed a forwarding bug: Host: $updated_host needs to get set when proxying to the notebook itself or you get cross-origin errors in the notebook. Things I have left to do:; - make the notebook non-clickable when it isn't ready; - write up a UI testing playbook; - link to notebook/workshop-admin somewhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145:554,error,errors,554,https://hail.is,https://github.com/hail-is/hail/pull/7145,1,['error'],['errors']
Availability,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:355,Error,Error,355,https://hail.is,https://github.com/hail-is/hail/issues/2527,2,"['Error', 'failure']","['Error', 'failure']"
Availability,Support checkpoint on BlockMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6933:8,checkpoint,checkpoint,8,https://hail.is,https://github.com/hail-is/hail/pull/6933,1,['checkpoint'],['checkpoint']
Availability,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6867:820,down,down,820,https://hail.is,https://github.com/hail-is/hail/pull/6867,1,['down'],['down']
Availability,"Sure thing, I think you basically want the following in the top-level Makefile instead of docker/Makefile (note that it is not PHONY):. ```make; vep-grch37-image: hail-ubuntu-image; $(eval VEP_GRCH37_IMAGE := $(DOCKER_PREFIX)/hailgenetics/vep-85-grch37:$(TOKEN)); 	python3 ci/jinja2_render.py '{""hail_ubuntu_image"":{""image"":""'$$(cat hail-ubuntu-image)'""}}' docker/vep/Dockerfile.GRCh37 docker/vep/Dockerfile.GRCh37.out; 	./docker-build.sh docker/vep Dockerfile.GRCh37.out $(VEP_GRCH37_IMAGE); 	echo $(VEP_GRCH37_IMAGE) > $@; ```. Side note: I do also agree with Dan's stylistic suggestion of having a directory structure like `docker/hailgenetics/vep/grch37/Dockerfile`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1503376729:494,echo,echo,494,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1503376729,1,['echo'],['echo']
Availability,"Sure, did that, though I kind of want a way to verify that it worked. Watching top doesn't seem super scientific. Linear regression still only takes like 20 seconds on my laptop. Watching top I see that the cpu usage spikes to 300% for a second at the beginning of each of the benchmark iterations, then falls back down to somewhere between 100% and 110% for the duration of the 20 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801:315,down,down,315,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801,1,['down'],['down']
Availability,"Surfaced because sometimes k8s secrets 404 for CI pipelines and we got FK constraint failures because there is no batch 0. No danger of bad data being written, just noise and unnecessary database load. Thank you foreign key checks!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14373:85,failure,failures,85,https://hail.is,https://github.com/hail-is/hail/pull/14373,1,['failure'],['failures']
Availability,"T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]’; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from ‘R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int32<4>; T = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:62:35: required from ‘simdpp::arch_avx2::int32<4>& simdpp::arch_avx2::int32<4>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]’; libsimdpp-2.0-rc2/simdpp/core/split.h:96:8: required from ‘void simdpp::arch_avx2::split(const simdpp::arch_avx2::int32<N>&, simdpp::arch_avx2::int32<(N / 2)>&, simdpp::arch_avx2::int32<(N / 2)>&) [with unsigned int N = 8]’; libsimdpp-2.0-rc2/simdpp/detail/insn/to_int64.h:67:20: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: ‘void* memcpy(void*, const void*, size_t)’ copying an object of type ‘class simdpp::arch_avx2::int32<4>’ with ‘private’ member ‘simdpp::arch_avx2::int32<4>::d_’ from an array of ‘const class simdpp::arch_avx2::uint32<4>’; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:33:7: note: ‘class simdpp::arch_avx2::int32<4>’ declared here; class int32<4, void> : public any_int32<4, int32<4,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of ‘R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int32<8>]’:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from ‘static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955:119124,error,error,119124,https://hail.is,https://github.com/hail-is/hail/issues/3955,1,['error'],['error']
Availability,TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2072,echo,echo,2072,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"THON-CRYPTOGRAPHY-5914629) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **479/1000** <br/> **Why?** Has a fix available, CVSS 5.3 | Missing Cryptographic Step <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6036192](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6036192) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **616/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 5.9 | NULL Pointer Dereference <br/>[SNYK-PYTHON-CRYPTOGRAPHY-6092044](https://snyk.io/vuln/SNYK-PYTHON-CRYPTOGRAPHY-6092044) | `cryptography:` <br> `3.3.2 -> 41.0.6` <br> | No | Proof of Concept ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **519/1000** <br/> **Why?** Has a fix available, CVSS 6.1 | Information Exposure <br/>[SNYK-PYTHON-REQUESTS-5595532](https://snyk.io/vuln/SNYK-PYTHON-REQUESTS-5595532) | `requests:` <br> `2.27.1 -> 2.31.0` <br> | No | No Known Exploit . (*) Note that the real score may have changed since the PR was raised. Some vulnerabilities couldn't be fully fixed and so Snyk will still find them when the project is tested again. This may be because the vulnerability existed within more than one direct dependency, but not all of the affected dependencies could be upgraded. Check the changes in this PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0Yzg3NGFkNy01NjNmLTQ5Y2QtOTc3My04YjlmMTA5NWUzNmMiLCJldmVudCI6IlBSIH",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14148:8652,avail,available,8652,https://hail.is,https://github.com/hail-is/hail/pull/14148,1,['avail'],['available']
Availability,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:929,mask,mask,929,https://hail.is,https://github.com/hail-is/hail/pull/5215,1,['mask'],['mask']
Availability,"TTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""><code>a8504f9</code></a> Bump up version number to 5.3.0</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/708067cd11c4a013da7a8c15d91f7f946967cf94""><code>708067c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0fdebf3c7ad43ed4739d0400c333a72b32f5d514""><code>0fdebf3</code></a> Improve verify example</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/019089b9554692674d6baee7df7d4d884f310cc9""><code>019089b</code></a> Correctly create list of output files</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/fa2739ded05333ba46d8f50bb3b2a3721cf0ca86""><code>fa2739d</code></a> Create target directories at a central place</li>; <li><a href=""https://github.com/mic",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:3216,down,download-task,3216,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['down'],['download-task']
Availability,Tabix just immediately creates a tiny index but does not throw any errors.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/160:67,error,errors,67,https://hail.is,https://github.com/hail-is/hail/issues/160,1,['error'],['errors']
Availability,Table field with '$' error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5120:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/5120,1,['error'],['error']
Availability,Table.show(-1) throws an error instead of showing everything,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6122:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/6122,1,['error'],['error']
Availability,Table.to_matrix_table bad error message on duplicate row keys,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['error'],['error']
Availability,"TableIRSuite extensively uses the function; ```; def collect(tir: TableIR): TableCollect = TableCollect(TableKeyBy(tir, FastIndexedSeq())); ```; to compare the result of a `TableIR` with the expected collection. But `TableCollect` makes no promises what order the results will be in, and in particular the optimizer is allowed to remove that `TableKeyBy`. This PR redefines that function to use the collect aggregator, which does promise the order rows are collected. It also fixes a small type error in the `TableJoin` lowering case that was uncovered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9054:495,error,error,495,https://hail.is,https://github.com/hail-is/hail/pull/9054,1,['error'],['error']
Availability,"Tags and digests have no affect on whether an image is one of the hailgenetics; images that are stored in both GCR and DockerHub. This change ignores the tag and; digest when checking if we should warn about using DockerHub. In doing this, I consolidated and made more robust our Docker-image-reference parsing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10327:269,robust,robust,269,https://hail.is,https://github.com/hail-is/hail/pull/10327,1,['robust'],['robust']
Availability,"TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20326,Error,Error,20326,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Error'],['Error']
Availability,"Teaches `hfs.ls('gs://bucket/')` to list the files and directories at the top-level of the bucket. In `main` that command raises because this line of `_ls_no_glob` raises:. ```python3; maybe_sb_and_t, maybe_contents = await asyncio.gather(; self._size_bytes_and_time_modified_or_none(path), ls_as_dir(); ); ```. In particular, `statfile` raises a cloud-specific, esoteric error about a malformed URL or empty object names:. ```python3; async def _size_bytes_and_time_modified_or_none(self, path: str) -> Optional[Tuple[int, float]]:; try:; # Hadoop semantics: creation time is used if the object has no notion of last modification time.; file_status = await self.afs.statfile(path); return (await file_status.size(), file_status.time_modified().timestamp()); except FileNotFoundError:; return None; ```. I decided to add a sub-class of `FileNotFoundError` which is self-describing: `IsABucketError`. I changed most methods to raise that error when given a bucket URL. The two interesting cases:. 1. `isdir`. This raises an error but I could also see this returning `True`. A bucket is like a directory whose path/name is empty. 2. `isfile`. This returns False but I could also see this raising an error. This just seems convenient, we know the bucket is not a file so we should say so. ---. Apparently `hfs.ls` had no current tests because the globbing system doesn't work with Azure https:// URLs. I fixed it to use `AsyncFSURL.with_new_path_component` which is resilient to Azure https weirdness. However, I had to change `with_new_path_component` to treat an empty path in a special way. I wanted this to hold:. ```; actual = str(afs.parse_url('gs://bucket').with_new_path_component('bar')); expected = 'gs://bucket/bar'; assert actual == expected; ```. But `with_new_path_component` interacts badly with `GoogleAsyncFSURL.__str__` to return this:. ```; 'gs://bucket//bar'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14176:372,error,error,372,https://hail.is,https://github.com/hail-is/hail/pull/14176,5,"['error', 'resilien']","['error', 'resilient']"
Availability,Test Errors from Digital China Health,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683:5,Error,Errors,5,https://hail.is,https://github.com/hail-is/hail/issues/683,1,['Error'],['Errors']
Availability,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:106,ERROR,ERROR,106,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Test failures seem odd, they don't seem to use FilterSamples?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2564#issuecomment-351422628:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/2564#issuecomment-351422628,1,['failure'],['failures']
Availability,"Test:. ```scala; @Test def testArrayLeftJoin() {; val l = Ref(genUID(), TInt32()); val r = Ref(genUID(), TInt32()); val left = ArrayRange(0, 10, 1); val right = MakeArray(Seq(2, 5, 8), TArray(TInt32())); assertEvalsTo(; ArrayLeftJoinDistinct(; left, right,; l.name, r.name,; l - r,; If(IsNA(r), l, 0)),; IndexedSeq(0, 1, 0, 3, 4, 0, 6, 7, 0, 9))(; ExecStrategy.javaOnly; ); }; ```. Fails with: `java.lang.IllegalStateException: Bytecode failed verification 1`. ```; Verify Output 2 for is/hail/codegen/generated/C1:; org.objectweb.asm.tree.analysis.AnalyzerException: Error at instruction 478: Argument 2: expected I, but found J; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6827:568,Error,Error,568,https://hail.is,https://github.com/hail-is/hail/issues/6827,1,['Error'],['Error']
Availability,"TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name of the test instead of just the function name for better readability of test output.(Krishnan Mahadevan); Fixed: GITHUB-2725: Honour custom attribute values in TestNG default reports (Krishnan Mahadevan); Fixed: GITHUB-2726: <a href=""https://github.com/AfterClass""><code>@​AfterClass</code></a> config method is executed for EACH <a href=""https://github.com/Test""><code>@​Test</code></a> method when parallel == methods (Krishnan Mahadevan); Fixed: GITHUB-2752: TestListener i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:11806,error,error,11806,https://hail.is,https://github.com/hail-is/hail/pull/12665,3,"['error', 'fail-safe']","['error', 'fail-safe']"
Availability,"Tested as follows from a clean environment.; 1. Build the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:859,echo,echo,859,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,2,['echo'],['echo']
Availability,Tests are passing again! I know there were concerns about error handling in the `vep.py` code. I tried to make it exactly the same as the existing Scala code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1522373710:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1522373710,1,['error'],['error']
Availability,"Tests fail, segmentation fault, issue in copyFromType added test in PBaseStruct, haven't solved yet, no remaining time today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7958:25,fault,fault,25,https://hail.is,https://github.com/hail-is/hail/pull/7958,1,['fault'],['fault']
Availability,"Tests not passing, not sure why yet. Errors are MatchError. Stacked on #6421, will see if I can unwind; ```sh; > Task :test; Running test: Test method bitPackedVectorCorrectWhenOffsetNotZero(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.bitPackedVectorCorrectWhenOffsetNotZero PASSED; Running test: Test method testBitPackUnpack(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testBitPackUnpack FAILED; scala.MatchError at LocalLDPruneSuite.scala:222; Running test: Test method testIsLocallyUncorrelated(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testIsLocallyUncorrelated FAILED; org.apache.spark.SparkException at LocalLDPruneSuite.scala:214; Caused by: scala.MatchError; Running test: Test method testR2(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testR2 FAILED; scala.MatchError at LocalLDPruneSuite.scala:244; Running test: Test method testRandom(is.hail.methods.LocalLDPruneSuite). Gradle suite > Gradle test > is.hail.methods.LocalLDPruneSuite.testRandom FAILED; java.lang.AssertionError at LocalLDPruneSuite.scala:323; Caused by: scala.MatchError at LocalLDPruneSuite.scala:323; ```. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6425:37,Error,Errors,37,https://hail.is,https://github.com/hail-is/hail/pull/6425,1,['Error'],['Errors']
Availability,"TextContext should attach the caught exception as the cause on the resulting fatal error. I have partial code for this and will make a PR, just leaving this here as a reminder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1556:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/1556,1,['error'],['error']
Availability,Thank @jbloom22 ! Hadn't seen that this had change (was `bool` back in the days). And indeed the error message did not put me on the right track :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088,1,['error'],['error']
Availability,"Thank you for getting back to me. I was using the solution provided by aws (https://github.com/awslabs/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/blob/master/source/GenomicsAnalysisCode/buildhail_buildspec.yml) also the main page for reference (https://docs.aws.amazon.com/solutions/latest/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/welcome.html). In order to use the latest Hail (because we have vcf format 4.3 which is not supported in Hail 0.1), we changed it to ; ```. echo 'Installing pre-reqs'; yum install -y g++ cmake git; yum install -y lz4; yum install -y lz4-devel; git clone $HAIL_REPO; cd hail/hail && git fetch && git checkout main; ./gradlew clean; make install HAIL_COMPILE_NATIVES=1; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.13 SPARK_VERSION=3.1.1; cd build; pip download decorator==4.2.1; aws s3 cp decorator-4.2.1-py2.py3-none-any.whl s3://${RESOURCES_BUCKET}/artifacts/decorator.zip; aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/; aws s3 cp libs/hail-all-spark.jar s3://${RESOURCES_BUCKET}/artifacts/; ``` . What else I should change in order to deploy this solution successfully? . Thank you for the help!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181:537,echo,echo,537,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181,2,"['down', 'echo']","['download', 'echo']"
Availability,"Thank you for taking a look!. > 1. I'm not opposed to adding tokens to the batches_n_jobs_in_complete_states table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:930,Error,Error,930,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,['Error'],['Error']
Availability,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404,4,"['FAILURE', 'error']","['FAILURE', 'error', 'errors']"
Availability,"Thank you for the quick response, @tpoterba . I understand, I think we are on the same page. I was mainly curious what would happen if overwrite was True. Whether it would start from scratch or ignore overwrite, or something else. That being said, a possible use case re overwrite could be to actually remove partial files re tasks being recovered. Re a single task, if it would fail, then it would be possible for there to exist a partial task file. Usually if a task fails, then Spark would retry it. And if the task's next attempt passes, then there would exist a full task file plus a partial task file. So, if Hail would recover the task, then I imagine there would also exist a full task file plus a partial task file. I have not seen partial task files affecting downstream processing yet, but I am generally wary of ""duplicate"" files. Just a thought",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485:338,recover,recovered,338,https://hail.is,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485,3,"['down', 'recover']","['downstream', 'recover', 'recovered']"
Availability,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:230,error,error,230,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410,4,['error'],['error']
Availability,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240395647:78,failure,failures,78,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647,1,['failure'],['failures']
Availability,"Thanks Alex, appreciated. I know it's a big change, but it is needed for a bunch of downstream stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581#issuecomment-617726597:84,down,downstream,84,https://hail.is,https://github.com/hail-is/hail/pull/8581#issuecomment-617726597,1,['down'],['downstream']
Availability,"Thanks a lot for reporting this error, Tim found the fix, and this detailed report was very helpful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-665775696:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-665775696,1,['error'],['error']
Availability,"Thanks for clarifying these points and pointing out the errors!. <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 01"" src=""https://user-images.githubusercontent.com/106194/219511417-004e5529-c41c-4068-bc85-d679fca58058.png"">; <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 04"" src=""https://user-images.githubusercontent.com/106194/219511421-ba3600ad-b0a3-4564-862a-78f6b7501467.png"">; <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 09"" src=""https://user-images.githubusercontent.com/106194/219511419-cfa45641-bcf4-4e29-a5f1-bf431981c0c6.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1433881634:56,error,errors,56,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1433881634,1,['error'],['errors']
Availability,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:174,down,download,174,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,3,"['avail', 'down']","['available', 'download']"
Availability,"Thanks for sharing the doctest failures. Those were indeed issues, and I think I have fixed them now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14250#issuecomment-1992494034:31,failure,failures,31,https://hail.is,https://github.com/hail-is/hail/pull/14250#issuecomment-1992494034,1,['failure'],['failures']
Availability,"Thanks for the detailed error report! This is a known problem, fix is in PR: #10523",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682#issuecomment-883350719:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/10682#issuecomment-883350719,1,['error'],['error']
Availability,Thanks for the ping!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106#issuecomment-793959393:15,ping,ping,15,https://hail.is,https://github.com/hail-is/hail/pull/10106#issuecomment-793959393,1,['ping'],['ping']
Availability,"Thanks for the report! This is a bug. The `export_vcf` method will currently just error universally on haploid calls, rather than checking if the haploid call is phased. https://github.com/hail-is/hail/blob/026a64d64c8a2905c6125fcd445302c2452fb37b/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala#L1012-L1017",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330#issuecomment-1969442933:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/issues/14330#issuecomment-1969442933,1,['error'],['error']
Availability,"Thanks for the report, @JacobBayer! I don't believe anyone on our team uses Spyder unfortunately, but I don't think this is a Hail issue. According to [this thread](https://community.developers.refinitiv.com/questions/88895/spyder-515-erroreikon-data-api.html?childToView=89408#answer-89408) a Spyder upgrade might resolve the issue if you're on an old version?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309:235,error,erroreikon-data-api,235,https://hail.is,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309,1,['error'],['erroreikon-data-api']
Availability,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:240,down,down,240,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790,1,['down'],['down']
Availability,Thanks for tracking this down Tim,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9425#issuecomment-689815711:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689815711,1,['down'],['down']
Availability,"Thanks so much Daniel!! This is awesome. I can't seem to be able to merge though, probably due to permissions?; <img width=""516"" alt=""Screen Shot 2021-04-22 at 9 58 00 am"" src=""https://user-images.githubusercontent.com/1575412/115636406-3ada5e00-a351-11eb-887d-3882271f6369.png"">. There are also conflicts, but I'm resolving them right now :). UPD: Ah, learned from Leo that the merge button will be available after the tests pass :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697:400,avail,available,400,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697,1,['avail'],['available']
Availability,Thanks! That helped. There a couple of other issues that came up that required some tinkering. I list them below in case any one runs into them also. 1. curl failed when trying to download the ibsimdpp lib. The workaround was to download it with wget and move it to the Make Directory. ```; wget --no-check-certificate https://storage.googleapis.com/hail-common/libsimdpp-2.0-rc2.tar.gz; mv libsimdpp-2.0-rc2.tar.gz src/main/c; ```. 2. Needed to compile with newer version of gcc; ```; module load gcc/7.2.0; ./gradlew -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar archiveZip. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411:180,down,download,180,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411,2,['down'],['download']
Availability,"Thanks! To clarify your other question, while the local backend segfault with this, I think it is caused by existing memory issues that show up in the Spark backend tests. I conjecture when you fix those, the local backend failure will go away. If not, we can flip to see who debugs it :-)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424:223,failure,failure,223,https://hail.is,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424,1,['failure'],['failure']
Availability,"Thanks, and sorry about that! Github's fault.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624157539:39,fault,fault,39,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624157539,1,['fault'],['fault']
Availability,"Thanks, that fixed it. I followed instructions here: https://stackoverflow.com/questions/46513639/how-to-downgrade-java-from-9-to-8-on-a-macos-eclipse-is-not-running-with-java-9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10524#issuecomment-849024403:105,down,downgrade-java-from-,105,https://hail.is,https://github.com/hail-is/hail/issues/10524#issuecomment-849024403,1,['down'],['downgrade-java-from-']
Availability,That definitely makes more sense. Thanks!. > This will hang if we have tasks that were not properly stopped. Do you see this as problematic? I figure a pod would get terminated if it took too long to shut down gracefully and it could more obviously reveal the underlying problem.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9944#issuecomment-769078938:205,down,down,205,https://hail.is,https://github.com/hail-is/hail/pull/9944#issuecomment-769078938,1,['down'],['down']
Availability,"That huge spike in ready jobs has got to be a mistake. Where do we create a 1500 partition dataset?. Service backend [split 35](https://batch.hail.is/batches/7489026/jobs/197) is again the critical path. I tried pushing a commit which checkpoints the VCF. The main problem is probably just that our PC-Relate implementation is slow.; ```; ============================= slowest 50 durations =============================; 240.09s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 168.03s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 93.98s call hail/vds/test_vds.py::test_truncate_reference_blocks; 86.56s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; 83.82s call hail/methods/test_qc.py::Tests::test_vep_grch38_against_dataproc; ```. 2a3fbd185e9255ed447dd80b983709d49c7a345e (45 minutes):. <img width=""2032"" alt=""Screen Shot 2023-05-26 at 14 39 28"" src=""https://github.com/hail-is/hail/assets/106194/2f98159e-e60e-4410-af4b-7bca72fc43a8"">. <img width=""305"" alt=""Screen Shot 2023-05-26 at 14 36 58"" src=""https://github.com/hail-is/hail/assets/106194/be2c4b32-db08-484f-a6de-08bc41149e65"">. <img width=""523"" alt=""Screen Shot 2023-05-26 at 14 38 18"" src=""https://github.com/hail-is/hail/assets/106194/981665f6-fe7c-45eb-8659-836d505fa280"">; <img width=""518"" alt=""Screen Shot 2023-05-26 at 14 38 13"" src=""https://github.com/hail-is/hail/assets/106194/feaf826a-57d2-4404-9844-69048e3ade69"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1564787141:235,checkpoint,checkpoints,235,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1564787141,1,['checkpoint'],['checkpoints']
Availability,"That is a good point! I think we could work around that by passing in the CSRF token from Batch in the query parameters as well, and then having the Auth service ping the Batch API with it to verify the token is valid (or perhaps we could just make this UI a single page application instead of a bunch of pages on different subdomains that resemble one) but I think for the short term this is a good fix!. Unrelated process note: I think in order to link the issue to the PR successfully, you have to use a [verb like ""fixes"" or ""closes""](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) in the PR description, rather than ""addresses"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2269566317:162,ping,ping,162,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2269566317,1,['ping'],['ping']
Availability,That's a bit weird. You're pushing to the branch on your fork and still getting that error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248074937:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248074937,1,['error'],['error']
Availability,"That's a good point about which fatal is called. I'm OK with removing it. On Wed, Apr 6, 2016 at 10:56 AM, cseed notifications@github.com wrote:. > I argue fatalIf(p, msg) is less readable than if (p) fatal(msg) and not; > any shorter. It also causes an error since you can't control which fatal to; > call, e.g., Utils.fatal vs Line.fatal. @tpoterba; > https://github.com/tpoterba Thoughts?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/issues/262",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/262#issuecomment-206416460:254,error,error,254,https://hail.is,https://github.com/hail-is/hail/issues/262#issuecomment-206416460,1,['error'],['error']
Availability,"That's all part of the same error. If you resolve the file permissions issue, then the HailContext can be successfully initialized. I'll report this confusing error message to the team.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138,2,['error'],['error']
Availability,That's correct. I saw an intermittent failure in Azure on one of my PRs. That's how I found out the debug info was wrong.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11203#issuecomment-1006815541:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/11203#issuecomment-1006815541,1,['failure'],['failure']
Availability,"That's handled in the typechecker on `StringExpression.matches` -- we don't generate `RegexMatch` AST nodes anywhere else. However, there's still a problem that I haven't yet resolved -- it's possible to get parse errors if the string given doesn't parse to a valid 0.1-expr-language string literal. I think there might be additional unescaping going on somewhere in the python/java connector. Still, I think we should merge this in and figure out how to solve this problem later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2637#issuecomment-354891277:214,error,errors,214,https://hail.is,https://github.com/hail-is/hail/pull/2637#issuecomment-354891277,1,['error'],['errors']
Availability,"The ""Arguments refer to no files"" error means that the hadoop file connector may not be working properly, but since you're able to read the file with `textFile` I'm baffled (we call sc.textFile internally to import a VCF!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321255324:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321255324,1,['error'],['error']
Availability,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594:345,FAILURE,FAILURE,345,https://hail.is,https://github.com/hail-is/hail/issues/594,1,['FAILURE'],['FAILURE']
Availability,The *really* weird thing is that I think even a write/read/collect doesn't trigger this error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498871032:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498871032,1,['error'],['error']
Availability,The AddressAndPort pylint failure will be fixed by https://github.com/hail-is/hail/pull/9126,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129#issuecomment-662812817:26,failure,failure,26,https://hail.is,https://github.com/hail-is/hail/pull/9129#issuecomment-662812817,1,['failure'],['failure']
Availability,The Azure blob storage client does not include the blob name in file not found errors. This adds that information to the `FileNotFoundError` that we raise on top of the azure error in the azure fs's `read`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11214:79,error,errors,79,https://hail.is,https://github.com/hail-is/hail/pull/11214,2,['error'],"['error', 'errors']"
Availability,"The GENCODE GTF files associated with gnomAD annotations are occasionally useful. For example, they are needed to get the gene and transcript version numbers for VEP annotations for Ensembl transcripts. Or they can be used to get an interval for a particular gene or transcript, which can then be used to efficiently filter the variants Hail tables. However, the files hosted by GENCODE are not block gzipped. Thus, they are slow to import into Hail because the import cannot be parallelized. To make working with this data in Hail easier, it would be nice if the relevant versions of GENCODE were available in [Hail's Datasets collection](https://hail.is/docs/0.2/datasets.html). It looks like GENCODE v19 and v31 are already there. https://www.gencodegenes.org/human/releases.html; https://gnomad.broadinstitute.org/help/what-version-of-gencode-was-used-to-annotate-variants. This is effectively the same request as broadinstitute/gnomad_production#1042.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11899:598,avail,available,598,https://hail.is,https://github.com/hail-is/hail/issues/11899,1,['avail'],['available']
Availability,"The IP address we got from `address` might be the IP of a pod that has been removed. When this happens; we get connection failed errors, which we treat as ""transient"". This change modifies the test to also; get a new IP address each time a transient error occurs. Not ideal, but more correct than previously.; There are forthcoming changes that I hope will more pervasively address this problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9963:129,error,errors,129,https://hail.is,https://github.com/hail-is/hail/pull/9963,2,['error'],"['error', 'errors']"
Availability,"The NativePtr test runs fine as a single test, but it fails when I run all the tests. I'm trying to track; down the problem. Somehow the (anonymous) global data in NativePtr.cpp is getting corrupted.; I can make it pass tests by reinitializing that data, but I really want to figure out how it gets corrupted; because that could cause trouble elsewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3595#issuecomment-390062140:107,down,down,107,https://hail.is,https://github.com/hail-is/hail/pull/3595#issuecomment-390062140,1,['down'],['down']
Availability,The SNP manipulation functions are huge and get inlined a million; times in sample_qc. This will help keep IR size down.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6200:115,down,down,115,https://hail.is,https://github.com/hail-is/hail/pull/6200,1,['down'],['down']
Availability,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:174,recover,recover,174,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189,2,"['failure', 'recover']","['failure', 'recover']"
Availability,"The VDS combiner is flaky on query on batch on GCP due to issues reading VCFs with intervals. Errors observed:. - BGZ validation errors; - Unexpected end of input. Both of these point to issues in the interface between the `FSSeekableInputStream` that underpins GoogleFS and the `BGZipInputStream` that contains it at least in the presence of more than one seek. Unfortunately, the conditions that reproduce this are rare, and when our clusters are quieter (nighttime) the errors are even less frequent.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356:94,Error,Errors,94,https://hail.is,https://github.com/hail-is/hail/issues/13356,3,"['Error', 'error']","['Errors', 'errors']"
Availability,The [API](https://docs.microsoft.com/en-us/azure/virtual-machines/states-billing) optionally postfixes an error code/modifier so it seems more robust to look at the prefixes here. This currently manifested with a `ProvisioningState/failed/AllocationFailed` state leaving an instance forever pending.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11393:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/11393,2,"['error', 'robust']","['error', 'robust']"
Availability,"The [`flake8` check job](https://ci.hail.is/batches/536128/jobs/11) is also complaining about whitespace in struct.py on a few lines:; ```; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; hail/utils/struct.py:120:1: W293 blank line contains whitespace; hail/utils/struct.py:122:1: W293 blank line contains whitespace; hail/utils/struct.py:124:1: W293 blank line contains whitespace; hail/utils/struct.py:126:1: W293 blank line contains whitespace; hail/utils/struct.py:129:1: W293 blank line contains whitespace; hail/utils/struct.py:131:1: W293 blank line contains whitespace; hail/utils/struct.py:134:1: W293 blank line contains whitespace; hail/utils/struct.py:136:1: W293 blank line contains whitespace; make[1]: *** [Makefile:11: check] Error 1; make[1]: Leaving directory '/io/repo/hail/python'; make: *** [Makefile:13: check-hail] Error 2; ```. I recommend enabling ""show whitespace"" in your editor. That ensure you see these issues before you make a commit. For PyCharm you can [try this](https://intellij-support.jetbrains.com/hc/en-us/community/posts/206349049-How-to-make-show-whitespace-turn-on-as-default-).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544:822,Error,Error,822,https://hail.is,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544,2,['Error'],['Error']
Availability,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8940:229,error,error,229,https://hail.is,https://github.com/hail-is/hail/issues/8940,3,"['Error', 'error']","['Error', 'error']"
Availability,"The `delete_azure_batch_instances` step is failing on various PRs with the error `jq: command not found`. This appears to be because we do not pin the version for the `mcr.microsoft.com/azure-cli` image, and while that image was previously based on the Alpine image, [now it is based on the Azure Linux image](https://learn.microsoft.com/en-us/cli/azure/run-azure-cli-docker), and does not appear to have `jq` (or `kubectl`) preinstalled on it. This change updates the commands run in the `azure-cli` container for this CI step to install `jq` and `kubectl` via `curl` before running the relevant commands. The `curl` commands were tested locally by running `docker run -it mcr.microsoft.com/azure-cli` and trying them out in the image's shell. This change also adds the installation commands in the other place where this image is used (when cleaning up from `buildImage2` jobs that are run in Azure).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14677:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/pull/14677,1,['error'],['error']
Availability,The `download` function does this already.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11813:5,down,download,5,https://hail.is,https://github.com/hail-is/hail/pull/11813,1,['down'],['download']
Availability,"The `log.exception` in the wrapping try/except means we log anything that raises as an error, even things like 503's and 403's from the workers which we explicitly log as info. I think we're abusing exception handling here to catch a potentially non-exceptional failure mode which is ""we couldn't schedule, we need to add back those cores"". Didn't rework things though, just pushed the exception logging in to the only chunk of code that wasn't already in a nested try/except.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11715:87,error,error,87,https://hail.is,https://github.com/hail-is/hail/pull/11715,2,"['error', 'failure']","['error', 'failure']"
Availability,The `repr` for SparkContext displays like this: `<pyspark.context.SparkContext object at 0x7ff241c5f690>`. We don't have a history available for a SparkContext object. Not clear how to fix this other than use `sc` if a non-default arg is given for `sc` in HailContext constructor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2180:131,avail,available,131,https://hail.is,https://github.com/hail-is/hail/issues/2180,1,['avail'],['available']
Availability,"The assertion failure in `mendel_errors` was due to a bug in `TableKeyBy` that was only triggered in this PR because the IR version of `Table.aggregateByKey` maintains an `OrderedRVD`, hence uses the `ordered` branch. The `key_by('s')` after the `group_by('s', 'fam').aggregate(...)` was leaving the partition key as both fields. h/t @patrick-schultz for seeing the faulty `TableKeyBy`, since `nPartitionKeys=None` means that all key fields are partition keys.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3730#issuecomment-396372654:14,failure,failure,14,https://hail.is,https://github.com/hail-is/hail/pull/3730#issuecomment-396372654,2,"['failure', 'fault']","['failure', 'faulty']"
Availability,"The asyncio event loop only keeps weak references to tasks, so wherever we call `asyncio.create_task` we need to ensure that we keep a strong reference to its result. Specifically, `BackgroundTaskManager` needs to keep strong references not weak references to the tasks it creates. This is easy to do without accumulating garbage by using a done callback on the task to remove itself from the set. However, this felt iffy with the threadsafe futures, which were only used in sync.py anyway, so I pushed that functionality directly into sync.py and removed it from the `BackgroundTaskManager`. To simplify the ownership story for tasks, this changes `BackgroundTaskManager` to *not* return the task and instead hold onto strong references. If a client wants a reference to the task it creates, it should call `asyncio.create_task` directly and manage the lifecycle of the spawned task. This required only a few small changes in worker.py since most of the codebase does not assign the result of `task_manager.ensure_future`. The only change that gave me pause was the handling of `mjs_fut`, whose lifetime is a little tricky since it is potentially passed to yet another task. I think this shows a general weakness in the handling of ownership and lifetimes in between the Job and Worker classes and think a larger refactor can make this less error-prone but is out of scope for this fix. So I'd appreciate an especially scrutinizing look at that piece of the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12692:1342,error,error-prone,1342,https://hail.is,https://github.com/hail-is/hail/pull/12692,1,['error'],['error-prone']
Availability,"The batch UI column is named ""Failed"" but it means ""bad"" or ""Failure and Error"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7770:61,Failure,Failure,61,https://hail.is,https://github.com/hail-is/hail/issues/7770,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"The call in the docs is wrong:; ```; [:makeHailDocs] Warning, treated as error:; [13:46:55]	[:makeHailDocs] WARNING: **********************************************************************; [13:46:55]	[:makeHailDocs] File ""hail.VariantDataset.rst"", line 16, in default; [13:46:55]	[:makeHailDocs] Failed example:; [13:46:55]	[:makeHailDocs] linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:73,error,error,73,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['error'],['error']
Availability,"The children field in `table2` is the number of children with any mendel errors, not the number of children in that family. This should probably be fixed by doing an `annotate_cols` aggregation and then aggregating the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5786:73,error,errors,73,https://hail.is,https://github.com/hail-is/hail/issues/5786,1,['error'],['errors']
Availability,"The code I'm trying to run is below. It uses functionality from my experimental vcf_combiner branch,; available [here](https://github.com/chrisvittal/hail/blob/35456f1e4766e00958c3f0aaf464f2089bc73dfd/python/hail/experimental/vcf_combiner.py). I have attached console output of the script below as well. ```python3; #!/user/bin/env python3; import sys; from time import perf_counter. import hail as hl; from hail.experimental.vcf_combiner import combine_vcfs. def main():; hl.init(default_reference='GRCh38', min_block_size=0); start = perf_counter(); mt = hl.read_matrix_table(sys.argv[1]); mts = [mt for _ in range(0, 2)]; combine_vcfs(*mts).write(sys.argv[2]); end = perf_counter(); print(f'Time elapsed {end - start}'). if __name__ == '__main__':; main(); ```. [script.err.txt](https://github.com/hail-is/hail/files/2328852/script.err.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4222:102,avail,available,102,https://hail.is,https://github.com/hail-is/hail/issues/4222,1,['avail'],['available']
Availability,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8411:1379,Error,Error,1379,https://hail.is,https://github.com/hail-is/hail/issues/8411,1,['Error'],['Error']
Availability,"The correct fix is to change this line in `lookup` from `if (tt.xs.size == typ.xs.size)` to `if (tt.xs.size == typ.xs.size && typ.getClass == tt.getClass)`. However, this will cause many test suite failures and we would have to fix 100s of lines of expr language in both the docs and the test suites. In the meantime we could change the AST code for `Select` to `lookupMethodReturnType` on failure of `lookupFieldReturnType`. . The user will not be able to trigger this error once the expression language is implemented in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1983#issuecomment-329826002:198,failure,failures,198,https://hail.is,https://github.com/hail-is/hail/issues/1983#issuecomment-329826002,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"The current `on_cleanup` code carefully attempts to close resources in the correct order (if B depends on A, we should close B before we close A). Doing so is pretty error prone though and we have messed it up in the past, leading to noisy error logs when pods are shut down. If we instead push `.close` methods onto a stack immediately after they are initialized, the exit stack cannot be executed in the wrong order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14172:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/pull/14172,3,"['down', 'error']","['down', 'error']"
Availability,"The default behavior, is, apparently, to error, not to return `None`. This change makes the missing value case return `None` which triggers the ValueError on the next line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8651:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/8651,1,['error'],['error']
Availability,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/pull/2930,2,['error'],['error']
Availability,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4979:98,error,error,98,https://hail.is,https://github.com/hail-is/hail/pull/4979,1,['error'],['error']
Availability,"The design of NativeModule's handling of files (which may involve several worker threads; each trying to initialize their own NativeModule referring to the same DLL) assumed that; /bin/mv would do an atomic rename to replace an old DLL with a newer version. But on; MacOS /bin/mv is non-atomic, and leaves a window between deleting the old file and ; replacing it with the new one. I'm working on details of a more robust file-synchronization scheme, once that's ok I'll; backport it here and also address the review comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218:415,robust,robust,415,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218,1,['robust'],['robust']
Availability,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:904,down,down,904,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,2,['down'],['down']
Availability,The docker file describes a sufficient environment to build and test hail 0.1. The Makefile wraps up Docker image production. The `hail-docs-trampoline.sh` delays the `git rev-parse` until the docs are actually built which allows `gradle downloadDependencies` to run without the `.git` folder present which allows me to cache some of the gradle dependencies once rather than per-build. `hail-ci-build-image` contains the name of a docker image in which to build and test hail 0.1. `hail-ci-build.sh` describes how to build and test hail 0.1 and populates the `artifacts` directory with the results and an index file.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4123:238,down,downloadDependencies,238,https://hail.is,https://github.com/hail-is/hail/pull/4123,1,['down'],['downloadDependencies']
Availability,"The error about a log file is an unfortunate red herring, the main container appears to fail. `/vep/vep` is returning exit code -9 but providing no further debug information. We need to assess who is generating the -9. Based on the contents of `docker/hailgenetics/vep/grch38/95/Dockerfile`, `/vep/vep` appears to be; ```; #!/bin/bash; export PERL5LIB=$PERL5LIB:/vep/ensembl-vep/Plugins/; exec perl /vep/ensembl-vep/vep \""\$@\""""; ```; It seems likely that `/vep/esnembl-vep/vep` is returning exit code -9. We need to determine under what conditions that happens. Main container output:; ```; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['error'],['error']
Availability,"The error encountered when importing multiple VCFs with different INFO fields was not a good one. This improves the situation somewhat, by making it print the types instead of the ptypes, by calling attention to the fact that the issue is likely info fields, and by adding a test of this error message. . The situation could still be improved by pushing error ids through `TableRead`, but I have not done so yet. I will save that for a future PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10819:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/10819,3,['error'],['error']
Availability,"The error in the buildkit image is my fault, I'll address and retry.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847#issuecomment-1163501994:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11847#issuecomment-1163501994,2,"['error', 'fault']","['error', 'fault']"
Availability,"The error is here:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/cloud/gcp/driver/resource_manager.py"", line 158, in create_vm; await self.compute_client.post(f'/zones/{location}/instances', params=params, json=vm_config); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/base_client.py"", line 30, in post; async with await self._session.post(url, **kwargs) as resp:; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await request_retry_transient_errors(self._http_session, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 770, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 147, in request_and_raise_for_status; body=body; hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances?requestId=e2555a38-1583-47e2-ab15-c3d7ad84e700') body='{\n ""error"": {\n ""code"": 400,\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""errors"": [\n {\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (defau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728,1,['error'],['error']
Availability,The error mentioned above is cause when container runs out of Memory. Try running with higher Memory parameters.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-333867936:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-333867936,1,['error'],['error']
Availability,"The error message suggests that multiple clients are writing to:; ```; gs://rwalters-hail-tmp/merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00; ```; Which is a Hail Query partition file within a matrix table. This is happening in a flush of GoogleStorageFS after we've filled up the local byte buffer. I don't think this is 0.2.114 because the line numbers don't line up. This appears to be before our recent changes to handle requester pays. A couple thoughts:; 1. We retryTransientErrors on the entire partition. What happens if a partition fails and we automatically retry it? Does the generated code clean up all the output streams when an exception occurs? If not, it's possible that there's still an open connection to GCS when we re-run the partition's generated code gain.; 2. Hail Batch guarantees at least once execution of a job. It's possible that two distinct workers are executing the partition's generated code. Does the Hail Query generated code create a distinct file name for each execution? (I think the answer is yes.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410,1,['error'],['error']
Availability,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8494:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/8494,2,"['Error', 'error']","['Error', 'error']"
Availability,"The error occurred on a gzipped VCF, and went away after converting to bgzipped.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301357111:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301357111,1,['error'],['error']
Availability,The error on Azure is unrelated and will post on Zulip.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144089195:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144089195,1,['error'],['error']
Availability,The error should come from logistic_regression_rows and it should complain that row_idx is row-indexed not col-indexed..,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13788#issuecomment-1755689086:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/13788#issuecomment-1755689086,1,['error'],['error']
Availability,"The error this fixes is in the line: ; `if not step.run_if_requested or step.name in requested_step_names`. when `requested_step_names is None`, you cannot check if a value is `in` it. I chose not to make the default value of `requested_step_names = []`, since it's dangerous to do that in python (that single mutable list will be shared across invocations of the constructor)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7793:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/7793,1,['error'],['error']
Availability,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6545:4,error,errors,4,https://hail.is,https://github.com/hail-is/hail/issues/6545,1,['error'],['errors']
Availability,"The failure doesn't appear to be related to my changes. Installing the docker requirements, which has `setuptools>=38.6.0`, is trying to upgrade to the latest setuptools (56.0.0). Another dependency might be forcing the upgrade. However, setuptools was installed via apt, not pip, and that is causing this:. ```; Attempting uninstall: setuptools; Found existing installation: setuptools 45.2.0; Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr; Can't uninstall 'setuptools'. No files were found to uninstall.; ```. So there's two things I don't understand. I'll keep investigating. I glad I PRed this separately!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733,1,['failure'],['failure']
Availability,"The failure here looked intermittent, a jar failed to download for a batch. Ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11811#issuecomment-1119687684:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/11811#issuecomment-1119687684,2,"['down', 'failure']","['download', 'failure']"
Availability,"The failure here:; ```; E org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (hostname-c5956f6f02 executor driver): java.io.EOFException: Invalid seek offset: position value (6) must be between 0 and 6 for 'gs://hail-services-requester-pays/hello'; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.validatePosition(GoogleCloudStorageReadChannel.java:665); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.position(GoogleCloudStorageReadChannel.java:546); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFSInputStream.seek(GoogleHadoopFSInputStream.java:178); E 	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65); ```. I hit this same error in Avro/GVS work recently -- I think the Google Hadoop API connector is wrong in that you cannot seek to the end of a file (N where N is the number of bytes in the file).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700,4,"['error', 'failure']","['error', 'failure']"
Availability,The failure is due to the new memory requirements. Apparently the python script uses a lot more memory than I thought. Trying to find the magic number now. The tests were passing before the memory limits.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832,1,['failure'],['failure']
Availability,"The failure is in apiserver somehow, probably trivial. Can't id the problem at the moment because ci is giving 401.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6078#issuecomment-494399462:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/6078#issuecomment-494399462,1,['failure'],['failure']
Availability,"The failure was a test_query one, I kicked it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9967#issuecomment-771112056:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/9967#issuecomment-771112056,1,['failure'],['failure']
Availability,"The final failures were doctests that called show on tables with duplicated keys, where the order of the duplicate rows, which we don't guarantee, were different. I disabled those tests. Also addressed comments, should be good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849:10,failure,failures,10,https://hail.is,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849,1,['failure'],['failures']
Availability,"The first series addressing https://github.com/hail-is/hail/issues/6952. If we like this, will implement Coalesce Node in a similar manner. As part of this NA node changes, so that elements of collections are set to required (effectively a hoop when taking the boolean and of requireness on element types of non-NA nodes). Implemented and tested for every collection type, besides PNDArray, because we currently don't support arrays of NDArray. This also fixes the ToDict node inference, which requires the union of top-down and bottom-up element requiredeness inference. cc @patrick-schultz @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6990:520,down,down,520,https://hail.is,https://github.com/hail-is/hail/pull/6990,1,['down'],['down']
Availability,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:599,error,error,599,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['error'],['error']
Availability,"The fix for Safari will take effect the next time the docs are deployed. In the meantime,; the docs are indeed downloaded, but Safari tells you there was a problem. If users navigate; to the Downloads directory, the file should indeed be present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10302:111,down,downloaded,111,https://hail.is,https://github.com/hail-is/hail/pull/10302,2,"['Down', 'down']","['Downloads', 'downloaded']"
Availability,"The flags PR fixed the test failures!!! I think the last thing is being in agreement on what tests are needed. As painful as it is, I think we should spin up a dataproc cluster, run VEP and save the output into the test_resources folder and use that for the test. Before I do that, is there anything else I need to add?. The last thing for this PR is to modify the cloud run functions for ACR cleanup to cleanup the vep images generated by CI. We can make a follow up PR for Azure once we've transferred the data to the public storage source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1446698641:28,failure,failures,28,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1446698641,1,['failure'],['failures']
Availability,"The flaky test failure can be seen here: https://ci.hail.is/batches/6627486/jobs/105; ```; During handling of the above exception, another exception occurred:. @skip_unless_service_backend(); def test_tiny_driver_has_tiny_memory():; try:; hl.utils.range_table(100_000_000, 50).to_pandas(); except Exception as exc:; # Sometimes the JVM properly OOMs, sometimes it just dies.; > assert (; 'java.lang.OutOfMemoryError: Java heap space' in exc.args[0] or; 'batch.worker.jvm_entryway_protocol.EndOfStream' in exc.args[0]; ); E assert ('java.lang.OutOfMemoryError: Java heap space' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:15,failure,failure,15,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['failure'],['failure']
Availability,"The following error occurs when trying to liftover GRCh37 to GRCh38 for the UK Biobank GWAS for chr22. Any suggestions for this?. The code used is:. ```; chr=sys.argv[1]. bgen=""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr""+chr+""_v3.bgen""; sample=""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr""+chr+""_v3_s487327.sample""; # hl.index_bgen(bgen); mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); print(mt.describe()); rg37 = hl.get_reference('GRCh37'); rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('file:///restricted/projectnb/ukbiobank/ad/analysis/liftover/grch37_to_grch38.over.chain.gz', rg38); mt = mt.annotate_rows(new_locus=hl.liftover(mt.locus, 'GRCh38'), old_locus=mt.locus); mt = mt.filter_rows(hl.is_defined(mt.new_locus)); # mt = mt.key_rows_by(locus=mt.new_locus); print(mt.describe()); mt = mt.key_rows_by(locus=mt.new_locus,alleles=mt.alleles); print(mt.describe()); hl.export_vcf(mt,""/project/ukbiobank/imp/uk.v3.GRCh38/uk.v3.r38.chr""+chr+"".vcf.bgz""); ```. ```; Version 0.2.19-c6ec8b76eb26; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/liftover/hail-20200214-1434-0.2.19-c6ec8b76eb26.log; 2020-02-14 14:35:19 Hail: INFO: Number of BGEN files parsed: 1; 2020-02-14 14:35:19 Hail: INFO: Number of samples in BGEN files: 487409; 2020-02-14 14:35:19 Hail: INFO: Number of variants across all BGEN files: 1255683. Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'new_locus': locus<GRCh38>; 'old_locus': locus<GRCh37>; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; None; 2020-02-14 14:35:22 Hail: WARN: export_vcf: ignored the following fields:; 'varid' (row); 'ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['error'],['error']
Availability,"The goal of this PR is to have all of the JVM container logs available where all the worker logs are. I tagged the entries with ""worker.log"" so they show up with the other worker log entries. However, it's plain text with no timestamp. We can improve the formatting as a separate project. Notice the two entries with ""*"" on the left instead of the normal ""I"". The design choice I made is to have the JVM containers write to a location that is static. We cannot easily change the fluentd configuration dynamically. It requires restarting the daemon which takes 1.5 seconds. Furthermore, the configuration for fluentd is on /etc/ on the host which the batch worker container cannot access. Hence, why I took the approach of specifying it in the startup script at known locations. . Before we merge this, I'd like to confirm that (a) we want these logs and (b) they don't contain any secrets.; <img width=""1585"" alt=""Screenshot 2023-06-16 at 4 06 43 PM"" src=""https://github.com/hail-is/hail/assets/1693348/0ce9f7dc-1188-4c66-ae6f-83fcc3744f95"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13190:61,avail,available,61,https://hail.is,https://github.com/hail-is/hail/pull/13190,1,['avail'],['available']
Availability,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:540,down,download,540,https://hail.is,https://github.com/hail-is/hail/pull/10314,2,['down'],"['download', 'downloading']"
Availability,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:539,down,download,539,https://hail.is,https://github.com/hail-is/hail/pull/10390,2,['down'],"['download', 'downloading']"
Availability,"The image below describes what I mean. The brown files are not tracked by git, because these are generated by the sphinx autosummary directives. However, they're still used as a base when we build the website. I've run into several errors today with broken references (from other git branches!) which were resolved by deleting all these untracked rsts and rebuilding. Ideally, gradle clean would delete all these, or we would copy this directory to a temporary staging area before starting the build. ![image](https://cloud.githubusercontent.com/assets/10562794/25728126/21712204-30fb-11e7-8938-964b68c940e6.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1766:232,error,errors,232,https://hail.is,https://github.com/hail-is/hail/issues/1766,1,['error'],['errors']
Availability,The improved error message isn't working because we catch TypeError instead of ExpressionException,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467,1,['error'],['error']
Availability,The invalid grant error below this line is wrapped in a `StorageException`. We want to open up these wrapping exceptions to see if there's a true error underneath.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11963:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/11963,2,['error'],['error']
Availability,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:190,ERROR,ERROR,190,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"The king toolset outside of hail seems to have a feature that allows the user to generate a tree based on the results. Is there a similar feature available for [king()](https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.king) in hail or an alternative to use on hail? For reference, information on the visualization can be found [here](https://www.kingrelatedness.com/KINGvisualization.shtml). Thank you.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12489:146,avail,available,146,https://hail.is,https://github.com/hail-is/hail/issues/12489,1,['avail'],['available']
Availability,"The last link has the right answer. For reasons not known to me, you must destroy the service and recreate the service to get correct behavior. You know you have correct behavior when the TCP Load Balancer in the GCP console shows most of your instances as unhealthy (because most of them are not hosting the service in question). This lead to at most 15 minutes of downtime and probably like 10 minutes, which seems unacceptable to me, but 🤷‍♀",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289:366,downtime,downtime,366,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289,1,['downtime'],['downtime']
Availability,"The left and right sources may provide more fields than are necessary. This is OK, but; previously this caused an error because `filterSet` expects the argument to be a subset; of the `self` argument.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10077:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/pull/10077,1,['error'],['error']
Availability,"The logic trying to infer the version of various Spark dependencies; was total garbage and almost certainly except for a few specific; cases. I was feeling aggressive. I nuked it. If we want to support; building with other versions of Spark reliably (whcih we don't test)`,; we should find another way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8676:241,reliab,reliably,241,https://hail.is,https://github.com/hail-is/hail/pull/8676,1,['reliab'],['reliably']
Availability,"The main change is to the communication protocol between the client and; the driver and between the driver and the worker. In main, both the driver and the client send messages back to the client; and driver (respectively) by writing to a file in cloud storage. In both; cases, the file (in main) has one of these two structures:. 0x00 # is_success (False); UTF-8 encoded string # the stack trace. 0x01 # is_success (True); UTF-8 encoded string # JSON message to send back to the client or driver. In this PR, the success case does not change. The failure case becomes:. 0x00 # is_sucess (False); UTF-8 encoded string # short message; UTF-8 encoded string # expanded message; 4-byte signed integer # error id. The Python client (in `service_backend.py`) and the Driver (in; `ServiceBackendSocketAPI2`) changes to read this and raise the right error if; an error id is present. I also uncovered three unrelated problems that are fixed in this PR:; 1. PlinkVariant needs to be serializable because it is broadcasted.; 2. We open an input stream in LoadPlink which ought to be closed, but there is no mechanism to do so in the ServiceBackend. I just ignore it for now. cc: @tpoterba, I'm not sure what the right answer is here.; 3. Two uses of the broadcasted file system that should use the ExecuteContext's file system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11624:548,failure,failure,548,https://hail.is,https://github.com/hail-is/hail/pull/11624,4,"['error', 'failure']","['error', 'failure']"
Availability,"The main goal here was to flatten out the aggregator states in the tuple of aggregator states, so that we could e.g. inline the value of a prevnonnull aggregator instead of storing a pointer to the state. The big change that I made was in creating a `TupleAggregatorState` that knows its own offset so that when we initialize a state, we can initialize the value offset directly from the value of the container. (The previous StateContainer got renamed `StateTuple` and was slimmed down accordingly.) I think I eventually want `TupleAggregatorState` to implement the `AggregatorState` interface; I haven't pushed it there yet because I haven't needed to, but I think it would fit a lot better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7110:482,down,down,482,https://hail.is,https://github.com/hail-is/hail/pull/7110,1,['down'],['down']
Availability,"The movie lens dataset (used by some of the tutorials, not hosted by us) failed to download. It happens sometimes. I pushed an empty commit to have it retest. In general, if a PR looks good but the tests are failing, I approve. The robots will handle the tests, so I don't have to. If fixing a bug requires significant changes, or changes not in the spirit of the original PR, as an author, I dismiss the review and request another one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431:83,down,download,83,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431,1,['down'],['download']
Availability,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8987:208,error,errors,208,https://hail.is,https://github.com/hail-is/hail/pull/8987,2,['error'],['errors']
Availability,"The old bucket did not use uniform access control and also was multi-regional (us). I created a new bucket using the random suffix ger0g which has uniform access control. I also switched the location to us-central1 (not pictured here because that is a variable). I copied all the JARs from `gs://hail-query/jars` to `gs://hail-query-ger0g/jars` using a GCE VM. Again, global-config is not present in our terraform, so I'll have to manually edit that to reflect this new location: `gs://hail-query-ger0g`. The deployment process is:. 1. Edit global-config to reflect new bucket.; 2. Delete batch and batch-driver pods.; 3. Delete old workers. The rollback process (if necessary) is the same. Since this requires wiping the workers, I'll wait for a time when no one is on the cluster to do it. Any users using explicit JAR URLs will need to switch to `gs://hail-query-ger0g/...`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12969:646,rollback,rollback,646,https://hail.is,https://github.com/hail-is/hail/pull/12969,1,['rollback'],['rollback']
Availability,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9307:243,rollback,rollback,243,https://hail.is,https://github.com/hail-is/hail/pull/9307,1,['rollback'],['rollback']
Availability,The old error message for matmul was impossible to read. This is much clearer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10298:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/10298,1,['error'],['error']
Availability,The one test failure was a transient blob Not Found on Azure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1372795141:13,failure,failure,13,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1372795141,1,['failure'],['failure']
Availability,"The only place outside of tests where `combine_gvcfs` is called is `drive_combiner`. And no user would have had the opportunity to use the old parameter, which was a workaround for a bug in GATK that's no longer relevant with the way the combiner output is used by downstream. Also this is still _highly experimental_ functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610:265,down,downstream,265,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610,1,['down'],['downstream']
Availability,The only related problem I can find is a bad error when trying to drop a key field from tables.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673#issuecomment-392912868:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/issues/3673#issuecomment-392912868,1,['error'],['error']
Availability,"The only remaining references are in the datasets scripts, but those are meant as references of how we created the files, and in the `hailctl dataproc` command. I chose not to change the latter because I fear some users might still have ancient versions of `gcloud`. The reviewer should verify that I got the right version of the `gcloud` command in each case. Hopefully this resolves the bizarre error we see in test-dataproc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12781:397,error,error,397,https://hail.is,https://github.com/hail-is/hail/pull/12781,1,['error'],['error']
Availability,"The original code now gives a more useful error. I'm closing as wontfix. ```; TypeError: flatten: parameter 'collection': expected expression of type set<set<any>> or array<array<any>>, found <ArrayExpression of type array<ndarray<int32, 1>>>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659,1,['error'],['error']
Availability,"The original goal of this PR was avoiding `Try` when we are not using the restartability provided by semantic hashing because I strongly suspect it is related to the loss of stacktraces in exceptions. Unrelatedly, we realized the semantic hash PR changed the semantics of Query-on-Spark even when semantic hash is disabled: previously we would abort RDD writing on the first exception. In Hail 0.2.123 through 0.2.126, the semantics were changed to only crash *after* we already ran every other partition. Two bad scenarios of which I can think:. 1. Suppose the first partition fails due to OOM. We now waste time/money on the rest of the partitions even though we cannot possibly get a valid output. 2. Suppose every partition hits a permission error. Users should get that feedback after paying for O(1) partitions run, not O(N). I created two Backend paths: the normal `parallelizeAndComputeWithIndex` with its pre-0.2.123 semantics as well as `parallelizeAndComputeWithIndexReturnAllErrors` which, as the name says, returns errors instead of raising them. While making this change, I think I found two other bugs in the ""return all errors"" path, only one of which I addressed in this PR:. 1. I'm pretty sure semantic-hash-enabled QoB batch submission is broken because it uses the logical partition ids as job indices. Suppose there are 10,000 partitions, but we only need to compute 1, 100, and 1543. 0.2.126 would try to submit a batch of size 3 but whose job indices are 1, 100, and 1543. 2. Likewise, the Query-on-Spark path returns an invalid `SparkTaskContext.partitionId` which, at best, produces confusing partition filenames. I only fixed the former because it was simple to fix. I wasn't exactly sure what to do about the latter. We should fix that separately because the changes in this PR need to urgently land in the next release to avoid unexpected cost when one partition fails.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14085:746,error,error,746,https://hail.is,https://github.com/hail-is/hail/pull/14085,3,['error'],"['error', 'errors']"
Availability,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10591:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/10591,4,"['down', 'error']","['downloading', 'error']"
Availability,"The original report was about `gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz`. That's the gnomad v2.1.1 GRCh38 liftover sites table. See [this section of the gnomAD downloads](https://gnomad.broadinstitute.org/downloads#v2-liftover). In particular it is the ""All chromosomes VCF"". That's 85GiB, so I don't want to download it. I believe the chr21 VCF should have just as many row, column, and entry fields, so I downloaded that and tested Hail's ability to import and write it. ```bash; gsutil -m cp \; gs://gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; .; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. With Hail 0.2.108-fc03e9d5dc08 it worked fine. It also worked fine on a recent 0.2.120 development version I had installed. Next I tried running on the first few thousand lines of the full sites table:. ```bash; curl \; https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; | bgzip -d -c\; | head -n 10000 \; | bgzip -c \; > /tmp/head-sites.vcf.bgz; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('/tmp/head-sites.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. This also succeeded with Hail 0.2.108",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525:170,down,downloads,170,https://hail.is,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525,4,['down'],"['download', 'downloaded', 'downloads']"
Availability,"The output of ht.describe():; ```. ----------------------------------------; Global fields:; 'downsamplings': array<int32> ; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'freq': array<struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>, ; meta: dict<str, str>; }> ; 'project_max': array<struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>, ; project: str; }> ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3668#issuecomment-392342812:94,down,downsamplings,94,https://hail.is,https://github.com/hail-is/hail/issues/3668#issuecomment-392342812,1,['down'],['downsamplings']
Availability,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8263:454,down,downcast,454,https://hail.is,https://github.com/hail-is/hail/pull/8263,1,['down'],['downcast']
Availability,The plink website gave us a 503. I retried again and have a PR (https://github.com/hail-is/hail/pull/11649) which fixes our plink download to actually retry (by using curl instead of wget).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11622#issuecomment-1076523299:130,down,download,130,https://hail.is,https://github.com/hail-is/hail/pull/11622#issuecomment-1076523299,1,['down'],['download']
Availability,"The pods currently look like this:. ```; site-deployment-5b5697d6bb-2p5rk 1/1 Running 0 6h; site-deployment-69f686bf7f-266kg 0/1 ContainerCreating 0 2h; ```. The problem is that site mounts a volume RWO with the certs. The problem is the second pod can't launch for seamless upgrade because it can't mount the volume. There is a further discussion here: https://github.com/kubernetes/kubernetes/issues/26567. Short-term fix: we could delete the pod and recreate on upgrade, which would lead to a short window of downtime. Long-term fix: Normally the certs are only read by nginx, but need to be written by the certbot renew cron job. We could keep the certs in a volume. We could put a copy of the certs in a secret which can be mounted by multiple pods (e.g. site including when it is upgraded). Then we run certbot periodically in its own pod and update the original certs stored in the volume. After it runs, we create a new secret (or upgrade it if we can) with the new certs and then do a seamless upgrade on site.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4463:512,downtime,downtime,512,https://hail.is,https://github.com/hail-is/hail/issues/4463,1,['downtime'],['downtime']
Availability,The previous PR has merged. Could you remove the Stacked PR label and rebase / drop any redundant commits? I thought GitHub wouldn't show the extra diff at this point but *shrug*,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699:88,redundant,redundant,88,https://hail.is,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699,1,['redundant'],['redundant']
Availability,"The previous formula, `(1/n) (sumsq - (2 * mean * sum) + (n * mean^2))` was weirdly redundant, since `mean * sum == n * mean^2`. This was added by #6728, which ported stats from Scala, but the weird formula did not come from the Scala implementation. I have no clue where this came from. The only reason for doing something like this might be for improved numerical stability, but that doesn't seem to be the case here. In fact, this current method of computing the variance (pre or post this pr) is known to be unstable when the mean is much larger than the stdev (as is easy to see: you're subtracting two nearly equal positive numbers). The Scala implementation used the spark `StatCounter`, which uses the more stable [Welford's algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm). We should probably fix any variance or stdev computation to use a stable method, which I think requires a dedicated stats aggregator that maintains count, mean, and variance in a smart way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10132:84,redundant,redundant,84,https://hail.is,https://github.com/hail-is/hail/pull/10132,1,['redundant'],['redundant']
Availability,"The previous idiom was mapAnnotations(...).copy(vaSignature =; newVASignature), but this results (temporarily) in a VDS with an; incorrect va type that which causes problems for downstream changes; and assertions like typecheck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2188:178,down,downstream,178,https://hail.is,https://github.com/hail-is/hail/pull/2188,1,['down'],['downstream']
Availability,"The previous implementation, while seeming to be well-abstracted at; first, actually had a rather devious property of creating agg states; for multiple classes multiple times. I'm still working on figuring out; *exactly* the place where our assumptions broke down, but this change; definitely fixes the problem, and simplifies the implementation by; directly using IR, instead of other compiled functions. This problem was a symptom of a larger issue, which is that the; ownership semantics of the current aggregator system is way too complex; to be coding against regularly. This all will go away when lowering is; complete, in favor of the *much* simpler set of IR nodes that are used; in lowering. We may need to address this problem sooner, though. CHANGELOG: Fixed memory leak affecting `Table.annotate` with scans, `hl.experimental.densify`, and `Table.group_by` / `aggregate`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9028:259,down,down,259,https://hail.is,https://github.com/hail-is/hail/pull/9028,1,['down'],['down']
Availability,The problem is that `gsutil` only works with python2. So you'll need a python2 binary available as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419854054:86,avail,available,86,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419854054,1,['avail'],['available']
Availability,"The problem was the eventually consistency of the object store. You could delete a large vds, make a new one at the same path with fewer partitions, and read the new one and see an error because partition 100010 was still there",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/859#issuecomment-317578194:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/issues/859#issuecomment-317578194,1,['error'],['error']
Availability,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:440,error,error,440,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972,1,['error'],['error']
Availability,"The real change here is changing the preemptible pool config from `preemptible = true` to `spot = true`, but the `spot` config was only available in the new provider which involved a major version upgrade. The only incompatibility was the addition of an explicit `project` input to `google_project_iam_member`, as opposed to picking it up from the provider configuration. Tested just now in my own project. If one wants to apply this change without incurring downtime for preemptible deployments, they should follow the instructions outlined in the [migrating node pools dev-docs](https://github.com/hail-is/hail/blob/main/dev-docs/kubernetes-operations.md#when-using-terraform).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12127:136,avail,available,136,https://hail.is,https://github.com/hail-is/hail/pull/12127,2,"['avail', 'downtime']","['available', 'downtime']"
Availability,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9163:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/9163,1,['error'],['error']
Availability,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:387,failure,failures,387,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474,3,"['error', 'failure']","['errors', 'failures']"
Availability,The repository in question was created at `2018-10-10T00:32:59Z`. GitHub indicates [no system failures](https://status.github.com/messages) on October the tenth or the ninth.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782:94,failure,failures,94,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782,1,['failure'],['failures']
Availability,The return value of these functions indicates if the containing loop; should wait or if we should immediately re-call the function. This; is intended to be used to allow functions which *know* they have more; work to eagerly invoke themselves again. The use of this variable seems to have been changed to basically always; eagerly re-run during the Azure work. This change restores the original behavior:; 1. Do not wait in job private if we saw 300 records (seems likely there were; 301 or more records in the db).; 2. Do not wait in pool scheduler if we exhaust a user's share. I do not; fully follow the pool scheduler's logic. There might be something; smarter we can do. I think we should really only re-call if we believe; the db contains more ready jobs and we have available cores.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11384:773,avail,available,773,https://hail.is,https://github.com/hail-is/hail/pull/11384,1,['avail'],['available']
Availability,"The root issue here was that sometimes exc.args[0] was a string and sometimes it was a dict. When it was a string the `in` condition worked fine. When it was a dict, it was looking at the keys of the dict and not finding the error message (which is buried under a few layers). The code was unnecessarily complex. I reworked the yaml printer to be simpler and work for any multiline string. I removed the regular expression that was used to discover the worker batch when the worker jobs were in a different batch from the driver jobs. I remove all specialized debugging information in favor of the general `debug_info` methods on `Batch` and `ServiceBackend`. I also have two clear error cases: if the driver does not write its output file, then something went horribly wrong. We dump all the debug info. If we do not receive valid JSON from the driver, again, something went horribly wrong. We dump all the debug info. The only remaining exceptional case is an error purposely serialized by the QoB driver to us (with or without an error id). In particular, note that we now completely ignore the number of failing or successful jobs. That doesn't matter. If the driver sends us an output file, we use the data found there. If the driver does not send us an output file or sends us an output file without valid JSON, we dump as much debug info as possible. cc: @tpoterba for visibility on your end; cc: @iris-garden because you're in this space (albeit, the bug you're fixing is in the QoB *driver* whereas this is the *client* [nb: *client* is the Python code which starts a batch with a *driver*. A *driver* adds zero or more *worker* jobs to its batch. You're addressing an issue with how the *driver* handles errors from the *workers*. This PR simplifies the logic for how the *client* handles errors from the *driver*.]).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470:225,error,error,225,https://hail.is,https://github.com/hail-is/hail/pull/12470,6,['error'],"['error', 'errors']"
Availability,"The router resolver incorrectly assumed the contents of the `Authorization` header was a session; id. In fact, the structure of that header and X-Hail-Internal-Authorization is:. ```; Bearer SESSION_ID; ```. where `SESSION_ID` is a 44 base64 characters representing a 32 byte secret session id. I also took this opportunity to centralize the parsing of bearer headers as; `gear.maybe_parse_bearer_header`. ---. This caused a failure because router-resolver, when checking that a user is properly authenticated,; would send:. ```; Bearer Bearer SESSION_ID; ```. which failed the 44-byte length check in auth/front_end.py.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9919:425,failure,failure,425,https://hail.is,https://github.com/hail-is/hail/pull/9919,1,['failure'],['failure']
Availability,The script in question is located at: gs://danking/1_Generate_Variant_Stats_NVXvC_v1.py . Ping me if you need access.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741:90,Ping,Ping,90,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741,1,['Ping'],['Ping']
Availability,"The script is running fine on the smaller chromosome 19 to 22 bgen files so far. However, I noticed each were running just 24 cores even though we have 16 nodes * 16 cores each available on the cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439073706:177,avail,available,177,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439073706,1,['avail'],['available']
Availability,"The search bar for the batch docs is broken and just says `Searching…` forever. Tracked it down to a bug in the `sphinx_rtd_theme` dependency that was fixed in a later release. The important files to look at are the `requirements.txt` files not the `pinned-requirements.txt` files as the latter bulk updated a bunch of patch releases when I regenerated them. . In the mess of version conflicts that updating a dependency appears to do here, I also removed `google-cloud-logging` as it appears to be an unused dependency and `pre-commit` because it is optional for developers and had a hard requirement on a `importlib-metadata` version that made it incompatible with other important libraries that we use. I also explicitly pinned `protobuf` as a major version upgrade that wasn't restricted by some google libraries we use broke those same google libraries that added that dependency.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12215:91,down,down,91,https://hail.is,https://github.com/hail-is/hail/pull/12215,1,['down'],['down']
Availability,"The semantics of setting a variable in make are kinda weird, example:. ```make; FOO := foo; BAR ?= bar; .PHONY: echo; echo:; 	echo $(FOO) $(BAR); ```. ```sh; $ FOO=baz make; echo foo bar; foo bar; $ make FOO=baz; echo baz bar; baz bar; $ BAR=baz make; echo foo baz; foo baz; ```. This will fix an issue where ci wasn't actually building with spark 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9556:112,echo,echo,112,https://hail.is,https://github.com/hail-is/hail/pull/9556,6,['echo'],['echo']
Availability,"The setuptools thing was actually a red herring, that error isn't fatal, and there was a version conflict elsewhere. I upgraded urllib3 and requests to resolve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422,1,['error'],['error']
Availability,"The sources for agg transformations weren't being recorded correctly for transformations of hl.agg.count(). This wasn't a problem for e.g. a single-level hl.agg.group_by(key, hl.agg.count()), but was throwing errors when that was nested within other transformations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6740:209,error,errors,209,https://hail.is,https://github.com/hail-is/hail/pull/6740,1,['error'],['errors']
Availability,"The stack trace looks like this now:. ```; Error; Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 73, in check; return self.coerce(to_expr(x)); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 101, in to_expr; dtype = impute_type(e); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 59, in impute_type; raise ExpressionException(""Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array.""); hail.expr.expressions.base_expression.ExpressionException: Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 426, in check_all; arg_ = tc.check(arg, name, argname); File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 75, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/tests/test_expr.py"", line 1085, in test_empty_collection_error_msg; self.assertRaisesRegex(hl.expr.ExpressionException, ""Cannot impute type of empty list."", hl.array([])); File ""<decorator-gen-420>"", line 2, in array; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 494, in _typecheck; args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 436, in check_all; )) from e; TypeError: array: parameter 'collection': expected expression of type set<any> or array<any> or dict<('any', 'any')>, found list: []; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3497:43,Error,Error,43,https://hail.is,https://github.com/hail-is/hail/pull/3497,1,['Error'],['Error']
Availability,"The stack trace reported:. ```; [Stage 7:> (0 + 132) / 194]Traceback (most recent call last):; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/genomes_sites_vcf.py"", line 42, in <module>; sites_vds.write(tmp_vds); File ""/home/teamcity/TeamCityAgent2/work/591c293e3f6bfb1d/python/pyhail/dataset.py"", line 595, in write; File ""/tmp/b54eac62-9ebc-43ff-b49c-80cc77f89aa2/utils.py"", line 211, in run_command; cmd_args); File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o309.run.; : org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:771,error,error,771,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['error'],['error']
Availability,"The standard field registrations lead to TERRIBLE behavior -- . this line in a VCF:; ```; ##INFO=<ID=MQ0,Number=.,Type=Integer,Description=""Number of MAPQ == 0 reads"">; ```. ...gets returned in our code as...; ```; INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ```. ...leading to match errors when we get to the values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2822:324,error,errors,324,https://hail.is,https://github.com/hail-is/hail/issues/2822,1,['error'],['errors']
Availability,The suppressed message is:; ```; 	Suppressed: java.lang.Exception: #block terminated with an error; 		at is.hail.shadedazure.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99); 		at is.hail.shadedazure.reactor.core.publisher.Mono.block(Mono.java:1742); 		at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['error'],['error']
Availability,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:74,error,error,74,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952,1,['error'],['error']
Availability,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9166:1070,echo,echo,1070,https://hail.is,https://github.com/hail-is/hail/pull/9166,2,['echo'],['echo']
Availability,"The test failure here is spurious, happening because batch is going through a transition / upgrade and things are a little broken right now. Will make sure this merges when that's resolved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556,1,['failure'],['failure']
Availability,The test failures were unrelated to the proposed changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1300509857:9,failure,failures,9,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1300509857,1,['failure'],['failures']
Availability,"The test that lists batches timed out. The main problem is the limit in the aioclient used by the test_batch tests was passing a string rather than an integer. I assumed downstream the function was passing an integer. Therefore, we were doing this:. ```; batch_id < ""137""; ```. and not `batch_id < 137`. So the query was running forever and scanning all batches from the `test` user. I also was missing a tag annotation on the queries, but that was not causing the timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046:170,down,downstream,170,https://hail.is,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046,1,['down'],['downstream']
Availability,"The test that lists batches timed out. The main problem is the limit in the aioclient used by the test_batch tests was passing a string rather than an integer. I assumed downstream the function was passing an integer. Therefore, we were doing this:. batch_id < ""137""; and not batch_id < 137. So the query was running forever and scanning all batches from the test user. I also was missing a tag annotation on the queries, but that was not causing the timeout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13237:170,down,downstream,170,https://hail.is,https://github.com/hail-is/hail/pull/13237,1,['down'],['downstream']
Availability,The tests are passing. Failures are from building images.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10497#issuecomment-844462253:23,Failure,Failures,23,https://hail.is,https://github.com/hail-is/hail/pull/10497#issuecomment-844462253,1,['Failure'],['Failures']
Availability,The tests are passing. The error on Azure was cleaning up the base image timed out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1443661758:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1443661758,1,['error'],['error']
Availability,The tests relying on Batch are getting slower because it takes a long time to download and build Docker images and we're putting more load on Batch. This will increase parallelism and reduce test failures due to timeouts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441:78,down,download,78,https://hail.is,https://github.com/hail-is/hail/pull/9441,2,"['down', 'failure']","['download', 'failures']"
Availability,"The tests use multiple threads which can race to download the references. This is a bit; of a blunt fix. In particular, this is not an asyncio-friendly lock (because I need; thread safety, which asyncio.Lock does not provide). In general, users should not try; to initialize hail multiple times in different coroutines in the same thread.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11949:49,down,download,49,https://hail.is,https://github.com/hail-is/hail/pull/11949,1,['down'],['download']
Availability,"The treatment of `ClientPayloadError` as a sometimes transient error was originally made in response to [an existing issue](https://github.com/aio-libs/aiohttp/issues/4581) in aiohttp that can cause transient errors on the client that are difficult to distinguish from a real broken server. What's in `main` matched exactly on the error message, but that error message has [since changed](https://github.com/aio-libs/aiohttp/commit/dc38630b168a169139974617d75e176530c91696) to include more information, breaking our transient error handling. This change relaxes the requirement of the error response string to fix transient error handling for our current version of `aiohttp`. I wish I had a better approach. `ClientPayloadError` can also be thrown in the case of malformed data, so I am reticent to treat it as always transient, but we could perhaps make it a `limited_retries_error` and avoid inspecting the error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14545:63,error,error,63,https://hail.is,https://github.com/hail-is/hail/pull/14545,8,['error'],"['error', 'errors']"
Availability,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:222,down,down,222,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500,1,['down'],['down']
Availability,The user on the forum tried a wheel I made for them and they encountered the exact same error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10915#issuecomment-931609566:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/pull/10915#issuecomment-931609566,1,['error'],['error']
Availability,"The way that the off-heap-memory-fraction argument currently works; limits total memory usage in hail value heavy (like lowered) pipelines; immensely. The default settings both reserve AND and limit hail off heap; allocations to 60% of executor's memory. This behavior is almost never; what a user wants as it will reduce total memory that they can use. We; can retain some of the characteristics that these limits give us by; reserving off-heap-memory-fraction as overhead, and setting the; worker_off_heap_memory_per_core to be the total available memory per; core. This should still give good error messaging on attempts to; allocate too much memory for hail values while allowing us to use all; the memory we have available. A flag, --off-heap-memory-hard-limit, has been added to preserve the; previous behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11531:540,avail,available,540,https://hail.is,https://github.com/hail-is/hail/pull/11531,3,"['avail', 'error']","['available', 'error']"
Availability,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:682,down,down,682,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,1,['down'],['down']
Availability,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8784:14,error,errors,14,https://hail.is,https://github.com/hail-is/hail/pull/8784,3,['error'],"['error', 'errors']"
Availability,"There are two problems with this:; - it is a massive de-optimization if we forward an expensive computation into a loop (e.g. arraymap); - in the above case, it is an error in if the body of the let is non-deterministic. To do this right we need to build a control flow graph.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4497:167,error,error,167,https://hail.is,https://github.com/hail-is/hail/pull/4497,1,['error'],['error']
Availability,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/840:137,down,downloading,137,https://hail.is,https://github.com/hail-is/hail/issues/840,1,['down'],['downloading']
Availability,"There is a [known issue](https://github.com/moby/moby/issues/41792) with the official Docker deb. If you uninstall docker and re-install it later, it might fail to start. The root cause is the `docker.socket` `systemd` unit failing to start because there are ""insufficient file descriptors available"". I think this is confusing verbiage. The socket's name must be `/var/run/docker.sock`. Clearly, if that filename is already in use, we cannot create a new socket at that filename. One of Google's [""Dataproc components""](https://cloud.google.com/dataproc/docs/concepts/components/overview) is Docker. I believe Google installed and then uninstalled docker in this image, thus leaving it in the broken state. For evidence of that:. <details>; <summary> find docker on a worker node of a *non-Hail* Dataproc cluster</summary>. ```; sudo find / -iname '*docker*'; ```. ```; /opt/conda/miniconda3/pkgs/dbus-1.13.6-h5008d03_3/info/recipe/patches/0004-disable-fd-limit-tests-not-supported-in-docker.patch; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/docker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:290,avail,available,290,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['avail'],['available']
Availability,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1019,alive,alive,1019,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857,3,"['alive', 'down']","['alive', 'download']"
Availability,"There is always a race because `crun` will delete the cgroup once the container completes. Looks like the memory tracking checks for this exact error but not cpu, it would assume it should be both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13861#issuecomment-1771461424:144,error,error,144,https://hail.is,https://github.com/hail-is/hail/issues/13861#issuecomment-1771461424,1,['error'],['error']
Availability,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:837,avail,available,837,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448,1,['avail'],['available']
Availability,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:772,error,error,772,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['error'],['error']
Availability,"There is still work to do here, but it is now complete enough that InterpretSuite can be run properly on a minimal example. Current TODOs:; - [x] Add C++ emit; - [x] Add Python api (experimental, for now); - [x] Proper type checking in python? *yes, but no type inference*; - [ ] Test ALL failure pathways; - [x] Mismatched Number of args between `Loop` and matching `Recur`; - [x] Mismatched types of args between `Loop` and matching `Recur`; - [ ] Infinite loop detection; - [ ] Not tail recursive detection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5228:289,failure,failure,289,https://hail.is,https://github.com/hail-is/hail/pull/5228,1,['failure'],['failure']
Availability,"There must be a blank line after three dashes, otherwise you get this error:; ```; [WARNING] Could not parse YAML metadata at line 64 column 1: :8:0: Unexpected '-'. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11960:70,error,error,70,https://hail.is,https://github.com/hail-is/hail/pull/11960,1,['error'],['error']
Availability,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8504:240,error,error,240,https://hail.is,https://github.com/hail-is/hail/pull/8504,1,['error'],['error']
Availability,"There was a logic error in constructFromIndicesUnsafe, if a missing value was pushed, pushing a present value with the same index would not clear the missing bit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13263:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/13263,1,['error'],['error']
Availability,There was a series of PRs that addressed most of this but there's still one issue. The cancel thread frequently logs an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1773562874:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1773562874,1,['error'],['error']
Availability,There was a subsequent error https://github.com/hail-is/hail/pull/8403,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606698835:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606698835,1,['error'],['error']
Availability,There were a few places where things that had `NAMESPACE` didn't use it in certain places. I also fixed that. I also standardized the error verbiage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9695:134,error,error,134,https://hail.is,https://github.com/hail-is/hail/pull/9695,1,['error'],['error']
Availability,"There were two sources of deadlocks:. 1. The attempt resources were not inserted in the same order in the aggregated_*_resources tabes between the two triggers `attempt_resources_after_insert` and `attempts_after_update`. One had jobs -> batches -> billing projects and the other had billing_projects -> batches -> jobs. We also were inserting the resources in a different order in the two triggers. I solved the ordering issue by making sure we `INSERT MANY` the resources in alphabetical order. 2. Once I fixed (1), then the next set of errors were in `add_attempt`. We were locking the `instances_free_cores_mcpu` table only if the attempt didn't already exist. This was causing cryptic deadlock errors like this:. ```; ------------------------; LATEST DETECTED DEADLOCK; ------------------------; 2022-06-23 18:08:12 0x7f1807665700; *** (1) TRANSACTION:; TRANSACTION 1215034153, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 21 lock struct(s), heap size 1136, 12 row lock(s), undo log entries 5; MySQL thread id 962402, OS thread handle 139741222766336, query id 6809292838 10.32.5.50 jigold updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1578686 page no 3 n bits 72 index PRIMARY of table `jigold`.`instances_free_cores_mcpu` trx id 1215034153 lock_mode X locks rec but not gap waiting; Record lock, heap no 3 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d6a69676f6c642d7374616e646172642d62; asc batch-worker-jigold-standard-b; (total 34 bytes);; 1: len 6; hex 0000486bf32c; asc Hk ,;;; 2: len 7; hex 600001287513cb; asc ` (u ;;; 3: len 4; hex 80002de6; asc - ;;. *** (2) TRANSACTION:; TRANSACTION 1215034156, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 22 lock struct(s), heap size 1136, 13 row lock(s), undo log entries",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11959:539,error,errors,539,https://hail.is,https://github.com/hail-is/hail/pull/11959,2,['error'],['errors']
Availability,"There's a few things happening here:. ### Node pool updates through terraform; I extended the node pool update documentation with how to deal with terraform-managed node pools. This is what I did on Azure and worked fine. The only real change in terraform other than changing the machine type is making the node pool name configurable to adhere to the naming guidelines and allow us to do a rolling migration. ### Updated the kubernetes and azurerm providers; I updated the azurerm provider without thinking much about it and even though it's a minor version had some breaking changes that after a half-successful `apply` made it hard to downgrade. So I decided just to appease the breaking change and leave us at the new version, which is what all the `blob_properties` changes are for. They are in no way related to the node pools. ### Troubleshooting; I added a section for a bug that I've seen a couple of times (and encountered again today) but never documented how I got around it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11636:638,down,downgrade,638,https://hail.is,https://github.com/hail-is/hail/pull/11636,1,['down'],['downgrade']
Availability,There's a new assertion error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3424#issuecomment-383742072:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/pull/3424#issuecomment-383742072,1,['error'],['error']
Availability,There's a seemingly endless stream of errors hidden by not testing impute_sex...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2952#issuecomment-369289579:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/pull/2952#issuecomment-369289579,1,['error'],['errors']
Availability,"There's a small CI feature where you can randomly assign someone from services and/or compiler team to review a PR by including a directive in the github PR body, e.g. #assign compiler. There's a slight bug where if the PR body is left blank, GitHub will report it as `None` instead of what I assumed would be """", which breaks the lines like `if ASSIGN_SERVICES in self.body` with the error that `None` is not iterable. This just inserts a guard against that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10767:385,error,error,385,https://hail.is,https://github.com/hail-is/hail/pull/10767,1,['error'],['error']
Availability,"There's not a good reason, this is just how it was originally designed. Whenever a job was cancelled, it would take until the start of the next step for a container's execution to be stopped. I replaced the dependency on this in `Container` with `run_until_done_or_deleted`, but stopped short of deleting the functionality entirely because there were other parts of the worker, specifically `JVMJob` that still relied on it. Hopefully that is no longer the case after the QoB changes, but @danking would know better. We've also both lamented about how it's impossible to use timings currently inside cleanup blocks because it could accidentally re-raise a deleted error. This is a great change, I would just take extra care to test job cancellation to make sure there isn't anywhere that's still relying on this functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382:664,error,error,664,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382,1,['error'],['error']
Availability,There's some major test errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343,1,['error'],['errors']
Availability,There's something fishy happening in the service backend. It keeps timing out the same three test jobs. We'll need to do a little grep+diff to figure which tests are getting started but not terminating. I suspect we'll need to either skip something or ensure that it errors early.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11345#issuecomment-1057654251:267,error,errors,267,https://hail.is,https://github.com/hail-is/hail/pull/11345#issuecomment-1057654251,1,['error'],['errors']
Availability,"There's still a compilation error. Otherwise, it's good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3751#issuecomment-397052753:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/3751#issuecomment-397052753,1,['error'],['error']
Availability,"There's the Python process running the benchmark driver Hail client in Python, and the java process running the hail backend. We want the benchmark driver to return 0 in all cases, with information about whether the hail command finished successfully and how long it took. In current main, this works well for successful runs as well as errors that don't kill the backend (assertion, HailException, etc). It doesn't work for an error that kills the backend (segfault / oom), because while the exception handler in run() works fine, we run stop() unhandled and assume it won't throw (it only throws if the JVM is dead).. In that case, this PR lets the benchmark driver return 0 after writing a file indicating the Hail function failed, just like in an assertion error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527658197:337,error,errors,337,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527658197,3,['error'],"['error', 'errors']"
Availability,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10946:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/10946,1,['error'],['error']
Availability,"These changes include a performance regression - instead of allocating memory once and filling in each member of the nested value (srvb) we are using struct constructors that allocate e.g. the locus an extra time and copy it into the container. I do not think it's worth creating constructors right now that prevent this regression -- the region value construction is much slower than the java calls here, and the right solution is coming down teh pike -- constructing containers using SStackStruct emit codes will have exactly the semantics we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644:439,down,down,439,https://hail.is,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644,1,['down'],['down']
Availability,"These functions have the same bytecode signature, and I'm tired of having cryptic compile errors due to changing imports. Now coerce is just for code/value/settable, and tcoerce is for types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12333:90,error,errors,90,https://hail.is,https://github.com/hail-is/hail/pull/12333,1,['error'],['errors']
Availability,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/597:397,error,errors,397,https://hail.is,https://github.com/hail-is/hail/issues/597,6,['error'],"['error', 'errors']"
Availability,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8812:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/pull/8812,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555#issuecomment-238118077:146,down,down,146,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077,1,['down'],['down']
Availability,Think this should resolve this [error](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22%0Aseverity%3DERROR;timeRange=PT3H?authuser=2&project=hail-vdc) where `auth-driver` is crashing on start,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9949:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/9949,1,['error'],['error']
Availability,"This ArrayIndexOutOfBounds is caused by a larger design flaw in Tables and ordering, which was previously masked by the horrible coerce-or-shuffle-every-time we were doing. Will try to nail it down today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3637#issuecomment-392746870:106,mask,masked,106,https://hail.is,https://github.com/hail-is/hail/pull/3637#issuecomment-392746870,2,"['down', 'mask']","['down', 'masked']"
Availability,"This CSV file is lacking any commas, it uses semicolons instead. https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv I don't think hail should generate this error message:. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-5-2e52d209a59b> in <module>; ----> 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; 3 broken_ht[:3]. </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-1110> in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/methods/impex.py in import_table(paths, key, min_partitions, impute, no_header, comment, delimiter, missing, types, quote, skip_blank_lines, force_bgz); 1326 jt = Env.hc()._jhc.importTable(paths, key, min_partitions, jtypes, comment,; 1327 delimiter, missing, no_header, impute, quote,; -> 1328 skip_blank_lines, force_bgz); 1329 return Table._from_java(jt); 1330 . ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/utils/java.py in deco(*args, **kwargs); 225 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 226 'Hail version: %s\n'; --> 227 'Error summary: %s' % (deepest, full, hail.__versi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5221:172,error,error,172,https://hail.is,https://github.com/hail-is/hail/issues/5221,1,['error'],['error']
Availability,"This PR attempts to make CI more useful as a developer feedback tool. Developers need to know the success/failure state of their PR ASAP. It is better to know that state for an old tip sha pair than to know nothing. It is better still to know that state for a more recent sha pair. It is best to know that state for the tip sha pair. We aim to test a PR's tip source sha (perhaps against an out of date target sha). ---. Our *target state* for remembering batches is:; - one is complete and the other is in-progress; the complete one is for an out-of-date tip sha pair, or ; - only one batch is in-progress or complete; it's for the tip sha pair. We forget an in-progress batch for a PR only if:; - a batch for a more recent sha pair is complete, or; - a more recent, but not tip, source sha build is also in progress. As is the case for our services, at any time we may not be in our target state. For CI, if we are not in our target state, `_refresh` and `_heal` are intended to move us towards the desired state. `_refresh` incorporates new GitHub information. `_heal` incorporates new batch state and perturbs batch as necessary. cc: services crew: @akotlar, @jigold . EDIT:. I've gone round and round with my thoughts on what the right idea here is. I would appreciate some discussion around what we should aim for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6007:106,failure,failure,106,https://hail.is,https://github.com/hail-is/hail/pull/6007,1,['failure'],['failure']
Availability,"This PR begins the implementation of checkpointing and restoring of jobs in Batch. Currently, a container is checkpointed after 10 seconds and before running a container (using crun run) the worker container checks if it should restore a job based on a checkpoint in Google storage. Currently, the kinds of Jobs that can be checkpointed/restored are: jobs that do simple operations and only print to stdout, jobs that redirect their output to local files. Changelist:; - Add copy method to RouterAsyncFS; - Add checkpointable flag to containers (make DockerJob containers checkpointable and JVMJob containers not checkpointable); - Create checkpoint method which pauses a container, checkpoints it, copies the checkpoint directory and upper directory of the overlay into Google storage, and then resumes the container; - Add logic in _run_container to try copying checkpoint directory and upper directory from cloud storage and then running `crun restore` if the job is checkpointable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11888:37,checkpoint,checkpointing,37,https://hail.is,https://github.com/hail-is/hail/pull/11888,12,['checkpoint'],"['checkpoint', 'checkpointable', 'checkpointed', 'checkpointing', 'checkpoints']"
Availability,"This PR breaks ""fatal"" out into two functions, ""fatal"" and ""abort"". Fatal is for unexpected error handling, and produces a python stacktrace, but 'abort' is for handled, expected errors (like invalid method inputs) and does not generate a stack trace in python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/pull/1552,2,['error'],"['error', 'errors']"
Availability,"This PR brings the `Streamify` pass back to the JVM emitter. Streamify is useful because it make explicit *in the structure of the IR*, which nodes are stream producers and which nodes are stream consumers. In particular, the following nodes **produce** streams:; ```; ReadPartition; MakeStream; StreamRange; ToStream. ArrayMap; ArrayFilter; ArrayFlatMap; ArrayScan; ArrayAggScan; ArrayLeftJoinDistinct. Let /* sometimes */; ```; And the following nodes **consume** streams (`#` indicates which arguments are streams):; ```; ToArray(#); ToDict(#); ToSet(#); GroupByKey(#) ; ArraySort(#, -, -, -); ArrayFold(#, -, -, -, -); ArrayFold2(#, -, -, -, -); ArrayFor(#, -, -); ArrayAgg(#, -, -); CollectDistributedArray(#, -, -, -, -). ArrayMap(#, -, -); ArrayFilter(#, -, -); ArrayFlatMap(#, -, #); ArrayScan(#, -, -, -, -); ArrayAggScan(#, -, -); ArrayLeftJoinDistinct(#, #, -, -, -, -). Let(-, -, #) /* sometimes */; ```. Thus, `Emit` may make better assumptions about the IR it is walking over. `emitArrayIterator` only deals with stream producers, and all stream consumers must call `emitArrayIterator` on their stream arguments. Additionally:; - `Streamify` was fixed to materialize less arrays than it used to (there were also bugs that caused errors on certain IR; see tests); - The producer `ToStream` may assume that its argument is a container.; - The IR nodes `ArrayRange` and `MakeArray` do not make their way to `Emit`.; - I'm not sure what the purpose of `{T,P}Streamable` is, I think all of its functionality is already covered by `{T,P}Iterable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7120:1243,error,errors,1243,https://hail.is,https://github.com/hail-is/hail/pull/7120,1,['error'],['errors']
Availability,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9200:82,error,errors,82,https://hail.is,https://github.com/hail-is/hail/pull/9200,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:478,error,errorTransformer,478,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,1,['error'],['errorTransformer']
Availability,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092:161,avail,available,161,https://hail.is,https://github.com/hail-is/hail/pull/1092,1,['avail'],['available']
Availability,"This PR incorporates @cseed 's changes from #3477, brings everything up to date with master, and adds/fixes the following:; - added a test for linreg with no covariates against R, and deleted old `test_linear_regression_with_no_cov` since that still had intercept.; - extended Skat to work without covariates and added test that it runs, but it’s hard to test result against R given that the latter fails with no covariates: `Error in solve.default(t(X1) %*% X1) : 'a' is 0-diml`. The result look ""reasonable"" to me.; - added req of at least one covariate for logreg in doc and code. It's going to be painful to get logistic to take no covariates, we can always come back to it if priority goes up. Related fun breeze behavior: `a(::, *) *:* b` with `a` an `(n, 0)` matrix and `b` an `n`-vector has dimensions `(0, 0)`.; - removed default value of empty list for `covariate`, both to help signal users to consider putting in the intercept (pipelines currently using intercept only with default empty `[]` will break) and because empty is not currently valid for logreg.; - noted in docs that intercept must be included explicitly.; - added comment of R code against which linreg and logreg are testing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4067:426,Error,Error,426,https://hail.is,https://github.com/hail-is/hail/pull/4067,1,['Error'],['Error']
Availability,"This PR introduces `SArray` and `SArrayValue`, lets us compile with arrays. I added implementations for `MakeArray` and `ArrayRef`. . To help myself out, I also registered the region allocation function on the `CompileModule` so it's always available to call. I added a `printf` function as well that takes a C++ string and a vector of `llvm::Value` to allow arbitrary debugging prints.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10128:241,avail,available,241,https://hail.is,https://github.com/hail-is/hail/pull/10128,1,['avail'],['available']
Availability,"This PR introduces `facet_wrap`, which will allow creating plots based on a specified facet. It also adds ; ` def _add_aesthetics_to_trace_args(self, trace_args, df):`; and; ` def _update_legend_trace_args(self, trace_args, legend_cache):`. two helper methods which let me clean up some of the redundant plotting work. A `legend_cache` was introduced to make sure we put traces that should have the same legend point in a `legendgroup` together. Without it, if I draw one red line in each of 4 different subplots created by a facet, then 4 entries appear in the legend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11725:294,redundant,redundant,294,https://hail.is,https://github.com/hail-is/hail/pull/11725,1,['redundant'],['redundant']
Availability,"This PR introduces a generic pretty-printing package, which for now is only used in `ir.Pretty`. The primary motivation was to separate format specification from rendering, to simplify the formatting code, and make it easier to add multiple formatting options, including forms with line number references. Some nice properties of the new pretty-printer:; * Generic. Should be able to be used for all pretty-printing in hail scala code. This simplifies the codebase by making client pretty-printers easy to understand and modify, without getting bogged down in low-level details.; * Composable. Formatting specifications are trivially combinable, without needing to manually track context like the current indentation level, max line length, etc.; * Stack safe. Uses constant stack space.; * Uses constant heap space. Only keeps in memory text which might print in the current line, if it fits. (The pretty printer writes to a `java.io.Writer`, and I'm ignoring any heap space used by the writer.); * Lazy. If the number of lines to print is capped, doesn't scan more of the tree than is needed to print those lines.; * Produces more readable output, printing nodes on a single line where possible. As we work to increase visibility into the compiler, I think this will be very helpful. A formatted document is represented by the `Doc` type. This defines a `render` method, which takes three parameters to control the output:; * `width`: the maximum length of a line, including indentation; * `ribbonLength`: the maximum length of a line, not including indentation (too many characters on a line is hard to read, regardless of indentation); * `maxLines`: the maximum number of lines to print. There are only a few `Doc` constructors, which suffice to define all methods in the richer api contained in the `prettyPrint` package object.; * `Text(t: String)`; * `Line(ifFlat: String)`; * `Indent(i: Int, body: Doc)`; * `Concat(it: Iterable[Doc])`; * `Group(body: Doc)`. Ignoring `Group` for the moment, th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:552,down,down,552,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['down'],['down']
Availability,"This PR is a step towards a general picture for generating debugging information in bytecode. The general picture is to write a sequence of files, each corresponding to a certain point in the compile pipeline, where each line in each file includes a line number pointing to the line in the previous file from which this line was derived. (We may eventually want richer source information than just a single line number, like a range or list of ranges.) The top of each file has the file name of the previous printout, which is the target of all line numbers in the file. We could in the future also print a list of transformations that were applied to get from the previous printed state to this one. The idea to implement this picture is simple. Each IR node stores a line number in a mutable variable. When we want to generate a printed checkpoint, we walk the IR, printing a representation of each node, including the stored line number, and then overwriting the node's line number with the current line count of the file being written to. Some work will be required to preserve this source information in all IR transformations. This PR implements this idea in lir only. If the `HAIL_WRITE_IR_FILES` environment variable is set, it is hardcoded to print the lir after the first `SimplifyControl` (because before that is very hard to read), and after method splitting right before emitting bytecode. It also prints out the class files themselves. Longer term we'll want to be able to control which points in the compilation get printed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9521:839,checkpoint,checkpoint,839,https://hail.is,https://github.com/hail-is/hail/pull/9521,1,['checkpoint'],['checkpoint']
Availability,"This PR is based on discussion here: https://dev.hail.is/t/better-python-error-messages-idea/201/9 The intention is to create a system to give better error messages from python in a generic way. Tim's work in #7792 does a good job introducing behavior like this this specifically for `ArrayRef` nodes, but I want to add three things on top of that:. 1. I don't want to have to do as much custom per IR node work; 2. I don't want to send the entire python stack trace over py4j to scala for every node; 3. I don't want the user to see a Java stack trace in this scenarios. This first PR is a proof of concept that adds this behavior for the `Die` node, which will catch any errors generated by uses of `CaseBuilder.or_error`. Follow up PRs should change `ArrayRef` to work this way, as well as catch things like looking up a key in a dictionary but not finding it. In an ideal future, we'd bolt on some extra mechanism to give types to these errors, and we could throw a proper `IndexError` in the `ArrayRef` case or `KeyError` in the dictionary case. . It feels a little bit messy right now, open to suggestions. I don't love using `-1` as the ""no error"" situation, but I thought it was probably easier than dealing with optionals between python and scala. . To give an example of what it looks like, the error message for this script:. ```; import hail as hl. ht = hl.utils.range_table(10); ht = ht.annotate(foo = hl.nd.array([[1], [2], [3]])); ht = ht.annotate(bar = ht.foo[0:4, 12]); ht.collect(); ```. is. ```; Traceback (most recent call last):; File ""better_error_test.py"", line 6, in <module>; ht.collect(); File ""<decorator-gen-1103>"", line 2, in collect; File ""/Users/johnc/Code/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/johnc/Code/hail/hail/python/hail/table.py"", line 1903, in collect; return Env.backend().execute(e._ir); File ""/Users/johnc/Code/hail/hail/python/hail/backend/spark_backend.py"", line 325, in ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398:73,error,error-messages-idea,73,https://hail.is,https://github.com/hail-is/hail/pull/9398,4,['error'],"['error', 'error-messages-idea', 'errors']"
Availability,This PR is stacked on #9593. The key files to look at are in `docker/hail-ubuntu`. I introduced `hail-apt-get-install` which packages up the `apt-get update` and the removal of temporary files. I also set the number of package-download retries to 5.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594:227,down,download,227,https://hail.is,https://github.com/hail-is/hail/pull/9594,1,['down'],['download']
Availability,"This PR is the first iteration of an AsyncFS-based copy interface. It adds RouterAsyncFS.copy. The goal of these changes is to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the sour",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:451,error,error,451,https://hail.is,https://github.com/hail-is/hail/pull/9822,3,['error'],"['error', 'errors']"
Availability,This PR makes docker calls idempotent and adds a timeout for docker calls in the retry function. I got the error codes to ignore from the older docker documentation at the bottom of each API call: https://docs.docker.com/engine/api/v1.30/#operation/ContainerDelete. FYI: @cseed since you had opinions on the timeout times,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8049:107,error,error,107,https://hail.is,https://github.com/hail-is/hail/pull/8049,1,['error'],['error']
Availability,This PR mitigates the 500 error Ben was getting on the UI page when a resource usage file was corrupt. It also implements a more graceful handling of an out of space error on the worker.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12609:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/12609,2,['error'],['error']
Availability,"This PR rewrites the hailctl command line argument parsing code. While the interface remains largely the same, a few changes were made to how options are handled. We first introduce a bit of terminology. In a shell command invocation like `$ cmd a -o c`, `a`, `-o` and `c` are called parameters. `a` and `c` which do not start with dashes, are called arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:784,error,error,784,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['error'],['error']
Availability,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8433:991,error,errors,991,https://hail.is,https://github.com/hail-is/hail/pull/8433,1,['error'],['errors']
Availability,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:916,downtime,downtime,916,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['downtime'],['downtime']
Availability,"This PR tries to address the error we saw last Friday on Azure where there was a stuck worker that could not pull ubuntu:20.04 from Dockerhub. The error message in the worker logs was. ```; DockerError(500, 'Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed'); ```. I looked at the system logs and the actual error message was this:. ```; Mar 03 16:56:12 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:56:12.112691249Z"" level=info msg=""Attempting next endpoint for pull after error: Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed""; ```. Higher up in the logs was:. ```; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.520878176Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""<nil>"" remote=""docker.io/library/ubuntu:20.04""; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.762789745Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""ref moby/1/index-sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab locked: unavailable"" remote=""docker.io/library/ubuntu:20.04""; ```. My working hypothesis is described in detail here that the image cache with locks got corrupted with the simultaneous pulls: https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/Azure.20CI.20appears.20hung/near/339452619. To mitigate this, when we get the error ""denied: retrieving permissions failed"", we try and delete the image and then try pulling again once more before erroring gracefully. At least for now, this errors the user's job, but that's better than the status quo where the job is stuck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12758:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/12758,9,['error'],"['error', 'erroring', 'errors']"
Availability,"This PR updates a lot of NDArray code that was using the old `Code[T]` and emit triplet interface in favor of using code builders and `IEmitCode`. This is going to make it easier to update the `PNDArray` interface to not use `Code[Long]` everywhere, among other things. . Before this PR, there existed `NDArrayEmitter`, which did the old thing, and `NDArrayEmitter2`, which was a prototype of a new emitter. . After this PR, a tweaked version of `NDArrayEmitter2` has become the new `NDArrayEmitter`. `outputElement` now returns a `PCode`, and all the missingness problems are handled by carrying a `IEmitCodeGen[NDArrayEmitter]` around throughout the deforesting process, meaning the `NDArrayEmitter` no longer needs internal state about missingness. . I think the diff for `Emit.scala` is going to be pretty confusing. I'd at least opt for a side by side view instead of the intermingled one, as I've mostly implemented the same logic, just on top of our new code builder primitives. . All tests pass, but marking WIP since I'm noticing some slow down in the slice test that I want to look into. . This PR also adds a `get` function to `EmitValue` that gets the underlying `PValue` and moves two functions off of the `PNDArray` interface into `LinalgCodeUtils`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9824:1049,down,down,1049,https://hail.is,https://github.com/hail-is/hail/pull/9824,1,['down'],['down']
Availability,"This PR updates the LocalBackend to match the behavior of the SparkBackend w.r.t. error handling. . - `handle_java_exception` and `execute` are lifted into the parent file, `Py4JBackend`. ; - Tests in `test_ndarrays` that were marked as failing local tests are now passing, since the only failure was inconsistent handling of errors. . The error handling changes in question here were introduced in #9398",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9569:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/9569,4,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6659:259,error,error,259,https://hail.is,https://github.com/hail-is/hail/pull/6659,1,['error'],['error']
Availability,"This addresses a user issue whereby adding an indicator covariate that only varied in controls caused Firth to fail. Currently the standard logistic MLE is computed even for Firth regression so this beta can be used to initialize the Firth per-variant models. But if the data has a (quasi-)separated coordinate, the standard MLE may not converge, throwing an error before the Firth models are fit. With the changes in this PR, if the MLE does not converge, Firth falls back to initializing with the intercept-only estimate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2737:359,error,error,359,https://hail.is,https://github.com/hail-is/hail/pull/2737,1,['error'],['error']
Availability,This addresses issues where the gradle download may fail. We retry a command; that is cheap (`--version`) but which requries downloading the gradle binary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8782:39,down,download,39,https://hail.is,https://github.com/hail-is/hail/pull/8782,2,['down'],"['download', 'downloading']"
Availability,"This adds SpillingCollectIterator which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. The number of results kept in memory is a flag on the HailContext. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. #### Implementation Notes. I had to add two new file operations to `FS` and `HadoopFS` because I need seekable file input streams. When we add non-hadoop `FS`'s we'll need to address the interface issue. When we overflow our in-memory buffer, we spill to a disk file. We use O(n_partitions / mem_limit) files. We stream through the files to `scanLeft`, to compute the globally valid scan state per partition. The stream writes its results to another file which must be on a cluster-visible file system (we use `HailContext.getTemporaryFile`). Finally, each partition reads that file and seeks to its scan state. I somewhat better solution would be to eagerly scan as results come in. I leave that as future work. #### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.36 s, sys: 297 ms, total: 1.66 s; Wall time: 27.3 s. In [2]: %%time ; ...: ; ...: import hail as hl ; ...:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345:228,error,error,228,https://hail.is,https://github.com/hail-is/hail/pull/6345,1,['error'],['error']
Availability,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that [listens for GC events](https://stackoverflow.com/questions/30041332/a-useful-metric-for-determining-when-the-jvm-is-about-to-get-into-memory-gc-trou) and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? The only workable solution I can think of is a HailContext setting. Maybe I should bite the bullet and respond to memory pressure? Either way this should get Laurent cooking with gas. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. Master 0.2.14-4da055db5a7b; ```ipython; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 6.23 ms, sys: 1.96 ms, total: 8.19 ms; Wall time: 1.33 s; ```; This branch:; ```ipython; In [2]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.41 s, sys: 313 ms, total: 1.72 s; Wall time: 25.2 s. In [3]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(1000000, n_partitions=1000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; ...: ; ...: ; CPU times: user 4.72 ms, sys: 1.82 ms, total: 6.53 ms; Wall time: 1.41 s; ```. ---. Minor implementation note: I did the rigamarole with `runJob` because I wasn't sure that using `synchronized` in a constructor was kosher and I'm also generally weary of Scala's constructor syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6306:359,error,error,359,https://hail.is,https://github.com/hail-is/hail/pull/6306,2,"['down', 'error']","['down', 'error']"
Availability,"This adds `SpillingCollectIterator` which avoids holding more than 1000 aggregation results in memory at one time. We could do something that listens for GC events and spills data if there's high memory pressure. That seems a bit error prone and hard. How should I pipe the size limit down to TableMapRows? I decided to make it a HailContext `flag` which means its not very user-visible, but Laurent can set it for now. In C++ we can design a system that is aware of its memory usage and adjusts memory allocated to scans accordingly. Spilling ten local files and then reading them in is probably in the noise of timings. 🎉. ---. ### Implementation Notes. I had to add two new file operations to the `RichHadoopConfiguration` because I need seekable file input streams. I don't like the names. I'm not sure what to do here. Hadoop really screws us with the seek-ability on compressed streams. The implementation is rather simple, it just maintains an array of the per-partition results. The index of the array corresponds to the partition index. The sparsity of that array is controlled by how often we spill. For an operation with a huge number of partitions that are often spilled (e.g. large number of partitions, each with a lot of data), we may want to use a `Map` instead of an `Array`. The use of `ObjectOutputStream` without a try-catch-finally block is non-standard. I was having trouble seeking to individual classes when I used one ObjectOutputStream to output each partition's array. There were these ""bad header"" messages. This seems to work. I don't close the OOS because I'm going to re-use the underlying output stream on the next partition. We use O(n_spills) files. ---. ### Timings. Master 0.2.14-4da055db5a7b; ```; In [1]: %%time ; ...: ; ...: import hail as hl ; ...: ht = hl.utils.range_table(10000, n_partitions=10000) ; ...: ht = ht.annotate(rank = hl.scan.count())._force_count() ; CPU times: user 1.45 s, sys: 333 ms, total: 1.78 s; Wall time: 24.6 s; In [3]: %%time ; ...: ;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6333:230,error,error,230,https://hail.is,https://github.com/hail-is/hail/pull/6333,2,"['down', 'error']","['down', 'error']"
Availability,"This adds a prometheus statefulset to track metrics like API request latency and uptime. It scrapes pods on a 15s interval and collects prometheus metrics from any container in a pod with `grafanak8sapp` label that exposes an https endpoint `/metrics`.; The batch front end was already exposing prometheus metrics, but I changed it up slightly. For any http endpoint there should be a single metric, `http_request_latency`. Prometheus adds app and namespace metrics so seeing latencies for batch in particular is just a filter applied to this single metric. You can track latency of an endpoint by adding the `@monitor_endpoint` decorator defined in `metrics.py`, which tracks latency as well as number of requests and status code per request, available in the `http_request_count` metric. I also added monitoring to all CI endpoints. This also includes an `up` metric for tracking uptime at the same 15s granularity. I'm not convinced prometheus will suit our finer-grained needs surrounding batch, but it should do well enough in the meantime for our more traditional SLIs and allows to focus on one problem at a time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10165:744,avail,available,744,https://hail.is,https://github.com/hail-is/hail/pull/10165,1,['avail'],['available']
Availability,"This adds some useful infrastructure for Scala-generated C++. 1. Initial support for a small number of C++-to-Scala upcalls. 2. C++ info/warn/error implemented as upcall going through is.hail.utils,{info,warn,error}. 3. Scala PrettyCode auto-indenter for Scala-generated C++, so you don't have to try to; get the indentation right while generating the C++ source. 4. src/main/c/Makefile has a variable HAIL_ENABLE_DEBUG if set as "":=1"", then libs will be; built with -O1, and the initialization in src/main/NativePtr.cpp will try to start gdb in an xterm; and attach back to the hail process, allowing gdb debugging of generated C++ called from; Scala. 5. ObjectArray is a NativeObj which can hold any number of Scala Objects, holding them in C++; as JNI global-ref jobjects. This can be used for example to hold InputBuffer or InputStream; objects, and to pass them down to a constructor for a C++ object (e.g. a decoder) which will then ; make upcalls to methods of those objects. . A subsequent commit will have the RowStore/C++ decoder using all this infrastructure (passing an; InputBuffer down to the C++ decoder, which holds it and makes upcalls to pull a block of data).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4302:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/pull/4302,4,"['down', 'error']","['down', 'error']"
Availability,"This adds terraform modules for CI azure resources and CI k8s resources. It also many roles to the test account that the batch account has so that we can run PR and test namespaces (internal batch instances use the test account for all the services so the test account needs tons of privilege. It looks scary, but this is the model we currently have). Thanks to #11053, which is the current version of the CI deployment in Azure, this required no change to the CI application code. ### Sidebar; It might look weird that I've added a block here for the kubernetes provider. That is because up until now I've kept all the k8s and azure terraform in separate root modules, so that they need to be `terraform apply`'d separately and therefore their provider blocks were separated as well. While keeping the code isolated is good (the k8s modules can be reused for GCP), putting them in separate apply's was purely because of [this bug](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028) in the kubernetes provider. I ran into it when experimenting tearing clusters down and putting them back up again. However, it has since proven very cumbersome to manage two different terraform states where one relies heavily on the other and I've changed my mind. The bug in question has a PR forthcoming and is really only a problem when tearing a K8s cluster down and rebuilding it while preserving other terraform state, which isn't something I see us dealing with often past initial development. Thus, I've added the k8s provider block so that I can directly invoke the CI k8s module. I'll follow up with a PR that moves the other k8s module invocations in here as well. If it would help, I can first start with a dev doc detailing our terraform structure (or where I want it to be).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11084:1082,down,down,1082,https://hail.is,https://github.com/hail-is/hail/pull/11084,2,['down'],['down']
Availability,"This adds the minimal resources to k8s to allow us to modify the gateway's configuration to include redirects for https://notebook.hail.is. Currently, that domain will timeout, but there should otherwise be no errors introduces to the k8s system. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4645:210,error,errors,210,https://hail.is,https://github.com/hail-is/hail/pull/4645,2,['error'],['errors']
Availability,"This adds the oauth2 client secret as a terraform resource instead of uploading it by hand to k8s (though you still have to download it manually from the console. It also makes use of the `bootstrap_utils.sh` script that I introduced for azure for cloud-agnostic steps, trimming down the GCP instructions by a lot.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11037:124,down,download,124,https://hail.is,https://github.com/hail-is/hail/pull/11037,2,['down'],"['down', 'download']"
Availability,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8562:301,down,download,301,https://hail.is,https://github.com/hail-is/hail/pull/8562,1,['down'],['download']
Availability,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279,2,['error'],['error']
Availability,This appears to have cut batch2 test time nearly in half (5-6m down from 11+m),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465:63,down,down,63,https://hail.is,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465,1,['down'],['down']
Availability,"This approach to joining doesn't work for me:; ```; In [1]: import hail as hl . In [2]: t = hl.utils.range_table(1) . In [3]: t2 = t.key_by(idx=t.idx, idx2=t.idx) . In [4]: t.annotate(foo=t2[t.key]) ; Traceback (most recent call last):; File ""<ipython-input-4-85e676382c80>"", line 1, in <module>; t.annotate(foo=t2[t.key]); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 368, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32, int32; Index Expressions: int32; ```. And since the annotation db is just built on joins, it wouldn't work in that setting either. Moreover, the annotation db needs to be careful with uniqueness of the key-row relationship. I try to avoid unnecessarily using `all_matches=True` which introduces arrays that complicate downstream work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587:1048,down,downstream,1048,https://hail.is,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587,1,['down'],['downstream']
Availability,"This astronomically cuts down on IR duplication. The following is the IR for one invocation to TableMultiWayZipJoin in the old pipeline, this would be replicated for every vcf imported:; ```; (CastMatrixToTable ""__entries"" ""__cols""; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixMapRows; (MatrixMapEntries; (MatrixLiteral); (InsertFields; (SelectFields (AD DP GQ GT MIN_DP PGT PID PL SB); (Ref g)); None; (END; (GetField END; (GetField info; (Ref va)))); (BaseQRankSum; (GetField BaseQRankSum; (GetField info; (Ref va)))); (ClippingRankSum; (GetField ClippingRankSum; (GetField info; (Ref va)))); (MQ; (GetField MQ; (GetField info; (Ref va)))); (MQRankSum; (GetField MQRankSum; (GetField info; (Ref va)))); (ReadPosRankSum; (GetField ReadPosRankSum; (GetField info; (Ref va)))); (LGT; (GetField GT; (Ref g))); (LAD; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR `[:*]`; (GetField AD; (Ref g)); (I32 -1)); (GetField AD; (Ref g)))); (LPL; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 2)); (ApplyIR `[:*]`; (GetField PL; (Ref g)); (ApplyUnaryPrimOp Negate; (ArrayLen; (GetField alleles; (Ref va))))); (NA Array[Int32])); (If; (ApplyComparisonOp GT; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (GetField PL; (Ref g)); (NA Array[Int32])))); (LPGT; (GetField PGT; (Ref g))); (RGQ; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va)); (I32 -1)); (Str ""<NON_REF>"")); (ApplyIR indexArray; (GetField PL; (Ref g)); (Apply unphasedDiploidGtIndex; (Apply Call; (I32 0); (ApplyBinaryPrimOp Subtract; (ArrayLen; (GetField alleles; (Ref va))); (I32 1)); (False)))); (NA Int32))))); (InsertFields; (SelectFields (locus alleles rsid qual filters info); (Ref va)); None; (alleles; (If; (ApplyComparisonOp EQ; (ApplyIR indexArray; (GetField alleles; (Ref va))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5527:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/5527,1,['down'],['down']
Availability,"This behavior can produce type errors, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268302819:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268302819,1,['error'],['errors']
Availability,"This came to mind yesterday during our pairing. This PR introduces the following properties that our image building targets do not currently have:; 1. If your intention is only to build images, you shouldn't need `kubectl`. When `DOCKER_PREFIX` is used as a docker build arg it is because we mirror some dockerhub images inside our registry (for reliability/rate limiting reasons). But for local building there's no reason you can't use the dockerhub image. Also, other people should be able to build the hail image if they want to!; 2. One should *only* need to use `kubectl` if they are intending to use an image in a kubernetes deployment. In other words, you should only need the private registry `DOCKER_PREFIX` for pushing images.; 3. One should not need to endure image pushing if the only goal is to build the image locally; 4. No intermediate tags should end up in the private registry. If we push on every image build, the private docker registry will accumulate a lot of `hail-ubuntu:dev-xxxxxx` tags that are never used again because `hail-ubuntu` is just an intermediate used to build other images. This does *not* change the number of layers that end up in the registry, but reduces a bit of the work that the registry cleanup job needs to do to untag and delete images and just seems cleaner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13890:346,reliab,reliability,346,https://hail.is,https://github.com/hail-is/hail/pull/13890,1,['reliab'],['reliability']
Availability,"This catch was masking a `org.json4s.package$MappingException: Parsed JSON values do not match with class constructor` as a `NullPointerException` on line 148. Exceptions always include their cause in the stack trace, I see no compelling reason to maintain this try-catch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6156:15,mask,masking,15,https://hail.is,https://github.com/hail-is/hail/pull/6156,1,['mask'],['masking']
Availability,"This caught a bug in the type inference in `GroupByKey`, which is also fixed in the PR. @konradjk I'm pretty sure that this makes #5147 work, but I haven't confirmed directly so if you could confirm that it does and then close (or re-ping) once this goes in, that would be great.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5155:234,ping,ping,234,https://hail.is,https://github.com/hail-is/hail/pull/5155,1,['ping'],['ping']
Availability,This caused assertion errors in Scala in `MakeArray.unify`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12750:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/pull/12750,1,['error'],['errors']
Availability,"This causes issues when starting interactive sessions on clusters. Before, I get lots of output like this:; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: ERRORNOTFOUND. SPARKMONITOR_LISTENER: Exception creating socket:java.lang.NumberFormatException: For input string: ""ERRORNOTFOUND"". SPARKMONITOR_LISTENER: Application Started: application_1569946119076_0001 ...Start Time: 1569946336092. SPARKMONITOR_LISTENER: Exception sending socket message:java.lang.NullPointerException. After:; <nothing>. I also tested to make sure the monitor still worked in a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7174:230,ERROR,ERRORNOTFOUND,230,https://hail.is,https://github.com/hail-is/hail/pull/7174,2,['ERROR'],['ERRORNOTFOUND']
Availability,"This causes the interpreter to try other possible implementations, producing weird errors. For example:. ```; if hl.dict([1], [1]):; pass; ```. raises an error about `__len__`, not `__nonzero__`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4169:83,error,errors,83,https://hail.is,https://github.com/hail-is/hail/issues/4169,2,['error'],"['error', 'errors']"
Availability,"This change exists as part of larger refactoring work. Herein, I've exchanged; hard-coded contextual strings passed to `ExecutionTimer.time` with implict; contexts, drawing inspiration from scalatest. These contexts are now supplied after entering functions like `Compile` and; `Emit` instead of before (see `ExecuteContext.time`). By sprinking calls to ; `time` throughout the codebase after entering functions, we obtain a nice trace; of the timings with `sourcecode.Enclosing`, minus the previous verbosity. See [1] for more information about what pre-built macros are available. We can; always build our own later. See comments in [pull request id] for example output.; Note that `ExectionTimer.time` still accepts a string to support uses like; `Optimise` and `LoweringPass` where those contexts are provided already.; It is also exception-safe now. This change exposed many similarities between the implementations of query; execution across all three backends. I've stopped short of full unification; which is a greater work, I've instead simplified and moved duplicated result; encoding into the various backend api implementations. More interesting changes are to `ExecuteContext`, which now supports; - `time`, as discussed above; - `local`, a generalisation for temporarily overriding properties of an ; `ExecuteContext` (inspired by [2]). While I've long wanted this for testing,; we were doing some questionable things when reporting timings back to python,; for which locally overriding the `timer` of a `ctx` has been very useful.; We also follow this pattern for local regions. [1] https://github.com/com-lihaoyi/sourcecode; [2] https://hackage.haskell.org/package/mtl-2.3.1/docs/Control-Monad-Reader.html#v:local",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679:572,avail,available,572,https://hail.is,https://github.com/hail-is/hail/pull/14679,1,['avail'],['available']
Availability,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:619,error,error,619,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"This change grew out of https://github.com/hail-is/hail/pull/13674.; The idea is simple - we shouldn't be appending code after control statements as such statements are redundant. That idea opened pandora's box, but now we're not generating and dropping dead code anymore. Main changes that rose form fixing fallout from adding assert in `Block.append`:; - Implement basic control-flow structures (if, while, for, switch) in `CodeBuilderLike` and remove the older implementations from `Code`.; - main difference is these are built from sequencing `Code` operations rather than being defined from LIR; - allows for a higher-level implementation that I think is simpler to read.; - Use the type-system to prevent foot-guns like `cb.ifx(cond, label.goto)`. Other changes:; - rename `ifx`, `forLoop` and `whileLoop` to just `if_`, `for_` and `while_`, respectively.; - Implement loops in-terms of one-another to remove code duplication.; - Fix logic for when to write IRs as some default value behaviour was broken when `HAIL_WRITE_IR_FILES` was set in tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13752:169,redundant,redundant,169,https://hail.is,https://github.com/hail-is/hail/pull/13752,1,['redundant'],['redundant']
Availability,"This change removes previous infrastructure for building generated C++; code. The previous infrastructure would write two files, a cpp file and; a makefile, then run make to build the shared object. This change gets rid of all that in favor of a pipe-fork-exec model,; using the ability of clang++/g++ to read from stdin via `-x <LANG>` and; `-` arguments. Some notes:. * We still invoke the shell to find JAVA_HOME if it is not defined. We; do this in a similar way to what we do in the makefiles.; * Because of the odd signatures of the `exec` family of functions, we; use a `const_cast` to discard the appropriate qualifiers. This is safe; because we only do it after forking, and only to exec, and never use; that data after the call to `execvp`.; * We ignore `SIGPIPE`, as it is raised when the process tries to write; to a pipe where the other end is closed. Not doing this could crash; hail, and means that the child process died before we wrote all of the; c++ source to the pipe, other error handling will catch what actually; went wrong, rather than being unable to write to the pipe.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6927:995,error,error,995,https://hail.is,https://github.com/hail-is/hail/pull/6927,1,['error'],['error']
Availability,"This change sorts the jobs for a PR correctly in the UI, so that all jobs are placed under a header with the correct state (see screenshot). Because jobs are split into separate tables by their state, `focusOnSlash` has been removed from the relevant CI pages, since it is unknown which tables will exist on the page until it is rendered. It also fixes the error that was causing the server to return a 500 when there were no jobs yet (e.g. when a retry had just been requested), which was caused by an assumption in the original job filtering code that there would always be at least one job to display. It also passes the list of developers through to the underlying `PR` object that a `WatchedBranch` has, which was missed in https://github.com/hail-is/hail/pull/13398 but is required for CI to display the PR page correctly in dev deploys only. <img width=""714"" alt=""Screenshot 2023-08-22 at 15 08 48"" src=""https://github.com/hail-is/hail/assets/84595986/6bcab38c-a48a-4e10-8f69-770d35ea51b7"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13254:357,error,error,357,https://hail.is,https://github.com/hail-is/hail/pull/13254,1,['error'],['error']
Availability,This change uncovered a problem with lowering -- `AggLet` nodes are not tolerated inside MatrixMapCols/MatrixMapRows. I'll follow up with a PR to fix this for MatrixMapRows and add targeted tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406:72,toler,tolerated,72,https://hail.is,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406,1,['toler'],['tolerated']
Availability,"This comes up with a number of match errors on Agg IR. Wanted to check that these should be implemented in InferPType, because my impression was that relation IR would not be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7959#issuecomment-578509253:37,error,errors,37,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-578509253,1,['error'],['errors']
Availability,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:145,down,downloads,145,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,4,"['Down', 'avail', 'down']","['Downloaded', 'available', 'downloads']"
Availability,This creates redundant bindings that interfere with our ability to apply certain simplification rules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7720:13,redundant,redundant,13,https://hail.is,https://github.com/hail-is/hail/pull/7720,1,['redundant'],['redundant']
Availability,"This doesn't look like the same error. This error is found in a genotype call in a biallelic variant with 3 AD values -- a violation of the VCF spec (AD is ""R""-numbered). . We've seen this before on VCFs that were split by certain tools, and since there were enough of them, we added an option `skip_bad_ad` on import_vcf: https://hail.is/hail/hail.HailContext.html#hail.HailContext.import_vcf. You'll want to run with that option set to true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840,2,['error'],['error']
Availability,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:205,down,down,205,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707,1,['down'],['down']
Availability,"This ended up being a great lesson in `asyncio`!. `kubernetes_asyncio` has a deeply hidden `asyncio.ClientSession` that doesn't get properly closed when we restart services with `SIGINT`. Turns out the `__del__` method on the `RESTClient` deep inside the library that holds this client session sets up a future with asyncio to close the session, but I suspect that this object is getting deleted _after_ the event loop closes. As a result we get a bunch of garbage in the logs that; - the event loop is already closed when something is trying to happen; - a `ClientSession.close` was never properly awaited. I initially tried to explicitly close the client session but still dealt with the problem that the `k8s_client` was trying to interact with the even loop after it closed. Explicitly deleting the `k8s_client` on cleanup and then awaiting any remaining futures (so the client session `close`) appears to fix the problem as I get no more errors on shutdown, but let me know if this isn't kosher. If this is alright I'll see this through to the rest of the services.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9944:943,error,errors,943,https://hail.is,https://github.com/hail-is/hail/pull/9944,1,['error'],['errors']
Availability,This failed in Azure when compiling the JVM Entryway. ```; > Task :compileScala; [Error] /io/batch/jvm-entryway/src/main/java/is/hail/JVMEntryway.java:126: error: cannot find symbol; [Error] /io/batch/jvm-entryway/src/main/java/is/hail/JVMEntryway.java:153: error: cannot find symbol; javac exited with exit code 1. > Task :compileScala FAILED; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13664#issuecomment-1728404619:82,Error,Error,82,https://hail.is,https://github.com/hail-is/hail/pull/13664#issuecomment-1728404619,4,"['Error', 'error']","['Error', 'error']"
Availability,This fails currently. I want CI and eyes on this. Validation errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175:61,error,errors,61,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175,1,['error'],['errors']
Availability,"This feature is not widely supported (only on MatrixTable & SparkBackend); and is not well-tested in CI (we only test that matrix writes run and return; the correct result with the _checkpoint_file parameter, not the performance; semantics). I've played around with this code on my laptop and the performance semantics; are what I expect. Logging messages provide some transparency too:. ```. In [2]: mt = hl.utils.range_matrix_table(100000, 10, 500). In [3]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:06 Hail: INFO: creating new checkpoint at /tmp/mt_checkpoint; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [4]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:14 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 192/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [5]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:22 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 300/500 partitions written; 	^C---------------------------------------------------------------------------00]; 	KeyboardInterrupt Traceback (most recent call last); 	<snip>; 	KeyboardInterrupt:. In [6]: mt.write('/tmp/mt_temp4.mt', _checkpoint_file='/tmp/mt_checkpoint'); 	2021-03-23 14:50:29 Hail: INFO: resuming matrix write from /tmp/mt_checkpoint with 372/500 partitions written; 	2021-03-23 14:50:36 Hail: INFO: wrote matrix table with 100000 rows and 10 columns in 500 partitions to /tmp/mt_temp4.mt; 	 Total size: 391.55 KiB; 	 * Rows/entries: 391.51 KiB; 	 * Columns: 31.00 B; 	 * Globals: 11.00 B; 	 * Smallest partition: 200 rows (505.00 B); 	 * Largest partition: 200 rows (835.00 B). In [7]: mt_r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215:575,checkpoint,checkpoint,575,https://hail.is,https://github.com/hail-is/hail/pull/10215,1,['checkpoint'],['checkpoint']
Availability,"This fixes an issue where we cannot retrieve old batches for a PR. Addresses this error:; ```; {""levelname"": ""ERROR"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_urldispatcher.py\"", line 157, in handler_wrapper\n result = await result\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_jinja2/__init__.py\"", line 91, in wrapped\n context = await coro(*args)\n File \""/ci/ci.py\"", line 170, in get_batch\n j['duration'] = humanize.naturaldelta(datetime.timedelta(seconds=sum(j['duration'])))\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'""}; {""levelname"": ""INFO"", ""asctime"": ""2019-08-01 15:59:17,119"", ""filename"": ""web_log.py"", ""funcNameAndLine"": ""log:233"", ""message"": ""10.32.0.177 [01/Aug/2019:15:59:17 +0000] \""GET /batches/584 HTTP/1.0\"" 500 315 \""-\"" \""Mozilla/5.0 (Macintosh;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6783:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/6783,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:251,error,error,251,https://hail.is,https://github.com/hail-is/hail/pull/5746,1,['error'],['error']
Availability,"This fixes two bugs:; 1. The container logs weren't being cached. This made the logs ""disappear"" for previous tasks while the job was still running. FYI @konradjk . 2. My job got stuck in ""running"" even though the job was deleted from the worker because writing the status to GCS timed out and we didn't actually mark the job complete. I'm not sure if we should always try to retry writing the status rather than failing on non-transient errors. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8054:438,error,errors,438,https://hail.is,https://github.com/hail-is/hail/pull/8054,1,['error'],['errors']
Availability,This frequently fails and triggers an error log.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12472:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/12472,1,['error'],['error']
Availability,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078:41,down,down,41,https://hail.is,https://github.com/hail-is/hail/pull/5078,1,['down'],['down']
Availability,"This groups IBD and pc_relate together. I will follow up with KING. I plan to; lift some of the documentation verbiage from PC-Relate up and to unify the; mathematical presentation for PC-Relate and KING. I did not change the implementations, I just moved them into the relatedness; package. Both functions are still available at `hl.methods.XXX`. The import changes are just me using `(` and `)` instead of line continuation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9324:317,avail,available,317,https://hail.is,https://github.com/hail-is/hail/pull/9324,1,['avail'],['available']
Availability,"This has slipped far enough down my todo list that it isn't gonna get done, at least not by me",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12911#issuecomment-1532088026:28,down,down,28,https://hail.is,https://github.com/hail-is/hail/pull/12911#issuecomment-1532088026,1,['down'],['down']
Availability,"This impacts all the methods that use _select_entries, e.g. within process_joins, like group_by.aggregate_rows/cols, annotate_rows/cols, transmute_rows/cols, etc. The error is expression source mismatch, which may be due to `annotate_entries` being done separately from `_annotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3763:167,error,error,167,https://hail.is,https://github.com/hail-is/hail/issues/3763,1,['error'],['error']
Availability,"This implementation lowers TableWrite with a TableNativeReader. Punting on the `stageLocally` path for now (it'll throw a lowering error) since our current implementation adds a task listener to the spark task to clean up files, and I'm not sure how we want to handle that in the general case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8926:131,error,error,131,https://hail.is,https://github.com/hail-is/hail/pull/8926,1,['error'],['error']
Availability,"This implementation should reduce the number of list HTTP requests and a faster implementation for large directories. Basically, the key assumption here is to only request at most 2 blob listings. If there are 0 results, then the blob doesn't exist. If there's 1 result, then it either is the `Blob` for a directory or file with the desired path. And if there are at least 2 results, then the path must be a directory. It could also be a file as well as a directory here. In Python, we throw an error `FileAndDirectoryError`. But currently we, just return the first blob item we see that matches (non-deterministic). I have maintained that same behavior here. A different PR can address what to do in this case where the path is both a file and a directory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390:495,error,error,495,https://hail.is,https://github.com/hail-is/hail/pull/13390,1,['error'],['error']
Availability,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053:952,down,down,952,https://hail.is,https://github.com/hail-is/hail/pull/11053,1,['down'],['down']
Availability,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10866:60,down,down,60,https://hail.is,https://github.com/hail-is/hail/pull/10866,2,"['down', 'error']","['down', 'errors']"
Availability,This is a compile error. MatrixIR has partitionCounts as an Option[IndexedSeq[String]],MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3877#issuecomment-401561119:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/3877#issuecomment-401561119,1,['error'],['error']
Availability,"This is a fix for an error Ben found. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1907, in run; await self.setup_io(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1848, in setup_io; await self.disk.create(labels=labels); File ""/usr/local/lib/python3.9/dist-packages/batch/cloud/gcp/worker/disk.py"", line 47, in create; await self._attach(); File ""/usr/local/lib/python3.9/dist-packages/batch/cloud/gcp/worker/disk.py"", line 112, in _attach; self.last_response = await self.compute_client.attach_disk(; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/compute_client.py"", line 83, in attach_disk; return await self._request_with_zonal_operations_response(self.post, path, params, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/compute_client.py"", line 126, in _request_with_zonal_operations_response; return await retry_transient_errors(request_and_wait); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 763, in retry_transient_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 775, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/compute_client.py"", line 116, in request_and_wait; raise GCPOperationError(result['httpErrorStatusCode'],; hailtop.aiocloud.aiogoogle.client.compute_client.GCPOperationError: GCPOperationError: 400:BAD REQUEST ['RESOURCE_IN_USE_BY_ANOTHER_RESOURCE'] [""The disk resource 'projects/hail-vdc/zones/us-central1-b/disks/batch-disk-82XXXXX' is already being used by 'projects/hail-vdc/zones/us-central1-b/instances/batch-worker-default-standard-yjXXXX'""]; {'kind': 'compute#operation', 'id': 'XXXXX', 'name': 'operation-XXXXX', 'zone': 'https://www.g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13955:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/13955,1,['error'],['error']
Availability,"This is a little broken and might require some care. I'll look into it and potentially just submit my own PR, but I do want to bump the version if possible because it might help with some of our session cleanup errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10115#issuecomment-786797234:211,error,errors,211,https://hail.is,https://github.com/hail-is/hail/pull/10115#issuecomment-786797234,1,['error'],['errors']
Availability,"This is a newer version of #9598. We decided to give users min(5Gi/core, 5 Gi) in `/` with mounting external storage at `/io` if they need more storage. All storage requests can be 0, 0 < storage < 10 will be rounded up to 10 Gi, or 10+Gi rounded up to the nearest integer. I added a loop to remove orphaned disks in gce.py. I changed how the resources appear in the spec. Now there's `req_cpu`, `req_storage`, `req_memory` which stores what the user specified. Then we also have `cores_mcpu`, `memory_bytes`, and `storage_gib` which are the actual resources allocated. I think this will be simpler and more understandable. Resources are computed in the front end now and the worker just uses the values from the front end (no more doing conversions on both the worker and front end). I kept backwards compatibility on the worker for now which can get deleted once there are no more jobs with batch format version < 6. I bumped the instance version to 16 so we know which workers have the new storage functionality. . I tested this by submitting 4 jobs on my 1 core test instance with 150Gi requests. I then looked at the worker logs to make sure the disks were created correctly and the value of the semaphore was correct. I also tested 0 Gi and 5 Gi by hand to make sure the resource fulfilled was 0Gi and 10Gi respectively. Lastly, I checked the billing to make sure we charged for the fraction of the SSD used as well as the cost of adding an extra persistent SSD for that job. I also looked at the disks in the GCE console to make sure they wear torn down correctly. Although there isn't a migration, we should make sure there are no non-ci jobs running so that we don't over allocate the storage available. Also, once this is merged, we should send an email to all users to let them know the cores must be a power of 2 now and about the storage now being mounted at '/io`. I put the WIP tag on so I can do this when I'm ready to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10090:1556,down,down,1556,https://hail.is,https://github.com/hail-is/hail/pull/10090,2,"['avail', 'down']","['available', 'down']"
Availability,"This is a pretty barebones implementation of split_multi with none of the flags that the usual split_multi has. Because I think the gVCF combiner ensures that min-repping alleles doesn't move them, I'm returning the min-repped locus/alleles and erroring if the locus changes (since this would mess up the ref blocks in that row, although I suppose I could always fall back to the non-minrepped version instead). It constructs 5 table nodes (two of which are key-bys and shouldn't need to touch the actual partitioned data) and three passes---map, explode, map---which is more-or-less the same as the usual split-multi. I haven't timed this on anything yet but I'm happy to if someone points me at a dataset I can use. @chrisvittal I've assigned this to you because I figure you're in the best position to make sure I've understood the format correctly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5394:245,error,erroring,245,https://hail.is,https://github.com/hail-is/hail/pull/5394,1,['error'],['erroring']
Availability,"This is a simplified implementation of `Process.communicate`. We feed lines into the log one-by-one; until we reach the end of both stdout and stderr. When both stdout and stderr have been closed by; the child process, we wait for the process to exit. At any point in time, the most recent log is; available to us.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10259:298,avail,available,298,https://hail.is,https://github.com/hail-is/hail/pull/10259,1,['avail'],['available']
Availability,"This is a temporary fix for the sporadic copy failures. The problem is that this code cancels a task managed by the online bounded pool, and the pool treats that cancellation as an exception that it propagates up. I need to think through the details of the bounded gather with respect to cancellation, and that's going to take a few days. We could put this back when that's done, but honestly, it doesn't seem like an important optimization (given how rarely this failure comes up), so I'll probably just leave it out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10260:46,failure,failures,46,https://hail.is,https://github.com/hail-is/hail/pull/10260,2,['failure'],"['failure', 'failures']"
Availability,"This is a total overhaul of our docker images. Though very verbose, I tried to stick to these main tenets:. - Any docker image has exactly 1 layer in it (all the way down to ubuntu) that installs pip dependencies. This primarily aims to protect the cache for this particularly large layer and also avoids a later layer silently upgrading the version of a dependency installed in an earlier layer. This pairs nicely with the following goal; - We only ever use 1 version of a dependency across the monorepo. Liberal use of pip's [constraint files](https://pip.pypa.io/en/stable/user_guide/#constraints-files) to ensure that the dependencies for a service must be compatible with dependencies from hail. The `install-dev-dependencies` target which install all our pinned requirements files would tell you if there's any incompatible versions of transitive dependencies across the repo; - The image graph is shallow and images don't contain more than they need. In order to have a single layer with requirements and hail code on top, I moved the service images to just be based on hail-ubuntu. This shortens the critical path and therefore reduces total image building time by reducing the number of times our image data needs to be downloaded and re-uploaded to the registry. I also removed a lot of unnecessary cruft like gcloud in places it wasn't used anymore, some unused/unnecessary pip requirements, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578:166,down,down,166,https://hail.is,https://github.com/hail-is/hail/pull/12578,2,['down'],"['down', 'downloaded']"
